link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/1905.07857,Transparency & Explainability, CERTIFAI: A common framework to provide explanations and analyse the fairness and robustness of black-box models,"As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.",[],[],"['Shubham Sharma', 'Jette Henderson', 'Joydeep Ghosh']","['University of Texas at Austin, Austin, TX, USA', 'CognitiveScale, Austin, TX, USA', 'CognitiveScale, Austin, TX, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2001.00964,Transparency & Explainability, Saving Face: Towards an Ethically-Informed Approach to Facial Recognition Auditing,"Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of five ethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.",[],[],"['Inioluwa Deborah Raji', 'Timnit Gebru', 'Margaret Mitchell', 'Joy Buolamwini', 'Joonseok Lee', 'Emily Denton']",[],[]
https://arxiv.org/abs/2207.01482,Transparency & Explainability, Good Explanation for Algorithmic Transparency,"Increasingly, laws are being proposed and passed by governments around the world to regulate Artificial Intelligence (AI) systems implemented into the public and private sectors. Many of these regulations address the transparency of AI systems, and related citizen-aware issues like allowing individuals to have the right to an explanation about how an AI system makes a decision that impacts them. Yet, almost all AI governance documents to date have a significant drawback: they have focused on what to do (or what not to do) with respect to making AI systems transparent, but have left the brunt of the work to technologists to figure out how to build transparent systems. We fill this gap by proposing a novel stakeholder-first approach that assists technologists in designing transparent, regulatory compliant systems. We also describe a real-world case-study that illustrates how this approach can be used in practice.",[],[],"['Andrew Bell', 'Oded Nov', 'Julia Stoyanovich']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/1912.07820,Transparency & Explainability, Balancing the Tradeoff Between Clustering Value and Interpretability,"Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a $\beta$-interpretable clustering algorithm that ensures that at least $\beta$ fraction of nodes in each cluster share the same feature value. The tunable parameter $\beta$ is user-specified. We also present a more efficient algorithm for scenarios with $\beta\!=\!1$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.",[],[],"['Sandhya Saisubramanian', 'Sainyam Galhotra', 'Shlomo Zilberstein']","['University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2006.06300,Transparency & Explainability, Artificial Intelligent and Indigenous Perspectives: Protecting and Empowering Intelligent Human Beings,"In January and February 2020, the Scottish Government released two documents for review by the public regarding their artificial intelligence (AI) strategy. The Montreal AI Ethics Institute (MAIEI) reviewed these documents and published a response on 4 June 2020. MAIEI's response examines several questions that touch on the proposed definition of AI; the people-centered nature of the strategy; considerations to ensure that everyone benefits from AI; the strategy's overarching vision; Scotland's AI ecosystem; the proposed strategic themes; and how to grow public confidence in AI by building responsible and ethical systems. In addition to examining the points above, MAIEI suggests that the strategy be extended to include considerations on biometric data and how that will be processed and used in the context of AI. It also highlights the importance of tackling head-on the inherently stochastic nature of deep learning systems and developing concrete guidelines to ensure that these systems are built responsibly and ethically, particularly as machine learning becomes more accessible. Finally, it concludes that any national AI strategy must clearly address the measurements of success in regards to the strategy's stated goals and vision to ensure that they are interpreted and applied consistently. To do this, there must be inclusion and transparency between those building the systems and those using them in their work.",[],[],['Abhishek Gupta'],[],[]
https://arxiv.org/abs/2005.04176,Transparency & Explainability," Towards Just, Fair and Interpretable Methods for Judicial Subset Selection","Objectives: We study interpretable recidivism prediction using machine learning (ML) models and analyze performance in terms of prediction ability, sparsity, and fairness. Unlike previous works, this study trains interpretable models that output probabilities rather than binary predictions, and uses quantitative fairness definitions to assess the models. This study also examines whether models can generalize across geographic locations. Methods: We generated black-box and interpretable ML models on two different criminal recidivism datasets from Florida and Kentucky. We compared predictive performance and fairness of these models against two methods that are currently used in the justice system to predict pretrial recidivism: the Arnold PSA and COMPAS. We evaluated predictive performance of all models on predicting six different types of crime over two time spans. Results: Several interpretable ML models can predict recidivism as well as black-box ML models and are more accurate than COMPAS or the Arnold PSA. These models are potentially useful in practice. Similar to the Arnold PSA, some of these interpretable models can be written down as a simple table. Others can be displayed using a set of visualizations. Our geographic analysis indicates that ML models should be trained separately for separate locations and updated over time. We also present a fairness analysis for the interpretable models. Conclusions: Interpretable machine learning models can perform just as well as non-interpretable methods and currently-used risk assessment scales, in terms of both prediction accuracy and fairness. Machine learning models might be more accurate when trained separately for distinct locations and kept up-to-date.",[],[],"['Caroline Wang', 'Bin Han', 'Bhrij Patel', 'Cynthia Rudin']","['Yale University, New Haven, CT, USA', 'Yale University, New Haven, CT, USA', 'Yale University, New Haven, CT, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2101.06203,Transparency & Explainability,Auditing Algorithms: On Lessons Learned and the Risks of Data Minimization,"This paper determines whether the two core data protection principles of data minimisation and purpose limitation can be meaningfully implemented in data-driven systems. While contemporary data processing practices appear to stand at odds with these principles, we demonstrate that systems could technically use much less data than they currently do. This observation is a starting point for our detailed techno-legal analysis uncovering obstacles that stand in the way of meaningful implementation and compliance as well as exemplifying unexpected trade-offs which emerge where data protection law is applied in practice. Our analysis seeks to inform debates about the impact of data protection on the development of artificial intelligence in the European Union, offering practical action points for data controllers, regulators, and researchers.",[],[],"['Asia J. Biega', 'Michèle Finck']","['Eticas Research and Consulting, Barcelona, Spain', 'Eticas Research and Consulting, Barcelona, Spain', 'Pompeu Fabra University, Barcelona, Spain', 'ALPHA Telefónica, Barcelona, Spain', 'ALPHA Telefónica, Barcelona, Spain']","['Spain', 'Spain', 'Spain', 'Spain', 'Spain']"
https://arxiv.org/abs/2102.06518,Transparency & Explainability, Assessing Post-hoc Explainability of the BKT Algorithm,"With recent progress in the field of Explainable Artificial Intelligence (XAI) and increasing use in practice, the need for an evaluation of different XAI methods and their explanation quality in practical usage scenarios arises. For this purpose, we present VitrAI, which is a web-based service with the goal of uniformly demonstrating four different XAI algorithms in the context of three real life scenarios and evaluating their performance and comprehensibility for humans. This work reveals practical obstacles when adopting XAI methods and gives qualitative estimates on how well different approaches perform in said scenarios.",[],[],"['Marc Hanussek', 'Falko Kötter', 'Maximilien Kintz', 'Jens Drawehn']","['Williams College, Williamstown, MA, USA', 'Williams College, Williamstown, MA, USA', 'Williams College, Williamstown, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2310.19819,Transparency & Explainability, Why Reliabilism Is not Enough: Epistemic and Moral Justification in Machine Learning,"Trusting machine learning algorithms requires having confidence in their outputs. Confidence is typically interpreted in terms of model reliability, where a model is reliable if it produces a high proportion of correct outputs. However, model reliability does not address concerns about the robustness of machine learning models, such as models relying on the wrong features or variations in performance based on context. I argue that the epistemic dimension of trust can instead be understood through the concept of knowledge, where the trustworthiness of an algorithm depends on whether its users are in the position to know that its outputs are correct. Knowledge requires beliefs to be formed for the right reasons and to be robust to error, so machine learning algorithms can only provide knowledge if they work well across counterfactual scenarios and if they make decisions based on the right features. This, I argue, can explain why we should care about model properties like interpretability, causal shortcut independence, and distribution shift robustness even if such properties are not required for model reliability.",[],[],['Jonathan Vandenburgh'],"['Google, San Francisco, CA, USA', 'Google, San Francisco, CA, USA', 'Google, San Francisco, CA, USA', 'Google, Seattle, WA, USA', 'Google, Mountain View, CA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2009.11186,Transparency & Explainability, A Deontic Logic for Programming Rightful Machines,"Transparency is a key requirement for ethical machines. Verified ethical behavior is not enough to establish justified trust in autonomous intelligent agents: it needs to be supported by the ability to explain decisions. Logic Programming (LP) has a great potential for developing such perspective ethical systems, as in fact logic rules are easily comprehensible by humans. Furthermore, LP is able to model causality, which is crucial for ethical decision making.",[],[],"['Abeer Dyoub', 'Stefania Costantini', 'Francesca A. Lisi']","['Northeastern University, Boston, MA, USA']",['US']
https://arxiv.org/abs/1912.09318,Transparency & Explainability, AI and Holistic Review: Informing Human Reading in College Admissions,"College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.",[],[],"['AJ Alvero', 'Noah Arthurs', 'anthony lising antonio', 'Benjamin W. Domingue', 'Ben Gebre-Medhin', 'Sonia Giebel', 'Mitchell L. Stevens']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2001.03071,Transparency & Explainability, Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification,"Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.",[],[],"['Chris Dulhanty', 'Alexander Wong']","['University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada']","['Canada', 'Canada']"
https://arxiv.org/abs/1912.09140,Transparency & Explainability, Meta Decision Trees for Explainable Recommendation Systems,"We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user's training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature.",[],[],"['Eyal Shulman', 'Lior Wolf']","['Tel Aviv University, Tel Aviv-Yafo, Israel', 'Tel Aviv University, Tel Aviv-Yafo, Israel']","['Israel', 'Israel']"
https://arxiv.org/abs/2304.11217,Fairness & Bias, When your only tool is a hammer: ethical limitations of computational fairness solutions in healthcare machine learning,"Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissimilar situations, and to capture differing interpretations of the same situation by different stakeholders.",[],[],"['Andrea Aler Tubella', 'Dimitri Coelho Mollo', 'Adam Dahlgren Lindström', 'Hannah Devinney', 'Virginia Dignum', 'Petter Ericson', 'Anna Jonsson', 'Timotheus Kampik', 'Tom Lenaerts', 'Julian Alfredo Mendez', 'Juan Carlos Nieves']",[],[]
https://arxiv.org/abs/2001.09773,Fairness & Bias, Algorithmic Fairness from a Non-ideal Perspective,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In efforts to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine learning} to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and the perfectly just world. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine learning literature and the ideal approach in political philosophy, and argue that the increasingly apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions, a reinterpretation of impossibility results, and directions for future research.",[],[],"['Sina Fazelpour', 'Zachary C. Lipton']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US']"
https://arxiv.org/abs/2001.00964,Fairness & Bias, Saving Face: Towards an Ethically-Informed Approach to Facial Recognition Auditing,"Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of five ethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.",[],[],"['Inioluwa Deborah Raji', 'Timnit Gebru', 'Margaret Mitchell', 'Joy Buolamwini', 'Joonseok Lee', 'Emily Denton']",[],[]
https://arxiv.org/abs/1911.02508,Fairness & Bias, Nothing to See Here: Hiding Model Biases by Fooling Post hoc Explanation Methods,"As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real-world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.",[],[],"['Dylan Slack', 'Sophie Hilgard', 'Emily Jia', 'Sameer Singh', 'Himabindu Lakkaraju']",[],[]
https://arxiv.org/abs/2007.14886,Fairness & Bias," Biased Priorities, Biased Outcomes: Three Recommendations for Ethics-oriented Data Annotation Practices","The interpretation of data is fundamental to machine learning. This paper investigates practices of image data annotation as performed in industrial contexts. We define data annotation as a sense-making practice, where annotators assign meaning to data through the use of labels. Previous human-centered investigations have largely focused on annotators subjectivity as a major cause for biased labels. We propose a wider view on this issue: guided by constructivist grounded theory, we conducted several weeks of fieldwork at two annotation companies. We analyzed which structures, power relations, and naturalized impositions shape the interpretation of data. Our results show that the work of annotators is profoundly informed by the interests, values, and priorities of other actors above their station. Arbitrary classifications are vertically imposed on annotators, and through them, on data. This imposition is largely naturalized. Assigning meaning to data is often presented as a technical matter. This paper shows it is, in fact, an exercise of power with multiple implications for individuals and society.",[],[],"['Milagros Miceli', 'Martin Schuessler', 'Tianling Yang']","['Technische Universität Berlin, Berlin, Germany', 'Technische Universität Berlin, Berlin, Germany']","['Germany', 'Germany']"
https://arxiv.org/abs/1912.08388,Fairness & Bias, Balancing the Tradeoff between Profit and Fairness in Rideshare Platforms during High-Demand Hours,"Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g. from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, \lpalg, that allows the platform designer to control the profit and fairness of the system via parameters $\alpha$ and $\beta$ respectively. We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use \lpalg to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using \lpalg, the competitive ratios for profit and fairness measures would be no worse than $\alpha/e$ and $\beta/e$ respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that $\lpalg$ under some choice of $(\alpha, \beta)$ can beat two natural heuristics, Greedy and Uniform, on \emph{both} fairness and profit.",[],[],"['Vedant Nanda', 'Pan Xu', 'Karthik Abinav Sankararaman', 'John P. Dickerson', 'Aravind Srinivasan']",[],[]
https://arxiv.org/abs/2107.12870,Fairness & Bias, Social Contracts for Non-Cooperative Games,"Frequent violations of fair principles in real-life settings raise the fundamental question of whether such principles can guarantee the existence of a self-enforcing equilibrium in a free economy. We show that elementary principles of distributive justice guarantee that a pure-strategy Nash equilibrium exists in a finite economy where agents freely (and non-cooperatively) choose their inputs and derive utility from their pay. Chief among these principles is that: 1) your pay should not depend on your name, and 2) a more productive agent should not earn less. When these principles are violated, an equilibrium may not exist. Moreover, we uncover an intuitive condition -- technological monotonicity -- that guarantees equilibrium uniqueness and efficiency. We generalize our findings to economies with social justice and inclusion, implemented in the form of progressive taxation and redistribution, and guaranteeing a basic income to unproductive agents. Our analysis uncovers a new class of strategic form games by incorporating normative principles into non-cooperative game theory. Our results rely on no particular assumptions, and our setup is entirely non-parametric. Illustrations of the theory include applications to exchange economies, surplus distribution in a firm, contagion and self-enforcing lockdown in a networked economy, and bias in the academic peer-review system. Keywords: Market justice; Social justice; Inclusion; Ethics; Discrimination; Self-enforcing contracts; Fairness in non-cooperative games; Pure strategy Nash equilibrium; Efficiency. JEL Codes: C72, D30, D63, J71, J38",[],[],"['Ghislain H. Demeze-Jouatsa', 'Roland Pongou', 'Jean-Baptiste Tondji']","['Université du Québec en Outaouais, Gatineau, PQ, Canada', 'The University of Edinburgh, Edinburgh, United Kingdom']","['Canada', 'United Kingdom']"
https://arxiv.org/abs/2001.00089,Fairness & Bias, Human Comprehension of Fairness in Machine Learning,"Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions--demographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.",[],[],"['Debjani Saha', 'Candice Schumann', 'Duncan C. McElfresh', 'John P. Dickerson', 'Michelle L. Mazurek', 'Michael Carl Tschantz']","['University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'University of Maryland, College Park, MD, USA', 'International Computer Science Institute, Berkeley, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2111.01872,Fairness & Bias, A Fairness-aware Incentive Scheme for Federated Learning,"Recent advances in Federated Learning (FL) have brought large-scale collaborative machine learning opportunities for massively distributed clients with performance and data privacy guarantees. However, most current works focus on the interest of the central controller in FL,and overlook the interests of the FL clients. This may result in unfair treatment of clients that discourages them from actively participating in the learning process and damages the sustainability of the FL ecosystem. Therefore, the topic of ensuring fairness in FL is attracting a great deal of research interest. In recent years, diverse Fairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve fairness in FL from different perspectives. However, there is no comprehensive survey that helps readers gain insight into this interdisciplinary field. This paper aims to provide such a survey. By examining the fundamental and simplifying assumptions, as well as the notions of fairness adopted by existing literature in this field, we propose a taxonomy of FAFL approaches covering major steps in FL, including client selection, optimization, contribution evaluation and incentive distribution. In addition, we discuss the main metrics for experimentally evaluating the performance of FAFL approaches, and suggest promising future research directions towards FAFL.",[],[],"['Yuxin Shi', 'Han Yu', 'Cyril Leung']","['Nanyang Technological University, Singapore, Singapore', 'Nanyang Technological University, Singapore, Singapore', 'WeBank, Shenzhen, China', 'WeBank, Shenzhen, China', 'The University of Hong Kong, Hong Kong, China', 'Peking University, Beijing, China', 'Nanyang Technological University, Singapore, Singapore', 'Hong Kong University of Science and Technology, Hong Kong, China']","['Singapore', 'Singapore', 'China', 'China', 'China', 'China', 'Singapore', 'China']"
https://arxiv.org/abs/2005.04176,Fairness & Bias," Towards Just, Fair and Interpretable Methods for Judicial Subset Selection","Objectives: We study interpretable recidivism prediction using machine learning (ML) models and analyze performance in terms of prediction ability, sparsity, and fairness. Unlike previous works, this study trains interpretable models that output probabilities rather than binary predictions, and uses quantitative fairness definitions to assess the models. This study also examines whether models can generalize across geographic locations. Methods: We generated black-box and interpretable ML models on two different criminal recidivism datasets from Florida and Kentucky. We compared predictive performance and fairness of these models against two methods that are currently used in the justice system to predict pretrial recidivism: the Arnold PSA and COMPAS. We evaluated predictive performance of all models on predicting six different types of crime over two time spans. Results: Several interpretable ML models can predict recidivism as well as black-box ML models and are more accurate than COMPAS or the Arnold PSA. These models are potentially useful in practice. Similar to the Arnold PSA, some of these interpretable models can be written down as a simple table. Others can be displayed using a set of visualizations. Our geographic analysis indicates that ML models should be trained separately for separate locations and updated over time. We also present a fairness analysis for the interpretable models. Conclusions: Interpretable machine learning models can perform just as well as non-interpretable methods and currently-used risk assessment scales, in terms of both prediction accuracy and fairness. Machine learning models might be more accurate when trained separately for distinct locations and kept up-to-date.",[],[],"['Caroline Wang', 'Bin Han', 'Bhrij Patel', 'Cynthia Rudin']","['Yale University, New Haven, CT, USA', 'Yale University, New Haven, CT, USA', 'Yale University, New Haven, CT, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1908.09635,Fairness & Bias, Measuring Fairness in an Unfair World,"With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",[],[],"['Ninareh Mehrabi', 'Fred Morstatter', 'Nripsuta Saxena', 'Kristina Lerman', 'Aram Galstyan']","['University of Rochester, Rochester, NY, USA']",['US']
https://arxiv.org/abs/2001.05046,Fairness & Bias, Robot Rights? Letâs talk about human welfare instead,"The 'robot rights' debate, and its related question of 'robot responsibility', invokes some of the most polarized positions in AI ethics. While some advocate for granting robots rights on a par with human beings, others, in a stark opposition argue that robots are not deserving of rights but are objects that should be our slaves. Grounded in post-Cartesian philosophical foundations, we argue not just to deny robots 'rights', but to deny that robots, as artifacts emerging out of and mediating human being, are the kinds of things that could be granted rights in the first place. Once we see robots as mediators of human being, we can understand how the `robots rights' debate is focused on first world problems, at the expense of urgent ethical concerns, such as machine bias, machine elicited human labour exploitation, and erosion of privacy all impacting society's least privileged individuals. We conclude that, if human being is our starting point and human welfare is the primary concern, the negative impacts emerging from machinic systems, as well as the lack of taking responsibility by people designing, selling and deploying such machines, remains the most pressing ethical discussion in AI.",[],[],"['Abeba Birhane', 'Jelle van Dijk']",[],[]
https://arxiv.org/abs/2207.02848,Fairness & Bias, Arbiter: A Domain-Specific Language for Ethical Machine Learning,"Datasets play a central role in the training and evaluation of machine learning (ML) models. But they are also the root cause of many undesired model behaviors, such as biased predictions. To overcome this situation, the ML community is proposing a data-centric cultural shift where data issues are given the attention they deserve, and more standard practices around the gathering and processing of datasets start to be discussed and established. So far, these proposals are mostly high-level guidelines described in natural language and, as such, they are difficult to formalize and apply to particular datasets. In this sense, and inspired by these proposals, we define a new domain-specific language (DSL) to precisely describe machine learning datasets in terms of their structure, data provenance, and social concerns. We believe this DSL will facilitate any ML initiative to leverage and benefit from this data-centric shift in ML (e.g., selecting the most appropriate dataset for a new project or better replicating other ML results). The DSL is implemented as a Visual Studio Code plugin, and it has been published under an open source license.",[],[],"['Joan Giner-Miguelez', 'Abel Gómez', 'Jordi Cabot']","['Northeastern University, Boston, MA, USA', 'Northeastern University, Boston, MA, USA']","['US', 'US']"
https://arxiv.org/abs/1903.09209,Fairness & Bias, An invitation to system-wide algorithmic fairness,"We propose the use of Agent Based Models (ABMs) inside a reinforcement learning framework in order to better understand the relationship between automated decision making tools, fairness-inspired statistical constraints, and the social phenomena giving rise to discrimination towards sensitive groups. There have been many instances of discrimination occurring due to the applications of algorithmic tools by public and private institutions. Until recently, these practices have mostly gone unchecked. Given the large-scale transformation these new technologies elicit, a joint effort of social sciences and machine learning researchers is necessary. Much of the research has been done on determining statistical properties of such algorithms and the data they are trained on. We aim to complement that approach by studying the social dynamics in which these algorithms are implemented. We show how bias can be accumulated and reinforced through automated decision making, and the possibility of finding a fairness inducing policy. We focus on the case of recidivism risk assessment by considering simplified models of arrest. We find that if we limit our attention to what is observed and manipulated by these algorithmic tools, we may determine some blatantly unfair practices as fair, illustrating the advantage of analyzing the otherwise elusive property with a system-wide model. We expect the introduction of agent based simulation techniques will strengthen collaboration with social scientists, arriving at a better understanding of the social systems affected by technology and to hopefully lead to concrete policy proposals that can be presented to policymakers for a true systemic transformation.",[],[],"['Efrén Cruz Cortés', 'Debashis Ghosh']","['The Pennsylvania State University, State College, PA, USA', 'University of Colorado Anschutz, Aurora, CO, USA']","['US', 'US']"
https://arxiv.org/abs/2002.01621,Fairness & Bias, Joint Optimization of AI Fairness and Utility: A Human-Centered Approach,"Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.",[],[],"['Yunfeng Zhang', 'Rachel K. E. Bellamy', 'Kush R. Varshney']","['IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA', 'IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA', 'IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1910.12854,Fairness & Bias, A Geometric Solution to Fair Representations,"To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and they are not easily transferable across models. To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.",[],[],"['Yuzi He', 'Keith Burghardt', 'Kristina Lerman']","['University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2309.07110,Fairness & Bias, Data Augmentation for Discrimination Prevention and Bias Disambiguation,"In this work, we propose data augmentation via pairwise mixup across subgroups to improve group fairness. Many real-world applications of machine learning systems exhibit biases across certain groups due to under-representation or training data that reflects societal biases. Inspired by the successes of mixup for improving classification performance, we develop a pairwise mixup scheme to augment training data and encourage fair and accurate decision boundaries for all subgroups. Data augmentation for group fairness allows us to add new samples of underrepresented groups to balance subpopulations. Furthermore, our method allows us to use the generalization ability of mixup to improve both fairness and accuracy. We compare our proposed mixup to existing data augmentation and bias mitigation approaches on both synthetic simulations and real-world benchmark fair classification data, demonstrating that we are able to achieve fair outcomes with robust if not improved accuracy.",[],[],"['Madeline Navarro', 'Camille Little', 'Genevera I. Allen', 'Santiago Segarra']","['IBM Research & University of Texas at Austin, Austin, TX, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Austin, TX, USA', 'IBM Research, Yorktown Heights, NY, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2109.13283,Fairness & Bias, More Than “If Time Allows”: The Role of Ethics in AI Education,"Justice-centered approaches to equitable computer science (CS) education frame CS learning as a means for advancing peace, antiracism, and social justice rather than war, empire, and corporations. However, most research in justice-centered approaches in CS education focus on K-12 learning environments. In this position paper, we review justice-centered approaches to CS education, problematize the lack of justice-centered approaches to CS in higher education in particular, and describe a justice-centered approach for undergraduate Data Structures and Algorithms. Our approach emphasizes three components: (1) ethics: critiques the sociopolitical values of data structure and algorithm design as well as the underlying logics of dominant computing culture; (2) identity: draws on culturally responsive-sustaining pedagogies to emphasize student identity as rooted in resistance to the dominant computing culture; and (3) political vision: ensures the rightful presence of political struggles by reauthoring rights to frame CS learning as a force for social justice. Through a case study of this Critical Comparative Data Structures and Algorithms pedagogy, we argue that justice-centered approaches to higher CS education can help all computing students not only learn about the ethical implications of nominally technical concepts, but also develop greater respect for diverse epistemologies, cultures, and experiences surrounding computing that are essential to creating the socially-just worlds we need.",[],[],['Kevin Lin'],[],[]
https://arxiv.org/abs/1912.06883,Fairness & Bias, The Perils of Objectivity: Towards a Normative Framework for Fair Judicial Decision-Making,"A distinction has been drawn in fair machine learning research between `group' and `individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on theoretical discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artifact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of `unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.",[],[],['Reuben Binns'],"['Microsoft Research, Redmond, WA, USA', 'University of Cambridge, Cambridge, United Kingdom']","['US', 'United Kingdom']"
https://arxiv.org/abs/1912.09318,Fairness & Bias, AI and Holistic Review: Informing Human Reading in College Admissions,"College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.",[],[],"['AJ Alvero', 'Noah Arthurs', 'anthony lising antonio', 'Benjamin W. Domingue', 'Ben Gebre-Medhin', 'Sonia Giebel', 'Mitchell L. Stevens']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2112.04359,Fairness & Bias,Monitoring Misuse for Accountable âArtificial Intelligence as a Serviceâ,"This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.",[],[],"['Laura Weidinger', 'John Mellor', 'Maribeth Rauh', 'Conor Griffin', 'Jonathan Uesato', 'Po-Sen Huang', 'Myra Cheng', 'Mia Glaese', 'Borja Balle', 'Atoosa Kasirzadeh', 'Zac Kenton', 'Sasha Brown', 'Will Hawkins', 'Tom Stepleton', 'Courtney Biles', 'Abeba Birhane', 'Julia Haas', 'Laura Rimell', 'Lisa Anne Hendricks', 'William Isaac', 'Sean Legassick', 'Geoffrey Irving', 'Iason Gabriel']",[],[]
https://arxiv.org/abs/2111.01872,Privacy & Data Governance, A Fairness-aware Incentive Scheme for Federated Learning,"Recent advances in Federated Learning (FL) have brought large-scale collaborative machine learning opportunities for massively distributed clients with performance and data privacy guarantees. However, most current works focus on the interest of the central controller in FL,and overlook the interests of the FL clients. This may result in unfair treatment of clients that discourages them from actively participating in the learning process and damages the sustainability of the FL ecosystem. Therefore, the topic of ensuring fairness in FL is attracting a great deal of research interest. In recent years, diverse Fairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve fairness in FL from different perspectives. However, there is no comprehensive survey that helps readers gain insight into this interdisciplinary field. This paper aims to provide such a survey. By examining the fundamental and simplifying assumptions, as well as the notions of fairness adopted by existing literature in this field, we propose a taxonomy of FAFL approaches covering major steps in FL, including client selection, optimization, contribution evaluation and incentive distribution. In addition, we discuss the main metrics for experimentally evaluating the performance of FAFL approaches, and suggest promising future research directions towards FAFL.",[],[],"['Yuxin Shi', 'Han Yu', 'Cyril Leung']","['Nanyang Technological University, Singapore, Singapore', 'Nanyang Technological University, Singapore, Singapore', 'WeBank, Shenzhen, China', 'WeBank, Shenzhen, China', 'The University of Hong Kong, Hong Kong, China', 'Peking University, Beijing, China', 'Nanyang Technological University, Singapore, Singapore', 'Hong Kong University of Science and Technology, Hong Kong, China']","['Singapore', 'Singapore', 'China', 'China', 'China', 'China', 'Singapore', 'China']"
https://arxiv.org/abs/2101.06203,Privacy & Data Governance,Auditing Algorithms: On Lessons Learned and the Risks of Data Minimization,"This paper determines whether the two core data protection principles of data minimisation and purpose limitation can be meaningfully implemented in data-driven systems. While contemporary data processing practices appear to stand at odds with these principles, we demonstrate that systems could technically use much less data than they currently do. This observation is a starting point for our detailed techno-legal analysis uncovering obstacles that stand in the way of meaningful implementation and compliance as well as exemplifying unexpected trade-offs which emerge where data protection law is applied in practice. Our analysis seeks to inform debates about the impact of data protection on the development of artificial intelligence in the European Union, offering practical action points for data controllers, regulators, and researchers.",[],[],"['Asia J. Biega', 'Michèle Finck']","['Eticas Research and Consulting, Barcelona, Spain', 'Eticas Research and Consulting, Barcelona, Spain', 'Pompeu Fabra University, Barcelona, Spain', 'ALPHA Telefónica, Barcelona, Spain', 'ALPHA Telefónica, Barcelona, Spain']","['Spain', 'Spain', 'Spain', 'Spain', 'Spain']"
https://arxiv.org/abs/2001.00463,Security, The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.",[],[],"['Toby Shevlane', 'Allan Dafoe']",[],[]
https://arxiv.org/abs/2108.03388,Security, âHow do I fool you?â: Manipulating User Trust via Misleading Black Box Explanations,"Graph Neural Networks (GNNs) have boosted the performance for many graph-related tasks. Despite the great success, recent studies have shown that GNNs are highly vulnerable to adversarial attacks, where adversaries can mislead the GNNs' prediction by modifying graphs. On the other hand, the explanation of GNNs (GNNExplainer) provides a better understanding of a trained GNN model by generating a small subgraph and features that are most influential for its prediction. In this paper, we first perform empirical studies to validate that GNNExplainer can act as an inspection tool and have the potential to detect the adversarial perturbations for graphs. This finding motivates us to further initiate a new problem investigation: Whether a graph neural network and its explanations can be jointly attacked by modifying graphs with malicious desires? It is challenging to answer this question since the goals of adversarial attacks and bypassing the GNNExplainer essentially contradict each other. In this work, we give a confirmative answer to this question by proposing a novel attack framework (GEAttack), which can attack both a GNN model and its explanations by simultaneously exploiting their vulnerabilities. Extensive experiments on two explainers (GNNExplainer and PGExplainer) under various real-world datasets demonstrate the effectiveness of the proposed method.",[],[],"['Wenqi Fan', 'Wei Jin', 'Xiaorui Liu', 'Han Xu', 'Xianfeng Tang', 'Suhang Wang', 'Qing Li', 'Jiliang Tang', 'Jianping Wang', 'Charu Aggarwal']",,
https://arxiv.org/abs/2104.09586,Security, Contextual Analysis of Social Media: The Promise and Challenge of Eliciting Context in Social Media Posts with Natural Language Processing,"Online forums provide a unique opportunity for online users to share comments and exchange information on a particular topic. Understanding user behaviour is valuable to organizations and has applications for social and security strategies, for instance, identifying user opinions within a community or predicting future behaviour. Discovering the semantic aspects in Incel forums are the main goal of this research; we apply Natural language processing techniques based on topic modeling to latent topic discovery and opinion mining of users from a popular online Incel discussion forum. To prepare the input data for our study, we extracted the comments from this http URL. The research experiments show that Artificial Intelligence (AI) based on NLP models can be effective for semantic and emotion knowledge discovery and retrieval of useful information from the Incel community. For example, we discovered semantic-related words that describe issues within a large volume of Incel comments, which is difficult with manual methods.",[],[],"['Hamed Jelodar', 'Richard Frank']","['Columbia University, New York, NY, USA', 'Columbia University, New York, NY, USA', 'Lankenau Institute for Medical Research, Philadelphia, PA, USA', 'Columbia University, New York, NY, USA', 'Columbia University, New York, NY, USA', 'City University of New York, New York, NY, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2102.02928,Security, Toward Implementing the ADC Model of Moral Judgment in Autonomous Vehicles,"The expansion of artificial intelligence (AI) and autonomous systems has shown the potential to generate enormous social good while also raising serious ethical and safety concerns. AI technology is increasingly adopted in transportation. A survey of various in-vehicle technologies found that approximately 64% of the respondents used a smartphone application to assist with their travel. The top-used applications were navigation and real-time traffic information systems. Among those who used smartphones during their commutes, the top-used applications were navigation and entertainment. There is a pressing need to address relevant social concerns to allow for the development of systems of intelligent agents that are informed and cognizant of ethical standards. Doing so will facilitate the responsible integration of these systems in society. To this end, we have applied Multi-Criteria Decision Analysis (MCDA) to develop a formal Multi-Attribute Impact Assessment (MAIA) questionnaire for examining the social and ethical issues associated with the uptake of AI. We have focused on the domain of autonomous vehicles (AVs) because of their imminent expansion. However, AVs could serve as a stand-in for any domain where intelligent, autonomous agents interact with humans, either on an individual level (e.g., pedestrians, passengers) or a societal level.",[],[],"['Veljko Dubljević', 'George F. List', 'Jovan Milojevich', 'Nirav Ajmeri', 'William Bauer', 'Munindar P. Singh', 'Eleni Bardaka', 'Thomas Birkland', 'Charles Edwards', 'Roger Mayer', 'Ioan Muntean', 'Thomas Powers', 'Hesham Rakha', 'Vance Ricks', 'M. Shoaib Samandar']",[],[]
https://arxiv.org/abs/1911.09005,Security, Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",[],[],"['Roel Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']",[],[]
https://arxiv.org/abs/2002.05672,Security, Steps Towards Value-Aligned Systems,"Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are indispensable tools for managing the flood of information needed to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the literature on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.",[],[],"['Osonde A. Osoba', 'Benjamin Boudreaux', 'Douglas Yeung']","['RAND Corporation, Santa Monica, CA, USA', 'RAND Corporation, Santa Monica, CA, USA', 'RAND Corporation, Santa Monica, CA, USA']","['US', 'US', 'US']"