link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/1710.06169,Transparency & Explainability,Detecting Bias in Black-Box Models Using Transparent Model Distillation,"Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, a model distillation and comparison approach to audit such models. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by black-box models. We compare the student model trained with distillation to a second un-distilled transparent model trained on ground-truth outcomes, and use differences between the two models to gain insight into the black-box model. Our approach can be applied in a realistic setting, without probing the black-box model API. We demonstrate the approach on four public data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.",[],[],"['Sarah Tan', 'Rich Caruana', 'Giles Hooker', 'Yin Lou']",[],[]
https://arxiv.org/abs/2306.05372,Transparency & Explainability,Towards an âEthics by Designâ methodology for AI research projects,"As artificial intelligence (AI) systems become more prevalent, ensuring fairness in their design becomes increasingly important. This survey focuses on the subdomains of social media and healthcare, examining the concepts of fairness, accountability, transparency, and ethics (FATE) within the context of AI. We explore existing research on FATE in AI, highlighting the benefits and limitations of current solutions, and provide future research directions. We found that statistical and intersectional fairness can support fairness in healthcare on social media platforms, and transparency in AI is essential for accountability. While solutions like simulation, data analytics, and automated systems are widely used, their effectiveness can vary, and keeping up-to-date with the latest research is crucial.",[],[],"['Aditya Singhal', 'Hasnaat Tanveer', 'Vijay Mago']",[],[]
https://arxiv.org/abs/1809.06061,Transparency & Explainability,Transparency and Explanation in Deep Reinforcement Learning Neural Networks,"Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of ""object saliency maps"", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.",[],[],"['Rahul Iyer', 'Yuezhang Li', 'Huao Li', 'Michael Lewis', 'Ramitha Sundar', 'Katia Sycara']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Google Inc., Mountain View, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2308.16538,Transparency & Explainability, Opportunities and Challenges for Artificial Intelligence in India,"This report examines Artificial Intelligence (AI) in the financial sector, outlining its potential to revolutionise the industry and identify its challenges. It underscores the criticality of a well-rounded understanding of AI, its capabilities, and its implications to effectively leverage its potential while mitigating associated risks. The potential of AI potential extends from augmenting existing operations to paving the way for novel applications in the finance sector. The application of AI in the financial sector is transforming the industry. Its use spans areas from customer service enhancements, fraud detection, and risk management to credit assessments and high-frequency trading. However, along with these benefits, AI also presents several challenges. These include issues related to transparency, interpretability, fairness, accountability, and trustworthiness. The use of AI in the financial sector further raises critical questions about data privacy and security. A further issue identified in this report is the systemic risk that AI can introduce to the financial sector. Being prone to errors, AI can exacerbate existing systemic risks, potentially leading to financial crises. Regulation is crucial to harnessing the benefits of AI while mitigating its potential risks. Despite the global recognition of this need, there remains a lack of clear guidelines or legislation for AI use in finance. This report discusses key principles that could guide the formation of effective AI regulation in the financial sector, including the need for a risk-based approach, the inclusion of ethical considerations, and the importance of maintaining a balance between innovation and consumer protection. The report provides recommendations for academia, the finance industry, and regulators.",[],[],"['Carsten Maple', 'Lukasz Szpruch', 'Gregory Epiphaniou', 'Kalina Staykova', 'Simran Singh', 'William Penwarden', 'Yisi Wen', 'Zijian Wang', 'Jagdish Hariharan', 'Pavle Avramovic']","['Indian Institute of Technology Bombay, Mumbai, India', 'Embrace Innovations, Bangalore, India', ""King's College London, London, United Kingdom"", 'Rao Law Chambers, Azim Premji University, Bangalore, India']","['India', 'India', 'United Kingdom', 'India']"
https://arxiv.org/abs/2307.00660,Transparency & Explainability,Towards Provably Moral AI Agents in Bottom-up Learning Frameworks,"As artificial intelligence (AI) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. For models involved in moral decision-making, also known as artificial moral agents (AMA), interpretability provides a way to trust and understand the agent's internal reasoning mechanisms for effective use and error correction. In this paper, we provide an overview of this rapidly-evolving sub-field of AI interpretability, introduce the concept of the Minimum Level of Interpretability (MLI) and recommend an MLI for various types of agents, to aid their safe deployment in real-world settings.",[],[],"['Avish Vijayaraghavan', 'Cosmin Badea']",[],[]
https://arxiv.org/abs/1903.04561,Fairness & Bias,Measuring and Mitigating Unintended Bias in Text Classification,"Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. In this paper, we introduce a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier's score distribution can vary across designated groups. We also introduce a large new test set of online comments with crowd-sourced annotations for identity references. We use this to show how our metrics can be used to find new and potentially subtle unintended bias in existing public models.",[],[],"['Daniel Borkan', 'Lucas Dixon', 'Jeffrey Sorensen', 'Nithum Thain', 'Lucy Vasserman']","['Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, New York, NY, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2010.04053,Fairness & Bias,Non-Discriminatory Machine Learning through Convex Fairness Criteria,"As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.",[],[],"['Simon Caton', 'Christian Haas']","['Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland', 'Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland', 'Ecole Polytechnique Federal de Lausanne, Lausanne, Switzerland']","['Switzerland', 'Switzerland', 'Switzerland']"
https://arxiv.org/abs/1712.08197,Fairness & Bias, Fair Forests: Regularized Tree Induction to Minimize Model Bias,"The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees. We show that our ""Fair Forest"" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both ""group fairness"" and ""individual fairness.'"" We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.",[],[],"['Edward Raff', 'Jared Sylvester', 'Steven Mills']",[],[]
https://arxiv.org/abs/1906.06425,Fairness & Bias,Toward Non-Intuition-Based Machine Ethics,"We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological ethics, and envision a research agenda prioritized by it.",[],[],"['Shrimai Prabhumoye', 'Elijah Mayfield', 'Alan W Black']",[],[]
https://arxiv.org/abs/1811.03435,Fairness & Bias,Partially Generative Neural Networks for Gang Crime Classification with Partial Information,"In response to public scrutiny of data-driven algorithms, the field of data science has adopted ethics training and principles. Although ethics can help data scientists reflect on certain normative aspects of their work, such efforts are ill-equipped to generate a data science that avoids social harms and promotes social justice. In this article, I argue that data science must embrace a political orientation. Data scientists must recognize themselves as political actors engaged in normative constructions of society and evaluate their work according to its downstream impacts on people's lives. I first articulate why data scientists must recognize themselves as political actors. In this section, I respond to three arguments that data scientists commonly invoke when challenged to take political positions regarding their work. In confronting these arguments, I describe why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why data science's attempts to promote ""social good"" dangerously rely on unarticulated and incrementalist political assumptions. I then propose a framework for how data science can evolve toward a deliberative and rigorous politics of social justice. I conceptualize the process of developing a politically engaged data science as a sequence of four stages. Pursuing these new approaches will empower data scientists with new methods for thoughtfully and rigorously contributing to social justice.",[],[],['Ben Green'],"['University of Southern California, Los Angeles, CA, USA', 'University of Nebraska-Lincoln, Lincoln, NE, USA', 'University of California, Los Angeles, Los Angeles, CA, USA', 'University of California, Los Angeles, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA', 'University of Southern California, Los Angeles, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/1710.06169,Fairness & Bias,Detecting Bias in Black-Box Models Using Transparent Model Distillation,"Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, a model distillation and comparison approach to audit such models. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by black-box models. We compare the student model trained with distillation to a second un-distilled transparent model trained on ground-truth outcomes, and use differences between the two models to gain insight into the black-box model. Our approach can be applied in a realistic setting, without probing the black-box model API. We demonstrate the approach on four public data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.",[],[],"['Sarah Tan', 'Rich Caruana', 'Giles Hooker', 'Yin Lou']",[],[]
https://arxiv.org/abs/2008.13404,Fairness & Bias, On the distinction between implicit and explicit ethical agency,"Personalization should take the human person seriously. This requires a deeper understanding of how recommender systems can shape both our self-understanding and identity. We unpack key European humanistic and philosophical ideas underlying the General Data Protection Regulation (GDPR) and propose a new paradigm of humanistic personalization. Humanistic personalization responds to the IEEE's call for Ethically Aligned Design (EAD) and is based on fundamental human capacities and values. Humanistic personalization focuses on narrative accuracy: the subjective fit between a person's self-narrative and both the input (personal data) and output of a recommender system. In doing so, we re-frame the distinction between implicit and explicit data collection as one of nonconscious (""organismic"") behavior and conscious (""reflective"") action. This distinction raises important ethical and interpretive issues related to agency, self-understanding, and political participation. Finally, we discuss how an emphasis on narrative accuracy can reduce opportunities for epistemic injustice done to data subjects.",[],[],"['Travis Greene', 'Galit Shmueli']","['Western Norway University of Applied Sciences, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway']","['Norway', 'Norway', 'Norway']"
https://arxiv.org/abs/1711.09050,Fairness & Bias,Ethical Challenges in Data-Driven Dialogue Systems,"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.",[],[],"['Peter Henderson', 'Koustuv Sinha', 'Nicolas Angelard-Gontier', 'Nan Rosemary Ke', 'Genevieve Fried', 'Ryan Lowe', 'Joelle Pineau']","['McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'Polytechnique Montréal, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada']","['Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada']"
https://arxiv.org/abs/2002.09471,Fairness & Bias,Fairness in Relational Domains,"The development of fair machine learning models that effectively avert bias and discrimination is an important problem that has garnered attention in recent years. The necessity of encoding complex relational dependencies among the features and variables for competent predictions require the development of fair, yet expressive relational models. In this work, we introduce Fair-A3SL, a fairness-aware structure learning algorithm for learning relational structures, which incorporates fairness measures while learning relational graphical model structures. Our approach is versatile in being able to encode a wide range of fairness metrics such as statistical parity difference, overestimation, equalized odds, and equal opportunity, including recently proposed relational fairness measures. While existing approaches employ the fairness measures on pre-determined model structures post prediction, Fair-A3SL directly learns the structure while optimizing for the fairness measures and hence is able to remove any structural bias in the model. We demonstrate the effectiveness of our learned model structures when compared with the state-of-the-art fairness models quantitatively and qualitatively on datasets representing three different modeling scenarios: i) a relational dataset, ii) a recidivism prediction dataset widely used in studying discrimination, and iii) a recommender systems dataset. Our results show that Fair-A3SL can learn fair, yet interpretable and expressive structures capable of making accurate predictions.",[],[],"['Yue Zhang', 'Arti Ramesh']","['UC Santa Cruz, Santa Cruz, CA, USA', 'Polytechnique Montreal, Montreal, PQ, Canada', 'UC Santa Cruz, Santa Cruz, CA, USA']","['US', 'Canada', 'US']"
https://arxiv.org/abs/1801.07593,Fairness & Bias,Mitigating Unwanted Biases with Adversarial Learning,"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.",[],[],"['Brian Hu Zhang', 'Blake Lemoine', 'Margaret Mitchell']","['Stanford University, Stanford, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1712.00846,Fairness & Bias, Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection,"Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.",[],[],"['Kyle Hundman', 'Thamme Gowda', 'Mayank Kejriwal', 'Benedikt Boecking']",[],[]
https://arxiv.org/abs/1711.07111,Fairness & Bias,Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions,"Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.",[],[],"['Marisa Vasconcelos', 'Carlos Cardonha', 'Bernardo Gonçalves']",[],[]
https://arxiv.org/abs/1808.00089,Fairness & Bias,Towards Composable Bias Rating of AI Systems,"A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance. However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf. A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias. Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about. The possibly biased behavior of a service is hard to detect and handle if the AI service is merely being used and not developed from scratch, since the training data set is not available. In this situation, we envisage a 3rd party rating agency that is independent of the API producer or consumer and has its own set of biased and unbiased data, with customizable distributions. We propose a 2-step rating approach that generates bias ratings signifying whether the AI service is unbiased compensating, data-sensitive biased, or biased. The approach also works on composite services. We implement it in the context of text translation and report interesting results.",[],[],"['Biplav Srivastava', 'Francesca Rossi']",[],[]
https://arxiv.org/abs/2008.13404,Privacy & Data Governance, On the distinction between implicit and explicit ethical agency,"Personalization should take the human person seriously. This requires a deeper understanding of how recommender systems can shape both our self-understanding and identity. We unpack key European humanistic and philosophical ideas underlying the General Data Protection Regulation (GDPR) and propose a new paradigm of humanistic personalization. Humanistic personalization responds to the IEEE's call for Ethically Aligned Design (EAD) and is based on fundamental human capacities and values. Humanistic personalization focuses on narrative accuracy: the subjective fit between a person's self-narrative and both the input (personal data) and output of a recommender system. In doing so, we re-frame the distinction between implicit and explicit data collection as one of nonconscious (""organismic"") behavior and conscious (""reflective"") action. This distinction raises important ethical and interpretive issues related to agency, self-understanding, and political participation. Finally, we discuss how an emphasis on narrative accuracy can reduce opportunities for epistemic injustice done to data subjects.",[],[],"['Travis Greene', 'Galit Shmueli']","['Western Norway University of Applied Sciences, Bergen, Norway', 'University of Bergen, Bergen, Norway', 'University of Bergen, Bergen, Norway']","['Norway', 'Norway', 'Norway']"
https://arxiv.org/abs/2212.14736,Privacy & Data Governance,Privacy-preserving Machine Learning Based Data Analytics on Edge Devices,"Consumer healthcare Internet of Things (IoT) devices are gaining popularity in our homes and hospitals. These devices provide continuous monitoring at a low cost and can be used to augment high-precision medical equipment. However, major challenges remain in applying pre-trained global models for anomaly detection on smart health monitoring, for a diverse set of individuals that they provide care for. In this paper, we propose PRISM, an edge-based system for experimenting with in-home smart healthcare devices. We develop a rigorous methodology that relies on automated IoT experimentation. We use a rich real-world dataset from in-home patient monitoring from 44 households of People Living With Dementia (PLWD) over two years. Our results indicate that anomalies can be identified with accuracy up to 99% and mean training times as low as 0.88 seconds. While all models achieve high accuracy when trained on the same patient, their accuracy degrades when evaluated on different patients.",[],[],"['Savvas Hadjixenophontos', 'Anna Maria Mandalari', 'Yuchen Zhao', 'Hamed Haddadi']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2308.16538,Privacy & Data Governance, Opportunities and Challenges for Artificial Intelligence in India,"This report examines Artificial Intelligence (AI) in the financial sector, outlining its potential to revolutionise the industry and identify its challenges. It underscores the criticality of a well-rounded understanding of AI, its capabilities, and its implications to effectively leverage its potential while mitigating associated risks. The potential of AI potential extends from augmenting existing operations to paving the way for novel applications in the finance sector. The application of AI in the financial sector is transforming the industry. Its use spans areas from customer service enhancements, fraud detection, and risk management to credit assessments and high-frequency trading. However, along with these benefits, AI also presents several challenges. These include issues related to transparency, interpretability, fairness, accountability, and trustworthiness. The use of AI in the financial sector further raises critical questions about data privacy and security. A further issue identified in this report is the systemic risk that AI can introduce to the financial sector. Being prone to errors, AI can exacerbate existing systemic risks, potentially leading to financial crises. Regulation is crucial to harnessing the benefits of AI while mitigating its potential risks. Despite the global recognition of this need, there remains a lack of clear guidelines or legislation for AI use in finance. This report discusses key principles that could guide the formation of effective AI regulation in the financial sector, including the need for a risk-based approach, the inclusion of ethical considerations, and the importance of maintaining a balance between innovation and consumer protection. The report provides recommendations for academia, the finance industry, and regulators.",[],[],"['Carsten Maple', 'Lukasz Szpruch', 'Gregory Epiphaniou', 'Kalina Staykova', 'Simran Singh', 'William Penwarden', 'Yisi Wen', 'Zijian Wang', 'Jagdish Hariharan', 'Pavle Avramovic']","['Indian Institute of Technology Bombay, Mumbai, India', 'Embrace Innovations, Bangalore, India', ""King's College London, London, United Kingdom"", 'Rao Law Chambers, Azim Premji University, Bangalore, India']","['India', 'India', 'United Kingdom', 'India']"
https://arxiv.org/abs/1803.02852,Security,"Value Alignment, Fair Play, and the Rights of Service Robots","Ethics and safety research in artificial intelligence is increasingly framed in terms of ""alignment"" with human values and interests. I argue that Turing's call for ""fair play for machines"" is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on ""fair play"" motivate a novel interpretation of Turing's notorious ""imitation game"" as a condition not of intelligence but instead of value alignment: a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is ""fair"" in precisely the sense that it encourages an alignment of interests between humans and machines.",[],[],['Daniel Estrada'],[],[]
https://arxiv.org/abs/2203.02662,Security,Regulating for ânormal AI accidentsâ: operational lessons for the responsible governance of AI deployment,"Metaverse, as an evolving paradigm of the next-generation Internet, aims to build a fully immersive, hyper spatiotemporal, and self-sustaining virtual shared space for humans to play, work, and socialize. Driven by recent advances in emerging technologies such as extended reality, artificial intelligence, and blockchain, metaverse is stepping from science fiction to an upcoming reality. However, severe privacy invasions and security breaches (inherited from underlying technologies or emerged in the new digital ecology) of metaverse can impede its wide deployment. At the same time, a series of fundamental challenges (e.g., scalability and interoperability) can arise in metaverse security provisioning owing to the intrinsic characteristics of metaverse, such as immersive realism, hyper spatiotemporality, sustainability, and heterogeneity. In this paper, we present a comprehensive survey of the fundamentals, security, and privacy of metaverse. Specifically, we first investigate a novel distributed metaverse architecture and its key characteristics with ternary-world interactions. Then, we discuss the security and privacy threats, present the critical challenges of metaverse systems, and review the state-of-the-art countermeasures. Finally, we draw open research directions for building future metaverse systems.",[],[],"['Yuntao Wang', 'Zhou Su', 'Ning Zhang', 'Rui Xing', 'Dongxiao Liu', 'Tom H. Luan', 'Xuemin Shen']",[],[]
https://arxiv.org/abs/1711.09050,Security,Ethical Challenges in Data-Driven Dialogue Systems,"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.",[],[],"['Peter Henderson', 'Koustuv Sinha', 'Nicolas Angelard-Gontier', 'Nan Rosemary Ke', 'Genevieve Fried', 'Ryan Lowe', 'Joelle Pineau']","['McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'Polytechnique Montréal, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada']","['Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada']"
https://arxiv.org/abs/2102.02928,Security,Sociotechnical Systems and Ethics in the Large,"The expansion of artificial intelligence (AI) and autonomous systems has shown the potential to generate enormous social good while also raising serious ethical and safety concerns. AI technology is increasingly adopted in transportation. A survey of various in-vehicle technologies found that approximately 64% of the respondents used a smartphone application to assist with their travel. The top-used applications were navigation and real-time traffic information systems. Among those who used smartphones during their commutes, the top-used applications were navigation and entertainment. There is a pressing need to address relevant social concerns to allow for the development of systems of intelligent agents that are informed and cognizant of ethical standards. Doing so will facilitate the responsible integration of these systems in society. To this end, we have applied Multi-Criteria Decision Analysis (MCDA) to develop a formal Multi-Attribute Impact Assessment (MAIA) questionnaire for examining the social and ethical issues associated with the uptake of AI. We have focused on the domain of autonomous vehicles (AVs) because of their imminent expansion. However, AVs could serve as a stand-in for any domain where intelligent, autonomous agents interact with humans, either on an individual level (e.g., pedestrians, passengers) or a societal level.",[],[],"['Veljko Dubljević', 'George F. List', 'Jovan Milojevich', 'Nirav Ajmeri', 'William Bauer', 'Munindar P. Singh', 'Eleni Bardaka', 'Thomas Birkland', 'Charles Edwards', 'Roger Mayer', 'Ioan Muntean', 'Thomas Powers', 'Hesham Rakha', 'Vance Ricks', 'M. Shoaib Samandar']","['Lancaster University, Lancaster, United Kingdom', 'North Carolina State University, Raleigh, NC, USA']","['United Kingdom', 'US']"
https://arxiv.org/abs/2308.16538,Security, Opportunities and Challenges for Artificial Intelligence in India,"This report examines Artificial Intelligence (AI) in the financial sector, outlining its potential to revolutionise the industry and identify its challenges. It underscores the criticality of a well-rounded understanding of AI, its capabilities, and its implications to effectively leverage its potential while mitigating associated risks. The potential of AI potential extends from augmenting existing operations to paving the way for novel applications in the finance sector. The application of AI in the financial sector is transforming the industry. Its use spans areas from customer service enhancements, fraud detection, and risk management to credit assessments and high-frequency trading. However, along with these benefits, AI also presents several challenges. These include issues related to transparency, interpretability, fairness, accountability, and trustworthiness. The use of AI in the financial sector further raises critical questions about data privacy and security. A further issue identified in this report is the systemic risk that AI can introduce to the financial sector. Being prone to errors, AI can exacerbate existing systemic risks, potentially leading to financial crises. Regulation is crucial to harnessing the benefits of AI while mitigating its potential risks. Despite the global recognition of this need, there remains a lack of clear guidelines or legislation for AI use in finance. This report discusses key principles that could guide the formation of effective AI regulation in the financial sector, including the need for a risk-based approach, the inclusion of ethical considerations, and the importance of maintaining a balance between innovation and consumer protection. The report provides recommendations for academia, the finance industry, and regulators.",[],[],"['Carsten Maple', 'Lukasz Szpruch', 'Gregory Epiphaniou', 'Kalina Staykova', 'Simran Singh', 'William Penwarden', 'Yisi Wen', 'Zijian Wang', 'Jagdish Hariharan', 'Pavle Avramovic']","['Indian Institute of Technology Bombay, Mumbai, India', 'Embrace Innovations, Bangalore, India', ""King's College London, London, United Kingdom"", 'Rao Law Chambers, Azim Premji University, Bangalore, India']","['India', 'India', 'United Kingdom', 'India']"
https://arxiv.org/abs/1801.07593,Security,Mitigating Unwanted Biases with Adversarial Learning,"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.",[],[],"['Brian Hu Zhang', 'Blake Lemoine', 'Margaret Mitchell']","['Stanford University, Stanford, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA']","['US', 'US', 'US']"