link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/2302.09157,Transparency & Explainability,Blind Justice: Algorithmically Masking Race in Charging Decisions.,"Predictive algorithms are now used to help distribute a large share of our society's resources and sanctions, such as healthcare, loans, criminal detentions, and tax audits. Under the right circumstances, these algorithms can improve the efficiency and equity of decision-making. At the same time, there is a danger that the algorithms themselves could entrench and exacerbate disparities, particularly along racial, ethnic, and gender lines. To help ensure their fairness, many researchers suggest that algorithms be subject to at least one of three constraints: (1) no use of legally protected features, such as race, ethnicity, and gender; (2) equal rates of ""positive"" decisions across groups; and (3) equal error rates across groups. Here we show that these constraints, while intuitively appealing, often worsen outcomes for individuals in marginalized groups, and can even leave all groups worse off. The inherent trade-off we identify between formal fairness constraints and welfare improvements -- particularly for the marginalized -- highlights the need for a more robust discussion on what it means for an algorithm to be ""fair"". We illustrate these ideas with examples from healthcare and the criminal-legal system, and make several proposals to help practitioners design more equitable algorithms.",[],[],"['Alex Chohlas-Wood', 'Madison Coots', 'Sharad Goel', 'Julian Nyarko']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2011.03654,Transparency & Explainability,Fair Machine Learning under Partial Compliance.,"Typically, fair machine learning research focuses on a single decisionmaker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decisionmakers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does the strategic behavior of decision subjects in partial compliance settings affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance (k% of employers) can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; and (4) partial compliance to local parity measures can induce extreme segregation.",[],[],"['Jessica Dai', 'Sina Fazelpour', 'Zachary C. Lipton']","['Brown University, Providence, RI, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2203.15370,Transparency & Explainability,Moral Disagreement and Artificial Intelligence.,"An assurance case is a structured argument, typically produced by safety engineers, to communicate confidence that a critical or complex system, such as an aircraft, will be acceptably safe within its intended context. Assurance cases often inform third party approval of a system. One emerging proposition within the trustworthy AI and autonomous systems (AI/AS) research community is to use assurance cases to instil justified confidence that specific AI/AS will be ethically acceptable when operational in well-defined contexts. This paper substantially develops the proposition and makes it concrete. It brings together the assurance case methodology with a set of ethical principles to structure a principles-based ethics assurance argument pattern. The principles are justice, beneficence, non-maleficence, and respect for human autonomy, with the principle of transparency playing a supporting role. The argument pattern, shortened to the acronym PRAISE, is described. The objective of the proposed PRAISE argument pattern is to provide a reusable template for individual ethics assurance cases, by which engineers, developers, operators, or regulators could justify, communicate, or challenge a claim about the overall ethical acceptability of the use of a specific AI/AS in a given socio-technical context. We apply the pattern to the hypothetical use case of an autonomous robo-taxi service in a city centre.",[],[],"['Zoe Porter', 'Ibrahim Habli', 'John McDermid', 'Marten Kaas']","['Australian National University, Canberra, Australia']",['Australia']
https://arxiv.org/abs/1907.00164,Transparency & Explainability,On the Privacy Risks of Model Explanations.,"Privacy and transparency are two key foundations of trustworthy machine learning. Model explanations offer insights into a model's decisions on input data, whereas privacy is primarily concerned with protecting information about the training data. We analyze connections between model explanations and the leakage of sensitive information about the model's training set. We investigate the privacy risks of feature-based model explanations using membership inference attacks: quantifying how much model predictions plus their explanations leak information about the presence of a datapoint in the training set of a model. We extensively evaluate membership inference attacks based on feature-based model explanations, over a variety of datasets. We show that backpropagation-based explanations can leak a significant amount of information about individual training datapoints. This is because they reveal statistical information about the decision boundaries of the model about an input, which can reveal its membership. We also empirically investigate the trade-off between privacy and explanation quality, by studying the perturbation-based model explanations.",[],[],"['Reza Shokri', 'Martin Strobel', 'Yair Zick']","['National University of Singapore, Singapore, Singapore', 'National University of Singapore, Singapore, Singapore', 'University of Massachusetts, Amherst, Amherst, MA, USA']","['Singapore', 'Singapore', 'US']"
https://arxiv.org/abs/2002.11836,Transparency & Explainability,Are AI Ethics Conferences Different and More Diverse Compared to Traditional Computer Science Conferences?,"The emergence and growth of research on issues of ethics in AI, and in particular algorithmic fairness, has roots in an essential observation that structural inequalities in society are reflected in the data used to train predictive models and in the design of objective functions. While research aiming to mitigate these issues is inherently interdisciplinary, the design of unbiased algorithms and fair socio-technical systems are key desired outcomes which depend on practitioners from the fields of data science and computing. However, these computing fields broadly also suffer from the same under-representation issues that are found in the datasets we analyze. This disconnect affects the design of both the desired outcomes and metrics by which we measure success. If the ethical AI research community accepts this, we tacitly endorse the status quo and contradict the goals of non-discrimination and equity which work on algorithmic fairness, accountability, and transparency seeks to address. Therefore, we advocate in this work for diversifying computing as a core priority of the field and our efforts to achieve ethical AI practices. We draw connections between the lack of diversity within academic and professional computing fields and the type and breadth of the biases encountered in datasets, machine learning models, problem formulations, and interpretation of results. Examining the current fairness/ethics in AI literature, we highlight cases where this lack of diverse perspectives has been foundational to the inequity in treatment of underrepresented and protected group data. We also look to other professional communities, such as in law and health, where disparities have been reduced both in the educational diversity of trainees and among professional practices. We use these lessons to develop recommendations that provide concrete steps for the computing community to increase diversity.",[],[],"['Caitlin Kuhlman', 'Latifa Jackson', 'Rumi Chunara']","['Syracuse University, Syracuse, NY, USA', 'Syracuse University, Syracuse, NY, USA']","['US', 'US']"
https://arxiv.org/abs/2105.00060,Transparency & Explainability,Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization.,"AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.",[],[],"['Michael Anis Mihdi Afnan', 'Cynthia Rudin', 'Vincent Conitzer', 'Julian Savulescu', 'Abhishek Mishra', 'Yanhe Liu', 'Masoud Afnan']","['Imperial College London, London, United Kingdom', 'Duke University, Durham, NC, USA', 'Duke University & Oxford University, Durham, NC, USA', ""Oxford University & Oxford University & Royal Children's Hospital, Oxford, United Kingdom"", 'Oxford University, Oxford, United Kingdom', 'Monash IVF Group & University of Western Australia & Edith Cowan University, Southport, Australia', 'Qingdao United Family Hospital, Qingdao, China']","['United Kingdom', 'US', 'US', 'United Kingdom', 'United Kingdom', 'Australia', 'China']"
https://arxiv.org/abs/2206.07173,Transparency & Explainability,"Person, Human, Neither: The Dehumanization Potential of Automated Image Tagging.","Previous work has largely considered the fairness of image captioning systems through the underspecified lens of ""bias."" In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.",[],[],"['Angelina Wang', 'Solon Barocas', 'Kristen Laird', 'Hanna Wallach']","['CYENS Centre of Excellence, Nicosia, Cyprus', 'CYENS Centre of Excellence, Nicosia, Cyprus', 'Cyprus Center for Algorithmic Transparency & Open University of Cyprus, Latsia, Cyprus', 'Cyprus Center for Algorithmic Transparency & Open University of Cyprus, Latsia, Cyprus']","['Cyprus', 'Cyprus', 'Cyprus', 'Cyprus']"
https://arxiv.org/abs/2101.02555,Transparency & Explainability,Explainable AI and Adoption of Financial Algorithmic Advisors: An Experimental Study.,"We study whether receiving advice from either a human or algorithmic advisor, accompanied by five types of Local and Global explanation labelings, has an effect on the readiness to adopt, willingness to pay, and trust in a financial AI consultant. We compare the differences over time and in various key situations using a unique experimental framework where participants play a web-based game with real monetary consequences. We observed that accuracy-based explanations of the model in initial phases leads to higher adoption rates. When the performance of the model is immaculate, there is less importance associated with the kind of explanation for adoption. Using more elaborate feature-based or accuracy-based explanations helps substantially in reducing the adoption drop upon model failure. Furthermore, using an autopilot increases adoption significantly. Participants assigned to the AI-labeled advice with explanations were willing to pay more for the advice than the AI-labeled advice with a No-explanation alternative. These results add to the literature on the importance of XAI for algorithmic adoption and trust.",[],[],"['Daniel Ben David', 'Yehezkel S. Resheff', 'Talia Tron']","['The Hebrew University of Jerusalem, Jerusalem, Israel', 'Holon Institude of Technology, Holon, Israel', 'Intuit Inc., Tel Aviv, Israel']","['Israel', 'Israel', 'Israel']"
https://arxiv.org/abs/2011.07586,Transparency & Explainability,"Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty.","Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.",[],[],"['Umang Bhatt', 'Javier Antorán', 'Yunfeng Zhang', 'Q. Vera Liao', 'Prasanna Sattigeri', 'Riccardo Fogliato', 'Gabrielle Gauthier Melançon', 'Ranganath Krishnan', 'Jason Stanley', 'Omesh Tickoo', 'Lama Nachman', 'Rumi Chunara', 'Madhulika Srikumar', 'Adrian Weller', 'Alice Xiang']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'ElementAI, Montreal, PQ, Canada', 'Intel, Portland, OR, USA', 'ElementAI, Montreal, PQ, Canada', 'Intel, Portland, OR, USA', 'Intel, Santa Clara, CA, USA', 'New York University, New York City, NY, USA', 'Partnership on AI, San Francisco, CA, USA', 'University of Cambridge, Cambridge, United Kingdom', 'Sony AI, Seattle, WA, USA']","['United Kingdom', 'United Kingdom', 'US', 'US', 'US', 'US', 'Canada', 'US', 'Canada', 'US', 'US', 'US', 'US', 'United Kingdom', 'US']"
https://arxiv.org/abs/2105.10174,Transparency & Explainability,Algorithmic Audit of Italian Car Insurance: Evidence of Unfairness in Access and Pricing.,"We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 euros more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.",[],[],"['Alessandro Fabris', 'Alan Mishler', 'Stefano Gottardi', 'Mattia Carletti', 'Matteo Daicampi', 'Gian Antonio Susto', 'Gianmaria Silvello']","['University of Padua, Padua, Italy', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'University of Padua, Padua, Italy', 'University of Padua, Padua, Italy', 'University of Udine, Udine, Italy', 'University of Padua, Padua, Italy', 'University of Padua, Padua, Italy']","['Italy', 'US', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy']"
https://arxiv.org/abs/2302.05284,Transparency & Explainability,Modeling and Guiding the Creation of Ethical Human-AI Teams.,"As Artificial Intelligence (AI) continues to advance rapidly, it becomes increasingly important to consider AI's ethical and societal implications. In this paper, we present a bottom-up mapping of the current state of research at the intersection of Human-Centered AI, Ethical, and Responsible AI (HCER-AI) by thematically reviewing and analyzing 164 research papers from leading conferences in ethical, social, and human factors of AI: AIES, CHI, CSCW, and FAccT. The ongoing research in HCER-AI places emphasis on governance, fairness, and explainability. These conferences, however, concentrate on specific themes rather than encompassing all aspects. While AIES has fewer papers on HCER-AI, it emphasizes governance and rarely publishes papers about privacy, security, and human flourishing. FAccT publishes more on governance and lacks papers on privacy, security, and human flourishing. CHI and CSCW, as more established conferences, have a broader research portfolio. We find that the current emphasis on governance and fairness in AI research may not adequately address the potential unforeseen and unknown implications of AI. Therefore, we recommend that future research should expand its scope and diversify resources to prepare for these potential consequences. This could involve exploring additional areas such as privacy, security, human flourishing, and explainability.",[],[],"['Mohammad Tahaei', 'Marios Constantinides', 'Daniele Quercia', 'Michael Muller']","['Clemson University, Clemson, SC, USA', 'Clemson University, Clemson, SC, USA', 'Clemson University, Clemson , SC, USA', 'Clemson University, Clemson , SC, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2102.03977,Transparency & Explainability,Learning to Generate Fair Clusters from Demonstrations.,"Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement, leading to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.",[],[],"['Sainyam Galhotra', 'Sandhya Saisubramanian', 'Shlomo Zilberstein']","['University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2007.11973,Transparency & Explainability,Ethical Obligations to Provide Novelty.,"In this paper, we provide a philosophical account of the value of creative systems for individuals and society. We characterize creativity in very broad philosophical terms, encompassing natural, existential, and social creative processes, such as natural evolution and entrepreneurship, and explain why creativity understood in this way is instrumental for advancing human well-being in the long term. We then explain why current mainstream AI tends to be anti-creative, which means that there are moral costs of employing this type of AI in human endeavors, although computational systems that involve creativity are on the rise. In conclusion, there is an argument for ethics to be more hospitable to creativity-enabling AI, which can also be in a trade-off with other values promoted in AI ethics, such as its explainability and accuracy.",[],[],"['Michele Loi', 'Eleonora Viganò', 'Lonneke van der Plas']","['University of Pennsylvania, Philadelphia, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US']"
https://arxiv.org/abs/2302.10329,Transparency & Explainability,An AI Ethics Course Highlighting Explicit Ethical Agents.,"Research in Fairness, Accountability, Transparency, and Ethics (FATE) has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed which threaten the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms. Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency -- notably, these include systemic and/or long-range impacts, often on marginalized stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.",[],[],"['Alan Chan', 'Rebecca Salganik', 'Alva Markelius', 'Chris Pang', 'Nitarshan Rajkumar', 'Dmitrii Krasheninnikov', 'Lauro Langosco', 'Zhonghao He', 'Yawen Duan', 'Micah Carroll', 'Michelle Lin', 'Alex Mayhew', 'Katherine Collins', 'Maryam Molamohammadi', 'John Burden', 'Wanru Zhao', 'Shalaleh Rismani', 'Konstantinos Voudouris', 'Umang Bhatt', 'Adrian Weller', 'David Krueger', 'Tegan Maharaj']","['University of North Carolina Greensboro, Greensboro, NC, USA']",['US']
https://arxiv.org/abs/2104.00950,Transparency & Explainability,Designing Shapelets for Interpretable Data-Agnostic Classification.,"Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.",[],[],"['Thomas Rojat', 'Raphaël Puget', 'David Filliat', 'Javier Del Ser', 'Rodolphe Gelin', 'Natalia Díaz-Rodríguez']","['University of Pisa, Pisa, Italy', 'University of Pisa, Pisa, Italy']","['Italy', 'Italy']"
https://arxiv.org/abs/2011.04917,Transparency & Explainability,Towards Unifying Feature Attribution and Counterfactual Explanations: Different Means to the Same End.,"Feature attributions and counterfactual explanations are popular approaches to explain a ML model. The former assigns an importance score to each input feature, while the latter provides input examples with minimal changes to alter the model's predictions. To unify these approaches, we provide an interpretation based on the actual causality framework and present two key results in terms of their use. First, we present a method to generate feature attribution explanations from a set of counterfactual examples. These feature attributions convey how important a feature is to changing the classification outcome of a model, especially on whether a subset of features is necessary and/or sufficient for that change, which attribution-based methods are unable to provide. Second, we show how counterfactual examples can be used to evaluate the goodness of an attribution-based explanation in terms of its necessity and sufficiency. As a result, we highlight the complementarity of these two approaches. Our evaluation on three benchmark datasets - Adult-Income, LendingClub, and German-Credit - confirms the complementarity. Feature attribution methods like LIME and SHAP and counterfactual explanation methods like Wachter et al. and DiCE often do not agree on feature importance rankings. In addition, by restricting the features that can be modified for generating counterfactual examples, we find that the top-k features from LIME or SHAP are often neither necessary nor sufficient explanations of a model's prediction. Finally, we present a case study of different explanation methods on a real-world hospital triage problem",[],[],"['Ramaravind Kommiya Mothilal', 'Divyat Mahajan', 'Chenhao Tan', 'Amit Sharma']","['Microsoft Research India, Bangalore, India', 'Microsoft Research India, Bangalore, India', 'University of Chicago, Chicago, IL, USA', 'Microsoft Research India, Bangalore, India']","['India', 'India', 'US', 'India']"
https://arxiv.org/abs/2109.09672,Transparency & Explainability,Risk Identification Questionnaire for Unintended Bias in Machine Learning Development Lifecycle.,"The widespread use of artificial intelligence (AI) in many domains has revealed numerous ethical issues from data and design to deployment. In response, countless broad principles and guidelines for ethical AI have been published, and following those, specific approaches have been proposed for how to encourage ethical outcomes of AI. Meanwhile, library and information services too are seeing an increase in the use of AI-powered and machine learning-powered information systems, but no practical guidance currently exists for libraries to plan for, evaluate, or audit the ethics of intended or deployed AI. We therefore report on several promising approaches for promoting ethical AI that can be adapted from other contexts to AI-powered information services and in different stages of the software lifecycle.",[],[],"['Helen Bubinger', 'Jesse David Dinneen']",[],[]
https://arxiv.org/abs/2105.01434,Transparency & Explainability,Towards Accountability in the Use of Artificial Intelligence for Public Administrations.,"We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer to processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.",[],[],"['Michele Loi', 'Matthias Spielkamp']","['University of Zurich, Zurich, Switzerland', 'AlgorithmWatch, Berlin, Germany']","['Switzerland', 'Germany']"
https://arxiv.org/abs/2006.04599,Transparency & Explainability,Disparate Impact of Artificial Intelligence Bias in Ridehailing Economyâs Price Discrimination Algorithms.,"Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications. The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.",[],[],"['Akshat Pandey', 'Aylin Caliskan']",[],[]
https://arxiv.org/abs/1912.02943,Transparency & Explainability,Face Mis-ID: Interrogating Facial Recognition Harms with Community Organizers Using an Interactive Demo.,"A wave of recent scholarship documenting the discriminatory harms of algorithmic systems has spurred widespread interest in algorithmic accountability and regulation. Yet effective accountability and regulation is stymied by a persistent lack of resources supporting public understanding of algorithms and artificial intelligence. Through interactions with a US-based civil rights organization and their coalition of community organizations, we identify a need for (i) heuristics that aid stakeholders in distinguishing between types of analytic and information systems in lay language, and (ii) risk assessment tools for such systems that begin by making algorithms more legible. The present work delivers a toolkit to achieve these aims. This paper both presents the Algorithmic Equity Toolkit (AEKit) Equity as an artifact, and details how our participatory process shaped its design. Our work fits within human-computer interaction scholarship as a demonstration of the value of HCI methods and approaches to problems in the area of algorithmic transparency and accountability.",[],[],"['Michael Katell', 'Meg Young', 'Bernease Herman', 'Dharma Dailey', 'Aaron Tam', 'Vivian Guetler', 'Corinne Binz', 'Daniella Raz', 'P. M. Krafft']",[],[]
https://arxiv.org/abs/2006.00305,Transparency & Explainability,RelEx: A Model-Agnostic Relational Model Explainer.,"In recent years, considerable progress has been made on improving the interpretability of machine learning models. This is essential, as complex deep learning models with millions of parameters produce state of the art results, but it can be nearly impossible to explain their predictions. While various explainability techniques have achieved impressive results, nearly all of them assume each data instance to be independent and identically distributed (iid). This excludes relational models, such as Statistical Relational Learning (SRL), and the recently popular Graph Neural Networks (GNNs), resulting in few options to explain them. While there does exist one work on explaining GNNs, GNN-Explainer, they assume access to the gradients of the model to learn explanations, which is restrictive in terms of its applicability across non-differentiable relational models and practicality. In this work, we develop RelEx, a model-agnostic relational explainer to explain black-box relational models with only access to the outputs of the black-box. RelEx is able to explain any relational model, including SRL models and GNNs. We compare RelEx to the state-of-the-art relational explainer, GNN-Explainer, and relational extensions of iid explanation models and show that RelEx achieves comparable or better performance, while remaining model-agnostic.",[],[],"['Yue Zhang', 'David Defazio', 'Arti Ramesh']","['SUNY Binghamton, Binghamton, NY, USA', 'SUNY Binghamton, Binghamton, NY, USA', 'SUNY Binghamton, Binghamton, NY, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2205.02526,Fairness & Bias,Gender Bias and Under-Representation in Natural Language Processing across Human Languages.,"The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how ""gender"" is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define ""bias."" Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.",[],[],"['Hannah Devinney', 'Jenny Björklund', 'Henrik Björklund']","['Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam, NY, USA', 'Clarkson University, Potsdam , NY, USA', 'University of Wisconsin, Madison, WI, USA', 'Clarkson University, Potsdam, NY, USA', 'Iona College, New York, NY, USA', 'Clarkson University, Potsdam, NY, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2302.09157,Fairness & Bias,Blind Justice: Algorithmically Masking Race in Charging Decisions.,"Predictive algorithms are now used to help distribute a large share of our society's resources and sanctions, such as healthcare, loans, criminal detentions, and tax audits. Under the right circumstances, these algorithms can improve the efficiency and equity of decision-making. At the same time, there is a danger that the algorithms themselves could entrench and exacerbate disparities, particularly along racial, ethnic, and gender lines. To help ensure their fairness, many researchers suggest that algorithms be subject to at least one of three constraints: (1) no use of legally protected features, such as race, ethnicity, and gender; (2) equal rates of ""positive"" decisions across groups; and (3) equal error rates across groups. Here we show that these constraints, while intuitively appealing, often worsen outcomes for individuals in marginalized groups, and can even leave all groups worse off. The inherent trade-off we identify between formal fairness constraints and welfare improvements -- particularly for the marginalized -- highlights the need for a more robust discussion on what it means for an algorithm to be ""fair"". We illustrate these ideas with examples from healthcare and the criminal-legal system, and make several proposals to help practitioners design more equitable algorithms.",[],[],"['Alex Chohlas-Wood', 'Madison Coots', 'Sharad Goel', 'Julian Nyarko']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2102.01203,Fairness & Bias,Emergent Unfairness: Normative Assumptions and Contradictions in Algorithmic Fairness-Accuracy Trade-Off Research.,"Across machine learning (ML) sub-disciplines, researchers make explicit mathematical assumptions in order to facilitate proof-writing. We note that, specifically in the area of fairness-accuracy trade-off optimization scholarship, similar attention is not paid to the normative assumptions that ground this approach. Such assumptions presume that 1) accuracy and fairness are in inherent opposition to one another, 2) strict notions of mathematical equality can adequately model fairness, 3) it is possible to measure the accuracy and fairness of decisions independent from historical context, and 4) collecting more data on marginalized individuals is a reasonable solution to mitigate the effects of the trade-off. We argue that such assumptions, which are often left implicit and unexamined, lead to inconsistent conclusions: While the intended goal of this work may be to improve the fairness of machine learning models, these unexamined, implicit assumptions can in fact result in emergent unfairness. We conclude by suggesting a concrete path forward toward a potential resolution.",[],[],"['A. Feder Cooper', 'Ellen Abrams']",[],[]
https://arxiv.org/abs/2011.03654,Fairness & Bias,Fair Machine Learning under Partial Compliance.,"Typically, fair machine learning research focuses on a single decisionmaker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decisionmakers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does the strategic behavior of decision subjects in partial compliance settings affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance (k% of employers) can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; and (4) partial compliance to local parity measures can induce extreme segregation.",[],[],"['Jessica Dai', 'Sina Fazelpour', 'Zachary C. Lipton']","['Brown University, Providence, RI, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2011.03108,Fairness & Bias,Minimax Group Fairness: Algorithms and Experiments.,"We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.",[],[],"['Emily Diana', 'Wesley Gill', 'Michael Kearns', 'Krishnaram Kenthapadi', 'Aaron Roth']","['University of Pennsylvania, Amazon AWS AI, Philadelphia, PA, USA', 'The University of Pennsylvania, Amazon AWS AI, Philadelphia, PA, USA', 'University of Pennsylvania, Amazon AWS AI, Philadelphia, PA, USA', 'Amazon AWS AI, Sunnyvale, CA, USA', 'University of Pennsylvania & Amazon AWS AI, Philadelphia, PA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2009.10576,Fairness & Bias,Co-Design and Ethical Artificial Intelligence for Health: Myths and Misconceptions.,"The use of machine learning (ML) in health care raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of health care. Specifically, we frame ethics of ML in health care through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to post-deployment considerations. We close by summarizing recommendations to address these challenges.",[],[],"['Irene Y. Chen', 'Emma Pierson', 'Sherri Rose', 'Shalmali Joshi', 'Kadija Ferryman', 'Marzyeh Ghassemi']","['University of Toronto, Toronto, ON, Canada', ""Women's College Hospital, Toronto, ON, Canada""]","['Canada', 'Canada']"
https://arxiv.org/abs/2105.04953,Fairness & Bias,On the Validity of Arrest as a Proxy for Offense: Race and the Likelihood of Arrest for Violent Crimes.,"The risk of re-offense is considered in decision-making at many stages of the criminal justice system, from pre-trial, to sentencing, to parole. To aid decision makers in their assessments, institutions increasingly rely on algorithmic risk assessment instruments (RAIs). These tools assess the likelihood that an individual will be arrested for a new criminal offense within some time window following their release. However, since not all crimes result in arrest, RAIs do not directly assess the risk of re-offense. Furthermore, disparities in the likelihood of arrest can potentially lead to biases in the resulting risk scores. Several recent validations of RAIs have therefore focused on arrests for violent offenses, which are viewed as being more accurate reflections of offending behavior. In this paper, we investigate biases in violent arrest data by analysing racial disparities in the likelihood of arrest for White and Black violent offenders. We focus our study on 2007--2016 incident-level data of violent offenses from 16 US states as recorded in the National Incident Based Reporting System (NIBRS). Our analysis shows that the magnitude and direction of the racial disparities depend on various characteristics of the crimes. In addition, our investigation reveals large variations in arrest rates across geographical locations and offense types. We discuss the implications of the observed disconnect between re-arrest and re-offense in the context of RAIs and the challenges around the use of data from NIBRS to correct for the sampling bias.",[],[],"['Riccardo Fogliato', 'Alice Xiang', 'Zachary Lipton', 'Daniel Nagin', 'Alexandra Chouldechova']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Sony AI, Seattle, WA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2006.03955,Fairness & Bias,Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-Like Biases.,"With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that appear in particular contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD and EIBD achieve high accuracy when detecting the intersectional and emergent biases of African American females and Mexican American females. Our results indicate that biases at the intersection of race and gender associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",[],[],"['Wei Guo', 'Aylin Caliskan']","['George Washington University, Washington, DC, USA', 'George Washington University and Institute for Data, Democracy & Politics, Washington, DC, USA']","['US', 'US']"
https://arxiv.org/abs/2110.02932,Fairness & Bias,Machine Learning Practices Outside Big Tech: How Resource Constraints Hinder Responsible Development.,"Practitioners from diverse occupations and backgrounds are increasingly using machine learning (ML) methods. Nonetheless, studies on ML Practitioners typically draw populations from Big Tech and academia, as researchers have easier access to these communities. Through this selection bias, past research often excludes the broader, lesser-resourced ML community -- for example, practitioners working at startups, at non-tech companies, and in the public sector. These practitioners share many of the same ML development difficulties and ethical conundrums as their Big Tech counterparts; however, their experiences are subject to additional under-studied challenges stemming from deploying ML with limited resources, increased existential risk, and absent access to in-house research teams. We contribute a qualitative analysis of 17 interviews with stakeholders from organizations which are less represented in prior studies. We uncover a number of tensions which are introduced or exacerbated by these organizations' resource constraints -- tensions between privacy and ubiquity, resource management and performance optimization, and access and monopolization. Increased academic focus on these practitioners can facilitate a more holistic understanding of ML limitations, and so is useful for prescribing a research agenda to facilitate responsible ML development for all.",[],[],"['Aspen Hopkins', 'Serena Booth']",[],[]
https://arxiv.org/abs/2102.13004,Fairness & Bias,Towards Unbiased and Accurate Deferral to Multiple Experts.,"Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on ""deferral systems"" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed learning framework outperforms baselines on this real-world dataset as well.",[],[],"['Vijay Keswani', 'Matthew Lease', 'Krishnaram Kenthapadi']","['Yale University, New Haven, CT, USA', 'University of Texas at Austin, Austin, TX, USA', 'Amazon AWS AI, East Palo Alto, CA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2309.13933,Fairness & Bias,Algorithmic Hiring in Practice: Recruiter and HR Professionalâs Perspectives on AI Use in Hiring.,"Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of, algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.",[],[],"['Alessandro Fabris', 'Nina Baranowska', 'Matthew J. Dennis', 'Philipp Hacker', 'Jorge Saldivar', 'Frederik Zuiderveen Borgesius', 'Asia J. Biega']",[],[]
https://arxiv.org/abs/2301.05753,Fairness & Bias,Ethically Compliant Planning within Moral Communities.,"As automated decision making and decision assistance systems become common in everyday life, research on the prevention or mitigation of potential harms that arise from decisions made by these systems has proliferated. However, various research communities have independently conceptualized these harms, envisioned potential applications, and proposed interventions. The result is a somewhat fractured landscape of literature focused generally on ensuring decision-making algorithms ""do the right thing"". In this paper, we compare and discuss work across two major subsets of this literature: algorithmic fairness, which focuses primarily on predictive systems, and ethical decision making, which focuses primarily on sequential decision making and planning. We explore how each of these settings has articulated its normative concerns, the viability of different techniques for these different settings, and how ideas from each setting may have utility for the other.",[],[],"['Samer B. Nashed', 'Justin Svegliato', 'Su Lin Blodgett']","['University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2203.15370,Fairness & Bias,Moral Disagreement and Artificial Intelligence.,"An assurance case is a structured argument, typically produced by safety engineers, to communicate confidence that a critical or complex system, such as an aircraft, will be acceptably safe within its intended context. Assurance cases often inform third party approval of a system. One emerging proposition within the trustworthy AI and autonomous systems (AI/AS) research community is to use assurance cases to instil justified confidence that specific AI/AS will be ethically acceptable when operational in well-defined contexts. This paper substantially develops the proposition and makes it concrete. It brings together the assurance case methodology with a set of ethical principles to structure a principles-based ethics assurance argument pattern. The principles are justice, beneficence, non-maleficence, and respect for human autonomy, with the principle of transparency playing a supporting role. The argument pattern, shortened to the acronym PRAISE, is described. The objective of the proposed PRAISE argument pattern is to provide a reusable template for individual ethics assurance cases, by which engineers, developers, operators, or regulators could justify, communicate, or challenge a claim about the overall ethical acceptability of the use of a specific AI/AS in a given socio-technical context. We apply the pattern to the hypothetical use case of an autonomous robo-taxi service in a city centre.",[],[],"['Zoe Porter', 'Ibrahim Habli', 'John McDermid', 'Marten Kaas']","['Australian National University, Canberra, Australia']",['Australia']
https://arxiv.org/abs/2012.03063,Fairness & Bias,FairOD: Fairness-Aware Outlier Detection.,"Fairness and Outlier Detection (OD) are closely related, as it is exactly the goal of OD to spot rare, minority samples in a given population. However, when being a minority (as defined by protected variables, such as race/ethnicity/sex/age) does not reflect positive-class membership (such as criminal/fraud), OD produces unjust outcomes. Surprisingly, fairness-aware OD has been almost untouched in prior work, as fair machine learning literature mainly focuses on supervised settings. Our work aims to bridge this gap. Specifically, we develop desiderata capturing well-motivated fairness criteria for OD, and systematically formalize the fair OD problem. Further, guided by our desiderata, we propose FairOD, a fairness-aware outlier detector that has the following desirable properties: FairOD (1) exhibits treatment parity at test time, (2) aims to flag equal proportions of samples from all groups (i.e. obtain group fairness, via statistical parity), and (3) strives to flag truly high-risk samples within each group. Extensive experiments on a diverse set of synthetic and real world datasets show that FairOD produces outcomes that are fair with respect to protected variables, while performing comparable to (and in some cases, even better than) fairness-agnostic detectors in terms of detection performance.",[],[],"['Shubhranshu Shekhar', 'Neil Shah', 'Leman Akoglu']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Snap Inc., Seattle, WA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2102.04257,Fairness & Bias,Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities.,"Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness--frequently, race and legal gender--can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.",[],[],"['Nenad Tomasev', 'Kevin R. McKee', 'Jackie Kay', 'Shakir Mohamed']","['DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind & University College London, London, United Kingdom', 'DeepMind, London, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2202.00705,Fairness & Bias,"Freedom at Work: Understanding, Alienation, and the AI-Driven Workplace.","In this paper, we situate the educational movement of ""Ethics in Mathematics,"" as outlined by the Cambridge University Ethics in Mathematics Project, in the wider area of mathematics ethics education. By focusing on the core message coming out of Ethics in Mathematics, its target group, and educational philosophy, we set it into relation with ""Mathematics for Social Justice"" and Paul Ernest's recent work on ethics of mathematics. We conclude that, although both Ethics in Mathematics and Mathematics for Social Justice appear antagonistic at first glance, they can be understood as complementary rather than competing educational strategies.",[],[],['Dennis Müller'],[],[]
https://arxiv.org/abs/2009.13676,Fairness & Bias,"The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat on Academic Integrity.","As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place.",[],[],"['Mohamed Abdalla', 'Moustafa Abdalla']","['University of Toronto, Toronto, ON, Canada', 'Harvard Medical School, Cambridge, MA, USA']","['Canada', 'US']âIâm Covered in Bloodâ: Persistent Anti-Muslim Bias in Large Language Models."
https://arxiv.org/abs/2112.04359,Fairness & Bias,âIâm Covered in Bloodâ: Persistent Anti-Muslim Bias in Large Language Models.,"This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.",[],[],"['Laura Weidinger', 'John Mellor', 'Maribeth Rauh', 'Conor Griffin', 'Jonathan Uesato', 'Po-Sen Huang', 'Myra Cheng', 'Mia Glaese', 'Borja Balle', 'Atoosa Kasirzadeh', 'Zac Kenton', 'Sasha Brown', 'Will Hawkins', 'Tom Stepleton', 'Courtney Biles', 'Abeba Birhane', 'Julia Haas', 'Laura Rimell', 'Lisa Anne Hendricks', 'William Isaac', 'Sean Legassick', 'Geoffrey Irving', 'Iason Gabriel']","['Syracuse University, Syracuse, NY, USA', 'Syracuse University, Syracuse, NY, USA']","['US', 'US']"
https://arxiv.org/abs/2002.11836,Fairness & Bias,Are AI Ethics Conferences Different and More Diverse Compared to Traditional Computer Science Conferences?,"The emergence and growth of research on issues of ethics in AI, and in particular algorithmic fairness, has roots in an essential observation that structural inequalities in society are reflected in the data used to train predictive models and in the design of objective functions. While research aiming to mitigate these issues is inherently interdisciplinary, the design of unbiased algorithms and fair socio-technical systems are key desired outcomes which depend on practitioners from the fields of data science and computing. However, these computing fields broadly also suffer from the same under-representation issues that are found in the datasets we analyze. This disconnect affects the design of both the desired outcomes and metrics by which we measure success. If the ethical AI research community accepts this, we tacitly endorse the status quo and contradict the goals of non-discrimination and equity which work on algorithmic fairness, accountability, and transparency seeks to address. Therefore, we advocate in this work for diversifying computing as a core priority of the field and our efforts to achieve ethical AI practices. We draw connections between the lack of diversity within academic and professional computing fields and the type and breadth of the biases encountered in datasets, machine learning models, problem formulations, and interpretation of results. Examining the current fairness/ethics in AI literature, we highlight cases where this lack of diverse perspectives has been foundational to the inequity in treatment of underrepresented and protected group data. We also look to other professional communities, such as in law and health, where disparities have been reduced both in the educational diversity of trainees and among professional practices. We use these lessons to develop recommendations that provide concrete steps for the computing community to increase diversity.",[],[],"['Caitlin Kuhlman', 'Latifa Jackson', 'Rumi Chunara']","['Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Ulm University, Ulm, Germany', 'Google, Mountain View, CA, USA', 'Ethical AI LLC, Seattle, WA, USA']","['US', 'US', 'Germany', 'US', 'US']"
https://arxiv.org/abs/2103.03417,Fairness & Bias,Measuring Model Biases in the Absence of Ground Truth.,"The measurement of bias in machine learning often focuses on model performance across identity subgroups (such as man and woman) with respect to groundtruth labels. However, these methods do not directly measure the associations that a model may have learned, for example between labels and identity subgroups. Further, measuring a model's bias requires a fully annotated evaluation dataset which may not be easily available in practice. We present an elegant mathematical solution that tackles both issues simultaneously, using image classification as a working example. By treating a classification model's predictions for a given image as a set of labels analogous to a bag of words, we rank the biases that a model has learned with respect to different identity labels. We use (man, woman) as a concrete example of an identity label set (although this set need not be binary), and present rankings for the labels that are most biased towards one identity or the other. We demonstrate how the statistical properties of different association metrics can lead to different rankings of the most ""gender biased"" labels, and conclude that normalized pointwise mutual information (nPMI) is most useful in practice. Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.",[],[],"['Osman Aka', 'Ken Burke', 'Alex Bäuerle', 'Christina Greer', 'Margaret Mitchell']","['Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Informatics, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany']","['Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/2105.04249,Fairness & Bias,Accounting for Model Uncertainty in Algorithmic Discrimination.,"Traditional approaches to ensure group fairness in algorithmic decision making aim to equalize ``total'' error rates for different subgroups in the population. In contrast, we argue that the fairness approaches should instead focus only on equalizing errors arising due to model uncertainty (a.k.a epistemic uncertainty), caused due to lack of knowledge about the best model or due to lack of data. In other words, our proposal calls for ignoring the errors that occur due to uncertainty inherent in the data, i.e., aleatoric uncertainty. We draw a connection between predictive multiplicity and model uncertainty and argue that the techniques from predictive multiplicity could be used to identify errors made due to model uncertainty. We propose scalable convex proxies to come up with classifiers that exhibit predictive multiplicity and empirically show that our methods are comparable in performance and up to four orders of magnitude faster than the current state-of-the-art. We further propose methods to achieve our goal of equalizing group error rates arising due to model uncertainty in algorithmic decision making and demonstrate the effectiveness of these methods using synthetic and real-world datasets.",[],[],"['Junaid Ali', 'Preethi Lahoti', 'Krishna P. Gummadi']","['CYENS Centre of Excellence, Nicosia, Cyprus', 'CYENS Centre of Excellence, Nicosia, Cyprus', 'Cyprus Center for Algorithmic Transparency & Open University of Cyprus, Latsia, Cyprus', 'Cyprus Center for Algorithmic Transparency & Open University of Cyprus, Latsia, Cyprus']","['Cyprus', 'Cyprus', 'Cyprus', 'Cyprus']"
https://arxiv.org/abs/2206.07173,Fairness & Bias,"Person, Human, Neither: The Dehumanization Potential of Automated Image Tagging.","Previous work has largely considered the fairness of image captioning systems through the underspecified lens of ""bias."" In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.",[],[],"['Angelina Wang', 'Solon Barocas', 'Kristen Laird', 'Hanna Wallach']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'ElementAI, Montreal, PQ, Canada', 'Intel, Portland, OR, USA', 'ElementAI, Montreal, PQ, Canada', 'Intel, Portland, OR, USA', 'Intel, Santa Clara, CA, USA', 'New York University, New York City, NY, USA', 'Partnership on AI, San Francisco, CA, USA', 'University of Cambridge, Cambridge, United Kingdom', 'Sony AI, Seattle, WA, USA']","['United Kingdom', 'United Kingdom', 'US', 'US', 'US', 'US', 'Canada', 'US', 'Canada', 'US', 'US', 'US', 'US', 'United Kingdom', 'US']"
https://arxiv.org/abs/2011.07586,Fairness & Bias,"Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty.","Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.",[],[],"['Umang Bhatt', 'Javier Antorán', 'Yunfeng Zhang', 'Q. Vera Liao', 'Prasanna Sattigeri', 'Riccardo Fogliato', 'Gabrielle Gauthier Melançon', 'Ranganath Krishnan', 'Jason Stanley', 'Omesh Tickoo', 'Lama Nachman', 'Rumi Chunara', 'Madhulika Srikumar', 'Adrian Weller', 'Alice Xiang']","['Microsoft Research, Bangalore, India', 'Indian Institute of Science, Bangalore, India', 'Microsoft Research, Bangalore, India', 'Indian Institute of Science, Bangalore, India']","['India', 'India', 'India', 'India']"
https://arxiv.org/abs/2105.14890,Fairness & Bias,Rawlsian Fair Adaptation of Deep Learning Classifiers.,"Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness.",[],[],"['Kulin Shah', 'Pooja Gupta', 'Amit Deshpande', 'Chiranjib Bhattacharyya']","['Harvard University, Cambridge, MA, USA', 'Microsoft Corporation, Redmond, WA, USA']","['US', 'US']"
https://arxiv.org/abs/2005.03474,Fairness & Bias,Ensuring Fairness under Prior Probability Shifts.,"In this paper, we study the problem of fair classification in the presence of prior probability shifts, where the training set distribution differs from the test set. This phenomenon can be observed in the yearly records of several real-world datasets, such as recidivism records and medical expenditure surveys. If unaccounted for, such shifts can cause the predictions of a classifier to become unfair towards specific population subgroups. While the fairness notion called Proportional Equality (PE) accounts for such shifts, a procedure to ensure PE-fairness was unknown. In this work, we propose a method, called CAPE, which provides a comprehensive solution to the aforementioned problem. CAPE makes novel use of prevalence estimation techniques, sampling and an ensemble of classifiers to ensure fair predictions under prior probability shifts. We introduce a metric, called prevalence difference (PD), which CAPE attempts to minimize in order to ensure PE-fairness. We theoretically establish that this metric exhibits several desirable properties. We evaluate the efficacy of CAPE via a thorough empirical evaluation on synthetic datasets. We also compare the performance of CAPE with several popular fair classifiers on real-world datasets like COMPAS (criminal risk assessment) and MEPS (medical expenditure panel survey). The results indicate that CAPE ensures PE-fair predictions, while performing well on other performance metrics.",[],[],"['Arpita Biswas', 'Suvam Mukherjee']","['Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2105.01774,Fairness & Bias,Envisioning Communities: A Participatory Approach towards AI for Social Good.,"Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be ""for"" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.",[],[],"['Elizabeth Bondi', 'Lily Xu', 'Diana Acosta-Navas', 'Jackson A. Killian']","['Florida International University, Miami, FL, USA', 'Tulane University, New Orleans, LA, USA', 'Case Western Reserve University, Cleveland, OH, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2010.04053,Fairness & Bias,Fairness and Machine Fairness.,"As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.",[],[],"['Simon Caton', 'Christian Haas']","['University of Padua, Padua, Italy', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'University of Padua, Padua, Italy', 'University of Padua, Padua, Italy', 'University of Udine, Udine, Italy', 'University of Padua, Padua, Italy', 'University of Padua, Padua, Italy']","['Italy', 'US', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy']"
https://arxiv.org/abs/2105.10174,Fairness & Bias,Algorithmic Audit of Italian Car Insurance: Evidence of Unfairness in Access and Pricing.,"We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 euros more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.",[],[],"['Alessandro Fabris', 'Alan Mishler', 'Stefano Gottardi', 'Mattia Carletti', 'Matteo Daicampi', 'Gian Antonio Susto', 'Gianmaria Silvello']",[],[]
https://arxiv.org/abs/1912.06883,Fairness & Bias,Whatâs Fair about Individual Fairness?,"A distinction has been drawn in fair machine learning research between `group' and `individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on theoretical discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artifact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of `unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.",[],[],['Reuben Binns'],"['University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2102.03977,Fairness & Bias,Learning to Generate Fair Clusters from Demonstrations.,"Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement, leading to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.",[],[],"['Sainyam Galhotra', 'Sandhya Saisubramanian', 'Shlomo Zilberstein']","['University at Buffalo, Buffalo, NY, USA', 'University at Buffalo, Buffalo, NY, USA', 'University at Buffalo, Buffalo, NY, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2105.04452,Fairness & Bias,"Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation.","Algorithmic fairness research has traditionally been linked to the disciplines of philosophy, ethics, and economics, where notions of fairness are prescriptive and seek objectivity. Increasingly, however, scholars are turning to the study of what different people perceive to be fair, and how these perceptions can or should help to shape the design of machine learning, particularly in the policy realm. The present work experimentally explores five novel research questions at the intersection of the ""Who,"" ""What,"" and ""How"" of fairness perceptions. Specifically, we present the results of a multi-factor conjoint analysis study that quantifies the effects of the specific context in which a question is asked, the framing of the given question, and who is answering it. Our results broadly suggest that the ""Who"" and ""What,"" at least, matter in ways that are 1) not easily explained by any one theoretical perspective, 2) have critical implications for how perceptions of fairness should be measured and/or integrated into algorithmic decision-making systems.",[],[],"['Jacqueline Hannan', 'Huei-Yen Winnie Chen', 'Kenneth Joseph']","['University of Maryland, Baltimore County, Baltimore, MD, USA', 'University of Maryland, Baltimore County, Baltimore, MD, USA', 'University of Maryland, Baltimore County, Baltimore, MD, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2202.00787,Fairness & Bias,Can We Obtain Fairness for Free?,"With the fast development of algorithmic governance, fairness has become a compulsory property for machine learning models to suppress unintentional discrimination. In this paper, we focus on the pre-processing aspect for achieving fairness, and propose a data reweighing approach that only adjusts the weight for samples in the training phase. Different from most previous reweighing methods which usually assign a uniform weight for each (sub)group, we granularly model the influence of each training sample with regard to fairness-related quantity and predictive utility, and compute individual weights based on influence under the constraints from both fairness and utility. Experimental results reveal that previous methods achieve fairness at a non-negligible cost of utility, while as a significant advantage, our approach can empirically release the tradeoff and obtain cost-free fairness for equal opportunity. We demonstrate the cost-free fairness through vanilla classifiers and standard training processes, compared to baseline methods on multiple real-world tabular datasets. Code available at this https URL.",[],[],"['Peizhao Li', 'Hongfu Liu']","['University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA']","['US', 'US']"
https://arxiv.org/abs/2105.06604,Fairness & Bias,Towards Equity and Algorithmic Fairness in Student Grade Prediction.,"Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.",[],[],"['Weijie Jiang', 'Zachary A. Pardos']","['University of Toronto & Australian National University, Canberra, Australia', 'Australian National University, Canberra, Australia']","['Australia', 'Australia']"
https://arxiv.org/abs/2206.00945,Fairness & Bias,The Ethical Gravity Thesis: Marrian Levels and the Persistence of Bias in Automated Decision-Making Systems.,"Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by -- or substantively rooted in -- ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices -- as pioneered in the contemporary philosophical literature by Iris Marion Young -- to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness.",[],[],['Atoosa Kasirzadeh'],[],[]
https://arxiv.org/abs/2303.15889,Fairness & Bias,Age Bias in Emotion Detection: Analysis of Facial Emotion Recognition Performance on Varying Age Groups.,"Demographic biases in source datasets have been shown as one of the causes of unfairness and discrimination in the predictions of Machine Learning models. One of the most prominent types of demographic bias are statistical imbalances in the representation of demographic groups in the datasets. In this paper, we study the measurement of these biases by reviewing the existing metrics, including those that can be borrowed from other disciplines. We develop a taxonomy for the classification of these metrics, providing a practical guide for the selection of appropriate metrics. To illustrate the utility of our framework, and to further understand the practical characteristics of the metrics, we conduct a case study of 20 datasets used in Facial Emotion Recognition (FER), analyzing the biases present in them. Our experimental results show that many metrics are redundant and that a reduced subset of metrics may be sufficient to measure the amount of demographic bias. The paper provides valuable insights for researchers in AI and related fields to mitigate dataset bias and improve the fairness and accuracy of AI models. The code is available at this https URL.",[],[],"['Iris Dominguez-Catena', 'Daniel Paternain', 'Mikel Galar']","['Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'University of Southern California, Los Angeles, CA, USA', 'Microsoft AI for Good, Redmond, WA, USA', 'Microsoft AI for Good, Redmond, WA, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2104.11757,Fairness & Bias,Becoming Good at AI for Good.,"AI for good (AI4G) projects involve developing and applying artificial intelligence (AI) based solutions to further goals in areas such as sustainability, health, humanitarian aid, and social justice. Developing and deploying such solutions must be done in collaboration with partners who are experts in the domain in question and who already have experience in making progress towards such goals. Based on our experiences, we detail the different aspects of this type of collaboration broken down into four high-level categories: communication, data, modeling, and impact, and distill eleven takeaways to guide such projects in the future. We briefly describe two case studies to illustrate how some of these takeaways were applied in practice during our past collaborations.",[],[],"['Meghana Kshirsagar', 'Caleb Robinson', 'Siyu Yang', 'Shahrzad Gholami', 'Ivan Klyuzhin', 'Sumit Mukherjee', 'Md Nasir', 'Anthony Ortiz', 'Felipe Oviedo', 'Darren Tanner', 'Anusua Trivedi', 'Yixi Xu', 'Ming Zhong', 'Bistra Dilkina', 'Rahul Dodhia', 'Juan M. Lavista Ferres']",[],[]
https://arxiv.org/abs/2109.09672,Fairness & Bias,Risk Identification Questionnaire for Unintended Bias in Machine Learning Development Lifecycle.,"The widespread use of artificial intelligence (AI) in many domains has revealed numerous ethical issues from data and design to deployment. In response, countless broad principles and guidelines for ethical AI have been published, and following those, specific approaches have been proposed for how to encourage ethical outcomes of AI. Meanwhile, library and information services too are seeing an increase in the use of AI-powered and machine learning-powered information systems, but no practical guidance currently exists for libraries to plan for, evaluate, or audit the ethics of intended or deployed AI. We therefore report on several promising approaches for promoting ethical AI that can be adapted from other contexts to AI-powered information services and in different stages of the software lifecycle.",[],[],"['Helen Bubinger', 'Jesse David Dinneen']","['Northeastern University, Boston, MA, USA', 'Northeastern University, Boston, MA, USA', 'Northeastern University, Boston, MA, USA', 'Northeastern University, Boston, MA, USA', 'Amherst College, Amherst, MA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2104.03909,Fairness & Bias,RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity.,"We present RAWLSNET, a system for altering Bayesian Network (BN) models to satisfy the Rawlsian principle of fair equality of opportunity (FEO). RAWLSNET's BN models generate aspirational data distributions: data generated to reflect an ideally fair, FEO-satisfying society. FEO states that everyone with the same talent and willingness to use it should have the same chance of achieving advantageous social positions (e.g., employment), regardless of their background circumstances (e.g., socioeconomic status). Satisfying FEO requires alterations to social structures such as school assignments. Our paper describes RAWLSNET, a method which takes as input a BN representation of an FEO application and alters the BN's parameters so as to satisfy FEO when possible, and minimize deviation from FEO otherwise. We also offer guidance for applying RAWLSNET, including on recognizing proper applications of FEO. We demonstrate the use of our system with publicly available data sets. RAWLSNET's altered BNs offer the novel capability of generating aspirational data for FEO-relevant tasks. Aspirational data are free from the biases of real-world data, and thus are useful for recognizing and detecting sources of unfairness in machine learning algorithms besides biased data.",[],[],"['David Liu', 'Zohair Shafi', 'William Fleisher', 'Tina Eliassi-Rad', 'Scott Alfeld']",[],[]
https://arxiv.org/abs/2210.10456,Fairness & Bias,Fair Equality of Chances: Fairness for Statistical Prediction-Based Decision-Making.,"Ensuring fairness of prediction-based decision making is based on statistical group fairness criteria. Which one of these criteria is the morally most appropriate one depends on the context, and its choice requires an ethical analysis. In this paper, we present a step-by-step procedure integrating three elements: (a) a framework for the moral assessment of what fairness means in a given context, based on the recently proposed general principle of ""Fair equality of chances"" (FEC) (b) a mapping of the assessment's results to established statistical group fairness criteria, and (c) a method for integrating the thus-defined fairness into optimal decision making. As a second contribution, we show new applications of the FEC principle and show that, with this extension, the FEC framework covers all types of group fairness criteria: independence, separation, and sufficiency. Third, we introduce an extended version of the FEC principle, which additionally allows accounting for morally irrelevant elements of the fairness assessment and links to well-known relaxations of the fairness criteria. This paper presents a framework to develop fair decision systems in a conceptually sound way, combining the moral and the computational elements of fair prediction-based decision-making in an integrated approach. Data and code to reproduce our results are available at this https URL.",[],[],"['Joachim Baumann', 'Christoph Heitz']","['Vrije Universiteit Brussel, Brussels, Belgium', 'Vrije Universiteit Brussel, Brussels, Belgium', 'Vrije Universiteit Brussel, Brussels, Belgium']","['Belgium', 'Belgium', 'Belgium']"
https://arxiv.org/abs/1904.09942,Fairness & Bias,How Do the Score Distributions of Subpopulations Influence Fairness Notions?,"As algorithmic prediction systems have become widespread, fears that these systems may inadvertently discriminate against members of underrepresented populations have grown. With the goal of understanding fundamental principles that underpin the growing number of approaches to mitigating algorithmic discrimination, we investigate the role of information in fair prediction. A common strategy for decision-making uses a predictor to assign individuals a risk score; then, individuals are selected or rejected on the basis of this score. In this work, we study a formal framework for measuring the information content of predictors. Central to this framework is the notion of a refinement, first studied by Degroot and Fienberg. Intuitively, a refinement of a predictor $z$ increases the overall informativeness of the predictions without losing the information already contained in $z$. We show that increasing information content through refinements improves the downstream selection rules across a wide range of fairness measures (e.g. true positive rates, false positive rates, selection rates). In turn, refinements provide a simple but effective tool for reducing disparity in treatment and impact without sacrificing the utility of the predictions. Our results suggest that in many applications, the perceived ""cost of fairness"" results from an information disparity across populations, and thus, may be avoided with improved information.",[],[],"['Sumegha Garg', 'Michael P. Kim', 'Omer Reingold']","['New York University, New York, NY, USA', 'New York University, New York, NY, USA']","['US', 'US']"
https://arxiv.org/abs/2010.07343,Fairness & Bias,Causal Multi-Level Fairness.,"Algorithmic systems are known to impact marginalized groups severely, and more so, if all sources of bias are not considered. While work in algorithmic fairness to-date has primarily focused on addressing discrimination due to individually linked attributes, social science research elucidates how some properties we link to individuals can be conceptualized as having causes at macro (e.g. structural) levels, and it may be important to be fair to attributes at multiple levels. For example, instead of simply considering race as a causal, protected attribute of an individual, the cause may be distilled as perceived racial discrimination an individual experiences, which in turn can be affected by neighborhood-level factors. This multi-level conceptualization is relevant to questions of fairness, as it may not only be important to take into account if the individual belonged to another demographic group, but also if the individual received advantaged treatment at the macro-level. In this paper, we formalize the problem of multi-level fairness using tools from causal inference in a manner that allows one to assess and account for effects of sensitive attributes at multiple levels. We show importance of the problem by illustrating residual unfairness if macro-level sensitive attributes are not accounted for, or included without accounting for their multi-level nature. Further, in the context of a real-world task of predicting income based on macro and individual-level attributes, we demonstrate an approach for mitigating unfairness, a result of multi-level sensitive attributes.",[],[],"['Vishwali Mhasawade', 'Rumi Chunara']","['Northwestern University, Evanston, IL, USA', 'Northwestern University, Evanston, IL, USA', 'Northwestern University, Evanston, IL, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2105.04760,Fairness & Bias,Unpacking the Expressed Consequences of AI Research in Broader Impact Statements.,"The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals.",[],[],"['Priyanka Nanayakkara', 'Jessica Hullman', 'Nicholas Diakopoulos']","['University of New South Wales, Sydney, NSW, Australia', 'University of New South Wales, Sydney, NSW, Australia', 'University of New South Wales, Sydney, NSW, Australia', 'Macquarie University, Sydney, NSW, Australia', 'University of New South Wales, Sydney, NSW, Australia']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://arxiv.org/abs/2306.13659,Fairness & Bias,Epistemic Reasoning for Machine Ethics with Situation Calculus.,"Fairness in machine learning is of considerable interest in recent years owing to the propensity of algorithms trained on historical data to amplify and perpetuate historical biases. In this paper, we argue for a formal reconstruction of fairness definitions, not so much to replace existing definitions but to ground their application in an epistemic setting and allow for rich environmental modelling. Consequently we look into three notions: fairness through unawareness, demographic parity and counterfactual fairness, and formalise these in the epistemic situation calculus.",[],[],['Vaishak Belle'],[],[]
https://arxiv.org/abs/2006.04599,Fairness & Bias,Disparate Impact of Artificial Intelligence Bias in Ridehailing Economyâs Price Discrimination Algorithms.,"Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications. The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.",[],[],"['Akshat Pandey', 'Aylin Caliskan']","['University of Technology, Sydney & Australian National University, Sydney, NSW, Australia']",['Australia']
https://arxiv.org/abs/2102.00753,Fairness & Bias,Quantum Fair Machine Learning.,"In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within $\epsilon$-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.",[],[],['Elija Perrier'],"['Amazon Web Services, Berlin, Germany', 'Amazon Web Services, Berlin, Germany', 'Amazon Web Services, Berlin, Germany', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Amazon Web Services, Palo Alto, CA, USA', 'Amazon Web Services, Berlin, Germany']","['Germany', 'Germany', 'Germany', 'US', 'US', 'Germany']"
https://arxiv.org/abs/2006.05109,Fairness & Bias,Fair Bayesian Optimization.,"Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.",[],[],"['Valerio Perrone', 'Michele Donini', 'Muhammad Bilal Zafar', 'Robin Schmucker', 'Krishnaram Kenthapadi', 'Cédric Archambeau']","['Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, San Bruno, CA, USA', 'Google, Mountain View, CA, USA', 'Google, New York, NY, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, New York, NY, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2105.09985,Fairness & Bias,Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective.,"In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts $\gamma$ and $\epsilon$. The first part $\gamma$ depends solely on the performance of the proxy such as precision and recall, whereas the second part $\epsilon$ captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by $\gamma$ via a linear dependence, whereas the dependence on the correlations $\epsilon$ only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.",[],[],"['Flavien Prost', 'Pranjal Awasthi', 'Nick Blumm', 'Aditee Kumthekar', 'Trevor Potter', 'Li Wei', 'Xuezhi Wang', 'Ed H. Chi', 'Jilin Chen', 'Alex Beutel']","['University of Texas at Austin, Austin, TX, USA', 'University of Texas at Austin, Austin, TX, USA', 'University of Texas at Austin, Austin, TX, USA', 'University of Texas at Austin, Austin, TX, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2010.06113,Fairness & Bias,FaiR-N: Fair and Robust Neural Networks for Structured Data.,"Fairness in machine learning is crucial when individuals are subject to automated decisions made by models in high-stake domains. Organizations that employ these models may also need to satisfy regulations that promote responsible and ethical A.I. While fairness metrics relying on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias, fairness in terms of the equalized ability to achieve recourse for different protected attribute groups has been relatively unexplored. We present a novel formulation for training neural networks that considers the distance of data points to the decision boundary such that the new objective: (1) reduces the average distance to the decision boundary between two groups for individuals subject to a negative outcome in each group, i.e. the network is more fair with respect to the ability to obtain recourse, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that training with this loss yields more fair and robust neural networks with similar accuracies to models trained without it. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse capabilities across groups are considered to train fairer neural networks, and a relation between error rates based fairness and recourse based fairness is investigated.",[],[],"['Shubham Sharma', 'Alan H. Gee', 'David Paydarfar', 'Joydeep Ghosh']","['Technische Universität Berlin, Berlin, Germany', 'Harvard University, Boston, MA, USA', 'Harvard University, Boston, MA, USA']","['Germany', 'US', 'US']"
https://arxiv.org/abs/2012.00423,Fairness & Bias,Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring.,"Ranking algorithms are being widely employed in various online hiring platforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has demonstrated that ranking algorithms employed by these platforms are prone to a variety of undesirable biases, leading to the proposal of fair ranking algorithms (e.g., Det-Greedy) which increase exposure of underrepresented candidates. However, there is little to no work that explores whether fair ranking algorithms actually improve real world outcomes (e.g., hiring decisions) for underrepresented groups. Furthermore, there is no clear understanding as to how other factors (e.g., job context, inherent biases of the employers) may impact the efficacy of fair ranking in practice. In this work, we analyze various sources of gender biases in online hiring platforms, including the job context and inherent biases of employers and establish how these factors interact with ranking algorithms to affect hiring decisions. To the best of our knowledge, this work makes the first attempt at studying the interplay between the aforementioned factors in the context of online hiring. We carry out a largescale user study simulating online hiring scenarios with data from TaskRabbit, a popular online freelancing site. Our results demonstrate that while fair ranking algorithms generally improve the selection rates of underrepresented minorities, their effectiveness relies heavily on the job contexts and candidate profiles.",[],[],"['Tom Sühr', 'Sophie Hilgard', 'Himabindu Lakkaraju']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2302.09157,Privacy & Data Governance,Blind Justice: Algorithmically Masking Race in Charging Decisions.,"Predictive algorithms are now used to help distribute a large share of our society's resources and sanctions, such as healthcare, loans, criminal detentions, and tax audits. Under the right circumstances, these algorithms can improve the efficiency and equity of decision-making. At the same time, there is a danger that the algorithms themselves could entrench and exacerbate disparities, particularly along racial, ethnic, and gender lines. To help ensure their fairness, many researchers suggest that algorithms be subject to at least one of three constraints: (1) no use of legally protected features, such as race, ethnicity, and gender; (2) equal rates of ""positive"" decisions across groups; and (3) equal error rates across groups. Here we show that these constraints, while intuitively appealing, often worsen outcomes for individuals in marginalized groups, and can even leave all groups worse off. The inherent trade-off we identify between formal fairness constraints and welfare improvements -- particularly for the marginalized -- highlights the need for a more robust discussion on what it means for an algorithm to be ""fair"". We illustrate these ideas with examples from healthcare and the criminal-legal system, and make several proposals to help practitioners design more equitable algorithms.",[],[],"['Alex Chohlas-Wood', 'Madison Coots', 'Sharad Goel', 'Julian Nyarko']","['Australian National University, Canberra, Australia']",['Australia']
https://arxiv.org/abs/2203.15370,Privacy & Data Governance,Moral Disagreement and Artificial Intelligence.,"An assurance case is a structured argument, typically produced by safety engineers, to communicate confidence that a critical or complex system, such as an aircraft, will be acceptably safe within its intended context. Assurance cases often inform third party approval of a system. One emerging proposition within the trustworthy AI and autonomous systems (AI/AS) research community is to use assurance cases to instil justified confidence that specific AI/AS will be ethically acceptable when operational in well-defined contexts. This paper substantially develops the proposition and makes it concrete. It brings together the assurance case methodology with a set of ethical principles to structure a principles-based ethics assurance argument pattern. The principles are justice, beneficence, non-maleficence, and respect for human autonomy, with the principle of transparency playing a supporting role. The argument pattern, shortened to the acronym PRAISE, is described. The objective of the proposed PRAISE argument pattern is to provide a reusable template for individual ethics assurance cases, by which engineers, developers, operators, or regulators could justify, communicate, or challenge a claim about the overall ethical acceptability of the use of a specific AI/AS in a given socio-technical context. We apply the pattern to the hypothetical use case of an autonomous robo-taxi service in a city centre.",[],[],"['Zoe Porter', 'Ibrahim Habli', 'John McDermid', 'Marten Kaas']",[],[]
https://arxiv.org/abs/2004.08602,Privacy & Data Governance,Watching the Watchers: Estimating the Prevalence of Surveillance Cameras across the United States with Street View Data.,"The prevalence of Closed Circuit Television (CCTV) in today's society has given rise to an inherent asymmetry of control between the watchers and the watched. A sense of unease relating to the unobservable observer (operator) often leads to a lack of trust in the camera and its purpose, despite security cameras generally being present as a protective device. In this paper, we detail our concept of Open Circuit Television and prototype CryptoCam, a novel system for secure sharing of video footage to individuals and potential subjects nearby. Utilizing point-of-capture encryption and wireless transfer of time-based access keys for footage, we have developed a system to encourage a more open approach to information sharing and consumption. Detailing concerns highlighted in existing literature we formalize our over-arching concept into a framework called Open Circuit Television (OCTV). Through CryptoCam we hope to address this asymmetry of control by providing subjects with data equity, discoverability and oversight.",[],[],"['Gerard Wilkinson', 'Dan Jackson', 'Andrew Garbett', 'Reuben Kirkham', 'Kyle Montague']","['National University of Singapore, Singapore, Singapore', 'National University of Singapore, Singapore, Singapore', 'University of Massachusetts, Amherst, Amherst, MA, USA']","['Singapore', 'Singapore', 'US']"
https://arxiv.org/abs/1907.00164,Privacy & Data Governance,On the Privacy Risks of Model Explanations.,"Privacy and transparency are two key foundations of trustworthy machine learning. Model explanations offer insights into a model's decisions on input data, whereas privacy is primarily concerned with protecting information about the training data. We analyze connections between model explanations and the leakage of sensitive information about the model's training set. We investigate the privacy risks of feature-based model explanations using membership inference attacks: quantifying how much model predictions plus their explanations leak information about the presence of a datapoint in the training set of a model. We extensively evaluate membership inference attacks based on feature-based model explanations, over a variety of datasets. We show that backpropagation-based explanations can leak a significant amount of information about individual training datapoints. This is because they reveal statistical information about the decision boundaries of the model about an input, which can reveal its membership. We also empirically investigate the trade-off between privacy and explanation quality, by studying the perturbation-based model explanations.",[],[],"['Reza Shokri', 'Martin Strobel', 'Yair Zick']","['DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind & University College London, London, United Kingdom', 'DeepMind, London, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2102.04257,Privacy & Data Governance,Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities.,"Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness--frequently, race and legal gender--can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.",[],[],"['Nenad Tomasev', 'Kevin R. McKee', 'Jackie Kay', 'Shakir Mohamed']","['Clemson University, Clemson, SC, USA', 'Clemson University, Clemson, SC, USA', 'Clemson University, Clemson , SC, USA', 'Clemson University, Clemson , SC, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2302.05284,Privacy & Data Governance,Modeling and Guiding the Creation of Ethical Human-AI Teams.,"As Artificial Intelligence (AI) continues to advance rapidly, it becomes increasingly important to consider AI's ethical and societal implications. In this paper, we present a bottom-up mapping of the current state of research at the intersection of Human-Centered AI, Ethical, and Responsible AI (HCER-AI) by thematically reviewing and analyzing 164 research papers from leading conferences in ethical, social, and human factors of AI: AIES, CHI, CSCW, and FAccT. The ongoing research in HCER-AI places emphasis on governance, fairness, and explainability. These conferences, however, concentrate on specific themes rather than encompassing all aspects. While AIES has fewer papers on HCER-AI, it emphasizes governance and rarely publishes papers about privacy, security, and human flourishing. FAccT publishes more on governance and lacks papers on privacy, security, and human flourishing. CHI and CSCW, as more established conferences, have a broader research portfolio. We find that the current emphasis on governance and fairness in AI research may not adequately address the potential unforeseen and unknown implications of AI. Therefore, we recommend that future research should expand its scope and diversify resources to prepare for these potential consequences. This could involve exploring additional areas such as privacy, security, human flourishing, and explainability.",[],[],"['Mohammad Tahaei', 'Marios Constantinides', 'Daniele Quercia', 'Michael Muller']","['Drexel University, Philadelphia, PA, USA', 'Drexel University, Philadelphia, PA, USA']","['US', 'US']"
https://arxiv.org/abs/2311.11415,Privacy & Data Governance,The Earth Is Flat and the Sun Is Not a Star: The Susceptibility of GPT-2 to Universal Adversarial Triggers.,"As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by focusing on the security risks posed by LLMs, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize the attacks by target and attack type within a prompt-based interaction scheme. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness.",[],[],"['Erik Derner', 'Kristina Batistič', 'Jan Zahálka', 'Robert Babuška']","['University of California, Berkeley, Berkeley, CA, USA', 'University of California, Berkeley, Berkeley, CA, USA']","['US', 'US']"
https://arxiv.org/abs/2105.06604,Privacy & Data Governance,Towards Equity and Algorithmic Fairness in Student Grade Prediction.,"Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.",[],[],"['Weijie Jiang', 'Zachary A. Pardos']","['The University of Texas at Austin, Austin, TX, USA', 'The University of Texas at Austin, Austin, TX, USA', 'The University of Texas at Austin, Austin, TX, USA', 'The University of Texas at Austin, Austin, TX, USA', 'University of California, San Diego, San Diego, CA, USA', 'University of California, San Diego, San Diego, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2401.13643,Privacy & Data Governance,Participatory Algorithmic Management: Elicitation Methods for Worker Well-Being Models.,"My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences.",[],[],['Christine P Lee'],"['Australian National University & University of Toronto, Canberra, ACT, Australia', 'Australian National University, Canberra, ACT, Australia']","['Australia', 'Australia']"
https://arxiv.org/abs/2109.06309,Security,Fairness and Data Protection Impact Assessments.,"In this paper, we critically examine the effectiveness of the requirement to conduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General Data Protection Regulation (GDPR) in light of fairness metrics. Through this analysis, we explore the role of the fairness principle as introduced in Article 5(1)(a) and its multifaceted interpretation in the obligation to conduct a DPIA. Our paper argues that although there is a significant theoretical role for the considerations of fairness in the DPIA process, an analysis of the various guidance documents issued by data protection authorities on the obligation to conduct a DPIA reveals that they rarely mention the fairness principle in practice.",[],[],"['Atoosa Kasirzadeh', 'Damian Clifford']","['Tel-Aviv University, Tel-Aviv, Israel', 'The Hebrew University of Jerusalem, Jerusalem, Israel', 'Bar-Ilan University, Ramat Gan, Israel', 'Aarhus University, Aarhus, Denmark', 'IISc Bangalore, Bangalore, Denmark', 'Bar-Ilan University, Ramat Gan, Israel']","['Israel', 'Israel', 'Israel', 'Denmark', 'Denmark', 'Israel']"
https://arxiv.org/abs/2311.11415,Security,The Earth Is Flat and the Sun Is Not a Star: The Susceptibility of GPT-2 to Universal Adversarial Triggers.,"As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by focusing on the security risks posed by LLMs, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize the attacks by target and attack type within a prompt-based interaction scheme. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness.",[],[],"['Erik Derner', 'Kristina Batistič', 'Jan Zahálka', 'Robert Babuška']","['Stanford University, Stanford, CA, USA', 'Columbia University, New York, NY, USA']","['US', 'US']"
https://arxiv.org/abs/2009.01534,Security,Fairness in the Eyes of the Data: Certifying Machine-Learning Models.,"We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.",[],[],"['Shahar Segal', 'Yossi Adi', 'Benny Pinkas', 'Carsten Baum', 'Chaya Ganesh', 'Joseph Keshet']",[],[]
https://arxiv.org/abs/2103.14068,Security,Differentially Private Normalizing Flows for Privacy-Preserving Density Estimation.,"Normalizing flow models have risen as a popular solution to the problem of density estimation, enabling high-quality synthetic data generation as well as exact probability density evaluation. However, in contexts where individuals are directly associated with the training data, releasing such a model raises privacy concerns. In this work, we propose the use of normalizing flow models that provide explicit differential privacy guarantees as a novel approach to the problem of privacy-preserving density estimation. We evaluate the efficacy of our approach empirically using benchmark datasets, and we demonstrate that our method substantially outperforms previous state-of-the-art approaches. We additionally show how our algorithm can be applied to the task of differentially private anomaly detection.",[],[],"['Chris Waites', 'Rachel Cummings']",[],[]