link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/2309.13965,Transparency & Explainability, Understanding Decision Subjects’ Fairness Perceptions and Retention in repeated Interactions with AI-Based Decision Systems,"Research in explainable AI (XAI) aims to provide insights into the decision-making process of opaque AI models. To date, most XAI methods offer one-off and static explanations, which cannot cater to the diverse backgrounds and understanding levels of users. With this paper, we investigate if free-form conversations can enhance users' comprehension of static explanations, improve acceptance and trust in the explanation methods, and facilitate human-AI collaboration. Participants are presented with static explanations, followed by a conversation with a human expert regarding the explanations. We measure the effect of the conversation on participants' ability to choose, from three machine learning models, the most accurate one based on explanations and their self-reported comprehension, acceptance, and trust. Empirical results show that conversations significantly improve comprehension, acceptance, trust, and collaboration. Our findings highlight the importance of customized model explanations in the format of free-form conversations and provide insights for the future design of conversational explanations.",[],[],"['Tong Zhang', 'X. Jessie Yang', 'Boyang Li']",[],[]
https://arxiv.org/abs/2304.08861,Transparency & Explainability, How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review,"While the emerging research field of explainable artificial intelligence (XAI) claims to address the lack of explainability in high-performance machine learning models, in practice, XAI targets developers rather than actual end-users. Unsurprisingly, end-users are often unwilling to use XAI-based decision support systems. Similarly, there is limited interdisciplinary research on end-users' behavior during XAI explanations usage, rendering it unknown how explanations may impact cognitive load and further affect end-user performance. Therefore, we conducted an empirical study with 271 prospective physicians, measuring their cognitive load, task performance, and task time for distinct implementation-independent XAI explanation types using a COVID-19 use case. We found that these explanation types strongly influence end-users' cognitive load, task performance, and task time. Further, we contextualized a mental efficiency metric, ranking local XAI explanation types best, to provide recommendations for future applications and implications for sociotechnical XAI research.",[],[],['Lukas-Valentin Herm'],"['Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France', 'Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France', 'Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France', 'Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France']","['France', 'France', 'France', 'France']"
https://arxiv.org/abs/2306.07023,Transparency & Explainability, FINS Auditing Framework: Group Fairness for Subset Selections,"Team assembly is a problem that demands trade-offs between multiple fairness criteria and computational optimization. We focus on four criteria: (i) fair distribution of workloads within the team, (ii) fair distribution of skills and expertise regarding project requirements, (iii) fair distribution of protected classes in the team, and (iv) fair distribution of the team cost among protected classes. For this problem, we propose a two-stage algorithmic solution. First, a multi-objective optimization procedure is executed and the Pareto candidates that satisfy the project requirements are selected. Second, N random groups are formed containing combinations of these candidates, and a second round of multi-objective optimization is executed, but this time for selecting the groups that optimize the team-assembly criteria.",[],[],"['Rodrigo Borges', 'Otto Sahlgrens', 'Sami Koivunen', 'Kostas Stefanidis', 'Thomas Olsson', 'Arto Laitinen']","['Worcester Polytechnic Institute, Worcester, MA, USA', 'Worcester Polytechnic Institute, Worcester, MA, USA']","['US', 'US']"
https://arxiv.org/abs/2103.12983,Transparency & Explainability, Counterfactual explanations for prediction and diagnosis in XAI,"Motivation: Many high-performance DTA models have been proposed, but they are mostly black-box and thus lack human interpretability. Explainable AI (XAI) can make DTA models more trustworthy, and can also enable scientists to distill biological knowledge from the models. Counterfactual explanation is one popular approach to explaining the behaviour of a deep neural network, which works by systematically answering the question ""How would the model output change if the inputs were changed in this way?"". Most counterfactual explanation methods only operate on single input data. It remains an open problem how to extend counterfactual-based XAI methods to DTA models, which have two inputs, one for drug and one for target, that also happen to be discrete in nature. Methods: We propose a multi-agent reinforcement learning framework, Multi-Agent Counterfactual Drug target binding Affinity (MACDA), to generate counterfactual explanations for the drug-protein complex. Our proposed framework provides human-interpretable counterfactual instances while optimizing both the input drug and target for counterfactual generation at the same time. Results: We benchmark the proposed MACDA framework using the Davis dataset and find that our framework produces more parsimonious explanations with no loss in explanation validity, as measured by encoding similarity and QED. We then present a case study involving ABL1 and Nilotinib to demonstrate how MACDA can explain the behaviour of a DTA model in the underlying substructure interaction between inputs in its prediction, revealing mechanisms that align with prior domain knowledge.",[],[],"['Tri Minh Nguyen', 'Thomas P Quinn', 'Thin Nguyen', 'Truyen Tran']","['Trinity College Dublin, University of Dublin, Dublin, Ireland', 'University College Dublin, Dublin, Ireland', 'Teagasc Moorepark, Cork, Ireland', 'Teagasc Moorepark, Cork, Ireland', 'Trinity College Dublin, University of Dublin, Dublin, Ireland']","['Ireland', 'Ireland', 'Ireland', 'Ireland', 'Ireland']"
https://arxiv.org/abs/2308.16377,Transparency & Explainability, Causal Framework of Artificial Autonomous Agent Responsibility,"Artificial Intelligence (AI) has a communication problem. XAI methods have been used to make AI more understandable and helped resolve some of the transparency issues that inhibit AI's broader usability. However, user evaluation studies reveal that the often numerical explanations provided by XAI methods have not always been effective for many types of users of AI systems. This article aims to adapt the major communications models from Science Communications into a framework for practitioners to understand, influence, and integrate the context of audiences both for their communications supporting AI literacy in the public and in designing XAI systems that are more adaptive to different users.",[],[],"['Simon Hudson', 'Matija Franklin']","['University College London, London, United Kingdom', 'University College London, London, United Kingdom', 'University of Exeter, Exeter, United Kingdom', 'University College London, London, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2205.00033,Transparency & Explainability, Data-Centric Factors in Algorithmic Fairness,"Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. This is not only problematic from a legal perspective, but non-transparent systems are also prone to yield unfair outcomes because their sanity is challenging to assess and calibrate in the first place -- which is particularly worrisome for human decision-subjects. Based on this observation and building upon existing work, I aim to make the following three main contributions through my doctoral thesis: (a) understand how (potential) decision-subjects perceive algorithmic decisions (with varying degrees of transparency of the underlying ADS), as compared to similar decisions made by humans; (b) evaluate different tools for transparent decision-making with respect to their effectiveness in enabling people to appropriately assess the quality and fairness of ADS; and (c) develop human-understandable technical artifacts for fair automated decision-making. Over the course of the first half of my PhD program, I have already addressed substantial pieces of (a) and (c), whereas (b) will be the major focus of the second half.",[],[],['Jakob Schoeffer'],"['ETH Zurich, Zurich, Switzerland', 'University of Oxford, Oxford, United Kingdom', 'ETH Zurich, Zurich, Switzerland']","['Switzerland', 'United Kingdom', 'Switzerland']"
https://arxiv.org/abs/2203.02013,Transparency & Explainability, DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations,"The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment. Code for our experiments can be found at this https URL.",[],[],"['Yiwei Lyu', 'Paul Pu Liang', 'Zihao Deng', 'Ruslan Salakhutdinov', 'Louis-Philippe Morency']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2302.07185,Transparency & Explainability, Investigating Debiasing Effects on Classification and Explainability,"Most works on the fairness of machine learning systems focus on the blind optimization of common fairness metrics, such as Demographic Parity and Equalized Odds. In this paper, we conduct a comparative study of several bias mitigation approaches to investigate their behaviors at a fine grain, the prediction level. Our objective is to characterize the differences between fair models obtained with different approaches. With comparable performances in fairness and accuracy, are the different bias mitigation approaches impacting a similar number of individuals? Do they mitigate bias in a similar way? Do they affect the same individuals when debiasing a model? Our findings show that bias mitigation approaches differ a lot in their strategies, both in the number of impacted individuals and the populations targeted. More surprisingly, we show these results even apply for several runs of the same mitigation approach. These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.",[],[],"['Natasa Krco', 'Thibault Laugel', 'Jean-Michel Loubes', 'Marcin Detyniecki']","['University of Pisa, Pisa, Italy', 'University of Pisa, Pisa, Italy']","['Italy', 'Italy']"
https://arxiv.org/abs/2306.00380,Transparency & Explainability, Managing Sustainability Tensions in Artificial Intelligence: Insights from Paradox Theory,"When making strategic decisions, we are often confronted with overwhelming information to process. The situation can be further complicated when some pieces of evidence are contradicted each other or paradoxical. The challenge then becomes how to determine which information is useful and which ones should be eliminated. This process is known as meta-decision. Likewise, when it comes to using Artificial Intelligence (AI) systems for strategic decision-making, placing trust in the AI itself becomes a meta-decision, given that many AI systems are viewed as opaque ""black boxes"" that process large amounts of data. Trusting an opaque system involves deciding on the level of Trustworthy AI (TAI). We propose a new approach to address this issue by introducing a novel taxonomy or framework of TAI, which encompasses three crucial domains: articulate, authentic, and basic for different levels of trust. To underpin these domains, we create ten dimensions to measure trust: explainability/transparency, fairness/diversity, generalizability, privacy, data governance, safety/robustness, accountability, reproducibility, reliability, and sustainability. We aim to use this taxonomy to conduct a comprehensive survey and explore different TAI approaches from a strategic decision-making perspective.",[],[],"['Caesar Wu', 'Yuan-Fang Lib', 'Pascal Bouvry']","['University of Surrey, Guildford, United Kingdom', 'University of Surrey, Guildford, United Kingdom', 'University of Surrey, Guildford, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2206.04737,Transparency & Explainability, Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance,"Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of interventions that allow for the effective participation of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.",[],[],"['Inioluwa Deborah Raji', 'Peggy Xu', 'Colleen Honigsberg', 'Daniel E. Ho']","['University of California, Berkeley, Berkeley, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2309.13933,Transparency & Explainability," Resume Format, LinkedIn URLs and Other Unexpected Influences on AI Personality Prediction in Hiring: Results of an Audit","Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of, algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.",[],[],"['Alessandro Fabris', 'Nina Baranowska', 'Matthew J. Dennis', 'Philipp Hacker', 'Jorge Saldivar', 'Frederik Zuiderveen Borgesius', 'Asia J. Biega']","['New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2108.06907,Transparency & Explainability, Select Wisely and Explain: Active Learning and Probabilistic Local Post-hoc Explainability,"Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active Learning Based Locally Faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression(GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.",[],[],"['Aditya Saini', 'Ranjitha Prasad']","['Indraprastha Institute of Information Technology, Delhi, New Delhi, India', 'Indraprastha Institute of Information Technology, Delhi, New Delhi, India']","['India', 'India']"
https://arxiv.org/abs/2205.05126,Transparency & Explainability, A Meta-Analysis of the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making,"Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysis to derive implications across existing research. We observe a statistically positive impact of XAI on users' performance. Additionally, the first results indicate that human-AI decision-making tends to yield better task performance on text data. However, we find no effect of explanations on users' performance compared to sole AI predictions. Our initial synthesis gives rise to future research investigating the underlying causes and contributes to further developing algorithms that effectively benefit human decision-makers by providing meaningful explanations.",[],[],"['Max Schemmer', 'Patrick Hemmer', 'Maximilian Nitsche', 'Niklas Kühl', 'Michael Vössing']","['Karlsruhe Institute of Technology, Karlsruhe, Germany', 'Karlsruhe Institute of Technology, Karlsruhe, Germany', 'Karlsruhe Institute of Technology, Karlsruhe, Germany', 'Karlsruhe Institute of Technology, Karlsruhe, Germany', 'Karlsruhe Institute of Technology, Karlsruhe, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/1906.11668,Transparency & Explainability, From Coded Bias to Existential Threat: Mapping Expert Frames for AI Governance,"In the last five years, private companies, research institutions as well as public sector organisations have issued principles and guidelines for ethical AI, yet there is debate about both what constitutes ""ethical AI"" and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analyzed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted; why they are deemed important; what issue, domain or actors they pertain to; and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.",[],[],"['Anna Jobin', 'Marcello Ienca', 'Effy Vayena']",[],[]
https://arxiv.org/abs/2308.07876,Transparency & Explainability, Multi-CoPED: A Multilingual Multi-Task Approach for Coding Political Event Data on Conflict and Mediation Domain,"Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classification. ZSP demonstrates competitive performance compared to supervised BERT models, positioning it as a valuable tool for event record validation and ontology development. Our work underscores the potential of leveraging transfer learning and existing expertise to enhance the efficiency and scalability of research in the field.",[],[],"['Yibo Hu', 'Erick Skorupa Parolin', 'Latifur Khan', 'Patrick T. Brandt', 'Javier Osorio', ""Vito J. D'Orazio""]","['The University of Texas at Dallas, Richardson, TX, USA', 'The University of Texas at Dallas, Richardson, TX, USA', 'The University of Texas at Dallas, Richardson, TX, USA', 'The University of Texas at Dallas, Richardson, TX, USA', 'The University of Texas at Dallas, Richardson, TX, USA', 'The University of Arizona, Tucson, AZ, USA', 'The University of Texas at Dallas, Richardson, TX, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2204.00302,Transparency & Explainability, Actual Causality and Responsibility Attribution in Decentralized Partially Observable Markov Decision Processes,"Actual causality and a closely related concept of responsibility attribution are central to accountable decision making. Actual causality focuses on specific outcomes and aims to identify decisions (actions) that were critical in realizing an outcome of interest. Responsibility attribution is complementary and aims to identify the extent to which decision makers (agents) are responsible for this outcome. In this paper, we study these concepts under a widely used framework for multi-agent sequential decision making under uncertainty: decentralized partially observable Markov decision processes (Dec-POMDPs). Following recent works in RL that show correspondence between POMDPs and Structural Causal Models (SCMs), we first establish a connection between Dec-POMDPs and SCMs. This connection enables us to utilize a language for describing actual causality from prior work and study existing definitions of actual causality in Dec-POMDPs. Given that some of the well-known definitions may lead to counter-intuitive actual causes, we introduce a novel definition that more explicitly accounts for causal dependencies between agents' actions. We then turn to responsibility attribution based on actual causality, where we argue that in ascribing responsibility to an agent it is important to consider both the number of actual causes in which the agent participates, as well as its ability to manipulate its own degree of responsibility. Motivated by these arguments we introduce a family of responsibility attribution methods that extends prior work, while accounting for the aforementioned considerations. Finally, through a simulation-based experiment, we compare different definitions of actual causality and responsibility attribution methods. The empirical results demonstrate the qualitative difference between the considered definitions of actual causality and their impact on attributed responsibility.",[],[],"['Stelios Triantafyllou', 'Adish Singla', 'Goran Radanovic']","['Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany']","['Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/2205.15258,Transparency & Explainability," Transparency, Governance and Regulation of Algorithmic Tools Deployed in the Criminal Justice System: a UK Case Study","We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.",[],[],"['Miri Zilka', 'Holli Sargeant', 'Adrian Weller']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2205.03931,Fairness & Bias, Write It Like You See It: Detectable Differences in Clinical Notes By Race Lead To Differential Model Recommendations,"Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.",[],[],"['Hammaad Adam', 'Ming Ying Yang', 'Kenrick Cato', 'Ioana Baldini', 'Charles Senteio', 'Leo Anthony Celi', 'Jiaming Zeng', 'Moninder Singh', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Columbia University, New York, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Rutgers University, New Brunswick, NJ, USA', 'Massachusetts Institute of Technology & Beth Israel Deaconess Medical Center, Cambridge, MA, USA', 'IBM Research, Cambridge, MA, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Massachusetts Institute of Technology & Vector Institute, Cambridge, MA, USA']","['US', 'US', 'US', 'US', 'US', 'Israel', 'US', 'US', 'US']"
https://arxiv.org/abs/2203.16432,Fairness & Bias, Long-term Dynamics of Fairness Intervention in Connection Recommender Systems,"Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Pólya urn model.",[],[],"['Nil-Jana Akpinar', 'Cyrus DiCiccio', 'Preetam Nandy', 'Kinjal Basu']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Work done at LinkedIn, Sunnyvale, CA, USA', 'LinkedIn, Sunnyvale, CA, USA', 'LinkedIn, Sunnyvale, CA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2304.08861,Fairness & Bias, How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review,"While the emerging research field of explainable artificial intelligence (XAI) claims to address the lack of explainability in high-performance machine learning models, in practice, XAI targets developers rather than actual end-users. Unsurprisingly, end-users are often unwilling to use XAI-based decision support systems. Similarly, there is limited interdisciplinary research on end-users' behavior during XAI explanations usage, rendering it unknown how explanations may impact cognitive load and further affect end-user performance. Therefore, we conducted an empirical study with 271 prospective physicians, measuring their cognitive load, task performance, and task time for distinct implementation-independent XAI explanation types using a COVID-19 use case. We found that these explanation types strongly influence end-users' cognitive load, task performance, and task time. Further, we contextualized a mental efficiency metric, ranking local XAI explanation types best, to provide recommendations for future applications and implications for sociotechnical XAI research.",[],[],['Lukas-Valentin Herm'],"['Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France', 'Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France', 'Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France', 'Télécom Paris, Institut Polytechnique de Paris, Palaiseau, France']","['France', 'France', 'France', 'France']"
https://arxiv.org/abs/2207.01510,Fairness & Bias, Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation,"With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.",[],[],"['Alejandra Bringas Colmenarejo', 'Luca Nannini', 'Alisa Rieger', 'Kristen M. Scott', 'Xuan Zhao', 'Gourab K. Patro', 'Gjergji Kasneci', 'Katharina Kinder-Kurlanda']","['University of Southampton, Southampton, United Kingdom', 'Minsait - Indra Sistemas & Universidade de Santiago de Compostela, Santiago de Compostela, Spain', 'Delft University of Technology, Delft, Netherlands', 'Katholieke Universiteit Leuven, Leuven, Belgium', 'SCHUFA Holding AG & University of Tuebingen, Tuebingen, Germany', 'IIT Kharagpur & L3S Research Center, Kharagpur, India', 'SCHUFA Holding AG & University of Tuebingen, Tuebingen, Germany', 'University of Klagenfurt, Klagenfurt, Austria']","['United Kingdom', 'Spain', 'Netherlands', 'Belgium', 'Germany', 'India', 'Germany', 'Austria']"
https://arxiv.org/abs/2203.11771,Fairness & Bias, Racial Disparities in the Enforcement of Marijuana Violations in the US,"Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.",[],[],"['Bradley Butcher', 'Chris Robinson', 'Miri Zilka', 'Riccardo Fogliato', 'Carolyn Ashurst', 'Adrian Weller']","['University of Sussex, Brighton, United Kingdom', 'University of Sussex, Brighton, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'The Alan Turing Institute, London, United Kingdom', 'University of Cambridge & The Alan Turing Institute, Cambridge, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'US', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2306.07023,Fairness & Bias, FINS Auditing Framework: Group Fairness for Subset Selections,"Team assembly is a problem that demands trade-offs between multiple fairness criteria and computational optimization. We focus on four criteria: (i) fair distribution of workloads within the team, (ii) fair distribution of skills and expertise regarding project requirements, (iii) fair distribution of protected classes in the team, and (iv) fair distribution of the team cost among protected classes. For this problem, we propose a two-stage algorithmic solution. First, a multi-objective optimization procedure is executed and the Pareto candidates that satisfy the project requirements are selected. Second, N random groups are formed containing combinations of these candidates, and a second round of multi-objective optimization is executed, but this time for selecting the groups that optimize the team-assembly criteria.",[],[],"['Rodrigo Borges', 'Otto Sahlgrens', 'Sami Koivunen', 'Kostas Stefanidis', 'Thomas Olsson', 'Arto Laitinen']","['Worcester Polytechnic Institute, Worcester, MA, USA', 'Worcester Polytechnic Institute, Worcester, MA, USA']","['US', 'US']"
https://arxiv.org/abs/2206.03390,Fairness & Bias," Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics","The statistical regularities in language corpora encode well-known social biases into word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). Using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence.",[],[],"['Aylin Caliskan', 'Pimparkar Parth Ajay', 'Tessa Charlesworth', 'Robert Wolfe', 'Mahzarin R. Banaji']","['University of Washington, Seattle, WA, USA', 'Birla Institute of Technology and Science, Pilani, India', 'Harvard University, Cambridge, MA, USA', 'University of Washington, Seattle, WA, USA', 'Harvard University, Cambridge, MA, USA']","['US', 'India', 'US', 'US', 'US']"
https://arxiv.org/abs/2205.07277,Fairness & Bias, Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations,"As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across various population subgroups including the minority groups. For instance, it should not be the case that explanations associated with instances belonging to a particular gender subgroup (e.g., female) are less accurate than those associated with other genders. However, there is little to no research that assesses if there exist such group-based disparities in the quality of the explanations output by state-of-the-art explanation methods. In this work, we address the aforementioned gaps by initiating the study of identifying group-based disparities in explanation quality. To this end, we first outline the key properties which constitute explanation quality and where disparities can be particularly problematic. We then leverage these properties to propose a novel evaluation framework which can quantitatively measure disparities in the quality of explanations output by state-of-the-art methods. Using this framework, we carry out a rigorous empirical analysis to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and highly non-linear. In addition, we also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit the aforementioned disparities. To the best of our knowledge, this work is the first to highlight and study the problem of group-based disparities in explanation quality. In doing so, our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.",[],[],"['Jessica Dai', 'Sohini Upadhyay', 'Ulrich Aivodji', 'Stephen H. Bach', 'Himabindu Lakkaraju']","['Brown University, Providence, RI, USA', 'Harvard University, Cambridge, MA, USA', 'Université du Québec à Montréal, Montreal, PQ, Canada', 'Brown University, Providence, RI, USA', 'Harvard University, Cambridge, MA, USA']","['US', 'US', 'Canada', 'US', 'US']"
https://arxiv.org/abs/2106.07057,Fairness & Bias, FairCanary: Rapid Continuous Explainable Fairness,"Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems. We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.",[],[],"['Avijit Ghosh', 'Aalok Shanbhag', 'Christo Wilson']","['Northeastern University, Boston, MA, USA', 'Snap Inc., Mountain View, CA, USA', 'Northeastern University, Boston, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2203.06498,Fairness & Bias, The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning,"Recent arguments that machine learning (ML) is facing a reproducibility and replication crisis suggest that some published claims in ML research cannot be taken at face value. These concerns inspire analogies to the replication crisis affecting the social and medical sciences. They also inspire calls for the integration of statistical approaches to causal inference and predictive modeling. A deeper understanding of what reproducibility concerns in supervised ML research have in common with the replication crisis in experimental science puts the new concerns in perspective, and helps researchers avoid ""the worst of both worlds,"" where ML researchers begin borrowing methodologies from explanatory modeling without understanding their limitations and vice versa. We contribute a comparative analysis of concerns about inductive learning that arise in causal attribution as exemplified in psychology versus predictive modeling as exemplified in ML. We identify themes that re-occur in reform discussions, like overreliance on asymptotic theory and non-credible beliefs about real-world data generating processes. We argue that in both fields, claims from learning are implied to generalize outside the specific environment studied (e.g., the input dataset or subject sample, modeling implementation, etc.) but are often impossible to refute due to undisclosed sources of variance in the learning pipeline. In particular, errors being acknowledged in ML expose cracks in long-held beliefs that optimizing predictive accuracy using huge datasets absolves one from having to consider a true data generating process or formally represent uncertainty in performance claims. We conclude by discussing risks that arise when sources of errors are misdiagnosed and the need to acknowledge the role of human inductive biases in learning and reform.",[],[],"['Jessica Hullman', 'Sayash Kapoor', 'Priyanka Nanayakkara', 'Andrew Gelman', 'Arvind Narayanan']","['Northwestern University, Evanston, IL, USA', 'Princeton University, Princeton, NJ, USA', 'Northwestern University, Evanston, IL, USA', 'Columbia University, New York, NY, USA', 'Princeton University, Princeton, NJ, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2206.00945,Fairness & Bias, Responsible Algorithmic Fairness: Insights from Feminist Political Philosophy,"Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by -- or substantively rooted in -- ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices -- as pioneered in the contemporary philosophical literature by Iris Marion Young -- to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness.",[],[],['Atoosa Kasirzadeh'],[],[]
https://arxiv.org/abs/2210.02516,Fairness & Bias, Fair Lending Regulation,"Credit is an essential component of financial wellbeing in America, and unequal access to it is a large factor in the economic disparities between demographic groups that exist today. Today, machine learning algorithms, sometimes trained on alternative data, are increasingly being used to determine access to credit, yet research has shown that machine learning can encode many different versions of ""unfairness,"" thus raising the concern that banks and other financial institutions could -- potentially unwittingly -- engage in illegal discrimination through the use of this technology. In the US, there are laws in place to make sure discrimination does not happen in lending and agencies charged with enforcing them. However, conversations around fair credit models in computer science and in policy are often misaligned: fair machine learning research often lacks legal and practical considerations specific to existing fair lending policy, and regulators have yet to issue new guidance on how, if at all, credit risk models should be utilizing practices and techniques from the research community. This paper aims to better align these sides of the conversation. We describe the current state of credit discrimination regulation in the United States, contextualize results from fair ML research to identify the specific fairness concerns raised by the use of machine learning in lending, and discuss regulatory opportunities to address these concerns.",[],[],"['I. Elizabeth Kumar', 'Keegan E. Hines', 'John P. Dickerson']","['Brown University, Providence, RI, USA', 'Arthur AI, New York City, NY, USA', 'Arthur AI, New York City, NY, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2107.03799,Fairness & Bias, CLEARE: Contrastive Learning with Robust Encoder for Better Detection of Biased Language,"Due to its open-source nature, Android operating system has been the main target of attackers to exploit. Malware creators always perform different code obfuscations on their apps to hide malicious activities. Features extracted from these obfuscated samples through program analysis contain many useless and disguised features, which leads to many false negatives. To address the issue, in this paper, we demonstrate that obfuscation-resilient malware family analysis can be achieved through contrastive learning. The key insight behind our analysis is that contrastive learning can be used to reduce the difference introduced by obfuscation while amplifying the difference between malware and other types of malware. Based on the proposed analysis, we design a system that can achieve robust and interpretable classification of Android malware. To achieve robust classification, we perform contrastive learning on malware samples to learn an encoder that can automatically extract robust features from malware samples. To achieve interpretable classification, we transform the function call graph of a sample into an image by centrality analysis. Then the corresponding heatmaps can be obtained by visualization techniques. These heatmaps can help users understand why the malware is classified as this family. We implement \emph{IFDroid} and perform extensive evaluations on two datasets. Experimental results show that \emph{IFDroid} is superior to state-of-the-art Android malware familial classification systems. Moreover, \emph{IFDroid} is capable of maintaining a 98.4\% F1 on classifying 69,421 obfuscated malware samples.",[],[],"['Yueming Wu', 'Shihan Dou', 'Deqing Zou', 'Wei Yang', 'Weizhong Qiang', 'Hai Jin']",[],[]
https://arxiv.org/abs/2205.00033,Fairness & Bias, Data-Centric Factors in Algorithmic Fairness,"Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. This is not only problematic from a legal perspective, but non-transparent systems are also prone to yield unfair outcomes because their sanity is challenging to assess and calibrate in the first place -- which is particularly worrisome for human decision-subjects. Based on this observation and building upon existing work, I aim to make the following three main contributions through my doctoral thesis: (a) understand how (potential) decision-subjects perceive algorithmic decisions (with varying degrees of transparency of the underlying ADS), as compared to similar decisions made by humans; (b) evaluate different tools for transparent decision-making with respect to their effectiveness in enabling people to appropriately assess the quality and fairness of ADS; and (c) develop human-understandable technical artifacts for fair automated decision-making. Over the course of the first half of my PhD program, I have already addressed substantial pieces of (a) and (c), whereas (b) will be the major focus of the second half.",[],[],['Jakob Schoeffer'],"['ETH Zurich, Zurich, Switzerland', 'University of Oxford, Oxford, United Kingdom', 'ETH Zurich, Zurich, Switzerland']","['Switzerland', 'United Kingdom', 'Switzerland']"
https://arxiv.org/abs/2302.13352,Fairness & Bias, Artificial Moral Advisors: A New Perspective from Moral Psychology,"Cognitive and psychological studies on morality have proposed underlying linguistic and semantic factors. However, laboratory experiments in the philosophical literature often lack the nuances and complexity of real life. This paper examines how well the findings of these cognitive studies generalize to a corpus of over 30,000 narratives of tense social situations submitted to a popular social media forum. These narratives describe interpersonal moral situations or misgivings; other users judge from the post whether the author (protagonist) or the opposing side (antagonist) is morally culpable. Whereas previous work focuses on predicting the polarity of normative behaviors, we extend and apply natural language processing (NLP) techniques to understand the effects of descriptions of the people involved in these posts. We conduct extensive experiments to investigate the effect sizes of features to understand how they affect the assignment of blame on social media. Our findings show that aggregating psychology theories enables understanding real-life moral situations. Moreover, our results suggest that there exist biases in blame assignment on social media, such as males are more likely to receive blame no matter whether they are protagonists or antagonists.",[],[],"['Ruijie Xi', 'Munindar P.Singh']",[],[]
https://arxiv.org/abs/2112.04359,Fairness & Bias," Does AI de-Bias Recruitment? Race, Gender, and AI’s âEradication of Differences Between Groupsâ","This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.",[],[],"['Laura Weidinger', 'John Mellor', 'Maribeth Rauh', 'Conor Griffin', 'Jonathan Uesato', 'Po-Sen Huang', 'Myra Cheng', 'Mia Glaese', 'Borja Balle', 'Atoosa Kasirzadeh', 'Zac Kenton', 'Sasha Brown', 'Will Hawkins', 'Tom Stepleton', 'Courtney Biles', 'Abeba Birhane', 'Julia Haas', 'Laura Rimell', 'Lisa Anne Hendricks', 'William Isaac', 'Sean Legassick', 'Geoffrey Irving', 'Iason Gabriel']",[],[]
https://arxiv.org/abs/2302.07185,Fairness & Bias, Investigating Debiasing Effects on Classification and Explainability,"Most works on the fairness of machine learning systems focus on the blind optimization of common fairness metrics, such as Demographic Parity and Equalized Odds. In this paper, we conduct a comparative study of several bias mitigation approaches to investigate their behaviors at a fine grain, the prediction level. Our objective is to characterize the differences between fair models obtained with different approaches. With comparable performances in fairness and accuracy, are the different bias mitigation approaches impacting a similar number of individuals? Do they mitigate bias in a similar way? Do they affect the same individuals when debiasing a model? Our findings show that bias mitigation approaches differ a lot in their strategies, both in the number of impacted individuals and the populations targeted. More surprisingly, we show these results even apply for several runs of the same mitigation approach. These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.",[],[],"['Natasa Krco', 'Thibault Laugel', 'Jean-Michel Loubes', 'Marcin Detyniecki']","['University of Pisa, Pisa, Italy', 'University of Pisa, Pisa, Italy']","['Italy', 'Italy']"
https://arxiv.org/abs/2206.01691,Fairness & Bias, Measuring Gender Bias in Word Embeddings of Gendered Languages Requires Disentangling Grammatical Gender Signals,"Does the grammatical gender of a language interfere when measuring the semantic gender information captured by its word embeddings? A number of anomalous gender bias measurements in the embeddings of gendered languages suggest this possibility. We demonstrate that word embeddings learn the association between a noun and its grammatical gender in grammatically gendered languages, which can skew social gender bias measurements. Consequently, word embedding post-processing methods are introduced to quantify, disentangle, and evaluate grammatical gender signals. The evaluation is performed on five gendered languages from the Germanic, Romance, and Slavic branches of the Indo-European language family. Our method reduces the strength of grammatical gender signals, which is measured in terms of effect size (Cohen's d), by a significant average of d = 1.3 for French, German, and Italian, and d = 0.56 for Polish and Spanish. Once grammatical gender is disentangled, the association between over 90% of 10,000 inanimate nouns and their assigned grammatical gender weakens, and cross-lingual bias results from the Word Embedding Association Test (WEAT) become more congruent with country-level implicit bias measurements. The results further suggest that disentangling grammatical gender signals from word embeddings may lead to improvement in semantic machine learning tasks.",[],[],"['Shiva Omrani Sabbaghi', 'Aylin Caliskan']","['George Washington University, Washington, DC, USA', 'University of Washington, Seattle, WA, USA']","['US', 'US']"
https://arxiv.org/abs/2309.13933,Fairness & Bias," Resume Format, LinkedIn URLs and Other Unexpected Influences on AI Personality Prediction in Hiring: Results of an Audit","Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of, algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.",[],[],"['Alessandro Fabris', 'Nina Baranowska', 'Matthew J. Dennis', 'Philipp Hacker', 'Jorge Saldivar', 'Frederik Zuiderveen Borgesius', 'Asia J. Biega']","['New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA', 'New York University, New York, NY, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2108.06907,Fairness & Bias, Select Wisely and Explain: Active Learning and Probabilistic Local Post-hoc Explainability,"Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active Learning Based Locally Faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression(GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.",[],[],"['Aditya Saini', 'Ranjitha Prasad']","['Indraprastha Institute of Information Technology, Delhi, New Delhi, India', 'Indraprastha Institute of Information Technology, Delhi, New Delhi, India']","['India', 'India']"
https://arxiv.org/abs/1912.07579,Fairness & Bias, Algorithms that “Don’t See Color”: Measuring Biases in Lookalike and Special Ad Audiences,"Researchers and journalists have repeatedly shown that algorithms commonly used in domains such as credit, employment, healthcare, or criminal justice can have discriminatory effects. Some organizations have tried to mitigate these effects by simply removing sensitive features from an algorithm's inputs. In this paper, we explore the limits of this approach using a unique opportunity. In 2019, Facebook agreed to settle a lawsuit by removing certain sensitive features from inputs of an algorithm that identifies users similar to those provided by an advertiser for ad targeting, making both the modified and unmodified versions of the algorithm available to advertisers. We develop methodologies to measure biases along the lines of gender, age, and race in the audiences created by this modified algorithm, relative to the unmodified one. Our results provide experimental proof that merely removing demographic features from a real-world algorithmic system's inputs can fail to prevent biased outputs. As a result, organizations using algorithms to help mediate access to important life opportunities should consider other approaches to mitigating discriminatory effects.",[],[],"['Piotr Sapiezynski', 'Avijit Ghosh', 'Levi Kaplan', 'Aaron Rieke', 'Alan Mislove']",[],[]
https://arxiv.org/abs/1906.11668,Fairness & Bias, From Coded Bias to Existential Threat: Mapping Expert Frames for AI Governance,"In the last five years, private companies, research institutions as well as public sector organisations have issued principles and guidelines for ethical AI, yet there is debate about both what constitutes ""ethical AI"" and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analyzed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted; why they are deemed important; what issue, domain or actors they pertain to; and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.",[],[],"['Anna Jobin', 'Marcello Ienca', 'Effy Vayena']",[],[]
https://arxiv.org/abs/2206.07555,Fairness & Bias, Respect as a Lens for the Design of AI Systems,"Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.",[],[],"['William Seymour', 'Max Van Kleek', 'Reuben Binns', 'Dave Murray-Rust']",[],[]
https://arxiv.org/abs/2207.04581,Fairness & Bias, Strategic Best Response Fairness in Fair Machine Learning,"With the introduction of machine learning in high-stakes decision making, ensuring algorithmic fairness has become an increasingly important problem to solve. In response to this, many mathematical definitions of fairness have been proposed, and a variety of optimisation techniques have been developed, all designed to maximise a defined notion of fairness. However, fair solutions are reliant on the quality of the training data, and can be highly sensitive to noise. Recent studies have shown that robustness (the ability for a model to perform well on unseen data) plays a significant role in the type of strategy that should be used when approaching a new problem and, hence, measuring the robustness of these strategies has become a fundamental problem. In this work, we therefore propose a new criterion to measure the robustness of various fairness optimisation strategies - the robustness ratio. We conduct multiple extensive experiments on five bench mark fairness data sets using three of the most popular fairness strategies with respect to four of the most popular definitions of fairness. Our experiments empirically show that fairness methods that rely on threshold optimisation are very sensitive to noise in all the evaluated data sets, despite mostly outperforming other methods. This is in contrast to the other two methods, which are less fair for low noise scenarios but fairer for high noise ones. To the best of our knowledge, we are the first to quantitatively evaluate the robustness of fairness optimisation strategies. This can potentially can serve as a guideline in choosing the most suitable fairness strategy for various data sets.",[],[],"['Edward Small', 'Wei Shao', 'Zeliang Zhang', 'Peihan Liu', 'Jeffrey Chan', 'Kacper Sokol', 'Flora Salim']","['McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'Purdue University, West Lafayette, IN, USA', 'McGill University, Montreal, PQ, Canada']","['Canada', 'Canada', 'US', 'Canada']"
https://arxiv.org/abs/2205.14725,Fairness & Bias, What are People Talking about in #BackLivesMatter and #StopAsianHate? Exploring and Categorizing Twitter Topics Emerged in Online Social Movements through the Latent Dirichlet Allocation Model,"Minority groups have been using social media to organize social movements that create profound social impacts. Black Lives Matter (BLM) and Stop Asian Hate (SAH) are two successful social movements that have spread on Twitter that promote protests and activities against racism and increase the public's awareness of other social challenges that minority groups face. However, previous studies have mostly conducted qualitative analyses of tweets or interviews with users, which may not comprehensively and validly represent all tweets. Very few studies have explored the Twitter topics within BLM and SAH dialogs in a rigorous, quantified and data-centered approach. Therefore, in this research, we adopted a mixed-methods approach to comprehensively analyze BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation model to understand the top high-level words and topics and (2) open-coding analysis to identify specific themes across the tweets. We collected more than one million tweets with the #blacklivesmatter and #stopasianhate hashtags and compared their topics. Our findings revealed that the tweets discussed a variety of influential topics in depth, and social justice, social movements, and emotional sentiments were common topics in both movements, though with unique subtopics for each movement. Our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of AI, ethics, and society in general.",[],[],"['Xin Tong', 'Yixuan Li', 'Jiayi Li', 'Rongqi Bei', 'Luyao Zhang']",[],[]
https://arxiv.org/abs/2207.00691,Fairness & Bias, American == White in Multimodal Language-and-Image AI,"Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97% of White individuals are identified as American, compared to only 3% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36% of the time, but never remarks upon race for White individuals. Finally, provided with an initialization image from the CFD and the text ""an American person,"" a synthetic image generator (VQGAN) using the text-based guidance of CLIP lightens the skin tone of individuals of all races (by 35% for Black individuals, based on pixel brightness). The results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.",[],[],"['Robert Wolfe', 'Aylin Caliskan']","['University of Washington, Seattle, WA, USA', 'University of Washington, Seattle, WA, USA']","['US', 'US']"
https://arxiv.org/abs/2106.05519,Fairness & Bias, Enhancing Fairness in Face Detection in Computer Vision Systems by Demographic Bias Mitigation,"Demographic bias is a significant challenge in practical face recognition systems. Existing methods heavily rely on accurate demographic annotations. However, such annotations are usually unavailable in real scenarios. Moreover, these methods are typically designed for a specific demographic group and are not general enough. In this paper, we propose a false positive rate penalty loss, which mitigates face recognition bias by increasing the consistency of instance False Positive Rate (FPR). Specifically, we first define the instance FPR as the ratio between the number of the non-target similarities above a unified threshold and the total number of the non-target similarities. The unified threshold is estimated for a given total FPR. Then, an additional penalty term, which is in proportion to the ratio of instance FPR overall FPR, is introduced into the denominator of the softmax-based loss. The larger the instance FPR, the larger the penalty. By such unequal penalties, the instance FPRs are supposed to be consistent. Compared with the previous debiasing methods, our method requires no demographic annotations. Thus, it can mitigate the bias among demographic groups divided by various attributes, and these attributes are not needed to be previously predefined during training. Extensive experimental results on popular benchmarks demonstrate the superiority of our method over state-of-the-art competitors. Code and trained models are available at this https URL.",[],[],"['Xingkun Xu', 'Yuge Huang', 'Pengcheng Shen', 'Shaoxin Li', 'Jilin Li', 'Feiyue Huang', 'Yong Li', 'Zhen Cui']","['Amazon Alexa AI, Los Angeles, CA, USA', 'Amazon Alexa AI, Seattle, WA, USA', 'Amazon Alexa AI, Vancouver, Canada', 'Amazon Alexa AI, Santa Clara, CA, USA', 'Amazon Alexa AI, Seattle, WA, USA', 'Amazon Alexa AI, Los Angeles, CA, USA', 'Amazon Alexa AI, Chicago, IL, USA', 'Amazon Alexa AI, Santa Clara, CA, USA', 'Amazon Alexa AI, Los Angeles, CA, USA']","['US', 'US', 'Canada', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2205.15258,Fairness & Bias," Transparency, Governance and Regulation of Algorithmic Tools Deployed in the Criminal Justice System: a UK Case Study","We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.",[],[],"['Miri Zilka', 'Holli Sargeant', 'Adrian Weller']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2304.01829,Privacy & Data Governance, Aegis: An Agent for Multi-party Privacy preservation,"Vertical federated learning (VFL) is a promising category of federated learning for the scenario where data is vertically partitioned and distributed among parties. VFL enriches the description of samples using features from different parties to improve model capacity. Compared with horizontal federated learning, in most cases, VFL is applied in the commercial cooperation scenario of companies. Therefore, VFL contains tremendous business values. In the past few years, VFL has attracted more and more attention in both academia and industry. In this paper, we systematically investigate the current work of VFL from a layered perspective. From the hardware layer to the vertical federated system layer, researchers contribute to various aspects of VFL. Moreover, the application of VFL has covered a wide range of areas, e.g., finance, healthcare, etc. At each layer, we categorize the existing work and explore the challenges for the convenience of further research and development of VFL. Especially, we design a novel MOSP tree taxonomy to analyze the core component of VFL, i.e., secure vertical federated machine learning algorithm. Our taxonomy considers four dimensions, i.e., machine learning model (M), protection object (O), security model (S), and privacy-preserving protocol (P), and provides a comprehensive investigation.",[],[],"['Liu Yang', 'Di Chai', 'Junxue Zhang', 'Yilun Jin', 'Leye Wang', 'Hao Liu', 'Han Tian', 'Qian Xu', 'Kai Chen']","['University of Montreal, Montréal, PQ, Canada', 'University of Montreal, Montréal, PQ, Canada', 'Notre Dame University-Louaize, Zouk Mosbeh, Lebanon']","['Canada', 'Canada', 'Lebanon']"
https://arxiv.org/abs/2306.00380,Privacy & Data Governance, Managing Sustainability Tensions in Artificial Intelligence: Insights from Paradox Theory,"When making strategic decisions, we are often confronted with overwhelming information to process. The situation can be further complicated when some pieces of evidence are contradicted each other or paradoxical. The challenge then becomes how to determine which information is useful and which ones should be eliminated. This process is known as meta-decision. Likewise, when it comes to using Artificial Intelligence (AI) systems for strategic decision-making, placing trust in the AI itself becomes a meta-decision, given that many AI systems are viewed as opaque ""black boxes"" that process large amounts of data. Trusting an opaque system involves deciding on the level of Trustworthy AI (TAI). We propose a new approach to address this issue by introducing a novel taxonomy or framework of TAI, which encompasses three crucial domains: articulate, authentic, and basic for different levels of trust. To underpin these domains, we create ten dimensions to measure trust: explainability/transparency, fairness/diversity, generalizability, privacy, data governance, safety/robustness, accountability, reproducibility, reliability, and sustainability. We aim to use this taxonomy to conduct a comprehensive survey and explore different TAI approaches from a strategic decision-making perspective.",[],[],"['Caesar Wu', 'Yuan-Fang Lib', 'Pascal Bouvry']","['University of Surrey, Guildford, United Kingdom', 'University of Surrey, Guildford, United Kingdom', 'University of Surrey, Guildford, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2304.01829,Security, Aegis: An Agent for Multi-party Privacy preservation,"Vertical federated learning (VFL) is a promising category of federated learning for the scenario where data is vertically partitioned and distributed among parties. VFL enriches the description of samples using features from different parties to improve model capacity. Compared with horizontal federated learning, in most cases, VFL is applied in the commercial cooperation scenario of companies. Therefore, VFL contains tremendous business values. In the past few years, VFL has attracted more and more attention in both academia and industry. In this paper, we systematically investigate the current work of VFL from a layered perspective. From the hardware layer to the vertical federated system layer, researchers contribute to various aspects of VFL. Moreover, the application of VFL has covered a wide range of areas, e.g., finance, healthcare, etc. At each layer, we categorize the existing work and explore the challenges for the convenience of further research and development of VFL. Especially, we design a novel MOSP tree taxonomy to analyze the core component of VFL, i.e., secure vertical federated machine learning algorithm. Our taxonomy considers four dimensions, i.e., machine learning model (M), protection object (O), security model (S), and privacy-preserving protocol (P), and provides a comprehensive investigation.",[],[],"['Liu Yang', 'Di Chai', 'Junxue Zhang', 'Yilun Jin', 'Leye Wang', 'Hao Liu', 'Han Tian', 'Qian Xu', 'Kai Chen']","['University of Montreal, Montréal, PQ, Canada', 'University of Montreal, Montréal, PQ, Canada', 'Notre Dame University-Louaize, Zouk Mosbeh, Lebanon']","['Canada', 'Canada', 'Lebanon']"
https://arxiv.org/abs/2209.10604,Security, Current and Near-Term AI as a Potential Existential Risk Factor,"There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole. A separate, parallel body of work has explored existential risks to humanity, including but not limited to that stemming from unaligned Artificial General Intelligence (AGI). In this paper, we problematise the notion that current and near-term artificial intelligence technologies have the potential to contribute to existential risk by acting as intermediate risk factors, and that this potential is not limited to the unaligned AGI scenario. We propose the hypothesis that certain already-documented effects of AI can act as existential risk factors, magnifying the likelihood of previously identified sources of existential risk. Moreover, future developments in the coming decade hold the potential to significantly exacerbate these risk factors, even in the absence of artificial general intelligence. Our main contribution is a (non-exhaustive) exposition of potential AI risk factors and the causal relationships between them, focusing on how AI can affect power dynamics and information security. This exposition demonstrates that there exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities.",[],[],"['Benjamin S. Bucknall', 'Shiri Dori-Hacohen']","['Uppsala University, Uppsala, Sweden', 'University of Connecticut, Storrs, CT, USA']","['Sweden', 'US']"
https://arxiv.org/abs/1802.07228,Security, Artificial Intelligence in the Government: Responses to Failures and Social Impact,"This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.",[],[],"['Miles Brundage', 'Shahar Avin', 'Jack Clark', 'Helen Toner', 'Peter Eckersley', 'Ben Garfinkel', 'Allan Dafoe', 'Paul Scharre', 'Thomas Zeitzoff', 'Bobby Filar', 'Hyrum Anderson', 'Heather Roff', 'Gregory C. Allen', 'Jacob Steinhardt', 'Carrick Flynn', 'Seán Ó hÉigeartaigh', 'Simon Beard', 'Haydn Belfield', 'Sebastian Farquhar', 'Clare Lyle', 'Rebecca Crootof', 'Owain Evans', 'Michael Page', 'Joanna Bryson', 'Roman Yampolskiy', 'Dario Amodei']","['Boston University, Boston, MA, USA', 'University of Virginia, Charlottesville, VA, USA', 'University of Pennsylvania, Philadelphia, PA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2306.00380,Security, Managing Sustainability Tensions in Artificial Intelligence: Insights from Paradox Theory,"When making strategic decisions, we are often confronted with overwhelming information to process. The situation can be further complicated when some pieces of evidence are contradicted each other or paradoxical. The challenge then becomes how to determine which information is useful and which ones should be eliminated. This process is known as meta-decision. Likewise, when it comes to using Artificial Intelligence (AI) systems for strategic decision-making, placing trust in the AI itself becomes a meta-decision, given that many AI systems are viewed as opaque ""black boxes"" that process large amounts of data. Trusting an opaque system involves deciding on the level of Trustworthy AI (TAI). We propose a new approach to address this issue by introducing a novel taxonomy or framework of TAI, which encompasses three crucial domains: articulate, authentic, and basic for different levels of trust. To underpin these domains, we create ten dimensions to measure trust: explainability/transparency, fairness/diversity, generalizability, privacy, data governance, safety/robustness, accountability, reproducibility, reliability, and sustainability. We aim to use this taxonomy to conduct a comprehensive survey and explore different TAI approaches from a strategic decision-making perspective.",[],[],"['Caesar Wu', 'Yuan-Fang Lib', 'Pascal Bouvry']","['University of Surrey, Guildford, United Kingdom', 'University of Surrey, Guildford, United Kingdom', 'University of Surrey, Guildford, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2206.04737,Security, Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance,"Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of interventions that allow for the effective participation of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.",[],[],"['Inioluwa Deborah Raji', 'Peggy Xu', 'Colleen Honigsberg', 'Daniel E. Ho']","['University of California, Berkeley, Berkeley, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2206.07555,Security, Respect as a Lens for the Design of AI Systems,"Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.",[],[],"['William Seymour', 'Max Van Kleek', 'Reuben Binns', 'Dave Murray-Rust']","['Oregon State University, Corvallis, OR, USA', 'Oregon State University, Corvallis, OR, USA']","['US', 'US']"
https://arxiv.org/abs/2105.02851,Security, Generating Deontic Obligations From Utility-Maximizing Systems,"We develop a formal framework for automatic reasoning about the obligations of autonomous cyber-physical systems, including their social and ethical obligations. Obligations, permissions and prohibitions are distinct from a system's mission, and are a necessary part of specifying advanced, adaptive AI-equipped systems. They need a dedicated deontic logic of obligations to formalize them. Most existing deontic logics lack corresponding algorithms and system models that permit automatic verification. We demonstrate how a particular deontic logic, Dominance Act Utilitarianism (DAU), is a suitable starting point for formalizing the obligations of autonomous systems like self-driving cars. We demonstrate its usefulness by formalizing a subset of Responsibility-Sensitive Safety (RSS) in DAU; RSS is an industrial proposal for how self-driving cars should and should not behave in traffic. We show that certain logical consequences of RSS are undesirable, indicating a need to further refine the proposal. We also demonstrate how obligations can change over time, which is necessary for long-term autonomy. We then demonstrate a model-checking algorithm for DAU formulas on weighted transition systems, and illustrate it by model-checking obligations of a self-driving car controller from the literature.",[],[],"['Colin Shea-Blymyer', 'Houssam Abbas']",[],[]
https://arxiv.org/abs/2109.05067,Security, Human Autonomy in Algorithmic Management,"As algorithms become an influential component of government decision-making around the world, policymakers have debated how governments can attain the benefits of algorithms while preventing the harms of algorithms. One mechanism that has become a centerpiece of global efforts to regulate government algorithms is to require human oversight of algorithmic decisions. Despite the widespread turn to human oversight, these policies rest on an uninterrogated assumption: that people are able to effectively oversee algorithmic decision-making. In this article, I survey 41 policies that prescribe human oversight of government algorithms and find that they suffer from two significant flaws. First, evidence suggests that people are unable to perform the desired oversight functions. Second, as a result of the first flaw, human oversight policies legitimize government uses of faulty and controversial algorithms without addressing the fundamental issues with these tools. Thus, rather than protect against the potential harms of algorithmic decision-making in government, human oversight policies provide a false sense of security in adopting algorithms and enable vendors and agencies to shirk accountability for algorithmic harms. In light of these flaws, I propose a shift from human oversight to institutional oversight as the central mechanism for regulating government algorithms. This institutional approach operates in two stages. First, agencies must justify that it is appropriate to incorporate an algorithm into decision-making and that any proposed forms of human oversight are supported by empirical evidence. Second, these justifications must receive democratic public review and approval before the agency can adopt the algorithm.",[],[],['Ben Green'],[],[]