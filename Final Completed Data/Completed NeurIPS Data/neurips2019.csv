link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://papers.nips.cc/paper_files/paper/2019/hash/07cb5f86508f146774a2fac4373a8e50-Abstract.html,Transparency & Explainability,A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits,"This study provides a normative theory for how Bayesian causal inference can be implemented in neural circuits. In both cognitive processes such as causal reasoning and perceptual inference such as cue integration, the nervous systems need to choose different models representing the underlying causal structures when making inferences on external stimuli. In multisensory processing, for example, the nervous system has to choose whether to integrate or segregate inputs from different sensory modalities to infer the sensory stimuli, based on whether the inputs are from the same or different sources. Making this choice is a model selection problem requiring the computation of Bayes factor, the ratio of likelihoods between the integration and the segregation models. In this paper, we consider the causal inference in multisensory processing and propose a novel generative model based on neural population code that takes into account both stimulus feature and stimulus reliability in the inference. In the case of circular variables such as heading direction, our normative theory yields an analytical solution for computing the Bayes factor, with a clear geometric interpretation, which can be implemented by simple additive mechanisms with neural population code. Numerical simulation shows that the tunings of the neurons computing Bayes factor are consistent with the ""opposite neurons"" discovered in dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas for visual-vestibular processing. This study illuminates a potential neural mechanism for causal inference in the brain.",[],[],"['Wenhao Zhang', 'Si Wu', 'Brent Doiron', 'Tai Sing Lee']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/11b9842e0a271ff252c1903e7132cd68-Abstract.html,Transparency & Explainability,CPM-Nets: Cross Partial Multi-View Networks,"Despite multi-view learning progressed fast in past decades, it is still challenging due to the difficulty in modeling complex correlation among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets). In this framework, we first give a formal definition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the latent representation learned from our algorithm. To achieve the completeness, the task of learning latent multi-view representation is specifically translated to degradation process through mimicking data transmitting, such that the optimal tradeoff between consistence and complementarity across different views could be achieved. In contrast with methods that either complete missing views or group samples according to view-missing patterns, our model fully exploits all samples and all views to produce structured representation for interpretability. Extensive experimental results validate the effectiveness of our algorithm over existing state-of-the-arts.",[],[],"['Changqing Zhang', 'Zongbo Han', 'yajie cui', 'Huazhu Fu', 'Joey Tianyi Zhou', 'Qinghua Hu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1b486d7a5189ebe8d8c46afc64b0d1b4-Abstract.html,Transparency & Explainability,On the Fairness of Disentangled Representations,"Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations.We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable.We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent.Analyzing the representations of more than 12600 trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.",[],[],"['Francesco Locatello', 'Gabriele Abbati', 'Thomas Rainforth', 'Stefan Bauer', 'Bernhard Schölkopf', 'Olivier Bachem']","['Dept. of Computer Science, ETH Zurich and Max-Planck Institute for Intelligent Systems', 'Dept. of Engineering Science, University of Oxford', 'Dept. of Statistics, University of Oxford', 'Max-Planck Institute for Intelligent Systems', 'Max-Planck Institute for Intelligent Systems', 'Google Research, Brain Team']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html,Transparency & Explainability,Calibration tests in multi-class classification: A unifying framework,"In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale.",[],[],"['David Widmann', 'Fredrik Lindsten', 'Dave Zachariah']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/4206e38996fae4028a26d43b24f68d32-Abstract.html,Transparency & Explainability,Provably robust boosted decision stumps and trees against adversarial attacks,"The problem of adversarial robustness has been studied extensively for neural networks. However, for boosted decision trees and decision stumps there are almost no results, even though they are widely used in practice (e.g. XGBoost) due to their accuracy, interpretability, and efficiency. We show in this paper that for boosted decision stumps the \textit{exact} min-max robust loss and test error for an $l_\infty$-attack can be computed in $O(T\log T)$ time per input, where $T$ is the number of decision stumps and the optimal update step of the ensemble can be done in $O(n^2\,T\log T)$, where $n$ is the number of data points. For boosted trees we show how to efficiently calculate and optimize an upper bound on the robust loss, which leads to state-of-the-art robust test error for boosted trees on MNIST (12.5\% for $\epsilon_\infty=0.3$), FMNIST (23.2\% for $\epsilon_\infty=0.1$), and CIFAR-10 (74.7\% for $\epsilon_\infty=8/255$). Moreover, the robust test error rates we achieve are competitive to the ones of provably robust convolutional networks. The code of all our experiments is available at \url{http://github.com/max-andr/provably-robust-boosting}.",[],[],"['Maksym Andriushchenko', 'Matthias Hein']","['University of Tübingen', 'University of Tübingen']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html,Transparency & Explainability,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,"A recent trend of fair machine learning is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However, one common challenge of all causality-based fairness notions is identifiability, i.e., whether they can be uniquely measured from observational data, which is a critical barrier to applying these notions to real-world situations. In this paper, we develop a framework for measuring different causality-based fairness. We propose a unified definition that covers most of previous causality-based fairness notions, namely the path-specific counterfactual fairness (PC fairness). Based on that, we propose a general method in the form of a constrained optimization problem for bounding the path-specific counterfactual fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method.",[],[],"['Yongkai Wu', 'Lu Zhang', 'Xintao Wu', 'Hanghang Tong']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/5352696a9ca3397beb79f116f3a33991-Abstract.html,Transparency & Explainability,Bat-G net: Bat-inspired High-Resolution 3D Image Reconstruction using Ultrasonic Echoes,"In this paper, a bat-inspired high-resolution ultrasound 3D imaging system is presented. Live bats demonstrate that the properly used ultrasound can be used to perceive 3D space. With this in mind, a neural network referred to as a Bat-G network is implemented to reconstruct the 3D representation of target objects from the hyperbolic FM (HFM) chirped ultrasonic echoes. The Bat-G network consists of an encoder emulating a bat's central auditory pathway, and a 3D graphical visualization decoder. For the acquisition of the ultrasound data, a custom-made Bat-I sensor module is used. The Bat-G network shows the uniform 3D reconstruction results and achieves precision, recall, and F1-score of 0.896, 0.899 and 0.895, respectively. The experimental results demonstrate the implementation feasibility of a high-resolution non-optical sound-based imaging system being used by live bats. The project web page (https://sites.google.com/view/batgnet) contains additional content summarizing our research.",[],[],"['Gunpil Hwang', 'Seohyeon Kim', 'Hyeon-Min Bae']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/54ebdfbbfe6c31c39aaba9a1ee83860a-Abstract.html,Transparency & Explainability,Discriminative Topic Modeling with Logistic LDA,"Despite many years of research into latent Dirichlet allocation (LDA), applying LDA to collections of non-categorical items is still challenging for practitioners. Yet many problems with much richer data share a similar structure and could benefit from the vast literature on LDA. We propose logistic LDA, a novel discriminative variant of latent Dirichlet allocation which is easy to apply to arbitrary inputs. In particular, our model can easily be applied to groups of images, arbitrary text embeddings, or integrate deep neural networks. Although it is a discriminative model, we show that logistic LDA can learn from unlabeled data in an unsupervised manner by exploiting the group structure present in the data. In contrast to other recent topic models designed to handle arbitrary inputs, our model does not sacrifice the interpretability and principled motivation of LDA.",[],[],"['Iryna Korshunova', 'Hanchen Xiong', 'Mateusz Fedoryszak', 'Lucas Theis']","['Ghent University', 'Twitter', 'Twitter', 'Twitter']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/55a988dfb00a914717b3000a3374694c-Abstract.html,Transparency & Explainability,Disentangling Influence: Using disentangled representations to audit model predictions,"Motivated by the need to audit complex and black box models, there has been extensive research on quantifying how data features influence model predictions. Feature influence can be direct (a direct influence on model outcomes) and indirect (model outcomes are influenced via proxy features). Feature influence can also be expressed in aggregate over the training or test data or locally with respect to a single point. Current research has typically focused on one of each of these dimensions. In this paper, we develop disentangled influence audits, a procedure to audit the indirect influence of features. Specifically, we show that disentangled representations provide a mechanism to identify proxy features in the dataset, while allowing an explicit computation of feature influence on either individual outcomes or aggregate-level outcomes. We show through both theory and experiments that disentangled influence audits can both detect proxy features and show, for each individual or in aggregate, which of these proxy features affects the classifier being audited the most. In this respect, our method is more powerful than existing methods for ascertaining feature influence.",[],[],"['Charles Marx', 'Richard Phillips', 'Sorelle Friedler', 'Carlos Scheidegger', 'Suresh Venkatasubramanian']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/567b8f5f423af15818a068235807edc0-Abstract.html,Transparency & Explainability,Demystifying Black-box Models with Symbolic Metamodels,"Understanding the predictions of a machine learning model can be as crucial as the model's accuracy in many application domains. However, the black-box nature of most highly-accurate (complex) models is a major hindrance to their interpretability. To address this issue, we introduce the symbolic metamodeling framework — a general methodology for interpreting predictions by converting ""black-box"" models into ""white-box"" functions that are understandable to human subjects. A symbolic metamodel is a model of a model, i.e., a surrogate model of a trained (machine learning) model expressed through a succinct symbolic expression that comprises familiar mathematical functions and can be subjected to symbolic manipulation. We parameterize symbolic metamodels using Meijer G-functions — a class of complex-valued contour integrals that depend on scalar parameters, and whose solutions reduce to familiar elementary, algebraic, analytic and closed-form functions for different parameter settings. This parameterization enables efficient optimization of metamodels via gradient descent, and allows discovering the functional forms learned by a machine learning model with minimal a priori assumptions. We show that symbolic metamodeling provides an all-encompassing framework for model interpretation — all common forms of global and local explanations of a model can be analytically derived from its symbolic metamodel.",[],[],"['Ahmed M. Alaa', 'Mihaela van der Schaar']","['ECE Department, UCLA', 'University of Cambridge, UCLA, and Alan Turing Institute']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/5e2b66750529d8ae895ad2591118466f-Abstract.html,Transparency & Explainability,Paraphrase Generation with Latent Bag of Words,"Paraphrase generation is a longstanding important problem in natural language processing.   Recent progress in deep generative models has shown promising results on discrete latent variables for text generation.   Inspired by variational autoencoders with discrete latent structures,   in this work, we propose a latent bag of words (BOW) model for paraphrase generation.  We ground the semantics of a discrete latent variable by the target BOW.   We use this latent variable to build a fully differentiable content planning and surface realization pipeline.   Specifically, we use source words to predict their neighbors and model the target BOW with a mixture of softmax.   We use gumbel top-k reparameterization to perform differentiable subset sampling from the predicted BOW distribution.  We retrieve the sampled word embeddings and use them to augment the decoder and guide its generation search space.   Our latent BOW model not only enhances the decoder, but also exhibits clear interpretability.  We show the model interpretability with regard to (1). unsupervised learning of word neighbors (2). the step-by-step generation procedure.   Extensive experiments demonstrate the model's transparent and effective generation process.",[],[],"['Yao Fu', 'Yansong Feng', 'John P. Cunningham']","['Department of Computer Science, Columbia University', 'Institute of Computer Science and Technology, Peking University', 'Department of Statistics, Columbia University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/6395ebd0f4b478145ecfbaf939454fa4-Abstract.html,Transparency & Explainability,Variational Denoising Network: Toward Blind Noise Modeling and Removal,"Blind image denoising is an important yet very challenging problem in computervision due to the complicated acquisition process of real images. In this work wepropose a new variational inference method, which integrates both noise estimation and image denoising into a unique Bayesian framework, for blind image denoising. Specifically, an approximate posterior, parameterized by deep neural networks, is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be easily implemented for blind image denoising with automatic noise estimation for the test noisy image. On one hand, as other data-driven deep learning methods, our method, namely variational denoising network (VDN), can perform denoising efficiently due to its explicit form of posterior expression. On the other hand, VDN inherits the advantages of traditional model-driven approaches, especially the good generalization capability of generative models. VDN has good interpretability and can be flexibly utilized to estimate and remove complicated non-i.i.d. noise collected in real scenarios. Comprehensive experiments are performed to substantiate the superiority of our method in blind image denoising.",[],[],"['Zongsheng Yue', 'Hongwei Yong', 'Qian Zhao', 'Deyu Meng', 'Lei Zhang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html,Transparency & Explainability,Deliberative Explanations: visualizing network insecurities,"A new approach to explainable AI, denoted {\it deliberative explanations,\/}  is proposed. Deliberative explanations are a visualization technique  that aims to go beyond the simple visualization of the image regions  (or, more generally, input variables) responsible for a network  prediction. Instead, they aim to expose the deliberations carried  by the network to arrive at that prediction, by uncovering the  insecurities of the network about the latter. The  explanation consists of a list of insecurities, each composed of  1) an image region (more generally, a set of input variables), and 2)  an ambiguity formed by the pair of classes responsible for the network  uncertainty about the region. Since insecurity detection requires  quantifying the difficulty of network predictions, deliberative  explanations combine ideas from the literatures on visual explanations and  assessment of classification difficulty. More specifically,  the proposed implementation  combines attributions with respect to both class  predictions and a difficulty score.  An evaluation protocol that leverages object recognition (CUB200)  and scene classification (ADE20K) datasets that combine part and  attribute annotations is also introduced to evaluate the accuracy of  deliberative explanations. Finally, an experimental evaluation shows that  the most accurate explanations are achieved by combining non self-referential  difficulty scores and second-order attributions. The resulting  insecurities are shown to correlate with regions of attributes that  are shared by different classes. Since these regions are also ambiguous  for humans, deliberative explanations are intuitive, suggesting that  the deliberative process of modern networks correlates with human  reasoning.",[],[],"['Pei Wang', 'Nuno Nvasconcelos']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/704cddc91e28d1a5517518b2f12bc321-Abstract.html,Transparency & Explainability,Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis,"Linear dimensionality reduction methods are commonly used to extract low-dimensional structure from high-dimensional data. However, popular methods disregard temporal structure, rendering them prone to extracting noise rather than meaningful dynamics when applied to time series data. At the same time, many successful unsupervised learning methods for temporal, sequential and spatial data extract features which are predictive of their surrounding context. Combining these approaches, we introduce Dynamical Components Analysis (DCA), a linear dimensionality reduction method which discovers a subspace of high-dimensional time series data with maximal predictive information, defined as the mutual information between the past and future. We test DCA on synthetic examples and demonstrate its superior ability to extract dynamical structure compared to commonly used linear methods. We also apply DCA to several real-world datasets, showing that the dimensions extracted by DCA are more useful than those extracted by other methods for predicting future states and decoding auxiliary variables. Overall, DCA robustly extracts dynamical structure in noisy, high-dimensional data while retaining the computational efficiency and geometric interpretability of linear dimensionality reduction methods.",[],[],"['David Clark', 'Jesse Livezey', 'Kristofer Bouchard']","['Center for Theoretical Neuroscience, Columbia University and Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory', 'Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory and Redwood Center for Theoretical Neuroscience, University of California, Berkeley', 'Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory and Redwood Center for Theoretical Neuroscience, University of California, Berkeley and Helen Wills Neuroscience Institute, University of California, Berkeley']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/73e0f7487b8e5297182c5a711d20bf26-Abstract.html,Transparency & Explainability,The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric,"Where machine-learned predictive risk scores inform high-stakes decisions, such as bail and sentencing in criminal justice, fairness has been a serious concern. Recent work has characterized the disparate impact that such risk scores can have when used for a binary classification task. This may not account, however, for the more diverse downstream uses of risk scores and their non-binary nature. To better account for this, in this paper, we investigate the fairness of predictive risk scores from the point of view of a bipartite ranking task, where one seeks to rank positive examples higher than negative ones. We introduce the xAUC disparity as a metric to assess the disparate impact of risk scores and define it as the difference in the probabilities of ranking a random positive example from one protected group above a negative one from another group and vice versa. We provide a decomposition of bipartite ranking loss into components that involve the discrepancy and components that involve pure predictive ability within each group. We use xAUC analysis to audit predictive risk scores for recidivism prediction, income prediction, and cardiac arrest prediction, where it describes disparities that are not evident from simply comparing within-group predictive performance.",[],[],"['Nathan Kallus', 'Angela Zhou']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html,Transparency & Explainability,Towards Automatic Concept-based Explanations,"Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions.     Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \emph{concept} based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.",[],[],"['Amirata Ghorbani', 'James Wexler', 'James Y. Zou', 'Been Kim']","['Stanford University and Google Brain', 'Google Brain', 'Stanford University', 'Google Brain']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/79a3308b13cd31f096d8a4a34f96b66b-Abstract.html,Transparency & Explainability,Sobolev Independence Criterion,"We propose the Sobolev Independence Criterion (SIC), an interpretable dependency measure between a high dimensional random variable X and a response variable Y. SIC decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. SIC can be seen as a gradient regularized Integral Probability Metric (IPM) between the joint distribution of the two random variables and the product of their marginals. We use sparsity inducing gradient penalties to promote input sparsity of the critic of the IPM. In the kernel version we show that SIC can be cast as a convex optimization problem by introducing auxiliary variables that play an important role in feature selection as they are normalized feature importance scores. We then present a neural version of SIC where the critic is parameterized as a homogeneous neural network, improving its representation power as well as its interpretability. We conduct experiments validating SIC for feature selection in synthetic and real-world experiments. We show that SIC enables reliable and interpretable discoveries, when used in conjunction with the holdout randomization test and knockoffs to control the False Discovery Rate. Code is available at http://github.com/ibm/sic.",[],[],"['Youssef Mroueh', 'Tom Sercu', 'Mattia Rigotti', 'Inkit Padhi', 'Cicero Nogueira dos Santos']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/7f278ad602c7f47aa76d1bfc90f20263-Abstract.html,Transparency & Explainability,Coresets for Archetypal Analysis,"Archetypal analysis represents instances as linear mixtures of prototypes (the archetypes) that lie on the boundary of the convex hull of the data. Archetypes are thus  often better interpretable than factors computed by other matrix factorization techniques. However, the interpretability comes with high computational cost due to additional convexity-preserving constraints. In this paper, we propose efficient coresets for archetypal analysis. Theoretical guarantees are derived by showing that quantization errors of k-means upper bound archetypal analysis; the computation of a provable absolute-coreset can be performed in only two passes over the data. Empirically, we show that the coresets lead to improved performance on several data sets.",[],[],"['Sebastian Mair', 'Ulf Brefeld']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html,Transparency & Explainability,Full-Gradient Representation for Neural Network Visualization,"We introduce a new tool for interpreting neural nets, namely full-gradients, which decomposes the neural net response into input sensitivity and per-neuron sensitivity components. This is the first proposed representation which satisfies two key properties: completeness and weak dependence, which provably cannot be satisfied by any saliency map-based interpretability method. Using full-gradients, we also propose an approximate saliency map representation for convolutional nets dubbed FullGrad, obtained by aggregating the full-gradient components.We experimentally evaluate the usefulness of FullGrad in explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behavior correctly, and more comprehensively than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods.",[],[],"['Suraj Srinivas', 'François Fleuret']","['Idiap Research Institute & EPFL', 'Idiap Research Institute & EPFL']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/82161242827b703e6acf9c726942a1e4-Abstract.html,Transparency & Explainability,Chasing Ghosts: Instruction Following as Bayesian State Tracking,"A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) within the framework of Bayesian state tracking - learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet baseline when predicting the goal location on the map. On the full VLN task, i.e. navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.",[],[],"['Peter Anderson', 'Ayush Shrivastava', 'Devi Parikh', 'Dhruv Batra', 'Stefan Lee']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html,Transparency & Explainability,Hierarchical Optimal Transport for Document Representation,"The ability to measure similarity between documents enables intelligent summarization and analysis of large corpora. Past distances between documents suffer from either an inability to incorporate semantic similarities between words or from scalability issues. As an alternative, we introduce hierarchical optimal transport as a meta-distance between documents, where documents are modeled as distributions over topics, which themselves are modeled as distributions over words. We then solve an optimal transport problem on the smaller topic space to compute a similarity score. We give conditions on the topics under which this construction defines a distance, and we relate it to the word mover's distance. We evaluate our technique for k-NN classification and show better interpretability and scalability with comparable performance to current methods at a fraction of the cost.",[],[],"['Mikhail Yurochkin', 'Sebastian Claici', 'Edward Chien', 'Farzaneh Mirzazadeh', 'Justin M. Solomon']","['IBM Research and MIT-IBM Watson AI Lab', 'MIT CSAIL and MIT-IBM Watson AI Lab', 'MIT CSAIL and MIT-IBM Watson AI Lab', 'IBM Research and MIT-IBM Watson AI Lab', 'MIT CSAIL and MIT-IBM Watson AI Lab']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html,Transparency & Explainability,Saccader: Improving Accuracy of Hard Attention Models for Vision,"Although deep convolutional neural networks achieve state-of-the-art performance across nearly all image classification tasks, their decisions are difficult to interpret. One approach that offers some level of interpretability by design is \textit{hard attention}, which uses only relevant portions of the image. However, training hard attention models with only class label supervision is challenging, and hard attention has proved difficult to scale to complex datasets. Here, we propose a novel hard attention model, which we term Saccader. Key to Saccader is a pretraining step that requires only class labels and provides initial attention locations for policy gradient optimization. Our best models narrow the gap to common ImageNet baselines, achieving $75\%$  top-1 and $91\%$ top-5 while attending to less than one-third of the image.",[],[],"['Gamaleldin Elsayed', 'Simon Kornblith', 'Quoc V. Le']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/90cc440b1b8caa520c562ac4e4bbcb51-Abstract.html,Transparency & Explainability,Structured Graph Learning Via Laplacian Spectral Constraints,"Learning a graph with a specific structure is essential for interpretability and identification of the relationships among data. But structured graph learning from observed samples is an NP-hard combinatorial problem. In this paper, we first show, for a set of important graph families it is possible to convert the combinatorial constraints of structure into eigenvalue constraints of the graph Laplacian matrix. Then we introduce a unified graph learning framework lying at the integration of the spectral properties of the Laplacian matrix with Gaussian graphical modeling, which is capable of learning structures of a large class of graph families. The proposed algorithms are provably convergent and practically amenable for big-data specific tasks. Extensive numerical experiments with both synthetic and real datasets demonstrate the effectiveness of the proposed methods. An R package containing codes for all the experimental results is submitted as a supplementary file.",[],[],"['Sandeep Kumar', 'Jiaxi Ying', 'Jose Vinicius de Miranda Cardoso', 'Daniel Palomar']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/947018640bf36a2bb609d3557a285329-Abstract.html,Transparency & Explainability,Causal Confusion in Imitation Learning,"Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive ""causal misidentification"" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.",[],[],"['Pim de Haan', 'Dinesh Jayaraman', 'Sergey Levine']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/95b431e51fc53692913da5263c214162-Abstract.html,Transparency & Explainability,Learning Compositional Neural Programs with Recursive Tree Search and Planning,"We propose a novel reinforcement learning algorithm, AlphaNPI, that incorpo-rates the strengths of Neural Programmer-Interpreters (NPI) and AlphaZero. NPIcontributes structural biases in the form of modularity, hierarchy and recursion,which are helpful to reduce sample complexity, improve generalization and in-crease interpretability. AlphaZero contributes powerful neural network guidedsearch algorithms, which we augment with recursion. AlphaNPI only assumesa hierarchical program specification with sparse rewards: 1 when the programexecution satisfies the specification, and 0 otherwise. This specification enablesus to overcome the need for strong supervision in the form of execution tracesand consequently train NPI models effectively with reinforcement learning. Theexperiments show that AlphaNPI can sort as well as previous strongly supervisedNPI variants. The AlphaNPI agent is also trained on a Tower of Hanoi puzzle withtwo disks and is shown to generalize to puzzles with an arbitrary number of disks.The experiments also show that when deploying our neural network policies, it isadvantageous to do planning with guided Monte Carlo tree search.",[],[],"['Thomas PIERROT', 'Guillaume Ligner', 'Scott E. Reed', 'Olivier Sigaud', 'Nicolas Perrin', 'Alexandre Laterre', 'David Kas', 'Karim Beguir', 'Nando de Freitas']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/a2186aa7c086b46ad4e8bf81e2a3a19b-Abstract.html,Transparency & Explainability,Learning Disentangled Representations for Recommendation,"User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users’ decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user’s preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.",[],[],"['Jianxin Ma', 'Chang Zhou', 'Peng Cui', 'Hongxia Yang', 'Wenwu Zhu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ac27b77292582bc293a51055bfc994ee-Abstract.html,Transparency & Explainability,Metamers of neural networks reveal divergence from human perceptual systems,"Deep neural networks have been embraced as models of sensory systems, instantiating representational transformations that appear to resemble those in the visual and auditory systems. To more thoroughly investigate their similarity to biological systems, we synthesized model metamers – stimuli that produce the same responses at some stage of a network’s representation. We generated model metamers for natural stimuli by performing gradient descent on a noise signal, matching the responses of individual layers of image and audio networks to a natural image or speech signal. The resulting signals reflect the invariances instantiated in the network up to the matched layer. We then measured whether model metamers were recognizable to human observers – a necessary condition for the model representations to replicate those of humans. Although model metamers from early network layers were recognizable to humans, those from deeper layers were not. Auditory model metamers became more human-recognizable with architectural modifications that reduced aliasing from pooling operations, but those from the deepest layers remained unrecognizable. We also used the metamer test to compare model representations. Cross-model metamer recognition dropped off for deeper layers, roughly at the same point that human recognition deteriorated, indicating divergence across model representations. The results reveal discrepancies between model and human representations, but also show how metamers can help guide model refinement and elucidate model representations.",[],[],"['Jenelle Feather', 'Alex Durango', 'Ray Gonzalez', 'Josh McDermott']","['Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology and McGovern Institute, Massachusetts Institute of Technology and Center for Brains Minds and Machines, Massachusetts Institute of Technology', 'Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology and McGovern Institute, Massachusetts Institute of Technology and Center for Brains Minds and Machines, Massachusetts Institute of Technology', 'Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology and McGovern Institute, Massachusetts Institute of Technology and Center for Brains Minds and Machines, Massachusetts Institute of Technology', 'Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology and McGovern Institute, Massachusetts Institute of Technology and Center for Brains Minds and Machines, Massachusetts Institute of Technology and Speech and Hearing Bioscience and Technology, Harvard University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html,Transparency & Explainability,This Looks Like That: Deep Learning for Interpretable Image Recognition,"When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.",[],[],"['Chaofan Chen', 'Oscar Li', 'Daniel Tao', 'Alina Barnett', 'Cynthia Rudin', 'Jonathan K. Su']","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'MIT Lincoln Laboratory', 'Duke University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/bbc12a3a98d8487f58a87d3a3070516e-Abstract.html,Transparency & Explainability,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks,"Recent efforts to improve the interpretability of deep neural networks use saliency to characterize the importance of input features to predictions made by models. Work on interpretability using saliency-based methods on Recurrent Neural Networks (RNNs) has mostly targeted language tasks, and their applicability to time series data is less understood. In this work we analyze saliency-based methods for RNNs, both classical and gated cell architectures. We show that RNN saliency vanishes over time, biasing detection of salient features only to later time steps and are, therefore, incapable of reliably detecting important features at arbitrary time intervals. To address this vanishing saliency problem, we propose a novel RNN cell structure (input-cell attention), which can extend any RNN cell architecture. At each time step, instead of only looking at the current input vector, input-cell attention uses a fixed-size matrix embedding, each row of the matrix attending to different inputs from current or previous time steps.  Using synthetic data, we show that the saliency map produced by the input-cell attention RNN is able to faithfully detect important features regardless of their occurrence in time. We also apply the input-cell attention RNN on a neuroscience task analyzing functional Magnetic Resonance Imaging (fMRI) data for human subjects performing a variety of tasks. In this case, we use saliency to characterize brain regions (input features) for which activity is important to distinguish between tasks. We show that standard RNN architectures are only capable of detecting important brain regions in the last few time steps of the fMRI data, while the input-cell attention model is able to detect important brain region activity across time without latter time step biases.",[],[],"['Aya Abdelsalam Ismail', 'Mohamed Gunady', 'Luiz Pessoa', 'Hector Corrada Bravo', 'Soheil Feizi']","['Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Psychology, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/c20a7ce2a627ba838cfbff082db35197-Abstract.html,Transparency & Explainability,Learning by Abstraction: The Neural State Machine,"We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we first predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model's strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efficacy of our approach.",[],[],"['Drew Hudson', 'Christopher D. Manning']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/cf34645d98a7630e2bcca98b3e29c8f2-Abstract.html,Transparency & Explainability,Self-attention with Functional Time Representation Learning,"Sequential modelling with self-attention has achieved cutting edge performances in natural language processing. With advantages in model flexibility, computation complexity and interpretability, self-attention is gradually becoming a key component in event sequence models. However, like most other sequence models, self-attention does not account for the time span between events and thus captures sequential signals rather than temporal patterns. Without relying on recurrent network structures, self-attention recognizes event orderings via positional encoding. To bridge the gap between modelling time-independent and time-dependent event sequence, we introduce a functional feature map that embeds time span into high-dimensional spaces. By constructing the associated translation-invariant time kernel function, we reveal the functional forms of the feature map under classic functional function analysis results, namely Bochner's Theorem and Mercer's Theorem. We propose several models to learn the functional time representation and the interactions with event representation. These methods are evaluated on real-world datasets under various continuous-time event sequence prediction tasks. The experiments reveal that the proposed methods compare favorably to baseline models while also capture useful time-event interactions.",[],[],"['Da Xu', 'Chuanwei Ruan', 'Evren Korpeoglu', 'Sushant Kumar', 'Kannan Achan']","['Walmart Labs, California, CA', 'Walmart Labs, California, CA', 'Walmart Labs, California, CA', 'Walmart Labs, California, CA', 'Walmart Labs, California, CA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d54e99a6c03704e95e6965532dec148b-Abstract.html,Transparency & Explainability,Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds,"Personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. Whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption via sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.",[],[],"['Nathan Kallus', 'Angela Zhou']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html,Transparency & Explainability,GNNExplainer: Generating Explanations for Graph Neural Networks,"Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs.GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models,and explaining predictions made by GNNs remains unsolved. Herewe propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNExplainer  can generate consistent and concise explanations for an entire class of instances.We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1% on average. GNNExplainer  provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.",[],[],"['Zhitao Ying', 'Dylan Bourgeois', 'Jiaxuan You', 'Marinka Zitnik', 'Jure Leskovec']","['Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University and Robust.AI', 'Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/dca5672ff3444c7e997aa9a2c4eb2094-Abstract.html,Transparency & Explainability,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components,"Abstract Neural networks are state-of-the-art classification approaches but are generally difficult to interpret. This issue can be partly alleviated by constructing a precise decision process within the neural network. In this work, a network architecture, denoted as Classification-By-Components network (CBC), is proposed. It is restricted to follow an intuitive reasoning based decision process inspired by Biederman's recognition-by-components theory from cognitive psychology. The network is trained to learn and detect generic components that characterize objects. In parallel, a class-wise reasoning strategy based on these components is learned to solve the classification problem. In contrast to other work on reasoning, we propose three different types of reasoning: positive, negative, and indefinite. These three types together form a probability space to provide a probabilistic classifier. The decomposition of objects into generic components combined with the probabilistic reasoning provides by design a clear interpretation of the classification decision process. The evaluation of the approach on MNIST shows that CBCs are viable classifiers. Additionally, we demonstrate that the inherent interpretability offers a profound understanding of the classification behavior such that we can explain the success of an adversarial attack. The method's scalability is successfully tested using the ImageNet dataset.",[],[],"['Sascha Saralajew', 'Lars Holdijk', 'Maike Rees', 'Ebubekir Asan', 'Thomas Villmann']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ddf354219aac374f1d40b7e760ee5bb7-Abstract.html,Transparency & Explainability,Learning Representations by Maximizing Mutual Information Across Views,"We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views – e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation.  This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.",[],[],"['Philip Bachman', 'R Devon Hjelm', 'William Buchwalter']","['Microsoft Research', 'Microsoft Research, MILA', 'Microsoft Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/e2db7186375992e729165726762cb4c1-Abstract.html,Transparency & Explainability,Untangling in Invariant Speech Recognition,"Encouraged by the success of deep convolutional neural networks on a variety of visual tasks, much theoretical and experimental work has been aimed at understanding and interpreting how vision networks operate.  At the same time, deep neural networks have also achieved impressive performance in audio processing applications, both as sub-components of larger systems and as complete end-to-end systems by themselves.  Despite their empirical successes, comparatively little is understood about how these audio models accomplish these tasks.In this work, we employ a recently developed statistical mechanical theory that connects geometric properties of network representations and the separability of classes to probe how information is untangled within neural networks trained to recognize speech.  We observe that speaker-specific nuisance variations are discarded by the network's hierarchy, whereas task-relevant properties such as words and phonemes are untangled in later layers. Higher level concepts such as parts-of-speech and context dependence also emerge in the later layers of the network. Finally, we find that the deep representations carry out significant temporal untangling by efficiently extracting task-relevant features at each time step of the computation.  Taken together, these findings shed light on how deep auditory models process their time dependent input signals to carry out invariant speech recognition, and show how different concepts emerge through the layers of the network.",[],[],"['Cory Stephenson', 'Jenelle Feather', 'Suchismita Padhy', 'Oguz Elibol', 'Hanlin Tang', 'Josh McDermott', 'SueYeon Chung']","['Intel AI Lab', 'MIT', 'Intel AI Lab', 'Intel AI Lab', 'Intel AI Lab', 'MIT/ Center for Brains, Minds, and Machines', 'Columbia University/ MIT']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/e6872f5bbe75073f8c7cfb93de7f6f3a-Abstract.html,Transparency & Explainability,Constraint-based Causal Structure Learning with Consistent Separating Sets,"We consider constraint-based methods for causal structure learning, such as the PC algorithm or any PC-derived algorithms whose ﬁrst step consists in pruning a complete graph to obtain an undirected graph skeleton, which is subsequently oriented. All constraint-based methods perform this ﬁrst step of removing dispensable edges, iteratively, whenever a separating set and corresponding conditional independence can be found. Yet, constraint-based methods lack robustness over sampling noise and are prone to uncover spurious conditional independences in ﬁnite datasets. In particular, there is no guarantee that the separating sets identiﬁed during the iterative pruning step remain consistent with the ﬁnal graph. In this paper, we propose a simple modiﬁcation of PC and PC-derived algorithms so as to ensure that all separating sets identiﬁed to remove dispensable edges are consistent with the ﬁnal graph,thus enhancing the explainability of constraint-basedmethods. It is achieved by repeating the constraint-based causal structure learning scheme, iteratively, while searching for separating sets that are consistent with the graph obtained at the previous iteration. Ensuring the consistency of separating sets can be done at a limited complexity cost, through the use of block-cut tree decomposition of graph skeletons, and is found to increase their validity in terms of actual d-separation. It also signiﬁcantly improves the sensitivity of constraint-based methods while retaining good overall structure learning performance. Finally and foremost, ensuring sepset consistency improves the interpretability of constraint-based models for real-life applications.",[],[],"['Honghao Li', 'Vincent Cabeli', 'Nadir Sella', 'Herve Isambert']","['Institut Curie, PSL Research University, CNRS UMR168, Paris', 'Institut Curie, PSL Research University, CNRS UMR168, Paris', 'Institut Curie, PSL Research University, CNRS UMR168, Paris', 'Institut Curie, PSL Research University, CNRS UMR168, Paris']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/fbefa505c8e8bf6d46f38f5277fed8d6-Abstract.html,Transparency & Explainability,Online Convex Matrix Factorization with Representative Regions,"Matrix factorization (MF) is a versatile learning method that has found wide applications in various data-driven disciplines. Still, many MF algorithms do not adequately scale with the size of available datasets and/or lack interpretability. To improve the computational efficiency of the method, an online (streaming) MF algorithm was proposed in Mairal et al., 2010. To enable data interpretability, a constrained version of MF, termed convex MF, was introduced in Ding et al., 2010. In the latter work, the basis vectors are required to lie in the convex hull of the data samples, thereby ensuring that every basis can be interpreted as a weighted combination of data samples. No current algorithmic solutions for online convex MF are known as it is challenging to find adequate convex bases without having access to the complete dataset. We address both problems by proposing the first online convex MF algorithm that maintains a collection of constant-size sets of representative data samples needed for interpreting each of the basis (Ding et al., 2010) and has the same almost sure convergence guarantees as the online learning algorithm of Mairal et al., 2010. Our proof techniques combine random coordinate descent algorithms with specialized quasi-martingale convergence analysis. Experiments on synthetic and real world datasets show significant computational savings of the proposed online convex MF method compared to classical convex MF. Since the proposed method maintains small representative sets of data samples needed for convex interpretations, it is related to a body of work in theoretical computer science, pertaining to generating point sets (Blum et al., 2016), and in computer vision, pertaining to archetypal analysis (Mei et al., 2018). Nevertheless, it differs from these lines of work both in terms of the objective and algorithmic implementations.",[],[],"['Jianhao Peng', 'Olgica Milenkovic', 'Abhishek Agarwal']","['Electrical and Computer Engineering, University of Illinois Urbana-Champaign', 'Electrical and Computer Engineering, University of Illinois Urbana-Champaign', 'Electrical and Computer Engineering, University of Illinois Urbana-Champaign']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html,Transparency & Explainability,A Benchmark for Interpretability Methods in Deep Neural Networks,"We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.",[],[],"['Sara Hooker', 'Dumitru Erhan', 'Pieter-Jan Kindermans', 'Been Kim']","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/0118a063b4aae95277f0bc1752c75abf-Abstract.html,Fairness & Bias,Multi-resolution Multi-task Gaussian Processes,"We consider evidence integration from potentially dependent observation processes under varying spatio-temporal sampling resolutions and noise levels. We offer a multi-resolution multi-task (MRGP) framework that allows for both inter-task and intra-task multi-resolution and multi-fidelity. We develop shallow Gaussian Process (GP) mixtures that approximate the difficult to estimate joint likelihood with a composite one and deep GP constructions that naturally handle biases. In doing so, we generalize existing approaches and offer information-theoretic corrections and efficient variational approximations. We demonstrate the competitiveness of MRGPs on synthetic settings and on the challenging problem of hyper-local estimation of air pollution levels across London from multiple sensing modalities operating at disparate spatio-temporal resolutions.",[],[],"['Oliver Hamelijnck', 'Theodoros Damoulas', 'Kangrui Wang', 'Mark Girolami']","['The Alan Turing Institute, Department of Computer Science, University of Warwick', 'The Alan Turing Institute, Depts. of Computer Science & Statistics, University of Warwick', 'The Alan Turing Institute, Department of Statistics, University of Warwick', 'The Alan Turing Institute, Department of Engineering, University of Cambridge']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/01d8bae291b1e4724443375634ccfa0e-Abstract.html,Fairness & Bias,Asymmetric Valleys: Beyond Sharp and Flat Local Minima,"Despite the non-convex nature of their loss functions, deep neural networks are known to generalize well when optimized with stochastic gradient descent (SGD). Recent work conjectures that SGD with proper conﬁguration is able to ﬁnd wide and ﬂat local minima, which are correlated with good generalization performance. In this paper, we observe that local minima of modern deep networks are more than being ﬂat or sharp. Instead, at a local minimum there exist many asymmetric directions such that the loss increases abruptly along one side, and slowly along the opposite side – we formally deﬁne such minima as asymmetric valleys. Under mild assumptions, we ﬁrst prove that for asymmetric valleys, a solution biased towards the ﬂat side generalizes better than the exact empirical minimizer. Then, we show that performing weight averaging along the SGD trajectory implicitly induces such biased solutions. This provides theoretical explanations for a series of intriguing phenomena observed in recent work [25, 5, 51]. Finally, extensive empirical experiments on both modern deep networks and simple 2 layer networks are conducted to validate our assumptions and analyze the intriguing properties of asymmetric valleys.",[],[],"['Haowei He', 'Gao Huang', 'Yang Yuan']","['Institute for Interdisciplinary Information Sciences, Tsinghua University', 'Department of Automation, Tsinghua University', 'Institute for Interdisciplinary Information Sciences, Tsinghua University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/05e97c207235d63ceb1db43c60db7bbb-Abstract.html,Fairness & Bias,Uniform convergence may be unable to explain generalization in deep learning,"Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning,  all  based on the fundamental learning-theoretic technique of uniform convergence. Whileit is well-known that many of these existing bounds are numerically large, through numerous experiments, we bring to light a more concerning aspect of these bounds: in practice,  these bounds can {\em increase} with the training dataset size. Guided by our observations,we then present examples of overparameterized linear classifiers and neural networks trained by  gradient descent (GD) where uniform convergence provably cannot ``explain generalization'' -- even if we take into account the implicit bias of GD {\em to the fullest extent possible}. More precisely, even if we consider only the set of classifiers output by GD, which have test errors less than some small $\epsilon$ in our settings, we show that applying (two-sided) uniform convergence on this set of classifiers will yield only a vacuous generalization guarantee larger than $1-\epsilon$. Through these findings,we cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well.",[],[],"['Vaishnavh Nagarajan', 'J. Zico Kolter']","['Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA', 'Department of Computer Science, Carnegie Mellon University & Bosch Center for Artificial Intelligence, Pittsburgh, PA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/07211688a0869d995947a8fb11b215d6-Abstract.html,Fairness & Bias,Reliable training and estimation of variance networks,"We propose and investigate new complementary methodologies for estimating predictive variance networks in regression neural networks. We derive a locally aware mini-batching scheme that results in sparse robust gradients, and we show how to make unbiased weight updates to a variance network. Further, we formulate a heuristic for robustly fitting both the mean and variance networks post hoc. Finally, we take inspiration from posterior Gaussian processes and propose a network architecture with similar extrapolation properties to Gaussian processes. The proposed methodologies are complementary, and improve upon baseline methods individually. Experimentally, we investigate the impact of predictive uncertainty on multiple datasets and tasks ranging from regression, active learning and generative modeling. Experiments consistently show significant improvements in predictive uncertainty estimation over state-of-the-art methods across tasks and datasets.",[],[],"['Nicki Skafte', 'Martin Jørgensen', 'Søren Hauberg']","['Section for Cognitive Systems, Technical University of Denmark', 'Section for Cognitive Systems, Technical University of Denmark', 'Section for Cognitive Systems, Technical University of Denmark']","['Denmark', 'Denmark', 'Denmark']"
https://papers.nips.cc/paper_files/paper/2019/hash/0d0fd7c6e093f7b804fa0150b875b868-Abstract.html,Fairness & Bias,Fast Low-rank Metric Learning for Large-scale and High-dimensional Data,"Low-rank metric learning aims to learn better discrimination of data subject to low-rank constraints. It keeps the intrinsic low-rank structure of datasets and reduces the time cost and memory usage in metric learning. However, it is still a challenge for current methods to handle datasets with both high dimensions and large numbers of samples. To address this issue, we present a novel fast low-rank metric learning (FLRML) method. FLRML casts the low-rank metric learning problem into an unconstrained optimization on the Stiefel manifold, which can be efficiently solved by searching along the descent curves of the manifold. FLRML significantly reduces the complexity and memory usage in optimization, which makes the method scalable to both high dimensions and large numbers of samples. Furthermore, we introduce a mini-batch version of FLRML to make the method scalable to larger datasets which are hard to be loaded and decomposed in limited memory. The outperforming experimental results show that our method is with high accuracy and much faster than the state-of-the-art methods under several benchmarks with large numbers of high-dimensional data. Code has been made available at https://github.com/highan911/FLRML.",[],[],"['Han Liu', 'Zhizhong Han', 'Yu-Shen Liu', 'Ming Gu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/0e095e054ee94774d6a496099eb1cf6a-Abstract.html,Fairness & Bias,Generalized Off-Policy Actor-Critic,"We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in the continuing reinforcement learning (RL) setting. Compared to the commonly used excursion objective, which can be misleading about the performance of the target policy when deployed, our new objective better predicts such performance. We prove the Generalized Off-Policy Policy Gradient Theorem to compute the policy gradient of the counterfactual objective and use an emphatic approach to get an unbiased sample from this policy gradient, yielding the Generalized Off-Policy Actor-Critic (Geoff-PAC) algorithm. We demonstrate the merits of Geoff-PAC over existing algorithms in Mujoco robot simulation tasks, the first empirical success of emphatic algorithms in prevailing deep RL benchmarks.",[],[],"['Shangtong Zhang', 'Wendelin Boehmer', 'Shimon Whiteson']","['Department of Computer Science, University of Oxford', 'Department of Computer Science, University of Oxford', 'Department of Computer Science, University of Oxford']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/0e1feae55e360ff05fef58199b3fa521-Abstract.html,Fairness & Bias,"Average Individual Fairness: Algorithms, Generalization and Experiments","We propose a new family of fairness definitions for classification problems that combine some of the best properties of both statistical and individual notions of fairness. We posit not only a distribution over individuals, but also a distribution over (or collection of) classification tasks. We then ask that standard statistics (such as error or false positive/negative rates) be (approximately) equalized across individuals, where the rate is defined as an expectation over the classification tasks. Because we are no longer averaging over coarse groups (such as race or gender), this is a semantically meaningful individual-level constraint. Given a sample of individuals and problems, we design an oracle-efficient algorithm (i.e. one that is given access to any standard, fairness-free learning heuristic) for the fair empirical risk minimization task. We also show that given sufficiently many samples, the ERM solution generalizes in two directions: both to new individuals, and to new classification tasks, drawn from their corresponding distributions. Finally we implement our algorithm and empirically verify its effectiveness.",[],[],"['Saeed Sharifi-Malvajerdi', 'Michael Kearns', 'Aaron Roth']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/124c3e4ada4a529aa0fedece80bb42ab-Abstract.html,Fairness & Bias,Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias,"Despite the phenomenal success of deep neural networks in a broad range of learning tasks, there is a lack of theory to understand the way they work. In particular, Convolutional Neural Networks (CNNs) are known to perform much better than Fully-Connected Networks (FCNs) on spatially structured data: the architectural structure of CNNs benefits from prior knowledge on the features of the data, for instance their translation invariance. The aim of this work is to understand this fact through the lens of dynamics in the loss landscape. We introduce a method that maps a CNN to its equivalent FCN (denoted as eFCN). Such an embedding enables the comparison of CNN and FCN training dynamics directly in the FCN space.We use this method to test a new training protocol, which consists in training a CNN, embedding it to FCN space at a certain ``relax time'', then resuming the training in FCN space. We observe that for all relax times, the deviation from the CNN subspace is small, and the final performance reached by the eFCN is higher than that reachable by a standard FCN of same architecture. More surprisingly, for some intermediate relax times, the eFCN outperforms the CNN it stemmed, by combining the prior information of the CNN and the expressivity of the FCN in a complementary way. The practical interest of our protocol is limited by the very large size of the highly sparse eFCN. However, it offers interesting insights into the persistence of architectural bias under stochastic gradient dynamics. It shows the existence of some rare basins in the FCN loss landscape associated with very good generalization. These can only be accessed thanks to the CNN prior, which helps navigate the landscape during the early stages of optimization.",[],[],"[""Stéphane d'Ascoli"", 'Levent Sagun', 'Giulio Biroli', 'Joan Bruna']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/149815eb972b3c370dee3b89d645ae14-Abstract.html,Fairness & Bias,Learning dynamic polynomial proofs,"Polynomial inequalities lie at the heart of many mathematical disciplines. In this paper, we consider the fundamental computational task of automatically searching for proofs of polynomial inequalities. We adopt the framework of semi-algebraic proof systems that manipulate polynomial inequalities via elementary inference rules that infer new inequalities from the premises. These proof systems are known to be very powerful, but searching for proofs remains a major difficulty. In this work, we introduce a machine learning based method to search for a dynamic proof within these proof systems. We propose a deep reinforcement learning framework that learns an embedding of the polynomials and guides the choice of inference rules, taking the inherent symmetries of the problem as an inductive bias. We compare our approach with powerful and widely-studied linear programming hierarchies based on static proof systems, and  show that our method reduces the size of the linear program by several orders of magnitude while also improving performance. These results hence pave the way towards augmenting powerful and well-studied semi-algebraic proof systems with machine learning guiding strategies for enhancing the expressivity of such proof systems.",[],[],"['Alhussein Fawzi', 'Mateusz Malinowski', 'Hamza Fawzi', 'Omar Fawzi']","['DeepMind', 'DeepMind', 'University of Cambridge', 'ENS Lyon']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html,Fairness & Bias,Poisson-Randomized Gamma Dynamical Systems,"This paper presents the Poisson-randomized gamma dynamical system (PRGDS), a model for sequentially observed count tensors that encodes a strong inductive bias toward sparsity and burstiness. The PRGDS is based on a new motif in Bayesian latent variable modeling, an alternating chain of discrete Poisson and continuous gamma latent states that is analytically convenient and computationally tractable. This motif yields closed-form complete conditionals for all variables by way of the Bessel distribution and a novel discrete distribution that we call the shifted confluent hypergeometric distribution. We draw connections to closely related models and compare the PRGDS to these models in studies of real-world count data sets of text, international events, and neural spike trains. We find that a sparse variant of the PRGDS, which allows the continuous gamma latent states to take values of exactly zero, often obtains better predictive performance than other models and is uniquely capable of inferring latent structures that are highly localized in time.",[],[],"['Aaron Schein', 'Scott Linderman', 'Mingyuan Zhou', 'David Blei', 'Hanna Wallach']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/16105fb9cc614fc29e1bda00dab60d41-Abstract.html,Fairness & Bias,RUDDER: Return Decomposition for Delayed Rewards,"We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD(λ), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards.",[],[],"['Jose A. Arjona-Medina', 'Michael Gillhofer', 'Michael Widrich', 'Thomas Unterthiner', 'Johannes Brandstetter', 'Sepp Hochreiter']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1a638db8311430c6c018bf21e1a0b7fb-Abstract.html,Fairness & Bias,Generalization Error Analysis of Quantized Compressive Learning,"Compressive learning is an effective method to deal with very high dimensional datasets by applying learning algorithms in a randomly projected lower dimensional space. In this paper, we consider the learning problem where the projected data is further compressed by scalar quantization, which is called quantized compressive learning. Generalization error bounds are derived for three models: nearest neighbor (NN) classifier, linear classifier and least squares regression. Besides studying finite sample setting, our asymptotic analysis shows that the inner product estimators have deep connection with NN and linear classification problem through the variance of their debiased counterparts. By analyzing the extra error term brought by quantization, our results provide useful implications to the choice of quantizers in applications involving different learning tasks. Empirical study is also conducted to validate our theoretical findings.",[],[],"['Xiaoyun Li', 'Ping Li']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1b486d7a5189ebe8d8c46afc64b0d1b4-Abstract.html,Fairness & Bias,On the Fairness of Disentangled Representations,"Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations.We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable.We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent.Analyzing the representations of more than 12600 trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.",[],[],"['Francesco Locatello', 'Gabriele Abbati', 'Thomas Rainforth', 'Stefan Bauer', 'Bernhard Schölkopf', 'Olivier Bachem']","['Dept. of Computer Science, ETH Zurich and Max-Planck Institute for Intelligent Systems', 'Dept. of Engineering Science, University of Oxford', 'Dept. of Statistics, University of Oxford', 'Max-Planck Institute for Intelligent Systems', 'Max-Planck Institute for Intelligent Systems', 'Google Research, Brain Team']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html,Fairness & Bias,Calibration tests in multi-class classification: A unifying framework,"In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale.",[],[],"['David Widmann', 'Fredrik Lindsten', 'Dave Zachariah']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1cc8a8ea51cd0adddf5dab504a285915-Abstract.html,Fairness & Bias,Accurate Uncertainty Estimation and Decomposition in Ensemble Learning,"Ensemble learning is a standard approach to building machine learning systems that capture complex phenomena in real-world data. An important aspect of these systems is the complete and valid quantification of model uncertainty. We introduce a Bayesian nonparametric ensemble (BNE) approach that augments an existing ensemble model to account for different sources of model uncertainty. BNE augments a model’s prediction and distribution functions using Bayesian nonparametric machinery. It has a theoretical guarantee in that it robustly estimates the uncertainty patterns in the data distribution, and can decompose its overall predictive uncertainty into distinct components that are due to different sources of noise and error. We show that our method achieves accurate uncertainty estimates under complex observational noise, and illustrate its real-world utility in terms of uncertainty decomposition and model bias detection for an ensemble in predict air pollution exposures in Eastern Massachusetts, USA.",[],[],"['Jeremiah Liu', 'John Paisley', 'Marianthi-Anna Kioumourtzoglou', 'Brent Coull']","['Google Research & Harvard University', 'Columbia University', 'Columbia University', 'Harvard University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1d54c76f48f146c3b2d66daf9d7f845e-Abstract.html,Fairness & Bias,On two ways to use determinantal point processes for Monte Carlo integration,"When approximating an integral by a weighted sum of function evaluations, determinantal point processes (DPPs) provide a way to enforce repulsion between the evaluation points.This negative dependence is encoded by a kernel.Fifteen years before the discovery of DPPs, Ermakov & Zolotukhin (EZ, 1960) had the intuition of sampling a DPP and solving a linear system to compute an unbiased Monte Carlo estimator of the integral.In the absence of DPP machinery to derive an efficient sampler and analyze their estimator, the idea of Monte Carlo integration with DPPs was stored in the cellar of numerical integration. Recently, Bardenet & Hardy (BH, 2019) came up with a more natural estimator with a fast central limit theorem (CLT).In this paper, we first take the EZ estimator out of the cellar, and analyze it using modern arguments.Second, we provide an efficient implementation to sample exactly a particular multidimensional DPP called multivariate Jacobi ensemble.The latter satisfies the assumptions of the aforementioned CLT. Third, our new implementation lets us investigate the behavior of the two unbiased Monte Carlo estimators in yet unexplored regimes.We demonstrate experimentally good properties when the kernel is adapted to basis of functions in which the integrand is sparse or has fast-decaying coefficients.If such a basis and the level of sparsity are known (e.g., we integrate a linear combination of kernel eigenfunctions), the EZ estimator can be the right choice, but otherwise it can display an erratic behavior.",[],[],"['Guillaume Gautier', 'Rémi Bardenet', 'Michal Valko']","[""Univ. Lille, CNRS, Centrale Lille, UMR 9189 – CRIStAL, Villeneuve d'Ascq, France and Inria Lille-Nord Europe, Villeneuve d'Ascq, France"", ""Univ. Lille, CNRS, Centrale Lille, UMR 9189 – CRIStAL, Villeneuve d'Ascq, France"", ""DeepMind Paris, Paris, France and Inria Lille-Nord Europe, Villeneuve d'Ascq, France and Univ. Lille, CNRS, Centrale Lille, UMR 9189 – CRIStAL, Villeneuve d'Ascq, France""]","['France', 'France', 'France']"
https://papers.nips.cc/paper_files/paper/2019/hash/1d7c2aae840867027b7edd17b6aaa0e9-Abstract.html,Fairness & Bias,Exploring Algorithmic Fairness in Robust Graph Covering Problems,"Fueled by algorithmic advances, AI algorithms are increasingly being deployed in settings subject to unanticipated challenges with complex social effects. Motivated by real-world deployment of AI driven, social-network based suicide prevention and landslide risk management interventions, this paper focuses on a robust graph covering problem subject to group fairness constraints. We show that, in the absence of fairness constraints, state-of-the-art algorithms for the robust graph covering problem result in biased node coverage: they tend to discriminate individuals (nodes) based on membership in traditionally marginalized groups. To remediate this issue, we propose a novel formulation of the robust covering problem with fairness constraints and a tractable approximation scheme applicable to real world instances. We provide a formal analysis of the price of group fairness (PoF) for this problem, where we show that uncertainty can lead to greater PoF. We demonstrate the effectiveness of our approach on several real-world social networks. Our method yields competitive node coverage while significantly improving group fairness relative to state-of-the-art methods.",[],[],"['Aida Rahmattalabi', 'Phebe Vayanos', 'Anthony Fulginiti', 'Eric Rice', 'Bryan Wilder', 'Amulya Yadav', 'Milind Tambe']","['University of Southern California', 'University of Southern California', 'University of Denver', 'University of Southern California', 'Harvard University', 'Pennsylvania State University', 'Harvard University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1dba5eed8838571e1c80af145184e515-Abstract.html,Fairness & Bias,Reducing the variance in online optimization by transporting past gradients,"Most stochastic optimization methods use gradients once before discarding them. While variance reduction methods have shown that reusing past gradients can be beneficial when there is a finite number of datapoints, they do not easily extend to the online setting. One issue is the staleness due to using past gradients. We propose to correct this staleness using the idea of {\em implicit gradient transport} (IGT) which transforms gradients computed at previous iterates into gradients evaluated at the current iterate without using the Hessian explicitly. In addition to reducing the variance and bias of our updates over time, IGT can be used as a drop-in replacement for the gradient estimate in a number of well-understood methods such as heavy ball or Adam. We show experimentally that it achieves state-of-the-art results on a wide range of architectures and benchmarks. Additionally, the IGT gradient estimator yields the optimal asymptotic convergence rate for online stochastic optimization in the restricted setting where the Hessians of all component functions are equal.",[],[],"['Sébastien Arnold', 'Pierre-Antoine Manzagol', 'Reza Babanezhad Harikandeh', 'Ioannis Mitliagkas', 'Nicolas Le Roux']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1fa6269f58898f0e809575c9a48747ef-Abstract.html,Fairness & Bias,TAB-VCR: Tags and Attributes based VCR Baselines,"Reasoning is an important ability that we learn from a very early age. Yet, reasoning is extremely hard for algorithms. Despite impressive recent progress that has been reported on tasks that necessitate reasoning, such as visual question answering and visual dialog, models often exploit biases in datasets.  To develop models with better reasoning abilities, recently, the new visual commonsense reasoning(VCR) task has been introduced. Not only do models have to answer questions, but also do they have to provide a reason for the given answer.  The proposed baseline achieved compelling results, leveraging a meticulously designed model composed of LSTM modules and attention nets. Here we show that a much simpler model obtained by ablating and pruning the existing intricate baseline can perform better with half the number of trainable parameters. By associating visual features with attribute information and better text to image grounding, we obtain further improvements for our simpler & effective baseline, TAB-VCR. We show that this approach results in a 5.3%, 4.4% and 6.5% absolute improvement over the previous state-of-the-art on question answering, answer justification and holistic VCR. Webpage: https://deanplayerljx.github.io/tabvcr/",[],[],"['Jingxiang Lin', 'Unnat Jain', 'Alexander Schwing']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/201d546992726352471cfea6b0df0a48-Abstract.html,Fairness & Bias,Assessing Social and Intersectional Biases in Contextualized Word Representations,"Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.",[],[],"['Yi Chern Tan', 'L. Elisa Celis']","['Yale University', 'Yale University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/21b29648a47a45ad16bb0da0c004dfba-Abstract.html,Fairness & Bias,Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond,"Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.",[],[],"['Lin Chen', 'Hossein Esfandiari', 'Gang Fu', 'Vahab Mirrokni']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/26cd8ecadce0d4efd6cc8a8725cbd1f8-Abstract.html,Fairness & Bias,Hamiltonian Neural Networks,"Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.",[],[],"['Samuel Greydanus', 'Misko Dzamba', 'Jason Yosinski']","['Google Brain', 'PetCube', 'Uber AI Labs']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/28f7241796510e838db4a1384ae1279d-Abstract.html,Fairness & Bias,Detecting Overfitting via Adversarial Examples,"The repeated community-wide reuse of test sets in popular benchmark problems raises doubts about the credibility of  reported test-error rates. Verifying whether a learned model is overfitted to a test set is challenging as independent test sets drawn from the same data distribution are usually unavailable, while other test sets may introduce a distribution shift. We propose a new hypothesis test that uses only the original test data to detect overfitting. It utilizes a new unbiased error estimate that is based on adversarial examples generated from the test data and importance weighting. Overfitting is detected if this error estimate is sufficiently different from the original test error rate. We develop a specialized variant of our test for multiclass image classification, and apply it to testing overfitting of recent models to the popular ImageNet benchmark. Our method correctly indicates overfitting of the trained model to the training set, but is not able to detect any overfitting to the test set, in line with other recent work on this topic.",[],[],"['Roman Werpachowski', 'András György', 'Csaba Szepesvari']","['DeepMind, London, UK', 'DeepMind, London, UK', 'DeepMind, London, UK']","['UK', 'UK', 'UK']"
https://papers.nips.cc/paper_files/paper/2019/hash/2b8501af7b64d1aaae7dd832805f0709-Abstract.html,Fairness & Bias,Inducing brain-relevant bias in natural language processing models,"Progress in natural language processing (NLP) models that estimate representations of word sequences has recently been leveraged to improve the understanding of language processing in the brain.  However, these models have not been specifically designed to capture the way the brain represents language meaning. We hypothesize that fine-tuning these models to predict recordings of brain activity of people reading text will lead to representations that encode more brain-activity-relevant language information. We demonstrate that a version of BERT, a recently introduced and powerful language model, can improve the prediction of brain activity after fine-tuning. We show that the relationship between language and brain activity learned by BERT during this fine-tuning transfers across multiple participants. We also show that, for some participants, the fine-tuned representations learned from both magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) are better for predicting fMRI than the representations learned from fMRI alone, indicating that the learned representations capture brain-activity-relevant information that is not simply an artifact of the modality. While changes to language representations help the model predict brain activity, they also do not harm the model's ability to perform downstream NLP tasks. Our findings are notable for research on language understanding in the brain.",[],[],"['Dan Schwartz', 'Mariya Toneva', 'Leila Wehbe']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/2c463dfdde588f3bfc60d53118c10d6b-Abstract.html,Fairness & Bias,Implicit Posterior Variational Inference for Deep Gaussian Processes,"A multi-layer deep Gaussian process (DGP) model is a hierarchical composition of GP models with a greater expressive power. Exact DGP inference is intractable, which has motivated the recent development of deterministic and stochastic approximation methods. Unfortunately, the deterministic approximation methods yield a biased posterior belief while the stochastic one is computationally costly. This paper presents an implicit posterior variational inference (IPVI) framework for DGPs that can ideally recover an unbiased posterior belief and still preserve time efficiency. Inspired by generative adversarial networks, our IPVI framework achieves this by casting the DGP inference problem as a two-player game in which a Nash equilibrium, interestingly, coincides with an unbiased posterior belief. This consequently inspires us to devise a best-response dynamics algorithm to search for a Nash equilibrium (i.e., an unbiased posterior belief). Empirical evaluation shows that IPVI outperforms the state-of-the-art approximation methods for DGPs.",[],[],"['Haibin YU', 'Yizhou Chen', 'Bryan Kian Hsiang Low', 'Patrick Jaillet', 'Zhongxiang Dai']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/2f4fe03d77724a7217006e5d16728874-Abstract.html,Fairness & Bias,Understanding and Improving Layer Normalization,"Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm.  Many of previous studies believe that the success of LayerNorm comes from  forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version  of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm  on seven out of eight datasets.",[],[],"['Jingjing Xu', 'Xu Sun', 'Zhiyuan Zhang', 'Guangxiang Zhao', 'Junyang Lin']","['MOE Key Lab of Computational Linguistics, School of EECS, Peking University', 'MOE Key Lab of Computational Linguistics, School of EECS, Peking University and Center for Data Science, Peking University', 'MOE Key Lab of Computational Linguistics, School of EECS, Peking University', 'Center for Data Science, Peking University', 'MOE Key Lab of Computational Linguistics, School of EECS, Peking University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/3335881e06d4d23091389226225e17c7-Abstract.html,Fairness & Bias,The Implicit Bias of AdaGrad on Separable Data,We study the implicit bias of AdaGrad on separable linear classification problems. We show that AdaGrad converges to a direction that can be  characterized as the solution of a quadratic optimization problem with the same feasible set as the hard SVM problem. We also give a discussion about how different choices of the hyperparameters of AdaGrad may impact this direction. This provides a deeper understanding of why adaptive methods do not seem to have the generalization ability as good as gradient descent does in practice.,[],[],"['Qian Qian', 'Xiaoyuan Qian']","['Department of Statistics, Ohio State University, Columbus, OH', 'School of Mathematical Sciences, Dalian University of Technology, Dalian, Liaoning, China']",['China']
https://papers.nips.cc/paper_files/paper/2019/hash/342285bb2a8cadef22f667eeb6a63732-Abstract.html,Fairness & Bias,Debiased Bayesian inference for average treatment effects,"Bayesian approaches have become increasingly popular in causal inference problems due to their conceptual simplicity, excellent performance and in-built uncertainty quantification ('posterior credible sets'). We investigate Bayesian inference for average treatment effects from observational data, which is a challenging problem due to the missing counterfactuals and selection bias. Working in the standard potential outcomes framework, we propose a data-driven modification to an arbitrary (nonparametric) prior based on the propensity score that corrects for the first-order posterior bias, thereby improving performance. We illustrate our method for Gaussian process (GP) priors using (semi-)synthetic data. Our experiments demonstrate significant improvement in both estimation accuracy and uncertainty quantification compared to the unmodified GP, rendering our approach highly competitive with the state-of-the-art.",[],[],"['Kolyan Ray', 'Botond Szabo']","[""Department of Mathematics, King's College London"", 'Mathematical Institute, Leiden University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/3493894fa4ea036cfc6433c3e2ee63b0-Abstract.html,Fairness & Bias,Explicit Disentanglement of Appearance and Perspective in Generative Models,"Disentangled representation learning finds compact, independent and easy-to-interpret factors of the data.Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Specifically, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and another that represents the transformed data. We find that the latter naturally captures the intrinsic appearance of the data. To realize the generative model, we propose a Variationally Inferred Transformational Autoencoder (VITAE) that incorporates a spatial ransformer into a variational autoencoder.  We show how to perform inference in the model efficiently by carefully designing the encoders and restricting the transformation class to be diffeomorphic. Empirically, our model separates the visual style from digit type on MNIST, separates shape and pose in images of human bodies and facial features from facial shape on CelebA.",[],[],"['Nicki Skafte', 'Søren Hauberg']","['Section for Cognitive Systems, Technical University of Denmark', 'Section for Cognitive Systems, Technical University of Denmark']","['Denmark', 'Denmark']"
https://papers.nips.cc/paper_files/paper/2019/hash/361440528766bbaaaa1901845cf4152b-Abstract.html,Fairness & Bias,Poisson-Minibatching for Gibbs Sampling with Convergence Rate Guarantees,"Gibbs sampling is a Markov chain Monte Carlo method that is often used for learning and inference on graphical models.Minibatching, in which a small random subset of the graph is used at each iteration, can help make Gibbs sampling scale to large graphical models by reducing its computational cost.In this paper, we propose a new auxiliary-variable minibatched Gibbs sampling method, {\it Poisson-minibatching Gibbs}, which both produces unbiased samples and has a theoretical guarantee on its convergence rate. In comparison to previous minibatched Gibbs algorithms, Poisson-minibatching Gibbs supports fast sampling from continuous state spaces and avoids the need for a Metropolis-Hastings correction on discrete state spaces.We demonstrate the effectiveness of our method on multiple applications and in comparison with both plain Gibbs and previous minibatched methods.",[],[],"['Ruqi Zhang', 'Christopher M. De Sa']","['Cornell University', 'Cornell University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/373e4c5d8edfa8b74fd4b6791d0cf6dc-Abstract.html,Fairness & Bias,Unlocking Fairness: a Trade-off Revisited,"The prevailing wisdom is that a model's fairness and its accuracy  are in tension with one another.  However, there is a pernicious  {\em modeling-evaluating dualism} bedeviling fair machine learning  in which phenomena such as label bias are appropriately acknowledged  as a source of unfairness when designing fair models,  only to be tacitly abandoned when evaluating them.  We investigate  fairness and accuracy, but this time under a variety of controlled  conditions in which we vary the amount and type of bias.  We find,  under reasonable assumptions, that the tension between fairness and  accuracy is illusive, and vanishes as soon as we account for these  phenomena during evaluation.  Moreover, our results are consistent  with an opposing conclusion: fairness and accuracy are sometimes in  accord.  This raises the question, {\em might there be a way to    harness fairness to improve accuracy after all?}  Since most  notions of fairness are with respect to the model's predictions and  not the ground truth labels, this provides an opportunity to see if  we can improve accuracy by harnessing appropriate notions of  fairness over large quantities of {\em unlabeled} data with  techniques like posterior regularization and generalized  expectation.  Indeed, we find that semi-supervision not only  improves fairness, but also accuracy and has advantages over  existing in-processing methods that succumb to selection bias on the  training set.",[],[],"['Michael Wick', 'swetasudha panda', 'Jean-Baptiste Tristan']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/37693cfc748049e45d87b8c7d8b9aacd-Abstract.html,Fairness & Bias,Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers,"In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity. Alternatively, we propose stochastically shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.",[],[],"['Liwei Wu', 'Shuqing Li', 'Cho-Jui Hsieh', 'James L. Sharpnack']","['Department of Statistics, University of California, Davis, Davis, CA', 'Department of Computer Science, University of California, Davis, Davis, CA', 'Department of Computer Science, University of California, Los Angles, Los Angles, CA', 'Department of Statistics, University of California, Davis, Davis, CA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/39d929972619274cc9066307f707d002-Abstract.html,Fairness & Bias,Function-Space Distributions over Kernels,"Gaussian processes are flexible function approximators, with inductive biases controlled by a covariance kernel. Learning the kernel is the key to representation learning and strong predictive performance. In this paper, we develop functional kernel learning (FKL) to directly infer functional posteriors over kernels. In particular, we place a transformed Gaussian process over a spectral density, to induce a non-parametric distribution over kernel functions. The resulting approach enables learning of rich representations, with support for any stationary kernel, uncertainty over the values of the kernel, and an interpretable specification of a prior directly over kernels, without requiring sophisticated initialization or manual intervention. We perform inference through elliptical slice sampling, which is especially well suited to marginalizing posteriors with the strongly correlated priors typical to function space modeling. We develop our approach for non-uniform, large-scale, multi-task, and multidimensional data, and show promising performance in a wide range of settings, including interpolation, extrapolation, and kernel recovery experiments.",[],[],"['Gregory Benton', 'Wesley J. Maddox', 'Jayson Salkey', 'Julio Albinati', 'Andrew Gordon Wilson']","['Courant Institute of Mathematical Sciences, New York University', 'Center for Data Science, New York University', 'Courant Institute of Mathematical Sciences, New York University', 'Microsoft', 'Courant Institute of Mathematical Sciences, New York University and Center for Data Science, New York University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/3cc697419ea18cc98d525999665cb94a-Abstract.html,Fairness & Bias,On Fenchel Mini-Max Learning,"Inference, estimation, sampling and likelihood evaluation are four primary goals of probabilistic modeling. Practical considerations often force modeling approaches to make compromises between these objectives. We present a novel probabilistic learning framework, called Fenchel Mini-Max Learning (FML), that accommodates all four desiderata in a flexible and scalable manner. Our derivation is rooted in classical maximum likelihood estimation, and it overcomes a longstanding challenge that prevents unbiased estimation of unnormalized statistical models. By reformulating MLE as a mini-max game, FML enjoys an unbiased training objective that (i) does not explicitly involve the intractable normalizing constant and (ii) is directly amendable to stochastic gradient descent optimization. To demonstrate the utility of the proposed approach, we consider learning unnormalized statistical models, nonparametric density estimation and training generative models, with encouraging empirical results presented.",[],[],"['Chenyang Tao', 'Liqun Chen', 'Shuyang Dai', 'Junya Chen', 'Ke Bai', 'Dong Wang', 'Jianfeng Feng', 'Wenlian Lu', 'Georgiy Bobashev', 'Lawrence Carin']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/3de568f8597b94bda53149c7d7f5958c-Abstract.html,Fairness & Bias,Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints,"Inverse reinforcement learning (IRL) enables an agent to learn complex behavior by observing demonstrations from a (near-)optimal policy. The typical assumption is that the learner's goal is to match the teacher’s demonstrated behavior. In this paper, we consider the setting where the learner has its own preferences that it additionally takes into consideration. These preferences can for example capture behavioral biases, mismatched worldviews, or physical constraints. We study two teaching approaches: learner-agnostic teaching, where the teacher provides demonstrations from an optimal policy ignoring the learner's preferences, and learner-aware teaching, where the teacher accounts for the learner’s preferences. We design learner-aware teaching algorithms and show that significant performance improvements can be achieved over learner-agnostic teaching.",[],[],"['Sebastian Tschiatschek', 'Ahana Ghosh', 'Luis Haug', 'Rati Devidze', 'Adish Singla']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html,Fairness & Bias,Defending Against Neural Fake News,"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",[],[],"['Rowan Zellers', 'Ari Holtzman', 'Hannah Rashkin', 'Yonatan Bisk', 'Ali Farhadi', 'Franziska Roesner', 'Yejin Choi']","['Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington and Allen Institute for Artificial Intelligence', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington and Allen Institute for Artificial Intelligence']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/42547f5a44d87da3bc40ee5d09624606-Abstract.html,Fairness & Bias,Graph-Based Semi-Supervised Learning with Non-ignorable Non-response,"Graph-based semi-supervised learning is a very powerful tool in classification tasks, while in most existing literature the labelled nodes are assumed to be randomly sampled. When the labelling status depends on the unobserved node response, ignoring the missingness can lead to significant estimation bias and handicap the classifiers. This situation is called non-ignorable non-response. To solve the problem, we propose a Graph-based joint model with Non-ignorable Non-response (GNN), followed by a joint inverse weighting estimation procedure incorporated with sampling imputation approach. Our method is proved to outperform some state-of-art models in both regression and classification problems, by simulations and real analysis on the Cora dataset.",[],[],"['Fan Zhou', 'Tengfei Li', 'Haibo Zhou', 'Hongtu Zhu', 'Ye Jieping']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html,Fairness & Bias,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,"A recent trend of fair machine learning is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However, one common challenge of all causality-based fairness notions is identifiability, i.e., whether they can be uniquely measured from observational data, which is a critical barrier to applying these notions to real-world situations. In this paper, we develop a framework for measuring different causality-based fairness. We propose a unified definition that covers most of previous causality-based fairness notions, namely the path-specific counterfactual fairness (PC fairness). Based on that, we propose a general method in the form of a constrained optimization problem for bounding the path-specific counterfactual fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method.",[],[],"['Yongkai Wu', 'Lu Zhang', 'Xintao Wu', 'Hanghang Tong']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/46d0671dd4117ea366031f87f3aa0093-Abstract.html,Fairness & Bias,A Model to Search for Synthesizable Molecules,"Deep generative models are able to suggest new organic molecules by generating strings, trees, and graphs representing their structure. While such models allow one to generate molecules with desirable properties, they give no guarantees that the molecules can actually be synthesized in practice. We propose a new molecule generation model, mirroring a more realistic real-world process, where (a) reactants are selected, and (b) combined to form more complex molecules. More specifically, our generative model proposes a bag of initial reactants (selected from a pool of commercially-available molecules) and uses a reaction model to predict how they react together to generate new molecules. We first show that the model can generate diverse, valid and unique molecules due to the useful inductive biases of modeling reactions. Furthermore, our model allows chemists to interrogate not only the properties of the generated molecules but also the feasibility of the synthesis routes. We conclude by using our model to solve retrosynthesis problems, predicting a set of reactants that can produce a target product.",[],[],"['John Bradshaw', 'Brooks Paige', 'Matt J. Kusner', 'Marwin Segler', 'José Miguel Hernández-Lobato']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/4d5b995358e7798bc7e9d9db83c612a5-Abstract.html,Fairness & Bias,Extending Stein's unbiased risk estimator to train deep denoisers with correlated pairs of noisy images,"Recently, Stein's unbiased risk estimator (SURE) has been applied to unsupervised training of deep neural network Gaussian denoisers that outperformed classical non-deep learning based denoisers and yielded comparable performance to those trained with ground truth. While SURE requires only one noise realization per image for training, it does not take advantage of having multiple noise realizations per image when they are available (e.g., two uncorrelated noise realizations per image for Noise2Noise). Here, we propose an extended SURE (eSURE) to train deep denoisers with correlated pairs of noise realizations per image and applied it to the case with two uncorrelated realizations per image to achieve better performance than SURE based method and comparable results to Noise2Noise. Then, we further investigated the case with imperfect ground truth (i.e., mild noise in ground truth) that may be obtained considering painstaking, time-consuming, and even expensive processes of collecting ground truth images with multiple noisy images. For the case of generating noisy training data by adding synthetic noise to imperfect ground truth to yield correlated pairs of images, our proposed eSURE based training method outperformed conventional SURE based method as well as Noise2Noise. Code is available at https://github.com/Magauiya/Extended_SURE",[],[],"['Magauiya Zhussip', 'Shakarim Soltanayev', 'Se Young Chun']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/4fdaa19b1f22a4d926fce9bfc7c61fa5-Abstract.html,Fairness & Bias,Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation,"Discovery of causal relations from observational data is essential for many disciplines of science and real-world applications. However, unlike other machine learning algorithms, whose development has been greatly fostered by a large amount of available benchmark datasets, causal discovery algorithms are notoriously difficult to be systematically evaluated because few datasets with known ground-truth causal relations are available. In this work, we handle the problem of evaluating causal discovery algorithms by building a flexible simulator in the medical setting. We develop a neuropathic pain diagnosis simulator, inspired by the fact that the biological processes of neuropathic pathophysiology are well studied with well-understood causal influences. Our simulator exploits the causal graph of the neuropathic pain pathology and its parameters in the generator are estimated from real-life patient cases. We show that the data generated from our simulator have similar statistics as real-world data. As a clear advantage, the simulator can produce infinite samples without jeopardizing the privacy of real-world patients. Our simulator provides a natural tool for evaluating various types of causal discovery algorithms, including those to deal with practical issues in causal discovery, such as unknown confounders, selection bias, and missing data. Using our simulator, we have evaluated extensively causal discovery algorithms under various settings.",[],[],"['Ruibo Tu', 'Kun Zhang', 'Bo Bertilson', 'Hedvig Kjellstrom', 'Cheng Zhang']","['KTH Royal Institute of Technology', 'Carnegie Mellon University', 'Karolinska Institute', 'KTH Royal Institute of Technology', 'Microsoft Research, Cambridge']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/51d92be1c60d1db1d2e5e7a07da55b26-Abstract.html,Fairness & Bias,RUBi: Reducing Unimodal Biases for Visual Question Answering,"Visual Question Answering (VQA) is the task of answering questions about animage.Some VQA models often exploit unimodal biases to provide the correct answer without using the image information.As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings.We propose RUBi, a new learning strategy to reduce biases in any VQA model.It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer.We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used.It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training.",[],[],"['Remi Cadene', 'Corentin Dancette', 'Hedi Ben younes', 'Matthieu Cord', 'Devi Parikh']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/5ac8bb8a7d745102a978c5f8ccdb61b8-Abstract.html,Fairness & Bias,The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies,"We study the relationship between the frequency of a function and the speed at which a neural network learns it.  We build on recent results that show that the dynamics of overparameterized neural networks trained with gradient descent can be well approximated by a linear system.  When normalized training data is uniformly distributed on a hypersphere, the eigenfunctions of this linear system are spherical harmonic functions.  We derive the corresponding eigenvalues for each frequency after introducing a bias term in the model.  This bias term had been omitted from the linear network model without significantly affecting previous theoretical results.  However, we show theoretically and experimentally that a shallow neural network without bias cannot represent or learn simple, low frequency functions with odd frequencies.  Our results lead to specific predictions of the time it will take a network to learn functions of varying frequency.  These predictions match the empirical behavior of both shallow and deep networks.",[],[],"['Basri Ronen', 'David Jacobs', 'Yoni Kasten', 'Shira Kritchman']","['Department of Computer Science, Weizmann Institute of Science, Rehovot, Israel', 'Department of Computer Science, University of Maryland, College Park, MD', 'Department of Computer Science, Weizmann Institute of Science, Rehovot, Israel', 'Department of Computer Science, Weizmann Institute of Science, Rehovot, Israel']","['Israel', 'Israel', 'Israel']"
https://papers.nips.cc/paper_files/paper/2019/hash/5c48ff18e0a47baaf81d8b8ea51eec92-Abstract.html,Fairness & Bias,Search on the Replay Buffer: Bridging Planning and Reinforcement Learning,"The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and relative values of states, but fails to plan over long horizons. Despite the successes of each method on various tasks, long horizon, sparse reward tasks with high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid injecting bias through reward shaping. We introduce a general-purpose control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our main idea is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a particular subgoal. We use goal-conditioned RL to learn a policy to reach each waypoint and to learn a distance metric for search. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over hundreds of steps, and generalizes substantially better than standard RL algorithms.",[],[],"['Ben Eysenbach', 'Russ R. Salakhutdinov', 'Sergey Levine']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/5d0d5594d24f0f955548f0fc0ff83d10-Abstract.html,Fairness & Bias,Residual Flows for Invertible Generative Modeling,"Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density, and reduce the memory required during training by a factor of ten. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid gradient saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.",[],[],"['Ricky T. Q. Chen', 'Jens Behrmann', 'David K. Duvenaud', 'Joern-Henrik Jacobsen']","['University of Toronto and Vector Institute', 'University of Bremen', 'University of Toronto and Vector Institute', 'University of Toronto and Vector Institute']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/5d2c2cee8ab0b9a36bd1ed7196bd6c4a-Abstract.html,Fairness & Bias,Copula Multi-label Learning,"A formidable challenge in multi-label learning is to model the interdependencies between labels and features. Unfortunately, the statistical properties of existing multi-label dependency modelings are still not well understood. Copulas are a powerful tool for modeling dependence of multivariate data, and achieve great success in a wide range of applications, such as finance, econometrics and systems neuroscience. This inspires us to develop a novel copula multi-label learning paradigm for modeling label and feature dependencies. The copula based paradigm enables to reveal new statistical insights in multi-label learning. In particular, the paper first leverages the kernel trick to construct continuous distribution in the output space, and then estimates our proposed model semiparametrically where the copula is modeled parametrically, while the marginal distributions are modeled nonparametrically. Theoretically, we show that our estimator is an unbiased and consistent estimator and follows asymptotically a normal distribution. Moreover, we bound the mean squared error of estimator. The experimental results from various domains validate the superiority of our proposed approach.",[],[],['Weiwei Liu'],"['School of Computer Science, Wuhan University, Wuhan, China']",['China']
https://papers.nips.cc/paper_files/paper/2019/hash/5e69fda38cda2060819766569fd93aa5-Abstract.html,Fairness & Bias,Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes,"Wide neural networks with random weights and biases are Gaussian processes, as observed by Neal (1995) for shallow networks, and more recently by Lee et al.~(2018) and Matthews et al.~(2018) for deep fully-connected networks, as well as by Novak et al.~(2019) and Garriga-Alonso et al.~(2019) for deep convolutional networks.We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization.More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks.This work serves as a tutorial on the \emph{tensor programs} technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there.We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at  github.com/thegregyang/GP4A.Please see our arxiv version for the complete and up-to-date version of this paper.",[],[],['Greg Yang'],[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/5faf461eff3099671ad63c6f3f094f7f-Abstract.html,Fairness & Bias,When to Trust Your Model: Model-Based Policy Optimization,"Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.",[],[],"['Michael Janner', 'Justin Fu', 'Marvin Zhang', 'Sergey Levine']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/65b1e92c585fd4c2159d5f33b5030ff2-Abstract.html,Fairness & Bias,Are sample means in multi-armed bandits positively or negatively biased?,"It is well known that in stochastic multi-armed bandits (MAB), the sample mean of an arm is typically not an unbiased estimator of its true mean. In this paper, we decouple three different sources of this selection bias: adaptive \emph{sampling} of arms, adaptive \emph{stopping} of the experiment, and adaptively \emph{choosing} which arm to study.  Through a new notion called ``optimism'' that captures certain natural monotonic behaviors of algorithms, we provide a clean and unified analysis of how optimistic rules affect the sign of the bias. The main takeaway message is that optimistic sampling induces a negative bias, but optimistic stopping and optimistic choosing both induce a positive bias. These results are derived in a general stochastic MAB setup that is entirely agnostic to the final aim of the experiment (regret minimization or best-arm identification or anything else). We provide examples of optimistic rules of each type, demonstrate that simulations confirm our theoretical predictions, and pose some natural but hard open problems.",[],[],"['Jaehyeok Shin', 'Aaditya Ramdas', 'Alessandro Rinaldo']","['Department of Statistics and Data Science, Carnegie Mellon University', 'Department of Statistics and Data Science, Carnegie Mellon University and Machine Learning Department, Carnegie Mellon University', 'Department of Statistics and Data Science, Carnegie Mellon University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/6950aa02ae8613af620668146dd11840-Abstract.html,Fairness & Bias,Grid Saliency for Context Explanations of Semantic Segmentation,"Recently, there has been a growing interest in developing saliency methods that provide visual explanations of network predictions. Still, the usability of existing methods is limited to image classification models. To overcome this limitation, we extend the existing approaches to generate grid saliencies, which provide spatially coherent visual explanations for (pixel-level) dense prediction networks. As the proposed grid saliency allows to spatially disentangle the object and its context, we specifically explore its potential to produce context explanations for semantic segmentation networks, discovering which context most influences the class predictions inside a target object area. We investigate the effectiveness of grid saliency on a synthetic dataset with an artificially induced bias between objects and their context as well as on the real-world Cityscapes dataset using state-of-the-art segmentation networks. Our results show that grid saliency can be successfully used to provide easily interpretable context explanations and, moreover, can be employed for detecting and localizing contextual biases present in the data.",[],[],"['Lukas Hoyer', 'Mauricio Munoz', 'Prateek Katiyar', 'Anna Khoreva', 'Volker Fischer']","['Bosch Center for Artificial Intelligence', 'Bosch Center for Artificial Intelligence', 'Bosch Center for Artificial Intelligence', 'Bosch Center for Artificial Intelligence', 'Bosch Center for Artificial Intelligence']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/6c7de1f27f7de61a6daddfffbe05c058-Abstract.html,Fairness & Bias,End-to-End Learning on 3D Protein Structure for Interface Prediction,"Despite an explosion in the number of experimentally determined, atomically detailed structures of biomolecules, many critical tasks in structural biology remain data-limited.  Whether performance in such tasks can be improved by using large repositories of tangentially related structural data remains an open question.  To address this question, we focused on a central problem in biology: predicting how proteins interact with one another—that is, which surfaces of one protein bind to those of another protein.  We built a training dataset, the Database of Interacting Protein Structures (DIPS), that contains biases but is two orders of magnitude larger than those used previously.  We found that these biases significantly degrade the performance of existing methods on gold-standard data.  Hypothesizing that assumptions baked into the hand-crafted features on which these methods depend were the source of the problem, we developed the first end-to-end learning model for protein interface prediction, the Siamese Atomic Surfacelet Network (SASNet).  Using only spatial coordinates and identities of atoms, SASNet outperforms state-of-the-art methods trained on gold-standard structural data, even when trained on only 3% of our new dataset.  Code and data available at https://github.com/drorlab/DIPS.",[],[],"['Raphael Townshend', 'Rishi Bedi', 'Patricia Suriana', 'Ron Dror']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/6d3a2d24eb109dddf78374fe5d0ee067-Abstract.html,Fairness & Bias,Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs,"Learning useful representations is a key ingredient to the success of modern machine learning. Currently, representation learning mostly relies on embedding data into Euclidean space. However, recent work has shown that data in some domains is better modeled by non-euclidean metric spaces, and inappropriate geometry can result in inferior performance. In this paper, we aim to eliminate the inductive bias imposed by the embedding space geometry. Namely, we propose to map data into more general non-vector metric spaces: a weighted graph with a shortest path distance. By design, such graphs can model arbitrary geometry with a proper configuration of edges and weights. Our main contribution is PRODIGE: a method that learns a weighted graph representation of data end-to-end by gradient descent. Greater generality and fewer model assumptions make PRODIGE  more powerful than existing embedding-based approaches. We confirm the superiority of our method via extensive experiments on a wide range of tasks, including classification, compression, and collaborative filtering.",[],[],"['Denis Mazur', 'Vage Egiazarian', 'Stanislav Morozov', 'Artem Babenko']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6fd6b030c6afec018415662d0db43f9d-Abstract.html,Fairness & Bias,Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning,"Gradient-based methods for optimisation of objectives in stochastic settings with unknown or intractable dynamics require estimators of derivatives. We derive an objective that, under automatic differentiation, produces low-variance unbiased estimators of derivatives at any order. Our objective is compatible with arbitrary advantage estimators, which allows the control of the bias and variance of any-order derivatives when using function approximation. Furthermore, we propose a method to trade off bias and variance of higher order derivatives by discounting the impact of more distant causal dependencies. We demonstrate the correctness and utility of our estimator in analytically tractable MDPs and in meta-reinforcement-learning for continuous control.",[],[],"['Gregory Farquhar', 'Shimon Whiteson', 'Jakob Foerster']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/70117ee3c0b15a2950f1e82a215e812b-Abstract.html,Fairness & Bias,Learning from brains how to regularize machines,"Despite impressive performance on numerous visual tasks, Convolutional Neural Networks (CNNs) --- unlike brains --- are often highly sensitive to small perturbations of their input, e.g. adversarial noise leading to erroneous decisions. We propose to regularize CNNs using large-scale neuroscience data to learn more robust neural features in terms of representational similarity. We presented natural images to mice and measured the responses of thousands of neurons from cortical visual areas. Next, we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the model's predictions. We then used the neural representation similarity to regularize CNNs trained on image classification by penalizing intermediate representations that deviated from neural ones. This preserved performance of baseline models when classifying images under standard benchmarks, while maintaining substantially higher performance compared to baseline or control models when classifying noisy images. Moreover, the models regularized with cortical representations also improved model robustness in terms of adversarial attacks. This demonstrates that regularizing with neural data can be an effective tool to create an inductive bias towards more robust inference.",[],[],"['Zhe Li', 'Wieland Brendel', 'Edgar Walker', 'Erick Cobos', 'Taliah Muhammad', 'Jacob Reimer', 'Matthias Bethge', 'Fabian Sinz', 'Zachary Pitkow', 'Andreas Tolias']","['Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Centre for Integrative Neuroscience, University of Tübingen and Bernstein Center for Computational Neuroscience, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Institute Bioinformatics and Medical Informatics, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Centre for Integrative Neuroscience, University of Tübingen and Bernstein Center for Computational Neuroscience, University of Tübingen and Institute for Theoretical Physics, University of Tübingen', 'Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Bernstein Center for Computational Neuroscience, University of Tübingen and Institute Bioinformatics and Medical Informatics, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Department of Electrical and Computer Engineering, Rice University', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Department of Electrical and Computer Engineering, Rice University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/702cafa3bb4c9c86e4a3b6834b45aedd-Abstract.html,Fairness & Bias,A Debiased MDI Feature Importance Measure for Random Forests,"Tree ensembles such as Random Forests have achieved impressive empirical success across a wide variety of applications. To understand how these models make predictions, people routinely turn to feature importance measures calculated from tree ensembles. It has long been known that Mean Decrease Impurity (MDI), one of the most widely used measures of feature importance, incorrectly assigns high importance to noisy features, leading to systematic bias in feature selection. In this paper, we address the feature selection bias of MDI from both theoretical and methodological perspectives. Based on the original definition of MDI by Breiman et al.  \cite{Breiman1984} for a single tree, we derive a tight non-asymptotic bound on the expected bias of MDI importance of noisy features, showing that deep trees have higher (expected) feature selection bias than shallow ones. However, it is not clear how to reduce the bias of MDI using its existing analytical expression. We derive a new analytical expression for MDI, and based on this new expression, we are able to propose a debiased MDI feature importance measure using out-of-bag samples, called MDI-oob. For both the simulated data and a genomic ChIP dataset, MDI-oob achieves state-of-the-art performance in feature selection from Random Forests for both deep and shallow trees.",[],[],"['Xiao Li', 'Yu Wang', 'Sumanta Basu', 'Karl Kumbier', 'Bin Yu']","['Statistics Department, UC Berkeley', 'Statistics Department, UC Berkeley', 'Statistics and Data Science Department, Computational Biology Department, Cornell University', 'Statistics Department, UC Berkeley', 'EECS, Statistics Department, UC Berkeley']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/73e0f7487b8e5297182c5a711d20bf26-Abstract.html,Fairness & Bias,The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric,"Where machine-learned predictive risk scores inform high-stakes decisions, such as bail and sentencing in criminal justice, fairness has been a serious concern. Recent work has characterized the disparate impact that such risk scores can have when used for a binary classification task. This may not account, however, for the more diverse downstream uses of risk scores and their non-binary nature. To better account for this, in this paper, we investigate the fairness of predictive risk scores from the point of view of a bipartite ranking task, where one seeks to rank positive examples higher than negative ones. We introduce the xAUC disparity as a metric to assess the disparate impact of risk scores and define it as the difference in the probabilities of ranking a random positive example from one protected group above a negative one from another group and vice versa. We provide a decomposition of bipartite ranking loss into components that involve the discrepancy and components that involve pure predictive ability within each group. We use xAUC analysis to audit predictive risk scores for recidivism prediction, income prediction, and cardiac arrest prediction, where it describes disparities that are not evident from simply comparing within-group predictive performance.",[],[],"['Nathan Kallus', 'Angela Zhou']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/7a674153c63cff1ad7f0e261c369ab2c-Abstract.html,Fairness & Bias,Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases,"Differential privacy has become a widely accepted notion of privacy, leading to the introduction and deployment of numerous privatization mechanisms. However, ensuring the privacy guarantee is an error-prone process, both in designing mechanisms and in implementing those mechanisms. Both types of errors will be greatly reduced, if we have a data-driven approach to verify privacy guarantees, from a black-box access to a mechanism. We pose it as a property estimation problem, and study the fundamental trade-offs involved in the accuracy in estimated privacy guarantees and the number of samples required. We introduce a novel estimator that uses polynomial approximation of a carefully chosen degree to optimally trade-off bias and variance. With n samples, we show that this estimator achieves performance of a straightforward plug-in estimator with n*log(n) samples, a phenomenon referred to as effective sample size amplification. The minimax optimality of the proposed estimator is proved by comparing it to a matching fundamental lower bound.",[],[],"['Xiyang Liu', 'Sewoong Oh']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/7a9a322cbe0d06a98667fdc5160dc6f8-Abstract.html,Fairness & Bias,Reconciling meta-learning and continual learning with online mixtures of tasks,"Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.",[],[],"['Ghassen Jerfel', 'Erin Grant', 'Tom Griffiths', 'Katherine A. Heller']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/7c4bf50b715509a963ce81b168ca674b-Abstract.html,Fairness & Bias,Policy Evaluation with Latent Confounders via Optimal Balance,"Evaluating novel contextual bandit policies using logged data is crucial in applications where exploration is costly, such as medicine. But it usually relies on the assumption of no unobserved confounders, which is bound to fail in practice. We study the question of policy evaluation when we instead have proxies for the latent confounders and develop an importance weighting method that avoids fitting a latent outcome regression model. Surprisingly, we show that there exist no single set of weights that give unbiased evaluation regardless of outcome model, unlike the case with no unobserved confounders where density ratios are sufficient. Instead, we propose an adversarial objective and weights that minimize it, ensuring sufficient balance in the latent confounders regardless of outcome model. We develop theory characterizing the consistency of our method and tractable algorithms for it. Empirical results validate the power of our method when confounders are latent.",[],[],"['Andrew Bennett', 'Nathan Kallus']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html,Fairness & Bias,Characterizing Bias in Classifiers using Generative Models,"Models that are learned from real-world data are often biased because the data used to train them is biased. This can propagate systemic human biases that exist and ultimately lead to inequitable treatment of people, especially minorities. To characterize bias in learned classifiers, existing approaches rely on human oracles labeling real-world examples to identify the ""blind spots"" of the classifiers; these are ultimately limited due to the human labor required and the finite nature of existing image examples. We propose a simulation-based approach for interrogating classifiers using generative adversarial models in a systematic manner. We incorporate a progressive conditional generative model for synthesizing photo-realistic facial images and Bayesian Optimization for an efficient interrogation of independent facial image classification systems. We show how this approach can be used to efficiently characterize racial and gender biases in commercial systems.",[],[],"['Daniel McDuff', 'Shuang Ma', 'Yale Song', 'Ashish Kapoor']","['Microsoft, Redmond, WA', 'Microsoft, Redmond, WA', 'Microsoft, Redmond, WA', 'SUNY Buffalo, Buffalo, NY']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/800b03685c22049f049801f6841861a2-Abstract.html,Fairness & Bias,Unified Sample-Optimal Property Estimation in Near-Linear Time,"We consider the fundamental learning problem of estimating properties of distributions over large domains. Using a novel piecewise-polynomial approximation technique, we derive the first unified methodology for constructing sample- and time-efficient estimators for all sufficiently smooth, symmetric and non-symmetric, additive properties. This technique yields near-linear-time computable estimators whose approximation values are asymptotically optimal and highly-concentrated, resulting in the first: 1) estimators achieving the $\mathcal{O}(k/(\varepsilon^2\log k))$ min-max $\varepsilon$-error sample complexity for all $k$-symbol Lipschitz properties; 2) unified near-optimal differentially private estimators for a variety of properties; 3) unified estimator achieving optimal bias and near-optimal variance for five important properties; 4) near-optimal sample-complexity estimators for several important symmetric properties over both domain sizes and confidence levels.",[],[],"['Yi Hao', 'Alon Orlitsky']","['Dept. of Electrical and Computer Engineering, University of California, San Diego', 'Dept. of Electrical and Computer Engineering, University of California, San Diego']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/816a6db41f0e44644bc65808b6db5ca4-Abstract.html,Fairness & Bias,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates,"We consider the core reinforcement-learning problem of on-policy value function approximation from a batch of trajectory data, and focus on various issues of Temporal Difference (TD) learning and Monte Carlo (MC) policy evaluation. The two methods are known to achieve complementary bias-variance trade-off properties, with TD tending to achieve lower variance but potentially higher bias. In this paper, we argue that the larger bias of TD can be a result of the amplification of local approximation errors. We address this by proposing an algorithm that adaptively switches between TD and MC in each state, thus mitigating the propagation of errors. Our method is based on learned confidence intervals that detect biases of TD estimates. We demonstrate in a variety of policy evaluation tasks that this simple adaptive algorithm performs competitively with the best approach in hindsight, suggesting that learned confidence intervals are a powerful technique for adapting policy evaluation to use TD or MC returns in a data-driven way.",[],[],"['Carlos Riquelme', 'Hugo Penedones', 'Damien Vincent', 'Hartmut Maennel', 'Sylvain Gelly', 'Timothy A. Mann', 'Andre Barreto', 'Gergely Neu']","['DeepMind', 'Google Brain', 'Google Brain', 'Google Brain', 'DeepMind', 'DeepMind', 'Google Brain', 'Universitat Pompeu Fabra']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/8558cb408c1d76621371888657d2eb1d-Abstract.html,Fairness & Bias,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift,"Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity.  In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted.  Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration.  We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods.  However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.",[],[],"['Yaniv Ovadia', 'Emily Fertig', 'Jie Ren', 'Zachary Nado', 'D. Sculley', 'Sebastian Nowozin', 'Joshua Dillon', 'Balaji Lakshminarayanan', 'Jasper Snoek']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/88bade49e98db8790df275fcebb37a13-Abstract.html,Fairness & Bias,From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization,"We present a new algorithm (ASEBO) for optimizing high-dimensional blackbox functions. ASEBO adapts to the geometry of the function and learns optimal sets of sensing directions, which are used to probe it, on-the-fly. It addresses the exploration-exploitation trade-off of blackbox optimization with expensive blackbox queries by continuously learning the bias of the lower-dimensional model used to approximate gradients of smoothings of the function via compressed sensing and contextual bandits methods. To obtain this model, it leverages techniques from the emerging theory of active subspaces in a novel ES blackbox optimization context. As a result, ASEBO learns the dynamically changing intrinsic dimensionality of the gradient space and adapts to the hardness of different stages of the optimization without external supervision. Consequently, it leads to more sample-efficient blackbox optimization than state-of-the-art algorithms. We provide theoretical results and test ASEBO advantages over other methods empirically by evaluating it on the set of reinforcement learning policy optimization tasks as well as functions from the recently open-sourced Nevergrad library.",[],[],"['Krzysztof M. Choromanski', 'Aldo Pacchiano', 'Jack Parker-Holder', 'Yunhao Tang', 'Vikas Sindhwani']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8c235f89a8143a28a1d6067e959dd858-Abstract.html,Fairness & Bias,Efficient Meta Learning via Minibatch Proximal Update,"We address the problem of meta-learning which learns a prior over hypothesis from a sample of meta-training tasks for fast adaptation on meta-testing tasks. A particularly simple yet successful paradigm for this research is model-agnostic meta-learning (MAML). Implementation and analysis of MAML, however, can be tricky; first-order approximation is usually adopted to avoid directly computing Hessian matrix but as a result the convergence and generalization guarantees remain largely mysterious for MAML. To remedy this deficiency, in this paper we propose a minibatch proximal update based meta-learning approach for learning to efficient hypothesis transfer. The principle is to learn a prior hypothesis shared across tasks such that the minibatch risk minimization biased regularized by this prior can quickly converge to the optimal hypothesis in each training task. The prior hypothesis training model can be efficiently optimized via SGD with provable convergence guarantees for both convex and non-convex problems. Moreover, we theoretically justify the benefit of the learnt prior hypothesis for fast adaptation to new few-shot learning tasks via minibatch proximal update. Experimental results on several few-shot regression and classification tasks demonstrate the advantages of our method over state-of-the-arts.",[],[],"['Pan Zhou', 'Xiaotong Yuan', 'Huan Xu', 'Shuicheng Yan', 'Jiashi Feng']","['Learning & Vision Lab, National University of Singapore, Singapore', 'B-DAT Lab, Nanjing University of Information Science & Technology, Nanjing, China', 'Alibaba and Georgia Institute of Technology', 'YITU Technology, Shanghai, China', 'Learning & Vision Lab, National University of Singapore, Singapore']","['Singapore', 'China', 'Georgia', 'China', 'Singapore']"
https://papers.nips.cc/paper_files/paper/2019/hash/8ca01ea920679a0fe3728441494041b9-Abstract.html,Fairness & Bias,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration,"Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.",[],[],"['Meelis Kull', 'Miquel Perello Nieto', 'Markus Kängsepp', 'Telmo Silva Filho', 'Hao Song', 'Peter Flach']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8d3369c4c086f236fabf61d614a32818-Abstract.html,Fairness & Bias,Perceiving the arrow of time in autoregressive motion,"Understanding the principles of causal inference in the visual system has a long history at least since the seminal studies by Albert Michotte. Many cognitive and machine learning scientists believe that intelligent behavior requires agents to possess causal models of the world. Recent ML algorithms exploit the dependence structure of additive noise terms for inferring causal structures from observational data, e.g. to detect the direction of time series; the arrow of time. This raises the question whether the subtle asymmetries between the time directions can also be perceived by humans. Here we show that human observers can indeed discriminate forward and backward autoregressive motion with non-Gaussian additive independent noise, i.e. they appear sensitive to subtle asymmetries between the time directions. We employ a so-called frozen noise paradigm enabling us to compare human performance with four different algorithms on a trial-by-trial basis: A causal inference algorithm exploiting the dependence structure of additive noise terms, a neurally inspired network, a Bayesian ideal observer model as well as a simple heuristic. Our results suggest that all human observers use similar cues or strategies to solve the arrow of time motion discrimination task, but the human algorithm is significantly different from the three machine algorithms we compared it to. In fact, our simple heuristic appears most similar to our human observers.",[],[],"['Kristof Meding', 'Dominik Janzing', 'Bernhard Schölkopf', 'Felix A. Wichmann']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8d5e957f297893487bd98fa830fa6413-Abstract.html,Fairness & Bias,Noise-tolerant fair classification,"Fairness-aware learning involves designing algorithms that do not discriminate with respect to some sensitive feature (e.g., race or gender). Existing work on the problem operates under the assumption that the sensitive feature available in one's training sample is perfectly reliable. This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features. In this paper, we answer the question in the affirmative: we show that if one measures fairness using the mean-difference score, and sensitive features are subject to noise from the mutually contaminated learning model, then owing to a simple identity we only need to change the desired fairness-tolerance. The requisite tolerance can be estimated by leveraging existing noise-rate estimators from the label noise literature. We finally show that our procedure is empirically effective on two case-studies involving sensitive feature censoring.",[],[],"['Alex Lamy', 'Ziyuan Zhong', 'Aditya K. Menon', 'Nakul Verma']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8e987cf1b2f1f6ffa6a43066798b4b7f-Abstract.html,Fairness & Bias,Estimating Entropy of Distributions in Constant Space,"We consider the task of estimating the entropy of $k$-ary distributions from samples in the streaming model, where space is limited. Our main contribution is an algorithm that requires $O\left(\frac{k \log (1/\varepsilon)^2}{\varepsilon^3}\right)$ samples and a constant $O(1)$ memory words of space and outputs a $\pm\varepsilon$ estimate of $H(p)$. Without space limitations, the sample complexity has been established as $S(k,\varepsilon)=\Theta\left(\frac k{\varepsilon\log k}+\frac{\log^2 k}{\varepsilon^2}\right)$, which is sub-linear in the domain size $k$, and the  current algorithms that achieve optimal sample complexity also require nearly-linear space in $k$. Our algorithm partitions $[0,1]$ into intervals and estimates the entropy contribution of probability values in each interval. The intervals are designed to trade bias and variance. Distribution property estimation and testing with limited memory is a largely unexplored research area. We hope our work will motivate research in this field.",[],[],"['Jayadev Acharya', 'Sourbh Bhadane', 'Piotr Indyk', 'Ziteng Sun']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8fb5f8be2aa9d6c64a04e3ab9f63feee-Abstract.html,Fairness & Bias,Adapting Neural Networks for the Estimation of Treatment Effects,"This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we ﬁt models for the expected outcome and the probability of treatment (propensity score). Second, we plug these ﬁtted models into a downstream estimator. Neural networks are a natural choice for the models in the ﬁrst step. Our question is: how can we adapt the design and training of the neural networks used in this ﬁrst step in order to improve the quality of the ﬁnal estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The ﬁrst is a new architecture, the Dragonnet, that exploits the sufﬁciency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties ‘out-of-the-box’. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods.",[],[],"['Claudia Shi', 'David Blei', 'Victor Veitch']","['Department of Computer Science, Columbia Unitversity', 'Department of Computer Science, Columbia Unitversity and Department of Statistics, Columbia University', 'Department of Statistics, Columbia University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/95b431e51fc53692913da5263c214162-Abstract.html,Fairness & Bias,Learning Compositional Neural Programs with Recursive Tree Search and Planning,"We propose a novel reinforcement learning algorithm, AlphaNPI, that incorpo-rates the strengths of Neural Programmer-Interpreters (NPI) and AlphaZero. NPIcontributes structural biases in the form of modularity, hierarchy and recursion,which are helpful to reduce sample complexity, improve generalization and in-crease interpretability. AlphaZero contributes powerful neural network guidedsearch algorithms, which we augment with recursion. AlphaNPI only assumesa hierarchical program specification with sparse rewards: 1 when the programexecution satisfies the specification, and 0 otherwise. This specification enablesus to overcome the need for strong supervision in the form of execution tracesand consequently train NPI models effectively with reinforcement learning. Theexperiments show that AlphaNPI can sort as well as previous strongly supervisedNPI variants. The AlphaNPI agent is also trained on a Tower of Hanoi puzzle withtwo disks and is shown to generalize to puzzles with an arbitrary number of disks.The experiments also show that when deploying our neural network policies, it isadvantageous to do planning with guided Monte Carlo tree search.",[],[],"['Thomas PIERROT', 'Guillaume Ligner', 'Scott E. Reed', 'Olivier Sigaud', 'Nicolas Perrin', 'Alexandre Laterre', 'David Kas', 'Karim Beguir', 'Nando de Freitas']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html,Fairness & Bias,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,"We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet (objects are largely centered and unoccluded) and harder (due to the controls). Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.",[],[],"['Andrei Barbu', 'David Mayo', 'Julian Alverio', 'William Luo', 'Christopher Wang', 'Dan Gutfreund', 'Josh Tenenbaum', 'Boris Katz']","['MIT, CSAIL & CBMM', 'MIT, CSAIL & CBMM', 'MIT, CSAIL', 'MIT, CSAIL', 'MIT, CSAIL', 'MIT-IBM Watson AI', 'MIT, BCS & CBMM', 'MIT, CSAIL & CBMM']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/98d8a23fd60826a2a474c5b4f5811707-Abstract.html,Fairness & Bias,Visual Concept-Metaconcept Learning,"Humans reason with concepts and metaconcepts: we recognize red and blue from visual input; we also understand that they are colors, i.e., red is an instance of color. In this paper, we propose the visual concept-metaconcept learner (VCML) for joint learning of concepts and metaconcepts from images and associated question-answer pairs. The key is to exploit the bidirectional connection between visual concepts and metaconcepts. Visual representations provide grounding cues for predicting relations between unseen pairs of concepts. Knowing that red and blue are instances of color, we generalize to the fact that green is also an instance of color since they all categorize the hue of objects. Meanwhile, knowledge about metaconcepts empowers visual concept learning from limited, noisy, and even biased data. From just a few examples of purple cubes we can understand a new color purple, which resembles the hue of the cubes instead of the shape of them. Evaluation on both synthetic and real-world datasets validates our claims.",[],[],"['Chi Han', 'Jiayuan Mao', 'Chuang Gan', 'Josh Tenenbaum', 'Jiajun Wu']","['MIT CSAIL and IIIS, Tsinghua University', 'MIT CSAIL', 'MIT-IBM Watson AI Lab', 'MIT BCS, CBMM, CSAIL', 'MIT CSAIL']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/9ac403da7947a183884c18a67d3aa8de-Abstract.html,Fairness & Bias,Importance Resampling for Off-policy Prediction,"Importance sampling (IS) is a common reweighting strategy for off-policy prediction in reinforcement learning.  While it is consistent and unbiased, it can result in high variance updates to the weights for the value function. In this work, we explore a resampling strategy as an alternative to reweighting. We propose Importance Resampling (IR) for off-policy prediction, which resamples experience from a replay buffer and applies standard on-policy updates. The approach avoids using importance sampling ratios in the update, instead correcting the distribution before the update. We characterize the bias and consistency of IR, particularly compared to Weighted IS (WIS). We demonstrate in several microworlds that IR has improved sample efficiency and lower variance updates, as compared to IS and several variance-reduced IS strategies, including variants of WIS and V-trace which clips IS ratios. We also provide a demonstration showing IR improves over IS for learning a value function from images in a racing car simulator.",[],[],"['Matthew Schlegel', 'Wesley Chung', 'Daniel Graves', 'Jian Qian', 'Martha White']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/9ce3c52fc54362e22053399d3181c638-Abstract.html,Fairness & Bias,Differentiable Convex Optimization Layers,"Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver’s solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.",[],[],"['Akshay Agrawal', 'Brandon Amos', 'Shane Barratt', 'Stephen Boyd', 'Steven Diamond', 'J. Zico Kolter']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/9e984c108157cea74c894b5cf34efc44-Abstract.html,Fairness & Bias,Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function,"We present an algorithm based on the \emph{Optimism in the Face of Uncertainty} (OFU) principle which is able to learn Reinforcement Learning (RL) modeled by Markov decision process (MDP) with finite state-action space efficiently. By evaluating the state-pair difference of the optimal bias function $h^{*}$, the proposed algorithm achieves a regret bound of $\tilde{O}(\sqrt{SATH})$\footnote{The symbol $\tilde{O}$ means $O$ with log factors ignored. } for MDP with S states and A actions, in the case that an upper bound $H$ on the span of $h^{*}$, i.e., $sp(h^{*})$ is known. This result outperforms the best previous regret bounds $\tilde{O}(HS\sqrt{AT})$\cite{bartlett2009regal} by a factor of $\sqrt{SH}$. Furthermore, this regret bound matches the lower bound of $\Omega(\sqrt{SATH})$\cite{jaksch2010near} up to a logarithmic factor. As a consequence,  we show that there is a near optimal regret bound of $\tilde{O}(\sqrt{DSAT})$ for MDPs with finite diameter $D$ compared to the lower bound of $\Omega(\sqrt{DSAT})$\cite{jaksch2010near}.",[],[],"['Zihan Zhang', 'Xiangyang Ji']","['Tsinghua University', 'Tsinghua University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/9f067d8d6df2d4b8c64fb4c084d6c208-Abstract.html,Fairness & Bias,Re-randomized Densification for One Permutation Hashing and Bin-wise Consistent Weighted Sampling,"Jaccard similarity is widely used as a distance measure in many machine learningand search applications. Typically, hashing methods are essential for the use ofJaccard similarity to be practical in large-scale settings. For hashing binary (0/1)data, the idea of one permutation hashing (OPH) with densification significantlyaccelerates traditional minwise hashing algorithms while providing unbiased andaccurate estimates. In this paper, we propose a strategy named “re-randomization”in the process of densification that could achieve the smallest variance among alldensification schemes. The success of this idea naturally inspires us to generalizeone permutation hashing to weighted (non-binary) data, which results in the socalled “bin-wise consistent weighted sampling (BCWS)” algorithm. We analyze thebehavior of BCWS and compare it with a recent alternative. Extensive experimentson various datasets illustrates the effectiveness of our proposed methods.",[],[],"['Ping Li', 'Xiaoyun Li', 'Cun-Hui Zhang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/9f9e8cba3700df6a947a8cf91035ab84-Abstract.html,Fairness & Bias,Finite-Sample Analysis for SARSA with Linear Function Approximation,"SARSA is an on-policy algorithm to learn a Markov decision process policy in reinforcement learning. We investigate the SARSA algorithm with linear function approximation under the non-i.i.d.\ setting, where a single sample trajectory is available. With a Lipschitz continuous policy improvement operator that is smooth enough, SARSA has been shown to converge asymptotically. However, its non-asymptotic analysis is challenging and remains unsolved due to the non-i.i.d.  samples, and the fact that the behavior policy changes dynamically with time. In this paper, we develop a novel technique to explicitly characterize the stochastic bias of a type of stochastic approximation procedures with time-varying Markov transition kernels. Our approach enables non-asymptotic convergence analyses of this type of stochastic approximation algorithms, which may be of independent interest. Using  our bias characterization technique and a  gradient descent type of analysis, we further provide the finite-sample analysis on the  mean square error of the SARSA algorithm.  In the end, we  present a fitted SARSA algorithm, which includes the original SARSA algorithm and its variant as special cases. This fitted SARSA algorithm provides a framework for \textit{iterative} on-policy fitted policy iteration, which is more memory and computationally efficient. For this fitted SARSA algorithm, we also present its finite-sample analysis.",[],[],"['Shaofeng Zou', 'Tengyu Xu', 'Yingbin Liang']","['Department of Electrical Engineering, University at Buffalo, The State University of New York, Buffalo, NY', 'Department of ECE, The Ohio State University, Columbus, OH', 'Department of ECE, The Ohio State University, Columbus, OH']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/a32d7eeaae19821fd9ce317f3ce952a7-Abstract.html,Fairness & Bias,Random Projections with Asymmetric Quantization,"The method of random projection has been a popular tool for data compression,similarity search, and machine learning. In many practical scenarios, applyingquantization on randomly projected data could be very helpful to further reducestorage cost and facilitate more efficient retrievals, while only suffering fromlittle loss in accuracy. In real-world applications, however, data collected fromdifferent sources may be quantized under different schemes, which calls for a need to study the asymmetric quantization problem. In this paper, we investigate the cosine similarity estimators derived in such setting under the Lloyd-Max (LM)quantization scheme. We thoroughly analyze the biases and variances of a series of estimators including the basic simple estimators, their normalized versions, andtheir debiased versions. Furthermore, by studying the monotonicity, we show thatthe expectation of proposed estimators increases with the true cosine similarity,on a broader family of stair-shaped quantizers. Experiments on nearest neighborsearch justify the theory and illustrate the effectiveness of our proposed estimators.",[],[],"['Xiaoyun Li', 'Ping Li']","['Department of Statistics, Rutgers University, Piscataway, NJ', 'Cognitive Computing Lab, Baidu Research USA, Bellevue, WA']",['US']
https://papers.nips.cc/paper_files/paper/2019/hash/a4613e8d72a61b3b69b32d040f89ad81-Abstract.html,Fairness & Bias,One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers,"The success of lottery ticket initializations (Frankle and Carbin, 2019) suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these ""winning ticket'' initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.",[],[],"['Ari Morcos', 'Haonan Yu', 'Michela Paganini', 'Yuandong Tian']","['Facebook AI Research', 'Facebook AI Research', 'Facebook AI Research', 'Facebook AI Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/a4d41b834ea903526373a9a1ae2ac66e-Abstract.html,Fairness & Bias,Learning Distributions Generated by One-Layer ReLU Networks,"We consider the problem of estimating the parameters of a $d$-dimensional rectified Gaussian distribution from i.i.d. samples. A rectified Gaussian distribution is defined by passing a standard Gaussian distribution through a one-layer ReLU neural network. We give a simple algorithm to estimate the parameters (i.e., the weight matrix and bias vector of the ReLU neural network) up to an error $\eps\norm{W}_F$ using $\widetilde{O}(1/\eps^2)$ samples and $\widetilde{O}(d^2/\eps^2)$ time (log factors are ignored for simplicity). This implies that we can estimate the distribution up to $\eps$ in total variation distance using $\widetilde{O}(\kappa^2d^2/\eps^2)$ samples, where $\kappa$ is the condition number of the covariance matrix. Our only assumption is that the bias vector is non-negative. Without this non-negativity assumption, we show that estimating the bias vector within any error requires the number of samples at least exponential in the infinity norm of the bias vector. Our algorithm is based on the key observation that vector norms and pairwise angles can be estimated separately. We use a recent result on learning from truncated samples. We also prove two sample complexity lower bounds: $\Omega(1/\eps^2)$ samples are required to estimate the parameters up to error $\eps$, while $\Omega(d/\eps^2)$ samples are necessary to estimate the distribution up to $\eps$ in total variation distance. The first lower bound implies that our algorithm is optimal for parameter estimation. Finally, we show an interesting connection between learning a two-layer generative model and non-negative matrix factorization. Experimental results are provided to support our analysis.",[],[],"['Shanshan Wu', 'Alexandros G. Dimakis', 'Sujay Sanghavi']","['Department of Electrical and Computer Engineering, University of Texas at Austin', 'Department of Electrical and Computer Engineering, University of Texas at Austin', 'Department of Electrical and Computer Engineering, University of Texas at Austin']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/a4d8e2a7e0d0c102339f97716d2fdfb6-Abstract.html,Fairness & Bias,Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules,"Deep learning has proven to yield fast and accurate predictions of quantum-chemical properties to accelerate the discovery of novel molecules and materials. As an exhaustive exploration of the vast chemical space is still infeasible, we require generative models that guide our search towards systems with desired properties. While graph-based models have previously been proposed, they are restricted by a lack of spatial information such that they are unable to recognize spatial isomerism and non-bonded interactions. Here, we introduce a generative neural network for 3d point sets that respects the rotational invariance of the targeted structures. We apply it to the generation of molecules and demonstrate its ability to approximate the distribution of equilibrium structures using spatial metrics as well as established measures from chemoinformatics. As our model is able to capture the complex relationship between 3d geometry and electronic properties, we bias the distribution of the generator towards molecules with a small HOMO-LUMO gap - an important property for the design of organic solar cells.",[],[],"['Niklas Gebauer', 'Michael Gastegger', 'Kristof Schütt']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/a68259547f3d25ab3c0a5c0adb4e3498-Abstract.html,Fairness & Bias,A Stochastic Composite Gradient Method with Incremental Variance Reduction,"We consider the problem of minimizing the composition of a smooth (nonconvex) function and a smooth vector mapping, where the inner mapping is in the form of an expectation over some random variable or a finite sum. We propose a stochastic composite gradient method that employs incremental variance-reduced estimators for both the inner vector mapping and its Jacobian. We show that this method achieves the same orders of complexity as the best known first-order methods for minimizing expected-value and finite-sum nonconvex functions, despite the additional outer composition which renders the composite gradient estimator biased. This finding enables a much broader range of applications in machine learning to benefit from the low complexity of incremental variance-reduction methods.",[],[],"['Junyu Zhang', 'Lin Xiao']","['University of Minnesota, Minneapolis, Minnesota', 'Microsoft Research, Redmond, Washington']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/ab49ef78e2877bfd2c2bfa738e459bf0-Abstract.html,Fairness & Bias,The Impact of Regularization on High-dimensional Logistic Regression,"Logistic regression is commonly used for modeling dichotomous outcomes. In the classical setting, where the number of observations is much larger than the number of parameters, properties of the maximum likelihood estimator in logistic regression are well understood. Recently, Sur and Candes~\cite{sur2018modern} have studied logistic regression in the high-dimensional regime, where the number of observations and parameters are comparable, and show, among other things, that the maximum likelihood estimator is biased. In the high-dimensional regime the underlying parameter vector is often structured (sparse, block-sparse, finite-alphabet, etc.) and so in this paper we study regularized logistic regression (RLR), where a convex regularizer that encourages the desired structure is added to the negative of the log-likelihood function. An advantage of RLR is that it allows parameter recovery even for instances where the (unconstrained) maximum likelihood estimate does not exist. We provide a precise analysis of the performance of RLR via the solution of a system of six nonlinear equations, through which any performance metric of interest (mean, mean-squared error, probability of support recovery, etc.) can be explicitly computed. Our results generalize those of Sur and Candes and we provide a detailed study for the cases of $\ell_2^2$-RLR and sparse ($\ell_1$-regularized) logistic regression. In both cases, we obtain explicit expressions for various performance metrics and can find the values of the regularizer parameter that optimizes the desired performance. The theory is validated by extensive numerical simulations across a range of parameter values and problem instances.",[],[],"['Fariborz Salehi', 'Ehsan Abbasi', 'Babak Hassibi']","['Department of Electrical Engineering, California Institute of Technology, Pasadena, CA', 'Department of Electrical Engineering, California Institute of Technology, Pasadena, CA', 'Department of Electrical Engineering, California Institute of Technology, Pasadena, CA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/ab817c9349cf9c4f6877e1894a1faa00-Abstract.html,Fairness & Bias,Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition,"Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.",[],[],"['Jinwoo Choi', 'Chen Gao', 'Joseph C. E. Messou', 'Jia-Bin Huang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/aec851e565646f6835e915293381e20a-Abstract.html,Fairness & Bias,Estimating Convergence of Markov chains with L-Lag Couplings,"Markov chain Monte Carlo (MCMC) methods generate samples that are asymptotically distributed from a target distribution of interest as the number of iterations goes to infinity. Various theoretical results provide upper bounds on the distance between the target and marginal distribution after a fixed number of iterations. These upper bounds are on a case by case basis and typically involve intractable quantities, which limits their use for practitioners. We introduce L-lag couplings to generate computable, non-asymptotic upper bound estimates for the total variation or the Wasserstein distance of general Markov chains. We apply L-lag couplings to the tasks of (i) determining MCMC burn-in, (ii) comparing different MCMC algorithms with the same target, and (iii) comparing exact and approximate MCMC. Lastly, we (iv) assess the bias of sequential Monte Carlo and self-normalized importance samplers.",[],[],"['Niloy Biswas', 'Pierre E. Jacob', 'Paul Vanetti']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/b1b20d09041289e6c3fbb81850c5da54-Abstract.html,Fairness & Bias,Distributed estimation of the inverse Hessian by determinantal averaging,"In distributed optimization and distributed numerical linear algebra,we often encounter an inversion bias: if we want to compute aquantity that depends on the inverse of a sum of distributed matrices,then the sum of the inverses does not equal the inverse of the sum. An example of this occurs in distributed Newton's method, where wewish to compute (or implicitly work with) the inverse Hessianmultiplied by the gradient. In this case, locally computed estimates are biased, and so taking auniform average will not recover the correct solution. To address this, we propose determinantal averaging, a newapproach for correcting the inversion bias. This approach involves reweighting the local estimates of the Newton'sstep proportionally to the determinant of the local Hessian estimate,and then averaging them together to obtain an improved globalestimate. This method provides the first known distributed Newton step that isasymptotically consistent, i.e., it recovers the exact step inthe limit as the number of distributed partitions grows to infinity. To show this, we develop new expectation identities and moment boundsfor the determinant and adjugate of a random matrix. Determinantal averaging can be applied not only to Newton's method,but to computing any quantity that is a linear tranformation of amatrix inverse, e.g., taking a trace of the inverse covariance matrix,which is used in data uncertainty quantification.",[],[],"['Michal Derezinski', 'Michael W. Mahoney']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/b4189d9de0fb2b9cce090bd1a15e3420-Abstract.html,Fairness & Bias,Inherent Tradeoffs in Learning Fair Representations,"With the prevalence of machine learning in high-stakes applications, especially the ones regulated by anti-discrimination laws or societal norms, it is crucial to ensure that the predictive models do not propagate any existing bias or discrimination. Due to the ability of deep neural nets to learn rich representations, recent advances in algorithmic fairness have focused on learning fair representations with adversarial techniques to reduce bias in data while preserving utility simultaneously. In this paper, through the lens of information theory, we provide the first result that quantitatively characterizes the tradeoff between demographic parity and the joint utility across different population groups. Specifically, when the base rates differ between groups, we show that any method aiming to learn fair representations admits an information-theoretic lower bound on the joint error across these groups. To complement our negative results, we also prove that if the optimal decision functions across different groups are close, then learning fair representations leads to an alternative notion of fairness, known as the accuracy parity, which states that the error rates are close between groups. Finally, our theoretical findings are also confirmed empirically on real-world datasets.",[],[],"['Han Zhao', 'Geoff Gordon']","['Machine Learning Department, School of Computer Science, Carnegie Mellon University', 'Microsoft Research, Montreal, Machine Learning Department, Carnegie Mellon University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/bbc12a3a98d8487f58a87d3a3070516e-Abstract.html,Fairness & Bias,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks,"Recent efforts to improve the interpretability of deep neural networks use saliency to characterize the importance of input features to predictions made by models. Work on interpretability using saliency-based methods on Recurrent Neural Networks (RNNs) has mostly targeted language tasks, and their applicability to time series data is less understood. In this work we analyze saliency-based methods for RNNs, both classical and gated cell architectures. We show that RNN saliency vanishes over time, biasing detection of salient features only to later time steps and are, therefore, incapable of reliably detecting important features at arbitrary time intervals. To address this vanishing saliency problem, we propose a novel RNN cell structure (input-cell attention), which can extend any RNN cell architecture. At each time step, instead of only looking at the current input vector, input-cell attention uses a fixed-size matrix embedding, each row of the matrix attending to different inputs from current or previous time steps.  Using synthetic data, we show that the saliency map produced by the input-cell attention RNN is able to faithfully detect important features regardless of their occurrence in time. We also apply the input-cell attention RNN on a neuroscience task analyzing functional Magnetic Resonance Imaging (fMRI) data for human subjects performing a variety of tasks. In this case, we use saliency to characterize brain regions (input features) for which activity is important to distinguish between tasks. We show that standard RNN architectures are only capable of detecting important brain regions in the last few time steps of the fMRI data, while the input-cell attention model is able to detect important brain region activity across time without latter time step biases.",[],[],"['Aya Abdelsalam Ismail', 'Mohamed Gunady', 'Luiz Pessoa', 'Hector Corrada Bravo', 'Soheil Feizi']","['Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Psychology, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/bbc92a647199b832ec90d7cf57074e9e-Abstract.html,Fairness & Bias,Paradoxes in Fair Machine Learning,"Equalized odds is a statistical notion of fairness in machine learning that ensures that classification algorithms do not discriminate against protected groups. We extend equalized odds to the setting of cardinality-constrained fair classification, where we have a bounded amount of a resource to distribute. This setting coincides with classic fair division problems, which allows us to apply concepts from that literature in parallel to equalized odds. In particular, we consider the axioms of resource monotonicity, consistency, and population monotonicity, all three of which relate different allocation instances to prevent paradoxes. Using a geometric characterization of equalized odds, we examine the compatibility of equalized odds with these axioms. We empirically evaluate the cost of allocation rules that satisfy both equalized odds and axioms of fair division on a dataset of FICO credit scores.",[],[],"['Paul Goelz', 'Anson Kahng', 'Ariel D. Procaccia']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/bc047286b224b7bfa73d4cb02de1238d-Abstract.html,Fairness & Bias,Variance Reduction in Bipartite Experiments through Correlation Clustering,"Causal inference in randomized experiments typically assumes that the units of randomization and the units of analysis are one and the same. In some applications, however, these two roles are played by distinct entities linked by a bipartite graph. The key challenge in such bipartite settings is how to avoid interference bias, which would typically arise if we simply randomized the treatment at the level of analysis units. One effective way of minimizing interference bias in standard experiments is through cluster randomization, but this design has not been studied in the bipartite setting where conventional clustering schemes can lead to poorly powered experiments. This paper introduces a novel clustering objective and a corresponding algorithm that partitions a bipartite graph so as to maximize the statistical power of a bipartite experiment on that graph. Whereas previous work relied on balanced partitioning, our formulation suggests the use of a correlation clustering objective. We use a publicly-available graph of Amazon user-item reviews to validate our solution and illustrate how it substantially increases the statistical power in bipartite experiments.",[],[],"['Jean Pouget-Abadie', 'Kevin Aydin', 'Warren Schudy', 'Kay Brodersen', 'Vahab Mirrokni']","['Google Research, New York, NY', 'Google Research, Mountain View, CA', 'Google Research, New York, NY', 'Google Zürich, Switzerland', 'Google Research, New York, NY']",['Switzerland']
https://papers.nips.cc/paper_files/paper/2019/hash/bc7f621451b4f5df308a8e098112185d-Abstract.html,Fairness & Bias,Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms,"Matrix Factorization is a popular non-convex optimization problem,  for which alternating minimization schemes are mostly used. They usually suffer from the major drawback that the solution is biased towards one of the optimization variables. A remedy is non-alternating schemes. However, due to a lack of Lipschitz continuity of the gradient in matrix factorization problems, convergence cannot be guaranteed. A recently developed approach relies on the concept of Bregman distances, which generalizes the standard Euclidean distance. We exploit this theory by proposing a novel Bregman distance for matrix factorization problems, which, at the same time, allows for simple/closed form update steps. Therefore,  for non-alternating schemes, such as the recently introduced Bregman Proximal Gradient (BPG) method and an inertial variant Convex--Concave Inertial BPG (CoCaIn BPG), convergence of the whole sequence to a stationary point is proved for Matrix Factorization. In several experiments, we observe a superior performance of our non-alternating schemes in terms of speed and objective value at the limit point.",[],[],"['Mahesh Chandra Mukkamala', 'Peter Ochs']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html,Fairness & Bias,Integrating Bayesian and Discriminative Sparse Kernel Machines for  Multi-class Active Learning,"We propose a novel active learning (AL) model that integrates Bayesian and discriminative kernel machines for fast and accurate multi-class data sampling. By joining a sparse Bayesian model and a maximum margin machine under a unified kernel machine committee (KMC), the proposed model is able to identify a small number of data samples that best represent the overall data space while accurately capturing the decision boundaries. The integration is conducted using the  maximum entropy discrimination framework, resulting in a joint objective function that contains generalized entropy as a regularizer. Such a property allows the proposed AL model to choose data samples that more effectively handle non-separable classification problems. Parameter learning is achieved through a principled optimization framework that leverages  convex duality and sparse structure of KMC to efficiently optimize the joint objective function. Key model parameters are used to design a novel sampling function  to choose data samples that can simultaneously improve multiple decision boundaries, making it an effective sampler for problems with a large number of classes. Experiments conducted over both synthetic and real data and comparison with competitive AL methods demonstrate the effectiveness of the proposed model.",[],[],"['Weishi Shi', 'Qi Yu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/c0c783b5fc0d7d808f1d14a6e9c8280d-Abstract.html,Fairness & Bias,Implicit Regularization in Deep Matrix Factorization,"Efforts to understand the generalization mystery in deep learning have led to the belief that gradient-based optimization induces a form of implicit regularization, a bias towards models of low ""complexity.""  We study the implicit regularization of gradient descent over deep linear neural networks for matrix completion and sensing, a model referred to as deep matrix factorization.  Our first finding, supported by theory and experiments, is that adding depth to a matrix factorization enhances an implicit tendency towards low-rank solutions, oftentimes leading to more accurate recovery.  Secondly, we present theoretical and empirical arguments questioning a nascent view by which implicit regularization in matrix factorization can be captured using simple mathematical norms.  Our results point to the possibility that the language of standard regularizers may not be rich enough to fully encompass the implicit regularization brought forth by gradient-based optimization.",[],[],"['Sanjeev Arora', 'Nadav Cohen', 'Wei Hu', 'Yuping Luo']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/c2e06e9a80370952f6ec5463c77cbace-Abstract.html,Fairness & Bias,Fisher Efficient Inference of Intractable Models,"Maximum Likelihood Estimators (MLE) has many good properties. For example, the asymptotic variance of MLE solution attains equality of the asymptotic Cram{\'e}r-Rao lower bound (efficiency bound), which is the minimum possible variance for an unbiased estimator. However, obtaining such MLE solution requires calculating the likelihood function which may not be tractable due to the normalization term of the density model. In this paper, we derive a Discriminative Likelihood Estimator (DLE) from the Kullback-Leibler divergence minimization criterion implemented via density ratio estimation and a Stein operator. We study the problem of model inference using DLE. We prove its consistency and show that the asymptotic variance of its solution can attain the equality of the efficiency bound under mild regularity conditions. We also propose a dual formulation of DLE which can be easily optimized. Numerical studies validate our asymptotic theorems and we give an example where DLE successfully estimates an intractable model constructed using a pre-trained deep neural network.",[],[],"['Song Liu', 'Takafumi Kanamori', 'Wittawat Jitkrittum', 'Yu Chen']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/c429429bf1f2af051f2021dc92a8ebea-Abstract.html,Fairness & Bias,A Step Toward Quantifying Independently Reproducible Machine Learning Research,"What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.",[],[],['Edward Raff'],"['Booz Allen Hamilton and University of Maryland, Baltimore County']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/c4ef9c39b300931b69a36fb3dbb8d60e-Abstract.html,Fairness & Bias,On the Inductive Bias of Neural Tangent Kernels,"State-of-the-art neural networks are heavily over-parameterized, making the optimization algorithm a crucial ingredient for learning predictive models with good generalization properties. A recent line of work has shown that in a certain over-parameterized regime, the learning dynamics of gradient descent are governed by a certain kernel obtained at initialization, called the neural tangent kernel. We study the inductive bias of learning in such a regime by analyzing this kernel and the corresponding function space (RKHS). In particular, we study smoothness, approximation, and stability properties of functions with finite norm, including stability to image deformations in the case of convolutional networks, and compare to other known kernels for similar architectures.",[],[],"['Alberto Bietti', 'Julien Mairal']","['Inria', 'Inria']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/c61aed648da48aa3893fb3eaadd88a7f-Abstract.html,Fairness & Bias,Implicit Regularization of Accelerated Methods in Hilbert Spaces,"We study learning properties of accelerated gradient descent methods for linear least-squares in Hilbert spaces. We analyze the implicit regularization properties of Nesterov acceleration and a variant of heavy-ball in terms of corresponding learning error bounds. Our results show that acceleration can provides faster bias decay  than gradient descent, but also suffers of a more unstable behavior. As a result acceleration cannot be in general expected to improve learning accuracy with respect to gradient descent, but rather to achieve the same accuracy with reduced computations. Our theoretical results are validated by numerical simulations. Our analysis is based on studying suitable polynomials induced by the accelerated dynamics and combining spectral techniques with concentration inequalities.",[],[],"['Nicolò Pagliana', 'Lorenzo Rosasco']","['University of Genoa', 'University of Genoa, Italian Institute of Technology, Massachusetts Institute of Technology']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/cb3ce9b06932da6faaa7fc70d5b5d2f4-Abstract.html,Fairness & Bias,Online Normalization for Training Neural Networks,"Online Normalization is a new technique for normalizing the hidden activations of a neural network. Like Batch Normalization, it normalizes the sample dimension. While Online Normalization does not use batches, it is as accurate as Batch Normalization. We resolve a theoretical limitation of Batch Normalization by introducing an unbiased technique for computing the gradient of normalized activations. Online Normalization works with automatic differentiation by adding statistical normalization as a primitive. This technique can be used in cases not covered by some other normalizers, such as recurrent networks, fully connected networks, and networks with activation memory requirements prohibitive for batching. We show its applications to image classification, image segmentation, and language modeling. We present formal proofs and experimental results on ImageNet, CIFAR, and PTB datasets.",[],[],"['Vitaliy Chiley', 'Ilya Sharapov', 'Atli Kosson', 'Urs Koster', 'Ryan Reece', 'Sofia Samaniego de la Fuente', 'Vishal Subbiah', 'Michael James']","['Cerebras Systems, Los Altos, California', 'Cerebras Systems, Los Altos, California', 'Cerebras Systems, Los Altos, California', 'Cerebras Systems, Los Altos, California', 'Cerebras Systems, Los Altos, California', 'Cerebras Systems, Los Altos, California', 'Cerebras Systems, Los Altos, California', 'Cerebras Systems, Los Altos, California']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html,Fairness & Bias,Equitable Stable Matchings in Quadratic Time,"Can a stable matching that achieves high equity among the two sides of a market be reached in quadratic time? The Deferred Acceptance (DA) algorithm finds a stable matching that is biased in favor of one side; optimizing apt equity measures is strongly NP-hard. A proposed approximation algorithm offers a guarantee only with respect to the DA solutions. Recent work introduced Deferred Acceptance with Compensation Chains (DACC), a class of algorithms that can reach any stable matching in O(n^4) time, but did not propose a way to achieve good equity. In this paper, we propose an alternative that is computationally simpler and achieves high equity too. We introduce Monotonic Deferred Acceptance (MDA), a class of algorithms that progresses monotonically towards a stable matching; we couple MDA with a mechanism we call Strongly Deferred Acceptance (SDA), to build an algorithm that reaches an equitable stable matching in quadratic time; we amend this algorithm with a few low-cost local search steps to what we call Deferred Local Search (DLS), and demonstrate experimentally that it outperforms previous solutions in terms of equity measures and matches the most efficient ones in runtime.",[],[],"['Nikolaos Tziavelis', 'Ioannis Giannakopoulos', 'Katerina Doka', 'Nectarios Koziris', 'Panagiotis Karras']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/cd474f6341aeffd65f93084d0dae3453-Abstract.html,Fairness & Bias,A Unifying Framework for Spectrum-Preserving Graph Sparsification and Coarsening,"How might one ``reduce'' a graph? That is, generate a smaller graph that preserves the global structure at the expense of discarding local details?  There has been extensive work on both graph sparsification (removing edges) and graph coarsening (merging nodes, often by edge contraction); however, these operations are currently treated separately.  Interestingly, for a planar graph, edge deletion corresponds to edge contraction in its planar dual (and more generally, for a graphical matroid and its dual).  Moreover, with respect to the dynamics induced by the graph Laplacian (e.g., diffusion), deletion and contraction are physical manifestations of two reciprocal limits: edge weights of $0$ and $\infty$, respectively.  In this work, we provide a unifying framework that captures both of these operations, allowing one to simultaneously sparsify and coarsen a graph while preserving its large-scale structure.  The limit of infinite edge weight is rarely considered, as many classical notions of graph similarity diverge.  However, its algebraic, geometric, and physical interpretations are reflected in the Laplacian pseudoinverse $\mat{L}^\dagger$, which remains finite in this limit.  Motivated by this insight, we provide a probabilistic algorithm that reduces graphs while preserving $\mat{L}^\dagger$, using an unbiased procedure that minimizes its variance. We compare our algorithm with several existing sparsification and coarsening algorithms using real-world datasets, and demonstrate that it more accurately preserves the large-scale structure.",[],[],"['Gecia Bravo Hermsdorff', 'Lee Gunderson']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d0ac1ed0c5cb9ecbca3d2496ec1ad984-Abstract.html,Fairness & Bias,Re-examination of the Role of Latent Variables in Sequence Modeling,"With latent variables, stochastic recurrent models have achieved state-of-the-art performance in modeling sound-wave sequence.However, opposite results are also observed in other domains, where standard recurrent networks often outperform stochastic models.To better understand this discrepancy, we re-examine the roles of latent variables in stochastic recurrent models for speech density estimation.Our analysis reveals that under the restriction of fully factorized output distribution in previous evaluations, the stochastic variants were implicitly leveraging intra-step correlation but the deterministic recurrent baselines were prohibited to do so, resulting in an unfair comparison.To correct the unfairness, we remove such restriction in our re-examination, where all the models can explicitly leverage intra-step correlation with an auto-regressive structure.Over a diverse set of univariate and multivariate sequential data, including human speech, MIDI music, handwriting trajectory, and frame-permuted speech, our results show that stochastic recurrent models fail to deliver the performance advantage claimed in previous work. %exhibit any practical advantage despite the claimed theoretical superiority. In contrast, standard recurrent models equipped with an auto-regressive output distribution consistently perform better, dramatically advancing the state-of-the-art results on three speech datasets.",[],[],"['Guokun Lai', 'Zihang Dai', 'Yiming Yang', 'Shinjae Yoo']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Brookhaven National Laboratory']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d3d80b656929a5bc0fa34381bf42fbdd-Abstract.html,Fairness & Bias,On Testing for Biases in Peer Review,"We consider the issue of biases in scholarly research, specifically, in peer review. There is a long standing debate on whether exposing author identities to reviewers induces biases against certain groups, and our focus is on designing tests to detect the presence of such biases. Our starting point is a remarkable recent work by Tomkins, Zhang and Heavlin which conducted a controlled, large-scale experiment to investigate existence of biases in the peer reviewing of the WSDM conference. We present two sets of results in this paper. The first set of results is negative, and pertains to the statistical tests and the experimental setup used in the work of Tomkins et al. We show that the test employed therein does not guarantee control over false alarm probability and under correlations between relevant variables, coupled with any of the following conditions, with high probability can declare a presence of bias when it is in fact absent: (a) measurement error, (b) model mismatch, (c) reviewer calibration. Moreover, we show that the setup of their experiment may itself inflate false alarm probability if (d) bidding is performed in non-blind manner or (e) popular reviewer assignment procedure is employed.  Our second set of results is positive, in that we present a general framework for testing for biases in (single vs. double blind) peer review. We then present a hypothesis test with guaranteed control over false alarm probability and non-trivial power even under conditions (a)--(c). Conditions (d) and (e) are more fundamental problems that are tied to the experimental setup and not necessarily related to the test.",[],[],"['Ivan Stelmakh', 'Nihar Shah', 'Aarti Singh']","['School of Computer Science, Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d464b5ac99e74462f321c06ccacc4bff-Abstract.html,Fairness & Bias,Manifold-regression to predict from MEG/EEG brain signals without source modeling,"Magnetoencephalography and electroencephalography (M/EEG) can reveal neuronal dynamics non-invasively in real-time and are therefore appreciated methods in medicine and neuroscience. Recent advances in modeling brain-behavior relationships have highlighted the effectiveness of Riemannian geometry for summarizing the spatially correlated time-series from M/EEG in terms of their covariance. However, after artefact-suppression, M/EEG data is often rank deficient which limits the application of Riemannian concepts. In this article, we focus on the task of regression with rank-reduced covariance matrices. We study two Riemannian approaches that vectorize the M/EEG covariance between sensors through projection into a tangent space. The Wasserstein distance readily applies to rank-reduced data but lacks affine-invariance. This can be overcome by finding a common subspace in which the covariance matrices are full rank, enabling the affine-invariant geometric distance. We investigated the implications of these two approaches in synthetic generative models, which allowed us to control estimation bias of a linear model for prediction. We show that Wasserstein and geometric distances allow perfect out-of-sample prediction on the generative models. We then evaluated the methods on real data with regard to their effectiveness in predicting age from M/EEG covariance matrices. The findings suggest that the data-driven Riemannian methods outperform different sensor-space estimators and that they get close to the performance of biophysics-driven source-localization model that requires MRI acquisitions and tedious data processing. Our study suggests that the proposed Riemannian methods can serve as fundamental building-blocks for automated large-scale analysis of M/EEG.",[],[],"['David Sabbagh', 'Pierre Ablin', 'Gael Varoquaux', 'Alexandre Gramfort', 'Denis A. Engemann']","['Université Paris-Saclay, Inria, CEA, Palaiseau, France and Inserm, UMRS-942, Paris Diderot University, Paris, France and Department of Anaesthesiology and Critical Care, Lariboisière Hospital, Assistance Publique Hôpitaux de Paris, Paris, France', 'Université Paris-Saclay, Inria, CEA, Palaiseau, France', 'Université Paris-Saclay, Inria, CEA, Palaiseau, France', 'Université Paris-Saclay, Inria, CEA, Palaiseau, France', 'Université Paris-Saclay, Inria, CEA, Palaiseau, France']","['France', 'France', 'France', 'France', 'France']"
https://papers.nips.cc/paper_files/paper/2019/hash/d4a93297083a23cc099f7bd6a8621131-Abstract.html,Fairness & Bias,Sim2real transfer learning for 3D human pose estimation: motion to the rescue,"Synthetic visual data can provide practicically infinite diversity and rich labels,while avoiding ethical issues with privacy and bias. However, for many tasks,current models trained on synthetic data generalize poorly to real data. The task of3D human pose estimation is a particularly interesting example of this sim2realproblem, because learning-based approaches perform reasonably well given realtraining data, yet labeled 3D poses are extremely difficult to obtain in the wild,limiting scalability. In this paper, we show that standard neural-network approaches,which perform poorly when trained on synthetic RGB images, can perform wellwhen the data is pre-processed to extract cues about the person’s motion, notablyas optical flow and the motion of 2D keypoints. Therefore, our results suggestthat motion can be a simple way to bridge a sim2real gap when video is available.We evaluate on the 3D Poses in the Wild dataset, the most challenging modernbenchmark for 3D pose estimation, where we show full 3D mesh recovery that ison par with state-of-the-art methods trained on real 3D sequences, despite trainingonly on synthetic humans from the SURREAL dataset.",[],[],"['Carl Doersch', 'Andrew Zisserman']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d630553e32ae21fb1a6df39c702d2c5c-Abstract.html,Fairness & Bias,A Universally Optimal Multistage Accelerated Stochastic Gradient Method,"We study the problem of minimizing a strongly convex, smooth function when we have noisy estimates of its gradient. We propose a novel multistage accelerated algorithm that is universally optimal in the sense that it achieves the optimal rate both in the deterministic and stochastic case and operates without knowledge of noise characteristics. The algorithm consists of stages that use a stochastic version of Nesterov's method with a specific restart and parameters selected to achieve the fastest reduction in the bias-variance terms in the convergence rate bounds.",[],[],"['Necdet Serhat Aybat', 'Alireza Fallah', 'Mert Gurbuzbalaban', 'Asuman Ozdaglar']","['Pennsylvania State University, University Park, PA', 'Massachusetts Institute of Technology, Cambridge, MA', 'Rutgers University, Piscataway, NJ', 'Massachusetts Institute of Technology, Cambridge, MA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d69768b3da745b77e82cdbddcc8bac98-Abstract.html,Fairness & Bias,Offline Contextual Bandits with High Probability Fairness Guarantees,"We present RobinHood, an ofﬂine contextual bandit algorithm designed to satisfy a broad family of fairness constraints. Our algorithm accepts multiple fairness deﬁnitions and allows users to construct their own unique fairness deﬁnitions for the problem at hand. We provide a theoretical analysis of RobinHood, which includes a proof that it will not return an unfair solution with probability greater than a user-speciﬁed threshold. We validate our algorithm on three applications: a tutoring system in which we conduct a user study and consider multiple unique fairness deﬁnitions; a loan approval setting (using the Statlog German credit data set) in which well-known fairness deﬁnitions are applied; and criminal recidivism (using data released by ProPublica). In each setting, our algorithm is able to produce fair policies that achieve performance competitive with other ofﬂine and online contextual bandit algorithms.",[],[],"['Blossom Metevier', 'Stephen Giguere', 'Sarah Brockman', 'Ari Kobren', 'Yuriy Brun', 'Emma Brunskill', 'Philip S. Thomas']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d76d8deea9c19cc9aaf2237d2bf2f785-Abstract.html,Fairness & Bias,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting,"A learned generative model often produces biased statistics relative to the underlying data distribution. A standard technique to correct this bias is importance sampling, where samples from the model are weighted  by the likelihood ratio under model and true distributions. When the likelihood ratio is unknown, it can be estimated by training a probabilistic classifier to distinguish samples from the two distributions. We employ this likelihood-free importance weighting method to correct for the bias in generative models. We find that this technique consistently improves standard goodness-of-fit metrics for evaluating the sample quality of state-of-the-art deep generative models, suggesting reduced bias. Finally, we demonstrate its utility on representative applications in a) data augmentation for classification using generative adversarial networks, and b) model-based policy evaluation using off-policy data.",[],[],"['Aditya Grover', 'Jiaming Song', 'Ashish Kapoor', 'Kenneth Tran', 'Alekh Agarwal', 'Eric J. Horvitz', 'Stefano Ermon']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d828725179d622a56f951e527a966ed7-Abstract.html,Fairness & Bias,Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption,"Matrix completion is often applied to data with entries missing not at random (MNAR). For example, consider a recommendation system where users tend to only reveal ratings for items they like. In this case, a matrix completion method that relies on entries being revealed at uniformly sampled row and column indices can yield overly optimistic predictions of unseen user ratings. Recently, various papers have shown that we can reduce this bias in MNAR matrix completion if we know the probabilities of different matrix entries being missing. These probabilities are typically modeled using logistic regression or naive Bayes, which make strong assumptions and lack guarantees on the accuracy of the estimated probabilities. In this paper, we suggest a simple approach to estimating these probabilities that avoids these shortcomings. Our approach follows from the observation that missingness patterns in real data often exhibit low nuclear norm structure. We can then estimate the missingness probabilities by feeding the (always fully-observed) binary matrix specifying which entries are revealed to an existing nuclear-norm-constrained matrix completion algorithm by Davenport et al. [2014]. Thus, we tackle MNAR matrix completion by solving a different matrix completion problem first that recovers missingness probabilities. We establish finite-sample error bounds for how accurate these probability estimates are and how well these estimates debias standard matrix completion losses for the original matrix to be completed. Our experiments show that the proposed debiasing strategy can improve a variety of existing matrix completion algorithms, and achieves downstream matrix completion accuracy at least as good as logistic regression and naive Bayes debiasing baselines that require additional auxiliary information.",[],[],"['Wei Ma', 'George H. Chen']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d9731321ef4e063ebbee79298fa36f56-Abstract.html,Fairness & Bias,Initialization of ReLUs for Dynamical Isometry,"Deep learning relies on good initialization schemes and hyperparameter choices prior to training a neural network. Random weight initializations induce random network ensembles, which give rise to the trainability, training speed, and sometimes also generalization ability of an instance. In addition, such ensembles provide theoretical insights into the space of candidate models of which one is selected during training. The results obtained so far rely on mean field approximations that assume infinite layer width and that study average squared signals. We derive the joint signal output distribution exactly, without mean field assumptions, for fully-connected networks with Gaussian weights and biases, and analyze deviations from the mean field results. For rectified linear units, we further discuss limitations of the standard initialization scheme, such as its lack of dynamical isometry, and propose a simple alternative that overcomes these by initial parameter sharing.",[],[],"['Rebekka Burkholz', 'Alina Dubatovka']","['Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA', 'Department of Computer Science, ETH Zurich, Zurich']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d97d404b6119214e4a7018391195240a-Abstract.html,Fairness & Bias,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset,"Learning meaningful and compact representations with disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect, many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper, we propose a novel data-set which consists of over 1 million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position. In order to be able to control all the factors of variation precisely, we built an experimental platform where the objects are being moved by a robotic arm. In addition, we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation, and how simulated data can be leveraged to build better representations of the real world. We provide a first experimental study of these questions and our results indicate that learned models transfer poorly, but that model and hyperparameter selection is an effective means of transferring information to the real world.",[],[],"['Muhammad Waleed Gondal', 'Manuel Wuthrich', 'Djordje Miladinovic', 'Francesco Locatello', 'Martin Breidt', 'Valentin Volchkov', 'Joel Akpo', 'Olivier Bachem', 'Bernhard Schölkopf', 'Stefan Bauer']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/de594ef5c314372edec29b93cab9d72e-Abstract.html,Fairness & Bias,Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs,"The exploration bonus is an effective approach to manage the exploration-exploitation trade-off in Markov Decision Processes (MDPs).While it has been analyzed in infinite-horizon discounted and finite-horizon problems, we focus on designing and analysing the exploration bonus in the more challenging infinite-horizon undiscounted setting.We first introduce SCAL+, a variant of SCAL (Fruit et al. 2018), that uses a suitable exploration bonus to solve any discrete unknown weakly-communicating MDP for which an upper bound $c$ on the span of the optimal bias function is known. We prove that SCAL+ enjoys the same regret guarantees as SCAL, which relies on the less efficient extended value iteration approach.Furthermore, we leverage the flexibility provided by the exploration bonus scheme to generalize SCAL+ to smooth MDPs with continuous state space and discrete actions. We show that the resulting algorithm (SCCAL+) achieves the same regret bound as UCCRL (Ortner and Ryabko, 2012) while being the first implementable algorithm for this setting.",[],[],"['Jian QIAN', 'Ronan Fruit', 'Matteo Pirotta', 'Alessandro Lazaric']","['Sequel Team - Inria Lille', 'Sequel Team - Inria Lille', 'Facebook AI Research', 'Facebook AI Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/e00406144c1e7e35240afed70f34166a-Abstract.html,Fairness & Bias,Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design,"The increasing popularity of online two-sided markets such as ride-sharing, accommodation and freelance labor platforms, goes hand in hand with new socioeconomic challenges.  One major issue remains the existence of bias and discrimination against certain social groups. We study this problem using  a two-sided large market model with  employers and workers mediated by a platform. Employers who seek to hire  workers face uncertainty about a candidate worker's  skill level. Therefore, they base their hiring decision  on learning from past reviews about an individual worker as well as on their (possibly misspecified)  prior beliefs about the ability level of the social group the worker belongs to. Drawing upon the  social learning literature with bounded rationality and limited information, uncertainty combined with social bias leads to  unequal hiring opportunities between workers of different social groups. Although the effect of social bias decreases as the number of reviews increases (consistent with empirical findings), minority workers still receive lower expected  payoffs. Finally, we  consider a simple directed matching policy (DM), which combines learning and matching to make better matching decisions for minority workers. Under this policy, there exists a steady-state equilibrium, in which  DM reduces the discrimination gap.",[],[],"['Faidra Georgia Monachou', 'Itai Ashlagi']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e0e2b58d64fb37a2527329a5ce093d80-Abstract.html,Fairness & Bias,Online-Within-Online Meta-Learning,"We study the problem of learning a series of tasks in a fully online Meta-Learningsetting. The goal is to exploit similarities among the tasks to incrementally adaptan inner online algorithm in order to incur a low averaged cumulative error overthe tasks. We focus on a family of inner algorithms based on a parametrizedvariant of online Mirror Descent. The inner algorithm is incrementally adaptedby an online Mirror Descent meta-algorithm using the corresponding within-taskminimum regularized empirical risk as the meta-loss. In order to keep the processfully online, we approximate the meta-subgradients by the online inner algorithm.An upper bound on the approximation error allows us to derive a cumulativeerror bound for the proposed method. Our analysis can also be converted to thestatistical setting by online-to-batch arguments. We instantiate two examples of theframework in which the meta-parameter is either a common bias vector or featuremap. Finally, preliminary numerical experiments confirm our theoretical findings.",[],[],"['Giulia Denevi', 'Dimitris Stamos', 'Carlo Ciliberto', 'Massimiliano Pontil']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e43739bba7cdb577e9e3e4e42447f5a5-Abstract.html,Fairness & Bias,Sampled Softmax with Random Fourier Features,"The computational cost of training with softmax cross entropy loss grows linearly with the number of classes. For the settings where a large number of classes are involved, a common method to speed up training is to sample a subset of classes and utilize an estimate of the loss gradient based on these classes, known as the sampled softmax method. However, the sampled softmax provides a biased estimate of the gradient unless the samples are drawn from the exact softmax distribution, which is again expensive to compute. Therefore, a widely employed practical approach involves sampling from a simpler distribution in the hope of approximating the exact softmax distribution. In this paper, we develop the first theoretical understanding of the role that different sampling distributions play in determining the quality of sampled softmax. Motivated by our analysis and the work on kernel-based sampling, we propose the Random Fourier Softmax (RF-softmax) method that utilizes the powerful Random Fourier Features to enable more efficient and accurate sampling from an approximate softmax distribution. We show that RF-softmax leads to low bias in estimation in terms of both the full softmax distribution and the full softmax gradient. Furthermore, the cost of RF-softmax scales only logarithmically with the number of classes.",[],[],"['Ankit Singh Rawat', 'Jiecao Chen', 'Felix Xinnan X. Yu', 'Ananda Theertha Suresh', 'Sanjiv Kumar']","['Google Research, New York', 'Google Research, New York', 'Google Research, New York', 'Google Research, New York', 'Google Research, New York']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/e49eb6523da9e1c347bc148ea8ac55d3-Abstract.html,Fairness & Bias,A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation,"Reinforcement learning is effective in optimizing policies for recommender systems. Current solutions mostly focus on model-free approaches, which require frequent interactions with a real environment, and thus are expensive in model learning. Offline evaluation methods, such as importance sampling, can alleviate such limitations, but usually request a large amount of logged data and do not work well when the action space is large. In this work, we propose a model-based reinforcement learning solution which models the user-agent interaction for offline policy learning via a generative adversarial network. To reduce bias in the learnt policy, we use the discriminator to evaluate the quality of generated sequences and rescale the generated rewards. Our theoretical analysis and empirical evaluations demonstrate the effectiveness of our solution in identifying patterns from given offline data and learning policies based on the offline and generated data.",[],[],"['Xueying Bai', 'Jian Guan', 'Hongning Wang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e4dd5528f7596dcdf871aa55cfccc53c-Abstract.html,Fairness & Bias,Predicting the Politics of an Image Using Webly Supervised Data,"The news media shape public opinion, and often, the visual bias they contain is evident for human observers. This bias can be inferred from how different media sources portray different subjects or topics. In this paper, we model visual political bias in contemporary media sources at scale, using webly supervised data. We collect a dataset of over one million unique images and associated news articles from left- and right-leaning news sources, and develop a method to predict the image's political leaning. This problem is particularly challenging because of the enormous intra-class visual and semantic diversity of our data. We propose a two-stage method to tackle this problem. In the first stage, the model is forced to learn relevant visual concepts that, when joined with document embeddings computed from articles paired with the images, enable the model to predict bias. In the second stage, we remove the requirement of the text domain and train a visual classifier from the features of the former model. We show this two-stage approach facilitates learning and outperforms several strong baselines. We also present extensive qualitative results demonstrating the nuances of the data.",[],[],"['Christopher Thomas', 'Adriana Kovashka']","['Department of Computer Science, University of Pittsburgh, Pittsburgh, PA', 'Department of Computer Science, University of Pittsburgh, Pittsburgh, PA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/e58cc5ca94270acaceed13bc82dfedf7-Abstract.html,Fairness & Bias,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,"Current deep neural networks(DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting function forms including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.",[],[],"['Jun Shu', 'Qi Xie', 'Lixuan Yi', 'Qian Zhao', 'Sanping Zhou', 'Zongben Xu', 'Deyu Meng']","[""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University""]",[]
https://papers.nips.cc/paper_files/paper/2019/hash/e88f243bf341ded9b4ced444795c3f17-Abstract.html,Fairness & Bias,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations,"From infancy, humans have expectations about how objects will move and interact. Even young children expect objects not to move through one another, teleport, or disappear. They are surprised by mismatches between physical expectations and perceptual observations, even in unfamiliar scenes with completely novel objects. A model that exhibits human-like understanding of physics should be similarly surprised, and adjust its beliefs accordingly.  We propose ADEPT, a model that uses a coarse (approximate geometry) object-centric representation for dynamic 3D scene understanding. Inference integrates deep recognition networks, extended probabilistic physical simulation, and particle filtering for forming predictions and expectations across occlusion. We also present a new test set for measuring violations of physical expectations, using a range of scenarios derived from developmental psychology.  We systematically compare ADEPT, baseline models, and human expectations on this test set.  ADEPT outperforms standard network architectures in discriminating physically implausible scenes, and often performs this discrimination at the same level as people.",[],[],"['Kevin Smith', 'Lingjie Mei', 'Shunyu Yao', 'Jiajun Wu', 'Elizabeth Spelke', 'Josh Tenenbaum', 'Tomer Ullman']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e9bf14a419d77534105016f5ec122d62-Abstract.html,Fairness & Bias,DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging,"We propose a recurrent neural-network for real-time reconstruction of acoustic camera spherical maps. The network, dubbed DeepWave, is both physically and algorithmically motivated: its recurrent architecture mimics iterative solvers from convex optimisation, and its parsimonious parametrisation is based on the natural structure of acoustic imaging problems.Each network layer applies successive filtering, biasing and activation steps to its input, which can be interpreted as generalised deblurring and sparsification steps. To comply with the irregular geometry of spherical maps, filtering operations are implemented efficiently by means of graph signal processing techniques.Unlike commonly-used imaging network architectures, DeepWave is moreover capable of directly processing the complex-valued raw microphone correlations, learning how to optimally back-project these into a spherical map. We propose moreover a smart physically-inspired initialisation scheme that attains much faster training and higher performance than random initialisation.Our real-data experiments show DeepWave has similar computational speed to the state-of-the-art delay-and-sum imager with vastly superior resolution. While developed primarily for acoustic cameras, DeepWave could easily be adapted to neighbouring  signal processing fields, such as radio astronomy, radar and sonar.",[],[],"['Matthieu SIMEONI', 'Sepand Kashani', 'Paul Hurley', 'Martin Vetterli']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html,Fairness & Bias,Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks,"When optimizing over-parameterized models, such as deep neural networks, a large set of parameters can achieve zero training error. In such cases, the choice of the optimization algorithm and its respective hyper-parameters introduces biases that will lead to convergence to specific minimizers of the objective. Consequently, this choice can be considered as an implicit regularization for the training of over-parametrized models. In this work, we push this idea further by studying the discrete gradient dynamics of the training of a two-layer linear network with the least-squares loss. Using a time rescaling, we show that, with a vanishing initialization and a small enough step size, this dynamics sequentially learns the solutions of a reduced-rank regression with a gradually increasing rank.",[],[],"['Gauthier Gidel', 'Francis Bach', 'Simon Lacoste-Julien']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/f7177163c833dff4b38fc8d2872f1ec6-Abstract.html,Fairness & Bias,High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks,"Predicting future video frames is extremely challenging, as there are many factors of variation that make up the dynamics of how frames change through time. Previously proposed solutions require complex inductive biases inside network architectures with highly specialized computation, including segmentation masks, optical flow, and foreground and background separation. In this work, we question if such handcrafted architectures are necessary and instead propose a different approach: finding minimal inductive bias for video prediction while maximizing network capacity. We investigate this question by performing the first large-scale empirical study and demonstrate state-of-the-art performance by learning large models on three different datasets: one for modeling object interactions, one for modeling human motion, and one for modeling car driving.",[],[],"['Ruben Villegas', 'Arkanath Pathak', 'Harini Kannan', 'Dumitru Erhan', 'Quoc V. Le', 'Honglak Lee']","['University of Michigan and Adobe Research', 'Google', 'Google Research', 'Google Research', 'Google Research', 'Google Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html,Fairness & Bias,Differential Privacy Has Disparate Impact on Model Accuracy,"Differential privacy (DP) is a popular mechanism for training machinelearning models with bounded leakage about the presence of specificpoints in the training data.  The cost of differential privacy is areduction in the model's accuracy.  We demonstrate that in the neuralnetworks trained using differentially private stochastic gradient descent(DP-SGD), this cost is not borne equally: accuracy of DP models dropsmuch more for the underrepresented classes and subgroups.For example, a gender classification model trained using DP-SGD exhibitsmuch lower accuracy for black faces than for white faces.  Critically,this gap is bigger in the DP model than in the non-DP model, i.e., ifthe original model is unfair, the unfairness becomes worse once DP isapplied.  We demonstrate this effect for a variety of tasks and models,including sentiment analysis of text and image classification.  We thenexplain why DP training mechanisms such as gradient clipping and noiseaddition have disproportionate effect on the underrepresented and morecomplex subgroups, resulting in a disparate reduction of model accuracy.",[],[],"['Eugene Bagdasaryan', 'Omid Poursaeed', 'Vitaly Shmatikov']","['Cornell Tech', 'Cornell Tech', 'Cornell Tech']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/fd0a5a5e367a0955d81278062ef37429-Abstract.html,Fairness & Bias,Unsupervised Meta-Learning for Few-Shot Image Classification,"Few-shot or one-shot learning of classifiers requires a significant inductive bias towards the type of task to be learned. One way to acquire this is by meta-learning on tasks similar to the target task. In this paper, we propose UMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning for classification tasks. The meta-learning step of UMTRA is performed on a flat collection of unlabeled images. While we assume that these images can be grouped into a diverse set of classes and are relevant to the target task, no explicit information about the classes or any labels are needed.  UMTRA uses random sampling and augmentation to create synthetic training tasks for meta-learning phase. Labels are only needed at the final target task learning step, and they can be as little as one sample per class. On the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested approach based on unsupervised learning of representations, while alternating for the best performance with the recent CACTUs algorithm. Compared to supervised model-agnostic meta-learning approaches, UMTRA trades off some classification accuracy for a reduction in the required labels of several orders of magnitude.",[],[],"['Siavash Khodadadeh', 'Ladislau Boloni', 'Mubarak Shah']","['Dept. of Computer Science, University of Central Florida', 'Dept. of Computer Science, University of Central Florida', 'Center for Research in Computer Vision, University of Central Florida']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html,Fairness & Bias,Biases for Emergent Communication in Multi-agent Reinforcement Learning,"We study the problem of emergent communication, in which language arises because speakers and listeners must communicate information in order to solve tasks. In temporally extended reinforcement learning domains, it has proved hard to learn such communication without centralized training of agents, due in part to a difficult joint exploration problem. We introduce inductive biases for positive signalling and positive listening, which ease this problem. In a simple one-step environment, we demonstrate how these biases ease the learning problem. We also apply our methods to a more extended environment, showing that agents with these inductive biases achieve better performance, and analyse the resulting communications protocols.",[],[],"['Tom Eccles', 'Yoram Bachrach', 'Guy Lever', 'Angeliki Lazaridou', 'Thore Graepel']","['DeepMind, London, UK', 'DeepMind, London, UK', 'DeepMind, London, UK', 'DeepMind, London, UK', 'DeepMind, London, UK']","['UK', 'UK', 'UK', 'UK', 'UK']"
https://papers.nips.cc/paper_files/paper/2019/hash/feab05aa91085b7a8012516bc3533958-Abstract.html,Fairness & Bias,Random deep neural networks are biased towards simple functions,"We prove that the binary classifiers of bit strings generated by random wide deep neural networks with ReLU activation function are biased towards simple functions. The simplicity is captured by the following two properties. For any given input bit string, the average Hamming distance of the closest input bit string with a different classification is at least sqrt(n / (2π log n)), where n is the length of the string. Moreover, if the bits of the initial string are flipped randomly, the average number of flips required to change the classification grows linearly with n. These results are confirmed by numerical experiments on deep neural networks with two hidden layers, and settle the conjecture stating that random deep neural networks are biased towards simple functions. This conjecture was proposed and numerically explored in [Valle Pérez et al., ICLR 2019] to explain the unreasonably good generalization properties of deep learning algorithms. The probability distribution of the functions generated by random deep neural networks is a good choice for the prior probability distribution in the PAC-Bayesian generalization bounds. Our results constitute a fundamental step forward in the characterization of this distribution, therefore contributing to the understanding of the generalization properties of deep learning algorithms.",[],[],"['Giacomo De Palma', 'Bobak Kiani', 'Seth Lloyd']","['MechE & RLE, MIT, Cambridge MA', 'MechE & RLE, MIT, Cambridge MA', 'MechE, Physics & RLE, MIT, Cambridge MA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html,Privacy & Data Governance,Capacity Bounded Differential Privacy,"Differential privacy, a notion of algorithmic stability, is a gold standard for measuring the additional risk an algorithm's output poses to the privacy of asingle record in the dataset. Differential privacy is defined as the distancebetween the output distribution of an algorithm on neighboring datasets thatdiffer in one entry. In this work, we present a novel relaxation of differentialprivacy, capacity bounded differential privacy, where the adversarythat distinguishes output distributions is assumed to becapacity-bounded -- i.e. bounded not in computational power, but interms of the function class from which their attack algorithm is drawn. We modeladversaries in terms of restricted f-divergences between probabilitydistributions, and study properties of the definition and algorithms thatsatisfy them.",[],[],"['Kamalika Chaudhuri', 'Jacob Imola', 'Ashwin Machanavajjhala']","['UC San Diego', 'UC San Diego', 'Duke University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html,Privacy & Data Governance,MixMatch: A Holistic Approach to Semi-Supervised Learning,"Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets.In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, thatguesses low-entropy labels for data-augmented unlabeled examples and mixes labeled and unlabeled data using MixUp.MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example,on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10.We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy.Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.Code is attached.",[],[],"['David Berthelot', 'Nicholas Carlini', 'Ian Goodfellow', 'Nicolas Papernot', 'Avital Oliver', 'Colin A. Raffel']","['Google Research', 'Google Research', 'Work done at Google', 'Google Research', 'Google Research', 'Google Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/376c6b9ff3bedbbea56751a84fffc10c-Abstract.html,Privacy & Data Governance,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors,"Differential privacy has emerged as the main definition for private data analysis and machine learning. The global model of differential privacy, which assumes that users trust the data collector, provides strong privacy guarantees and introduces small errors in the output. In contrast, applications of differential privacy in commercial systems by Apple, Google, and Microsoft, use the local model. Here, users do not trust the data collector, and hence randomize their data before sending it to the data collector. Unfortunately, local model is too strong for several important applications and hence is limited in its applicability. In this work, we propose a framework based on trusted processors and a new definition of differential privacy called Oblivious Differential Privacy, which combines the best of both local and global models. The algorithms we design in this framework show interesting interplay of ideas from the streaming algorithms, oblivious algorithms, and differential privacy.",[],[],"['Joshua Allen', 'Bolin Ding', 'Janardhan Kulkarni', 'Harsha Nori', 'Olga Ohrimenko', 'Sergey Yekhanin']","['Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/3c88c1db16b9523b4dcdcd572aa1e16a-Abstract.html,Privacy & Data Governance,Differentially Private Distributed Data Summarization under Covariate Shift,"We envision Artificial Intelligence marketplaces to be platforms where consumers, with very less data for a target task, can obtain a relevant model by accessing many private data sources with vast number of data samples.  One of the key challenges is to construct a training dataset that matches a target task without compromising on privacy of the data sources. To this end, we consider the following distributed data summarizataion problem. Given K private source datasets denoted by $[D_i]_{i\in [K]}$ and a small target validation set $D_v$, which may involve a considerable covariate shift with respect to the sources, compute a summary dataset $D_s\subseteq \bigcup_{i\in [K]} D_i$ such that its statistical distance from the validation dataset $D_v$ is minimized. We use the popular Maximum Mean Discrepancy as the measure of statistical distance. The non-private problem has received considerable attention in prior art, for example in prototype selection (Kim et al., NIPS 2016). Our work is the first to obtain strong differential privacy guarantees while ensuring the quality guarantees of the non-private version. We study this problem in a Parsimonious Curator Privacy Model, where a trusted curator coordinates the summarization process while minimizing the amount of private information accessed. Our central result is a novel protocol that (a) ensures the curator does not access more than $O(K^{\frac{1}{3}}|D_s| + |D_v|)$ points (b) has formal privacy guarantees on the leakage of information between the data owners and (c) closely matches the  best known non-private greedy algorithm. Our protocol uses two hash functions, one inspired by the Rahimi-Recht random features method and the second leverages state of the art differential privacy mechanisms. We introduce a novel ``noiseless'' differentially private auctioning protocol, which may be of independent interest.  Apart from theoretical guarantees, we demonstrate the efficacy of our protocol using real-world datasets.",[],[],"['Kanthi Sarpatwar', 'Karthikeyan Shanmugam', 'Venkata Sitaramagiridharganesh Ganapavarapu', 'Ashish Jagmohan', 'Roman Vaculin']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/3ef815416f775098fe977004015c6193-Abstract.html,Privacy & Data Governance,Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation,"The simplest and most widely applied method for guaranteeing differential privacy is to add instance-independent noise to a statistic of interest that is scaled to its global sensitivity. However, global sensitivity is a worst-case notion that is often too conservative for realized dataset instances. We provide methods for scaling noise in an instance-dependent way and demonstrate that they provide greater accuracy under average-case distributional assumptions. Specifically, we consider the basic problem of privately estimating the mean of a real distribution from i.i.d. samples. The standard empirical mean estimator can have arbitrarily-high global sensitivity. We propose the trimmed mean estimator, which interpolates between the mean and the median, as a way of attaining much lower sensitivity on average while losing very little in terms of statistical accuracy. To privately estimate the trimmed mean, we revisit the smooth sensitivity framework of Nissim, Raskhodnikova, and Smith (STOC 2007), which provides a framework for using instance-dependent sensitivity. We propose three new additive noise distributions which provide concentrated differential privacy when scaled to smooth sensitivity. We provide theoretical and experimental evidence showing that our noise distributions compare favorably to others in the literature, in particular, when applied to the mean estimation problem.",[],[],"['Mark Bun', 'Thomas Steinke']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/4158f6d19559955bae372bb00f6204e4-Abstract.html,Privacy & Data Governance,Differentially Private Covariance Estimation,"The covariance matrix of a dataset is a fundamental statistic that can be used for calculating optimum regression weights as well as in many other learning and data analysis settings. For datasets containing private user information, we often want to estimate the covariance matrix in a way that preserves differential privacy. While there are known methods for privately computing the covariance matrix, they all have one of two major shortcomings. Some, like the Gaussian mechanism, only guarantee (epsilon, delta)-differential privacy, leaving a non-trivial probability of privacy failure. Others give strong epsilon-differential privacy guarantees, but are impractical, requiring complicated sampling schemes, and tend to perform poorly on real data. In this work we propose a new epsilon-differentially private algorithm for computing the covariance matrix of a dataset that addresses both of these limitations. We show that it has lower error than existing state-of-the-art approaches, both analytically and empirically. In addition, the algorithm is significantly less complicated than other methods and can be efficiently implemented with rejection sampling.",[],[],"['Kareem Amin', 'Travis Dick', 'Alex Kulesza', 'Andres Munoz', 'Sergei Vassilvitskii']","['Google Research NY', 'Carnegie Mellon University', 'Google Research NY', 'Google Research NY', 'Google Research NY']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/41c576a3bac4220845f9427b002a2a9d-Abstract.html,Privacy & Data Governance,Facility Location Problem in Differential Privacy Model Revisited,"In this paper we study the facility location problem in the model of differential privacy (DP) with uniform facility cost. Specifically, we first show that under the hierarchically well-separated tree (HST) metrics and the super-set output setting that was introduced in Gupta et. al., there is an $\epsilon$-DP algorithm that achieves an $O(\frac{1}{\epsilon})$(expected multiplicative) approximation ratio; this implies an $O(\frac{\log n}{\epsilon})$ approximation ratio for the general metric case, where $n$ is the size of the input metric. These bounds improve the best-known results given by Gupta et. al.  In particular, our approximation ratio for HST-metrics is independent of $n$, and the ratio for general metrics is independent of the aspect ratio of the input metric. On the negative side, we show that the approximation ratio of any $\epsilon$-DP algorithm is lower bounded by $\Omega(\frac{1}{\sqrt{\epsilon}})$, even for instances on HST metrics with uniform facility cost, under the super-set output setting. The lower bound shows that the dependence of the approximation ratio for HST metrics on $\epsilon$ can not be removed or greatly improved. Our novel methods and techniques for both the upper and lower bound may find additional applications.",[],[],"['Yunus Esencayi', 'Marco Gaboardi', 'Shi Li', 'Di Wang']","['SUNY at Buffalo', 'Boston University', 'SUNY at Buffalo', 'SUNY at Buffalo']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/596f713f9a7376fe90a62abaaedecc2d-Abstract.html,Privacy & Data Governance,Knowledge Extraction with No Observable Data,"Knowledge distillation is to transfer the knowledge of a large neural network into a smaller one and has been shown to be effective especially when the amount of training data is limited or the size of the student model is very small. To transfer the knowledge, it is essential to observe the data that have been used to train the network since its knowledge is concentrated on a narrow manifold rather than the whole input space. However, the data are not accessible in many cases due to the privacy or confidentiality issues in medical, industrial, and military domains. To the best of our knowledge, there has been no approach that distills the knowledge of a neural network when no data are observable. In this work, we propose KegNet (Knowledge Extraction with Generative Networks), a novel approach to extract the knowledge of a trained deep neural network and to generate artificial data points that replace the missing training data in knowledge distillation. Experiments show that KegNet outperforms all baselines for data-free knowledge distillation. We provide the source code of our paper in https://github.com/snudatalab/KegNet.",[],[],"['Jaemin Yoo', 'Minyong Cho', 'Taebum Kim', 'U Kang']","['Seoul National University', 'Seoul National University', 'Seoul National University', 'Seoul National University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/5dec707028b05bcbd3a1db5640f842c5-Abstract.html,Privacy & Data Governance,Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate,"Differential Privacy is a popular and well-studied notion of privacy. In the era ofbig data that we are in, privacy concerns are becoming ever more prevalent and thusdifferential privacy is being turned to as one such solution. A popular method forensuring differential privacy of a classifier is known as subsample-and-aggregate,in which the dataset is divided into distinct chunks and a model is learned on eachchunk, after which it is aggregated. This approach allows for easy analysis of themodel on the data and thus differential privacy can be easily applied. In this paper,we extend this approach by dividing the data several times (rather than just once)and learning models on each chunk within each division. The first benefit of thisapproach is the natural improvement of utility by aggregating models trained ona more diverse range of subsets of the data (as demonstrated by the well-knownbagging technique). The second benefit is that, through analysis that we provide inthe paper, we can derive tighter differential privacy guarantees when several queriesare made to this mechanism.  In order to derive these guarantees, we introducethe upwards and downwards moments accountants and derive bounds for thesemoments accountants in a data-driven fashion. We demonstrate the improvementsour model makes over standard subsample-and-aggregate in two datasets (HeartFailure (private) and UCI Adult (public)).",[],[],"['James Jordon', 'Jinsung Yoon', 'Mihaela van der Schaar']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6646b06b90bd13dabc11ddba01270d23-Abstract.html,Privacy & Data Governance,Privacy-Preserving Q-Learning with Functional Noise in Continuous Spaces,"We consider differentially private algorithms for reinforcement learning in continuous spaces, such that neighboring reward functions are indistinguishable. This protects the reward information from being exploited by methods such as inverse reinforcement learning. Existing studies that guarantee differential privacy are not extendable to infinite state spaces, as the noise level to ensure privacy will scale accordingly to infinity. Our aim is to protect the value function approximator, without regard to the number of states queried to the function. It is achieved by adding functional noise to the value function iteratively in the training. We show rigorous privacy guarantees by a series of analyses on the kernel of the noise space, the probabilistic bound of such noise samples, and the composition over the iterations. We gain insight into the utility analysis by proving the algorithm's approximate optimality when the state space is discrete. Experiments corroborate our theoretical findings and show improvement over existing approaches.",[],[],"['Baoxiang Wang', 'Nidhi Hegde']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6832a7b24bc06775d02b7406880b93fc-Abstract.html,Privacy & Data Governance,Uncoupled Regression from Pairwise Comparison Data,"Uncoupled regression is the problem to learn a model from unlabeled data and the set of target values while the correspondence between them is unknown. Such a situation arises in predicting anonymized targets that involve sensitive information, e.g., one's annual income. Since existing methods for uncoupled regression often require strong assumptions on the true target function, and thus, their range of applications is limited, we introduce a novel framework that does not require such assumptions in this paper. Our key idea is to utilize \emph{pairwise comparison data, which consists of pairs of unlabeled data that we know which one has a larger target value. Such pairwise comparison data is easy to collect, as typically discussed in the learning-to-rank scenario, and does not break the anonymity of data. We propose two practical methods for uncoupled regression from pairwise comparison data and show that the learned regression model converges to the optimal model with the optimal parametric convergence rate when the target variable distributes uniformly. Moreover, we empirically show that for linear models the proposed methods are comparable to ordinary supervised regression with labeled data.",[],[],"['Liyuan Xu', 'Junya Honda', 'Gang Niu', 'Masashi Sugiyama']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/68d30a9594728bc39aa24be94b319d21-Abstract.html,Privacy & Data Governance,Differentially Private Algorithms for Learning Mixtures of Separated Gaussians,"Learning the parameters of Gaussian mixture models is a fundamental and widely studied problem with numerous applications. In this work, we give new algorithms for learning the parameters of a high-dimensional, well separated, Gaussian mixture model subject to the strong constraint of differential privacy. In particular, we give a differentially private analogue of the algorithm of Achlioptas and McSherry. Our algorithm has two key properties not achieved by prior work: (1) The algorithm’s sample complexity matches that of the corresponding non-private algorithm up to lower order terms in a wide range of parameters. (2) The algorithm requires very weak a priori bounds on the parameters of the mixture components.",[],[],"['Gautam Kamath', 'Or Sheffet', 'Vikrant Singhal', 'Jonathan Ullman']","['David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada', 'Department of Computer Science, Faculty of Exact Sciences, Bar-Ilan University, Ramat-Gan, Israel', 'Khoury College of Computer Sciences, Northeastern University, Boston, MA', 'Khoury College of Computer Sciences, Northeastern University, Boston, MA']","['Canada', 'Israel']"
https://papers.nips.cc/paper_files/paper/2019/hash/700fdb2ba62d4554dc268c65add4b16e-Abstract.html,Privacy & Data Governance,Private Learning Implies Online Learning: An Efficient Reduction,"We study the relationship between the notions of differentially private learning and online learning. Several recent works have shown that differentially private learning implies online learning, but an open problem of Neel, Roth, and Wu \cite{NeelAaronRoth2018} asks whether this implication is {\it efficient}. Specifically, does an efficient differentially private learner imply an efficient online learner? In this paper we resolve this open question in the context of pure differential privacy.We derive an efficient black-box reduction from differentially private learning to online learning from expert advice.",[],[],"['Alon Gonen', 'Elad Hazan', 'Shay Moran']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/73231e53eeef362c814c8522f5257286-Abstract.html,Privacy & Data Governance,Oblivious Sampling Algorithms for Private Data Analysis,"We study secure and privacy-preserving data analysisbased on queries executed on samples from a dataset.Trusted execution environments (TEEs) can be used toprotect the content of the data during query computation,while supporting differential-private (DP) queries in TEEsprovides record privacy when query output is revealed.Support for sample-based queries is attractivedue to \emph{privacy amplification}since not all dataset is used to answer a query but only a small subset.However, extracting data samples with TEEswhile proving strong DP guarantees is nottrivial as secrecy of sample indices has to be preserved.To this end, we design efficient secure variants of common sampling algorithms.Experimentally we show that accuracy of modelstrained with shuffling and sampling is the same fordifferentially private models for MNIST and CIFAR-10,while sampling provides stronger privacy guarantees than shuffling.",[],[],"['Sajin Sasy', 'Olga Ohrimenko']","['University of Waterloo', 'Microsoft Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/7a674153c63cff1ad7f0e261c369ab2c-Abstract.html,Privacy & Data Governance,Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases,"Differential privacy has become a widely accepted notion of privacy, leading to the introduction and deployment of numerous privatization mechanisms. However, ensuring the privacy guarantee is an error-prone process, both in designing mechanisms and in implementing those mechanisms. Both types of errors will be greatly reduced, if we have a data-driven approach to verify privacy guarantees, from a black-box access to a mechanism. We pose it as a property estimation problem, and study the fundamental trade-offs involved in the accuracy in estimated privacy guarantees and the number of samples required. We introduce a novel estimator that uses polynomial approximation of a carefully chosen degree to optimally trade-off bias and variance. With n samples, we show that this estimator achieves performance of a straightforward plug-in estimator with n*log(n) samples, a phenomenon referred to as effective sample size amplification. The minimax optimality of the proposed estimator is proved by comparing it to a matching fundamental lower bound.",[],[],"['Xiyang Liu', 'Sewoong Oh']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8e036cc193d0af59aa9b22821248292b-Abstract.html,Privacy & Data Governance,Private Testing of Distributions via Sample Permutations,"Statistical tests are at the heart of many scientific tasks.To validate their hypothesis, researchers in medical and social sciences use individuals' data. The sensitivity of participants' data requires the design of statistical tests that ensure the privacy of the individuals in the most efficient way. In this paper, we use the framework of property testing to design algorithms to test the properties of the distribution that the data is drawn from with respect to differential privacy. In particular, we investigate testing two fundamental properties of distributions:  (1) testing the equivalence of two distributions when we have unequal numbers of samples from the two distributions. (2) Testing independence of two random variables. In both cases, we show that our testers achieve near optimal sample complexity (up to logarithmic factors). Moreover, our dependence on the privacy parameter is an additive term, which indicates that differential privacy can be obtained in most regimes of parameters for free.",[],[],"['Maryam Aliakbarpour', 'Ilias Diakonikolas', 'Daniel Kane', 'Ronitt Rubinfeld']","['CSAIL, MIT', 'University of Wisconsin, Madison', 'University of California, San Diego', 'CSAIL, MIT, TAU']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/955cb567b6e38f4c6b3f28cc857fc38c-Abstract.html,Privacy & Data Governance,Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy,"We give a simple, computationally efficient, and node-differentially-private algorithm for estimating the parameter of an Erdos-Renyi graph---that is, estimating p in a G(n,p)---with near-optimal accuracy.  Our algorithm nearly matches the information-theoretically optimal exponential-time algorithm for the same problem due to Borgs et al. (FOCS 2018).  More generally, we give an optimal, computationally efficient, private algorithm for estimating the edge-density of any graph whose degree distribution is concentrated in a small interval.",[],[],"['Jonathan Ullman', 'Adam Sealfon']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/9778d5d219c5080b9a6a17bef029331c-Abstract.html,Privacy & Data Governance,Private Hypothesis Selection,"We provide a differentially private algorithm for hypothesis selection.   Given samples from an unknown probability distribution $P$ and a set of $m$ probability distributions $\mathcal{H}$, the goal is to output, in a $\varepsilon$-differentially private manner, a distribution from $\mathcal{H}$ whose total variation distance to $P$ is comparable to that of the best such distribution (which we denote by $\alpha$).  The sample complexity of our basic algorithm is $O\left(\frac{\log m}{\alpha^2} + \frac{\log m}{\alpha \varepsilon}\right)$, representing a minimal cost for privacy when compared to the non-private algorithm. We also can handle infinite hypothesis classes $\mathcal{H}$ by relaxing to $(\varepsilon,\delta)$-differential privacy.  We apply our hypothesis selection algorithm to give learning algorithms for a number of natural distribution classes, including Gaussians, product distributions, sums of independent random variables, piecewise polynomials, and mixture classes.  Our hypothesis selection procedure allows us to generically convert a cover for a class to a learning algorithm, complementing known learning lower bounds which are in terms of the size of the packing number of the class.  As the covering and packing numbers are often closely  related, for constant $\alpha$, our algorithms achieve the optimal sample complexity for many classes of interest.  Finally, we describe an application to private distribution-free PAC learning.",[],[],"['Mark Bun', 'Gautam Kamath', 'Thomas Steinke', 'Steven Z. Wu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/9a6a1aaafe73c572b7374828b03a1881-Abstract.html,Privacy & Data Governance,Limits of Private Learning with Access to Public Data,"We consider learning problems where the training set consists of two types of examples: private and public. The goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples. This setting interpolates between private learning (where all examples are private) and classical learning (where all examples are public). We study the limits of learning in this setting in terms of private and public sample complexities. We show that any hypothesis class of VC-dimension $d$ can be agnostically learned up to an excess error of $\alpha$ using only (roughly) $d/\alpha$ public examples and $d/\alpha^2$ private labeled examples. This result holds even when the public examples are unlabeled. This gives a quadratic improvement over the standard $d/\alpha^2$ upper bound on the public sample complexity (where private examples can be ignored altogether if the public examples are labeled). Furthermore, we give a nearly matching lower bound, which we prove via a generic reduction from this setting to the one of private learning without public data.",[],[],"['Noga Alon', 'Raef Bassily', 'Shay Moran']","['Department of Mathematics, Princeton University', 'Department of Computer Science & Engineering, The Ohio State University', 'Google AI, Princeton']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/9d28de8ff9bb6a3fa41fddfdc28f3bc1-Abstract.html,Privacy & Data Governance,Partially Encrypted Deep Learning using Functional Encryption,"Machine learning on encrypted data has received a lot of attention thanks to recent breakthroughs in homomorphic encryption and secure multi-party computation. It allows outsourcing computation to untrusted servers without sacrificing privacy of sensitive data. We propose a practical framework to perform partially encrypted and privacy-preserving predictions which combines adversarial training and functional encryption. We first present a new functional encryption scheme to efficiently compute quadratic functions so that the data owner controls what can be computed but is not involved in the calculation: it provides a decryption key which allows one to learn a specific function evaluation of some encrypted data. We then show how to use it in machine learning to partially encrypt neural networks with quadratic activation functions at evaluation time and we provide a thorough analysis of the information leaks based on indistinguishability of data items of the same label. Last, since several encryption schemes cannot deal with the last thresholding operation used for classification, we propose a training method to prevent selected sensitive features from leaking which adversarially optimizes the network against an adversary trying to identify these features. This is of great interest for several existing works using partially encrypted machine learning as it comes with almost no cost on the model's accuracy and significantly improves data privacy.",[],[],"['Théo Ryffel', 'David Pointcheval', 'Francis Bach', 'Edouard Dufour-Sans', 'Romain Gay']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html,Privacy & Data Governance,Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation,"Classification of personal text messages has many useful applications in surveillance, e-commerce, and mental health care, to name a few. Giving applications access to personal texts can easily lead to (un)intentional privacy violations. We propose the first privacy-preserving solution for text classification that is provably secure. Our method, which is based on Secure Multiparty Computation (SMC), encompasses both feature extraction from texts, and subsequent classification with logistic regression and tree ensembles. We prove that when using our secure text classification method, the application does not learn anything about the text, and the author of the text does not learn anything about the text classification model used by the application beyond what is given by the classification result itself. We perform end-to-end experiments with an application for detecting hate speech against women and immigrants, demonstrating excellent runtime results without loss of accuracy.",[],[],"['Devin Reich', 'Ariel Todoki', 'Rafael Dowsley', 'Martine De Cock', 'anderson nascimento']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/a588a6199feff5ba48402883d9b72700-Abstract.html,Privacy & Data Governance,Locally Private Gaussian Estimation,"We study a basic private estimation problem: each of n users draws a single i.i.d. sample from an unknown Gaussian distribution N(\mu,\sigma^2), and the goal is to estimate \mu while guaranteeing local differential privacy for each user. As minimizing the number of rounds of interaction is important in the local setting, we provide adaptive two-round solutions and nonadaptive one-round solutions to this problem. We match these upper bounds with an information-theoretic lower bound showing that our accuracy guarantees are tight up to logarithmic factors for all sequentially interactive locally private protocols.",[],[],"['Matthew Joseph', 'Janardhan Kulkarni', 'Jieming Mao', 'Steven Z. Wu']","['University of Pennsylvania', 'Microsoft Research Redmond', 'Google Research New York', 'University of Minnesota']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/b139e104214a08ae3f2ebcce149cdf6e-Abstract.html,Privacy & Data Governance,Practical Differentially Private Top-k Selection with Pay-what-you-get Composition,"We study the problem of top-k selection over a large domain universe subject to user-level differential privacy.  Typically, the exponential mechanism or report noisy max are the algorithms used to solve this problem.  However, these algorithms require querying the database for the count of each domain element.  We focus on the setting where the data domain is unknown, which is different than the setting of frequent itemsets where an apriori type algorithm can help prune the space of domain elements to query.  We design algorithms that ensures (approximate) differential privacy and only needs access to the true top-k' elements from the data for any chosen k' ≥ k.  This is a highly desirable feature for making differential privacy practical, since the algorithms require no knowledge of the domain.  We consider both the setting where a user's data can modify an arbitrary number of counts by at most 1, i.e. unrestricted sensitivity, and the setting where a user's data can modify at most some small, fixed number of counts by at most 1, i.e. restricted sensitivity.  Additionally, we provide a pay-what-you-get privacy composition bound for our algorithms.  That is, our algorithms might return fewer than k elements when the top-k elements are queried, but the overall privacy budget only decreases by the size of the outcome set.",[],[],"['David Durfee', 'Ryan M. Rogers']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/b3dd760eb02d2e669c604f6b2f1e803f-Abstract.html,Privacy & Data Governance,Elliptical Perturbations for Differential Privacy,"We study elliptical distributions in locally convex vector spaces, and determine conditions when they can or cannot be used to satisfy differential privacy (DP). A requisite condition for a sanitized statistical summary to satisfy DP is that the corresponding privacy mechanism must induce equivalent probability measures for all possible input databases. We show that elliptical distributions with the same dispersion operator, $C$, are equivalent if the difference of their means lies in the Cameron-Martin space of $C$. In the case of releasing finite-dimensional summaries using elliptical perturbations, we show that the privacy parameter $\ep$ can be computed in terms of a one-dimensional maximization problem. We apply this result to consider multivariate Laplace, $t$, Gaussian, and $K$-norm noise. Surprisingly, we show that the multivariate Laplace noise does not achieve $\ep$-DP in any dimension greater than one. Finally, we show that when the dimension of the space is infinite, no elliptical distribution can be used to give $\ep$-DP; only $(\epsilon,\delta)$-DP is possible.",[],[],"['Matthew Reimherr', 'Jordan Awan']","['Department of Statistics, Pennsylvania State University, University Park, PA', 'Department of Statistics, Pennsylvania State University, University Park, PA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/c14a2a57ead18f3532a5a8949382c536-Abstract.html,Privacy & Data Governance,Learning Auctions with Robust Incentive Guarantees,"We study the problem of learning Bayesian-optimal revenue-maximizing auctions. The classical approach to maximizing revenue requires a known prior distribution on the demand of the bidders, although recent work has shown how to replace the knowledge of a prior distribution with a polynomial sample. However, in an online setting, when buyers can participate in multiple rounds, standard learning techniques are susceptible to \emph{strategic overfitting}: bidders can improve their long-term wellbeing by manipulating the trajectory of the learning algorithm in earlier rounds. For example, they may be able to strategically adjust their behavior in earlier rounds to achieve lower, more favorable future prices. Such non-truthful behavior can hinder learning and harm revenue.  In this paper, we combine tools from differential privacy, mechanism design, and sample complexity to give a repeated auction that (1) learns bidder demand from past data, (2) is approximately revenue-optimal, and (3) strategically robust, as it incentivizes bidders to behave truthfully.",[],[],"['Jacob D. Abernethy', 'Rachel Cummings', 'Bhuvesh Kumar', 'Sam Taggart', 'Jamie H. Morgenstern']","['Georgia Tech', 'Georgia Tech', 'Georgia Tech', 'Georgia Tech', 'Oberlin College']","['Georgia', 'Georgia', 'Georgia', 'Georgia']"
https://papers.nips.cc/paper_files/paper/2019/hash/c36b1132ac829ece87dda55d77ac06a4-Abstract.html,Privacy & Data Governance,Online Learning via the Differential Privacy Lens,"In this paper, we use differential privacy as a lens to examine online learning in both full and partial information settings. The differential privacy framework is, at heart, less about privacy and more about algorithmic stability, and thus has found application in domains well beyond those where information security is central. Here we develop an algorithmic property called one-step differential stability which facilitates a more refined regret analysis for online learning methods. We show that tools from the differential privacy literature can yield regret bounds for many interesting online learning problems including online convex optimization and online linear optimization. Our stability notion is particularly well-suited for deriving first-order regret bounds for follow-the-perturbed-leader algorithms, something that all previous analyses have struggled to achieve. We also generalize the standard max-divergence to obtain a broader class called Tsallis max-divergences. These define stronger notions of stability that are useful in deriving bounds in partial information settings such as multi-armed bandits and bandits with experts.",[],[],"['Jacob D. Abernethy', 'Young Hun Jung', 'Chansoo Lee', 'Audra McMillan', 'Ambuj Tewari']","['College of Computing, Georgia Institute of Technology', 'Department of Statistics, University of Michigan', 'Google Brain', 'Simons Inst. for the Theory of Computing, Department of Computer Science, Boston University, Khoury College of Computer Sciences, Northeastern University', 'Department of Statistics, Department of EECS, University of Michigan']",['Georgia']
https://papers.nips.cc/paper_files/paper/2019/hash/c4c42505a03f2e969b4c0a97ee9b34e7-Abstract.html,Privacy & Data Governance,Privacy Amplification by Mixing and Diffusion Mechanisms,"A fundamental result in differential privacy states that the privacy guarantees of a mechanism are preserved by any post-processing of its output. In this paper we investigate under what conditions stochastic post-processing can amplify the privacy of a mechanism. By interpreting post-processing as the application of a Markov operator, we first give a series of amplification results in terms of uniform mixing properties of the Markov process defined by said operator. Next we provide amplification bounds in terms of coupling arguments which can be applied in cases where uniform mixing is not available. Finally, we introduce a new family of mechanisms based on diffusion processes which are closed under post-processing, and analyze their privacy via a novel heat flow argument. On the applied side, we generalize the analysis of ""privacy amplification by iteration"" in Noisy SGD and show it admits an exponential improvement in the strongly convex case, and study a mechanism based on the Ornstein–Uhlenbeck diffusion process which contains the Gaussian mechanism with optimal post-processing on bounded inputs as a special case.",[],[],"['Borja Balle', 'Gilles Barthe', 'Marco Gaboardi', 'Joseph Geumlek']","['', 'MPI for Security and Privacy, IMDEA Software Institute', 'Boston University', 'University of California, San Diego']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d01c25576ff1c53de58e0e6970a2d510-Abstract.html,Privacy & Data Governance,Locally Private Learning without Interaction Requires Separation,"We consider learning under the constraint of local differential privacy (LDP). For many learning problems known efficient algorithms in this model require many rounds of communication between the server and the clients holding the data points. Yet multi-round protocols are prohibitively slow in practice due to network latency and, as a result, currently deployed large-scale systems are limited to a single round. Despite significant research interest, very little is known about which learning problems can be solved by such non-interactive systems. The only lower bound we are aware of is for PAC learning an artificial class of functions with respect to a uniform distribution (Kasiviswanathan et al., 2008).We show that the margin complexity of a class of Boolean functions is a lower bound on the complexity of any non-interactive LDP algorithm for distribution-independent PAC learning of the class. In particular, the classes of linear separators and decision lists require exponential number of samples to learn non-interactively even though they can be learned in polynomial time by an interactive LDP algorithm. This gives the first example of a natural problem that is significantly harder to solve without interaction and also resolves an open problem of Kasiviswanathan et al.~(2008). We complement this lower bound with a new efficient learning algorithm whose complexity is polynomial in the margin complexity of the class. Our algorithm is non-interactive on labeled samples but still needs interactive access to unlabeled samples. All of our results also apply to the statistical query model and any model in which the number of bits communicated about each data point is constrained.",[],[],"['Amit Daniely', 'Vitaly Feldman']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ddb4955263e6c08179393d1beaf18602-Abstract.html,Privacy & Data Governance,User-Specified Local Differential Privacy in Unconstrained Adaptive Online Learning,"Local differential privacy is a strong notion of privacy in which the provider of the data guarantees privacy by perturbing the data with random noise. In the standard application of local differential differential privacy the distribution of the noise is constant and known by the learner. In this paper we generalize this approach by allowing the provider of the data to choose the distribution of the noise without disclosing any parameters of the distribution to the learner, under the constraint that the distribution is symmetrical. We consider this problem in the unconstrained Online Convex Optimization setting with noisy feedback. In this setting the learner receives the subgradient of a loss function, perturbed by noise, and aims to achieve sublinear regret with respect to some competitor, without constraints on the norm of the competitor. We derive the first algorithms that have adaptive regret bounds in this setting, i.e. our algorithms adapt to the unknown competitor norm, unknown noise, and unknown sum of the norms of the subgradients, matching state of the art bounds in all cases.",[],[],['Dirk van der Hoeven'],[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e44e875c12109e4fa3716c05008048b2-Abstract.html,Privacy & Data Governance,On Differentially Private Graph Sparsification and Applications,"In this paper, we study private sparsification of graphs. In particular, we give an algorithm that given an input graph, returns a sparse graph which approximates the spectrum of the input graph while ensuring differential privacy. This allows one to solve many graph problems privately yet efficiently and accurately. This is exemplified with application of the proposed meta-algorithm to graph algorithms for privately answering cut-queries, as well as practical algorithms for computing {\scshape MAX-CUT} and {\scshape SPARSEST-CUT} with better accuracy than previously known. We also give the first efficient private algorithm to learn Laplacian eigenmap on a graph.",[],[],"['Raman Arora', 'Jalaj Upadhyay']","['Johns Hopkins University', 'Rutgers University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/faefec47428cf9a2f0875ba9c2042a81-Abstract.html,Privacy & Data Governance,KNG: The K-Norm Gradient Mechanism,"This paper presents a new mechanism for producing sanitized statistical summaries that achieve {\it differential privacy}, called the {\it K-Norm Gradient} Mechanism, or KNG. This new approach maintains the strong flexibility of the exponential mechanism, while achieving the powerful utility performance of objective perturbation. KNG starts with an inherent objective function (often an empirical risk), and promotes summaries that are close to minimizing the objective by weighting according to how far the gradient of the objective function is from zero.  Working with the gradient instead of the original objective function allows for additional flexibility as one can penalize using different norms.  We show that, unlike the exponential mechanism, the noise added by KNG is asymptotically negligible compared to the statistical error for many problems. In addition to theoretical guarantees on privacy and utility, we confirm the utility of KNG empirically in the settings of linear and quantile regression through simulations.",[],[],"['Matthew Reimherr', 'Jordan Awan']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html,Privacy & Data Governance,Differential Privacy Has Disparate Impact on Model Accuracy,"Differential privacy (DP) is a popular mechanism for training machinelearning models with bounded leakage about the presence of specificpoints in the training data.  The cost of differential privacy is areduction in the model's accuracy.  We demonstrate that in the neuralnetworks trained using differentially private stochastic gradient descent(DP-SGD), this cost is not borne equally: accuracy of DP models dropsmuch more for the underrepresented classes and subgroups.For example, a gender classification model trained using DP-SGD exhibitsmuch lower accuracy for black faces than for white faces.  Critically,this gap is bigger in the DP model than in the non-DP model, i.e., ifthe original model is unfair, the unfairness becomes worse once DP isapplied.  We demonstrate this effect for a variety of tasks and models,including sentiment analysis of text and image classification.  We thenexplain why DP training mechanisms such as gradient clipping and noiseaddition have disproportionate effect on the underrepresented and morecomplex subgroups, resulting in a disparate reduction of model accuracy.",[],[],"['Eugene Bagdasaryan', 'Omid Poursaeed', 'Vitaly Shmatikov']","['Cornell Tech', 'Cornell Tech', 'Cornell Tech']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/02bf86214e264535e3412283e817deaa-Abstract.html,Security,Lower Bounds on Adversarial Robustness from Optimal Transport,"While progress has been made in understanding the robustness of machine learning classifiers to test-time adversaries (evasion attacks), fundamental questions remain unresolved. In this paper, we use optimal transport to characterize the maximum achievable accuracy in an adversarial classification scenario. In this setting, an adversary receives a random labeled example from one of two classes, perturbs the example subject to a neighborhood constraint, and presents the modified example to the classifier. We define an appropriate cost function such that the minimum transportation cost between the distributions of the two classes determines the \emph{minimum $0-1$ loss for any classifier}. When the classifier comes from a restricted hypothesis class, the optimal transportation cost provides a lower bound. We apply our framework to the case of Gaussian data with norm-bounded adversaries and explicitly show matching bounds for the classification and transport problems and the optimality of linear classifiers. We also characterize the sample complexity of learning in this setting, deriving and extending previously known results as a special case. Finally, we use our framework to study the gap between the optimal classification performance possible and that currently achieved by state-of-the-art robustly trained neural networks for datasets of interest, namely, MNIST, Fashion MNIST and CIFAR-10.",[],[],"['Arjun Nitin Bhagoji', 'Daniel Cullina', 'Prateek Mittal']","['Department of Electrical Engineering, Princeton University', 'Department of Electrical Engineering, Pennsylvania State University', 'Department of Electrical Engineering, Princeton University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/093b60fd0557804c8ba0cbf1453da22f-Abstract.html,Security,Coda: An End-to-End Neural Program Decompiler,"Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda1, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into of two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior with performance compared to baseline approaches. We assess Coda’s performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70% program accuracy. Our work reveals the vulnerability of binary executables and imposes a new threat to the protection of Intellectual Property (IP) for software development.",[],[],"['Cheng Fu', 'Huili Chen', 'Haolan Liu', 'Xinyun Chen', 'Yuandong Tian', 'Farinaz Koushanfar', 'Jishen Zhao']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/09a8a8976abcdfdee15128b4cc02f33a-Abstract.html,Security,Linear Stochastic Bandits Under Safety Constraints,"Bandit algorithms have various application in safety-critical systems, where it is important to respect the system constraints that rely on the bandit's unknown parameters at every round. In this paper, we formulate a linear stochastic multi-armed bandit problem with safety constraints that depend (linearly) on an unknown parameter vector. As such, the learner is unable to identify all safe actions and must act conservatively in ensuring that her actions satisfy the safety constraint at all rounds (at least with high probability). For these bandits, we propose a new UCB-based algorithm called Safe-LUCB, which includes necessary modifications to respect safety constraints. The algorithm has two phases. During the pure exploration phase the learner chooses her actions at random from a restricted set of safe actions with the goal of learning a good approximation of the entire unknown safe set. Once this goal is achieved, the algorithm begins a safe exploration-exploitation phase where the learner gradually expands their estimate of the set of safe actions while controlling the growth of regret. We provide a general regret bound for the algorithm, as well as a problem dependent bound that is connected to the location of the optimal action within the safe set. We then propose a modified heuristic that exploits our problem dependent analysis to improve the regret.",[],[],"['Sanae Amani', 'Mahnoosh Alizadeh', 'Christos Thrampoulidis']","['University of California, Santa Barbara', 'University of California, Santa Barbara', 'University of California, Santa Barbara']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/0defd533d51ed0a10c5c9dbf93ee78a5-Abstract.html,Security,Adversarial Robustness through Local Linearization,"Adversarial training is an effective methodology for training deep neural networks that are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47% adversarial accuracy for ImageNet with L-infinity norm adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.",[],[],"['Chongli Qin', 'James Martens', 'Sven Gowal', 'Dilip Krishnan', 'Krishnamurthy Dvijotham', 'Alhussein Fawzi', 'Soham De', 'Robert Stanforth', 'Pushmeet Kohli']","['DeepMind', 'DeepMind', 'DeepMind', 'Google', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/107878346e1d8f8fe6af7a7a588aa807-Abstract.html,Security,On Robustness to Adversarial Examples and Polynomial Optimization,"We study the design of computationally efficient algorithms with provable guarantees, that are robust to adversarial (test time) perturbations. While there has been an explosion of recent work on this topic due to its connections to test time robustness of deep networks, there is limited theoretical understanding of several basic questions like (i) when and how can one design provably robust learning algorithms? (ii) what is the price of achieving robustness to adversarial examples in a computationally efficient manner?The main contribution of this work is to exhibit a strong connection between achieving robustness to adversarial examples, and a rich class of polynomial optimization problems, thereby making progress on the above questions. In particular, we leverage this connection to (a) design computationally efficient robust algorithms with provable guarantees for a large class of hypothesis, namely linear classifiers and degree-2 polynomial threshold functions~(PTFs), (b) give a precise characterization of the price of achieving robustness in a computationally efficient manner for these classes, (c) design efficient algorithms to certify robustness and generate adversarial attacks in a principled manner for 2-layer neural networks. We empirically demonstrate the effectiveness of these attacks on real data.",[],[],"['Pranjal Awasthi', 'Abhratanu Dutta', 'Aravindan Vijayaraghavan']","['Department of Computer Science, Rutgers University', 'Department of Computer Science, Northwestern University', 'Department of Computer Science, Northwestern University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1091660f3dff84fd648efe31391c5524-Abstract.html,Security,In-Place Zero-Space Memory Protection for CNN,"Convolutional Neural Networks (CNN) are being actively explored for safety-critical applications such as autonomous vehicles and aerospace, where it is essential to ensure the reliability of inference results in the presence of possible memory faults. Traditional methods such as error correction codes (ECC) and Triple Modular Redundancy (TMR) are CNN-oblivious and incur substantial memory overhead and energy cost. This paper introduces in-place zero-space ECC assisted with a new training scheme weight distribution-oriented training. The new method provides the first known zero space cost memory protection for CNNs without compromising the reliability offered by traditional ECC.",[],[],"['Hui Guan', 'Lin Ning', 'Zhen Lin', 'Xipeng Shen', 'Huiyang Zhou', 'Seung-Hwan Lim']","['North Carolina State University, Raleigh, NC', 'North Carolina State University, Raleigh, NC', 'North Carolina State University, Raleigh, NC', 'North Carolina State University, Raleigh, NC', 'North Carolina State University, Raleigh, NC', 'Oak Ridge National Laboratory, Oak Ridge, TN']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/16bda725ae44af3bb9316f416bd13b1b-Abstract.html,Security,Theoretical Analysis of Adversarial Learning: A Minimax Approach,"In this paper, we propose a general theoretical method for analyzing the risk bound in the presence of adversaries. Specifically, we try to fit the adversarial learning problem into the minimax framework. We first show that the original adversarial learning problem can be transformed into a minimax statistical learning problem by introducing a transport map between distributions. Then, we prove a new risk bound for this minimax problem in terms of covering numbers under a weak version of Lipschitz condition. Our method can be applied to multi-class classification and popular loss functions including the hinge loss and ramp loss. As some illustrative examples, we derive the adversarial risk bounds for SVMs and deep neural networks, and our bounds have two data-dependent terms, which can be optimized for achieving adversarial robustness.",[],[],"['Zhuozhuo Tu', 'Jingwei Zhang', 'Dacheng Tao']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html,Security,Calibration tests in multi-class classification: A unifying framework,"In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale.",[],[],"['David Widmann', 'Fredrik Lindsten', 'Dave Zachariah']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/246a3c5544feb054f3ea718f61adfa16-Abstract.html,Security,A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks,"Verification of neural networks enables us to gauge their robustness against adversarial attacks. Verification algorithms fall into two categories: exact verifiers that run in exponential time and relaxed verifiers that are efficient but incomplete. In this paper, we unify all existing LP-relaxed verifiers, to the best of our knowledge, under a general convex relaxation framework. This framework works for neural networks with diverse architectures and nonlinearities and covers both primal and dual views of neural network verification. Next, we perform large-scale experiments, amounting to more than 22 CPU-years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks. We find the exact solution does not significantly improve upon the gap between PGD and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR datasets. Our results suggest there is an inherent barrier to tight verification for the large class of methods captured by our framework. We discuss possible causes of this barrier and potential future directions for bypassing it.",[],[],"['Hadi Salman', 'Greg Yang', 'Huan Zhang', 'Cho-Jui Hsieh', 'Pengchuan Zhang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html,Security,Model Compression with Adversarial Robustness: A Unified Optimization Framework,"Deep model compression has been extensively studied, and state-of-the-art methods can now achieve high compression ratios with minimal accuracy loss. This paper studies model compression through a different lens: could we compress models without hurting their robustness to adversarial attacks, in addition to maintaining accuracy? Previous literature suggested that the goals of robustness and compactness might sometimes contradict. We propose a novel Adversarially Trained Model Compression (ATMC) framework. ATMC constructs a unified constrained optimization formulation, where existing compression means (pruning, factorization, quantization) are all integrated into the constraints. An efficient algorithm is then developed. An extensive group of experiments are presented, demonstrating that ATMC obtains remarkably more favorable trade-off among model size, accuracy and robustness, over currently available alternatives in various settings. The codes are publicly available at: https://github.com/shupenggui/ATMC.",[],[],"['Shupeng Gui', 'Haotao Wang', 'Haichuan Yang', 'Chen Yu', 'Zhangyang Wang', 'Ji Liu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/2cad8fa47bbef282badbb8de5374b894-Abstract.html,Security,Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks,"Unlike the white-box counterparts that are widely studied and readily accessible, adversarial examples in black-box settings are generally more Herculean on account of the difficulty of estimating gradients. Many methods achieve the task by issuing numerous queries to target classification systems, which makes the whole procedure costly and suspicious to the systems. In this paper, we aim at reducing the query complexity of black-box attacks in this category. We propose to exploit gradients of a few reference models which arguably span some promising search subspaces. Experimental results show that, in comparison with the state-of-the-arts, our method can gain up to 2x and 4x reductions in the requisite mean and medium numbers of queries with much lower failure rates even if the reference models are trained on a small and inadequate dataset disjoint to the one for training the victim model. Code and models for reproducing our results will be made publicly available.",[],[],"['Yiwen Guo', 'Ziang Yan', 'Changshui Zhang']","['Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Tsinghua University, Beijing, China and Intel Labs China', 'Bytedance AI Lab and Intel Labs China', 'Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Tsinghua University, Beijing, China']","['China', 'China', 'China']"
https://papers.nips.cc/paper_files/paper/2019/hash/2d71b2ae158c7c5912cc0bbde2bb9d95-Abstract.html,Security,An Adaptive Empirical  Bayesian Method for Sparse Deep Learning,"We propose a novel adaptive empirical Bayesian (AEB) method for sparse deep learning, where the sparsity is ensured via a class of self-adaptive spike-and-slab priors. The proposed method works by alternatively sampling from an adaptive hierarchical posterior distribution using stochastic gradient Markov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters using stochastic approximation (SA). The convergence of the proposed method to the asymptotically correct distribution is established under mild conditions. Empirical applications of the proposed method lead to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks (CNN) and the state-of-the-art compression performance on CIFAR10 with Residual Networks. The proposed method also improves resistance to adversarial attacks.",[],[],"['Wei Deng', 'Xiao Zhang', 'Faming Liang', 'Guang Lin']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/315f006f691ef2e689125614ea22cc61-Abstract.html,Security,Policy Poisoning in Batch Reinforcement Learning and Control,"We study a security threat to batch reinforcement learning and control where the attacker aims to poison the learned policy. The victim is a reinforcement learner / controller which first estimates the dynamics and the rewards from a batch data set, and then solves for the optimal policy with respect to the estimates. The attacker can modify the data set slightly before learning happens, and wants to force the learner into learning a target policy chosen by the attacker. We present a unified framework for solving batch policy poisoning attacks, and instantiate the attack on two standard victims: tabular certainty equivalence learner in reinforcement learning and linear quadratic regulator in control. We show that both instantiation result in a convex optimization problem on which global optimality is guaranteed, and provide analysis on attack feasibility and attack cost. Experiments show the effectiveness of policy poisoning attacks.",[],[],"['Yuzhe Ma', 'Xuezhou Zhang', 'Wen Sun', 'Jerry Zhu']","['University of Wisconsin–Madison', 'University of Wisconsin–Madison', 'Microsoft Research New York', 'University of Wisconsin–Madison']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/32508f53f24c46f685870a075eaaa29c-Abstract.html,Security,Improving Black-box Adversarial Attacks with a Transfer-based Prior,"We consider the black-box adversarial setting, where the adversary has to generate adversarial perturbations without access to the target models to compute gradients. Previous methods tried to approximate the gradient either by using a transfer gradient of a surrogate white-box model, or based on the query feedback. However, these methods often suffer from low attack success rates or poor query efficiency since it is non-trivial to estimate the gradient in a high-dimensional space with limited information. To address these problems, we propose a prior-guided random gradient-free (P-RGF) method to improve black-box adversarial attacks, which takes the advantage of a transfer-based prior and the query information simultaneously. The transfer-based prior given by the gradient of a surrogate model is appropriately integrated into our algorithm by an optimal coefficient derived by a theoretical analysis. Extensive experiments demonstrate that our method requires much fewer queries to attack black-box models with higher success rates compared with the alternative state-of-the-art methods.",[],[],"['Shuyu Cheng', 'Yinpeng Dong', 'Tianyu Pang', 'Hang Su', 'Jun Zhu']","['Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China', 'Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China', 'Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China', 'Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China', 'Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China']","['China', 'China', 'China', 'China', 'China']"
https://papers.nips.cc/paper_files/paper/2019/hash/32e0bd1497aa43e02a42f47d9d6515ad-Abstract.html,Security,Unlabeled Data Improves Adversarial Robustness,"We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning.  Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) $\ell_\infty$ robustness against several strong attacks via adversarial training and (ii) certified $\ell_2$ and $\ell_\infty$ robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.",[],[],"['Yair Carmon', 'Aditi Raghunathan', 'Ludwig Schmidt', 'John C. Duchi', 'Percy S. Liang']","['Stanford University', 'Stanford University', 'UC Berkeley', 'Stanford University', 'Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/335cd1b90bfa4ee70b39d08a4ae0cf2d-Abstract.html,Security,Certified Adversarial Robustness with Additive Noise,"The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defense models has been developed, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that our method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.",[],[],"['Bai Li', 'Changyou Chen', 'Wenlin Wang', 'Lawrence Carin']","['Department of Statistical Science, Duke University', 'Department of CSE, University at Buffalo, SUNY', 'Department of ECE, Duke University', 'Department of ECE, Duke University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/351869bde8b9d6ad1e3090bd173f600d-Abstract.html,Security,Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices,"In real-world machine learning applications, data subsets correspond to especially critical outcomes: vulnerable cyclist detections are safety-critical in an autonomous driving task, and ""question"" sentences might be important to a dialogue agent's language understanding for product purposes.  While machine learning models can achieve quality performance on coarse-grained metrics like F1-score and overall accuracy, they may underperform on these critical subsets---we define these as slices, the key abstraction in our approach. To address slice-level performance, practitioners often train separate ""expert"" models on slice subsets or use multi-task hard parameter sharing.  We propose Slice-based Learning, a new programming model in which the slicing function (SF), a programmer abstraction, is used to specify additional model capacity for each slice.  Any model can leverage SFs to learn slice-specific representations, which are combined with an attention mechanism to make slice-aware predictions.  We show that our approach improves over baselines in terms of computational complexity and slice-specific performance by up to 19.0 points, and overall performance by up to 4.6 F1 points on applications spanning natural language understanding and computer vision benchmarks as well as production-scale industrial systems.",[],[],"['Vincent Chen', 'Sen Wu', 'Alexander J. Ratner', 'Jen Weng', 'Christopher Ré']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/36ab62655fa81ce8735ce7cfdaf7c9e8-Abstract.html,Security,Theoretical evidence for adversarial robustness through randomization,"This paper investigates the theory of robustness against adversarial attacks. Itfocuses on the family of randomization techniques that consist in injecting noisein the network at inference time. These techniques have proven effective in manycontexts, but lack theoretical arguments. We close this gap by presenting a theo-retical analysis of these approaches, hence explaining why they perform well inpractice. More precisely, we make two new contributions. The first one relatesthe randomization rate to robustness to adversarial attacks. This result applies forthe general family of exponential distributions, and thus extends and unifies theprevious approaches. The second contribution consists in devising a new upperbound on the adversarial risk gap of randomized neural networks. We support ourtheoretical claims with a set of experiments.",[],[],"['Rafael Pinot', 'Laurent Meunier', 'Alexandre Araujo', 'Hisashi Kashima', 'Florian Yger', 'Cedric Gouy-Pailler', 'Jamal Atif']","['Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France and nstitut LIST, CEA, Université Paris-Saclay', 'Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France and Facebook AI Research, Paris, France', 'Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France and Wavestone, Paris, France', 'Kyoto University, Kyoto, Japan and RIKEN Center for AIP, Japan', 'Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France', 'Institut LIST, CEA, Université Paris-Saclay', 'Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France']","['France', 'France', 'France', 'Japan', 'France', 'France']"
https://papers.nips.cc/paper_files/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html,Security,Defending Against Neural Fake News,"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",[],[],"['Rowan Zellers', 'Ari Holtzman', 'Hannah Rashkin', 'Yonatan Bisk', 'Ali Farhadi', 'Franziska Roesner', 'Yejin Choi']","['Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington and Allen Institute for Artificial Intelligence', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington and Allen Institute for Artificial Intelligence']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/4206e38996fae4028a26d43b24f68d32-Abstract.html,Security,Provably robust boosted decision stumps and trees against adversarial attacks,"The problem of adversarial robustness has been studied extensively for neural networks. However, for boosted decision trees and decision stumps there are almost no results, even though they are widely used in practice (e.g. XGBoost) due to their accuracy, interpretability, and efficiency. We show in this paper that for boosted decision stumps the \textit{exact} min-max robust loss and test error for an $l_\infty$-attack can be computed in $O(T\log T)$ time per input, where $T$ is the number of decision stumps and the optimal update step of the ensemble can be done in $O(n^2\,T\log T)$, where $n$ is the number of data points. For boosted trees we show how to efficiently calculate and optimize an upper bound on the robust loss, which leads to state-of-the-art robust test error for boosted trees on MNIST (12.5\% for $\epsilon_\infty=0.3$), FMNIST (23.2\% for $\epsilon_\infty=0.1$), and CIFAR-10 (74.7\% for $\epsilon_\infty=8/255$). Moreover, the robust test error rates we achieve are competitive to the ones of provably robust convolutional networks. The code of all our experiments is available at \url{http://github.com/max-andr/provably-robust-boosting}.",[],[],"['Maksym Andriushchenko', 'Matthias Hein']","['University of Tübingen', 'University of Tübingen']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/47d1e990583c9c67424d369f3414728e-Abstract.html,Security,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection,"In this paper, we aim to understand the generalization properties of generative adversarial networks (GANs) from a new perspective of privacy protection. Theoretically, we prove that a differentially private learning algorithm used for training the GAN does not overfit to a certain degree, i.e., the generalization gap can be bounded. Moreover, some recent works, such as the Bayesian GAN, can be re-interpreted based on our theoretical insight from privacy protection. Quantitatively, to evaluate the information leakage of well-trained GAN models, we perform various membership attacks on these models. The results show that previous Lipschitz regularization techniques are effective in not only reducing the generalization gap but also alleviating the information leakage of the training dataset.",[],[],"['Bingzhe Wu', 'Shiwan Zhao', 'Chaochao Chen', 'Haoyang Xu', 'Li Wang', 'Xiaolu Zhang', 'Guangyu Sun', 'Jun Zhou']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html,Security,Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks,"Vulnerability identification is crucial to protect the software systems from attacksfor cyber security. It is especially important to localize the vulnerable functionsamong the source code to facilitate the fix. However, it is a challenging and tediousprocess, and also requires specialized security expertise. Inspired by the workon manually-defined patterns of vulnerabilities from various code representationgraphs and the recent advance on graph neural networks, we propose Devign, ageneral graph neural network based model for graph-level classification throughlearning on a rich set of code semantic representations. It includes a novel Convmodule to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.",[],[],"['Yaqin Zhou', 'Shangqing Liu', 'Jingkai Siow', 'Xiaoning Du', 'Yang Liu']","['Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/4f398cb9d6bc79ae567298335b51ba8a-Abstract.html,Security,Safe Exploration for Interactive Machine Learning,"In interactive machine learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.",[],[],"['Matteo Turchetta', 'Felix Berkenkamp', 'Andreas Krause']","['Dept. of Computer Science, ETH Zurich', 'Dept. of Computer Science, ETH Zurich', 'Dept. of Computer Science, ETH Zurich']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/4fc848051e4459b8a6afeb210c3664ec-Abstract.html,Security,Learning from Label Proportions with Generative Adversarial Networks,"In this paper, we leverage generative adversarial networks (GANs) to derive an effective algorithm LLP-GAN for learning from label proportions (LLP), where only the bag-level proportional information in labels is available. Endowed with end-to-end structure, LLP-GAN performs approximation in the light of an adversarial learning mechanism, without imposing restricted assumptions on distribution. Accordingly, we can directly induce the final instance-level classifier upon the discriminator. Under mild assumptions, we give the explicit generative representation and prove the global optimality for LLP-GAN. Additionally, compared with existing methods, our work empowers LLP solver with capable scalability inheriting from deep models. Several experiments on benchmark datasets demonstrate vivid advantages of the proposed approach.",[],[],"['Jiabin Liu', 'Bo Wang', 'Zhiquan Qi', 'YingJie Tian', 'Yong Shi']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/4fe5149039b52765bde64beb9f674940-Abstract.html,Security,Budgeted Reinforcement Learning in Continuous State Space,"A Budgeted Markov Decision Process (BMDP) is an extension of a Markov Decision Process to critical applications requiring safety constraints. It relies on a notion of risk implemented in the shape of an upper bound on a constrains violation signal that -- importantly -- can be modified in real-time. So far, BMDPs could only be solved in the case of finite state spaces with known dynamics. This work extends the state-of-the-art to continuous spaces environments and unknown dynamics. We show that the solution to a BMDP is the fixed point of a novel Budgeted Bellman Optimality operator. This observation allows us to introduce natural extensions of Deep Reinforcement Learning algorithms to address large-scale BMDPs. We validate our approach on two simulated applications: spoken dialogue and autonomous driving.",[],[],"['Nicolas Carrara', 'Edouard Leurent', 'Romain Laroche', 'Tanguy Urvoy', 'Odalric-Ambrym Maillard', 'Olivier Pietquin']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/54e36c5ff5f6a1802925ca009f3ebb68-Abstract.html,Security,Real-Time Reinforcement Learning,"Markov Decision Processes (MDPs), the mathematical framework underlying most algorithms in Reinforcement Learning (RL), are often used in a way that wrongfully assumes that the state of an agent's environment does not change during action selection. As RL systems based on MDPs begin to find application in real-world safety critical situations, this mismatch between the assumptions underlying classical MDPs and the reality of real-time computation may lead to undesirable outcomes. In this paper, we introduce a new framework, in which states and actions evolve simultaneously and show how it is related to the classical MDP formulation. We analyze existing algorithms under the new real-time formulation and show why they are suboptimal when used in real-time. We then use those insights to create a new algorithm Real-Time Actor Critic (RTAC) that outperforms the existing state-of-the-art continuous control algorithm Soft Actor Critic both in real-time and non-real-time settings.",[],[],"['Simon Ramstedt', 'Chris Pal']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/55a988dfb00a914717b3000a3374694c-Abstract.html,Security,Disentangling Influence: Using disentangled representations to audit model predictions,"Motivated by the need to audit complex and black box models, there has been extensive research on quantifying how data features influence model predictions. Feature influence can be direct (a direct influence on model outcomes) and indirect (model outcomes are influenced via proxy features). Feature influence can also be expressed in aggregate over the training or test data or locally with respect to a single point. Current research has typically focused on one of each of these dimensions. In this paper, we develop disentangled influence audits, a procedure to audit the indirect influence of features. Specifically, we show that disentangled representations provide a mechanism to identify proxy features in the dataset, while allowing an explicit computation of feature influence on either individual outcomes or aggregate-level outcomes. We show through both theory and experiments that disentangled influence audits can both detect proxy features and show, for each individual or in aggregate, which of these proxy features affects the classifier being audited the most. In this respect, our method is more powerful than existing methods for ascertaining feature influence.",[],[],"['Charles Marx', 'Richard Phillips', 'Sorelle Friedler', 'Carlos Scheidegger', 'Suresh Venkatasubramanian']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/56a3107cad6611c8337ee36d178ca129-Abstract.html,Security,SHE: A Fast and Accurate Deep Neural Network for Encrypted Data,"Homomorphic Encryption (HE) is one of the most promising security solutions to emerging Machine Learning as a Service (MLaaS). Several Leveled-HE (LHE)-enabled Convolutional Neural Networks (LHECNNs) are proposed to implement MLaaS to avoid the large bootstrapping overhead. However, prior LHECNNs have to pay significant computational overhead but achieve only low inference accuracy, due to their polynomial approximation activations and poolings. Stacking many polynomial approximation activation layers in a network greatly reduces the inference accuracy, since the polynomial approximation activation errors lead to a low distortion of the output distribution of the next batch normalization layer. So the polynomial approximation activations and poolings have become the obstacle to a fast and accurate LHECNN model.In this paper, we propose a Shift-accumulation-based LHE-enabled deep neural network (SHE) for fast and accurate inferences on encrypted data. We use the binary-operation-friendly leveled-TFHE (LTFHE) encryption scheme to implement ReLU activations and max poolings. We also adopt the logarithmic quantization to accelerate inferences by replacing expensive LTFHE multiplications with cheap LTFHE shifts. We propose a mixed bitwidth accumulator to expedite accumulations. Since the LTFHE ReLU activations, max poolings, shifts and accumulations have small multiplicative depth, SHE can implement much deeper network architectures with more convolutional and activation layers. Our experimental results show SHE achieves the state-of-the-art inference accuracy and reduces the inference latency by 76.21% ~ 94.23% over prior LHECNNs on MNIST and CIFAR-10.",[],[],"['Qian Lou', 'Lei Jiang']","['Indiana University Bloomington', 'Indiana University Bloomington']","['India', 'India']"
https://papers.nips.cc/paper_files/paper/2019/hash/56bd37d3a2fda0f2f41925019c81011d-Abstract.html,Security,Non-Cooperative Inverse Reinforcement Learning,"Making decisions in the presence of a strategic opponent requires one to take into account the opponent’s ability to actively mask its intended objective. To describe such strategic situations, we introduce the non-cooperative inverse reinforcement learning (N-CIRL) formalism. The N-CIRL formalism consists of two agents with completely misaligned objectives, where only one of the agents knows the true objective function. Formally, we model the N-CIRL formalism as a zero-sum Markov game with one-sided incomplete information. Through interacting with the more informed player, the less informed player attempts to both infer and optimize the true objective function. As a result of the one-sided incomplete information, the multi-stage game can be decomposed into a sequence of single- stage games expressed by a recursive formula. Solving this recursive formula yields the value of the N-CIRL game and the more informed player’s equilibrium strategy. Another recursive formula, constructed by forming an auxiliary game, termed the dual game, yields the less informed player’s strategy. Building upon these two recursive formulas, we develop a computationally tractable algorithm to approximately solve for the equilibrium strategies. Finally, we demonstrate the benefits of our N-CIRL formalism over the existing multi-agent IRL formalism via extensive numerical simulation in a novel cyber security setting.",[],[],"['Xiangyuan Zhang', 'Kaiqing Zhang', 'Erik Miehling', 'Tamer Basar']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/576d026223582a390cd323bef4bad026-Abstract.html,Security,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization,"The adaptive momentum method (AdaMM), which uses past gradients to update descent directions and learning rates simultaneously, has become one of the most popular first-order optimization methods for solving machine learning  problems. However,  AdaMM is not suited for solving black-box optimization problems, where explicit gradient forms are difficult or infeasible to obtain. In this paper, we propose a zeroth-order  AdaMM (ZO-AdaMM) algorithm, that generalizes AdaMM to the gradient-free regime. We show that the convergence rate of ZO-AdaMM for  both  convex and nonconvex optimization is roughly a factor of $O(\sqrt{d})$ worse than that of the first-order AdaMM algorithm, where $d$ is problem size. In particular, we provide a deep understanding on why  Mahalanobis distance matters in convergence of ZO-AdaMM and other AdaMM-type methods. As a byproduct, our analysis   makes the first step toward understanding adaptive learning rate methods for nonconvex constrained optimization.Furthermore, we demonstrate two applications, designing  per-image and universal adversarial attacks from black-box neural networks, respectively. We perform extensive experiments on ImageNet and empirically show that  ZO-AdaMM converges much faster to a solution of high accuracy compared with  $6$ state-of-the-art ZO optimization methods.",[],[],"['Xiangyi Chen', 'Sijia Liu', 'Kaidi Xu', 'Xingguo Li', 'Xue Lin', 'Mingyi Hong', 'David Cox']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/5cde6dedeb8892e3794f22db57ada073-Abstract.html,Security,A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning,"In this paper, we proposed a general framework for data poisoning attacks to graph-based semi-supervised learning (G-SSL). In this framework, we first unify different tasks, goals and constraints into a single formula for data poisoning attack in G-SSL, then we propose two specialized algorithms to efficiently solve two important cases --- poisoning regression tasks under $\ell_2$-norm constraint and classification tasks under $\ell_0$-norm constraint. In the former case, we transform it into a non-convex trust region problem and show that our gradient-based algorithm with delicate initialization and update scheme finds the (globally) optimal perturbation. For the latter case, although it is an NP-hard integer programming problem, we propose a probabilistic solver that works much better than the classical greedy method. Lastly, we test our framework on real datasets and evaluate the robustness of G-SSL algorithms. For instance, on the MNIST binary classification problem (50000 training data with 50 labeled), flipping two labeled data is enough to make the model perform like random guess (around 50\% error).",[],[],"['Xuanqing Liu', 'Si Si', 'Jerry Zhu', 'Yang Li', 'Cho-Jui Hsieh']","['Department of Computer Science, UCLA', 'Google Research', 'Department of Computer Science, University of Wisconsin-Madison', 'Google Research', 'Department of Computer Science, UCLA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/5d4ae76f053f8f2516ad12961ef7fe97-Abstract.html,Security,Adversarial Training and Robustness for Multiple Perturbations,"Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small $\ell_\infty$-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model's vulnerability.Our aim is to understand the reasons underlying this robustness trade-off, and to train models that are simultaneously robust to multiple perturbation types.We prove that a trade-off in robustness to different types of $\ell_p$-bounded and spatial perturbations must exist in a natural and simple statistical setting.We corroborate our formal analysis by demonstrating similar robustness trade-offs on MNIST and CIFAR10. We propose new multi-perturbation adversarial training schemes, as well as an efficient attack for the $\ell_1$-norm, and use these to show that models trained against multiple attacks fail to achieve robustness competitive with that of models trained on each attack individually. In particular, we find that adversarial training with first-order $\ell_\infty, \ell_1$ and $\ell_2$ attacks on MNIST achieves merely $50\%$ robust accuracy, partly because of gradient-masking.Finally, we propose affine attacks that linearly interpolate between perturbation types and further degrade the accuracy of adversarially trained models.",[],[],"['Florian Tramer', 'Dan Boneh']","['Stanford University', 'Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html,Security,Deep Leakage from Gradients,"Passing gradient is a widely used scheme in  modern multi-node learning system (e.g, distributed training, collaborative learning). In a long time, people used to believe that gradients are safe to share: i.e, the training set will not be leaked by gradient sharing.  However, in this paper, we show that we can obtain the private training set from the publicly shared gradients.  The leaking only takes few gradient steps to process and can obtain the original training set instead of look-alike alternatives.  We name this leakage as \textit{deep leakage from gradient}  and practically validate the effectiveness of our algorithm on both computer vision and natural language processing tasks. We empirically show that our attack is much stronger than previous approaches and thereby and raise people's awareness to rethink the gradients' safety. We also discuss some possible strategies to defend this deep leakage.",[],[],"['Ligeng Zhu', 'Zhijian Liu', 'Song Han']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html,Security,Deliberative Explanations: visualizing network insecurities,"A new approach to explainable AI, denoted {\it deliberative explanations,\/}  is proposed. Deliberative explanations are a visualization technique  that aims to go beyond the simple visualization of the image regions  (or, more generally, input variables) responsible for a network  prediction. Instead, they aim to expose the deliberations carried  by the network to arrive at that prediction, by uncovering the  insecurities of the network about the latter. The  explanation consists of a list of insecurities, each composed of  1) an image region (more generally, a set of input variables), and 2)  an ambiguity formed by the pair of classes responsible for the network  uncertainty about the region. Since insecurity detection requires  quantifying the difficulty of network predictions, deliberative  explanations combine ideas from the literatures on visual explanations and  assessment of classification difficulty. More specifically,  the proposed implementation  combines attributions with respect to both class  predictions and a difficulty score.  An evaluation protocol that leverages object recognition (CUB200)  and scene classification (ADE20K) datasets that combine part and  attribute annotations is also introduced to evaluate the accuracy of  deliberative explanations. Finally, an experimental evaluation shows that  the most accurate explanations are achieved by combining non self-referential  difficulty scores and second-order attributions. The resulting  insecurities are shown to correlate with regions of attributes that  are shared by different classes. Since these regions are also ambiguous  for humans, deliberative explanations are intuitive, suggesting that  the deliberative process of modern networks correlates with human  reasoning.",[],[],"['Pei Wang', 'Nuno Nvasconcelos']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6a4d5952d4c018a1c1af9fa590a10dda-Abstract.html,Security,DM2C: Deep Mixed-Modal Clustering,"Data exhibited with multiple modalities are ubiquitous in real-world clustering tasks. Most existing methods, however, pose a strong assumption that the pairing information for modalities is available for all instances. In this paper, we consider a more challenging task where each instance is represented in only one modality, which we call mixed-modal data. Without any extra pairing supervision across modalities, it is difficult to find a universal semantic space for all of them. To tackle this problem, we present an adversarial learning framework for clustering with mixed-modal data. Instead of transforming all the samples into a joint modality-independent space, our framework learns the mappings across individual modal spaces by virtue of cycle-consistency. Through these mappings, we could easily unify all the samples into a single modal space and perform the clustering. Evaluations on several real-world mixed-modal datasets could demonstrate the superiority of our proposed framework.",[],[],"['Yangbangyan Jiang', 'Qianqian Xu', 'Zhiyong Yang', 'Xiaochun Cao', 'Qingming Huang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6e923226e43cd6fac7cfe1e13ad000ac-Abstract.html,Security,Functional Adversarial Attacks,"We propose functional adversarial attacks, a novel class of threat models for crafting adversarial examples to fool machine learning models. Unlike a standard lp-ball threat model, a functional adversarial threat model allows only a single function to be used to perturb input features to produce an adversarial example. For example, a functional adversarial attack applied on colors of an image can change all red pixels simultaneously to light red. Such global uniform changes in images can be less perceptible than perturbing pixels of the image individually. For simplicity, we refer to functional adversarial attacks on image colors as ReColorAdv, which is the main focus of our experiments. We show that functional threat models can be combined with existing additive (lp) threat models to generate stronger threat models that allow both small, individual perturbations and large, uniform changes to an input. Moreover, we prove that such combinations encompass perturbations that would not be allowed in either constituent threat model. In practice, ReColorAdv can significantly reduce the accuracy of a ResNet-32 trained on CIFAR-10. Furthermore, to the best of our knowledge, combining ReColorAdv with other attacks leads to the strongest existing attack even after adversarial training.",[],[],"['Cassidy Laidlaw', 'Soheil Feizi']","['University of Maryland', 'University of Maryland']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/70117ee3c0b15a2950f1e82a215e812b-Abstract.html,Security,Learning from brains how to regularize machines,"Despite impressive performance on numerous visual tasks, Convolutional Neural Networks (CNNs) --- unlike brains --- are often highly sensitive to small perturbations of their input, e.g. adversarial noise leading to erroneous decisions. We propose to regularize CNNs using large-scale neuroscience data to learn more robust neural features in terms of representational similarity. We presented natural images to mice and measured the responses of thousands of neurons from cortical visual areas. Next, we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the model's predictions. We then used the neural representation similarity to regularize CNNs trained on image classification by penalizing intermediate representations that deviated from neural ones. This preserved performance of baseline models when classifying images under standard benchmarks, while maintaining substantially higher performance compared to baseline or control models when classifying noisy images. Moreover, the models regularized with cortical representations also improved model robustness in terms of adversarial attacks. This demonstrates that regularizing with neural data can be an effective tool to create an inductive bias towards more robust inference.",[],[],"['Zhe Li', 'Wieland Brendel', 'Edgar Walker', 'Erick Cobos', 'Taliah Muhammad', 'Jacob Reimer', 'Matthias Bethge', 'Fabian Sinz', 'Zachary Pitkow', 'Andreas Tolias']","['Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Centre for Integrative Neuroscience, University of Tübingen and Bernstein Center for Computational Neuroscience, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Institute Bioinformatics and Medical Informatics, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Centre for Integrative Neuroscience, University of Tübingen and Bernstein Center for Computational Neuroscience, University of Tübingen and Institute for Theoretical Physics, University of Tübingen', 'Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Bernstein Center for Computational Neuroscience, University of Tübingen and Institute Bioinformatics and Medical Informatics, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Department of Electrical and Computer Engineering, Rice University', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Department of Electrical and Computer Engineering, Rice University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/7503cfacd12053d309b6bed5c89de212-Abstract.html,Security,Adversarial training for free!,"Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our ""free"" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks.",[],[],"['Ali Shafahi', 'Mahyar Najibi', 'Mohammad Amin Ghiasi', 'Zheng Xu', 'John Dickerson', 'Christoph Studer', 'Larry S. Davis', 'Gavin Taylor', 'Tom Goldstein']","['University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'Cornell University', 'University of Maryland', 'United States Naval Academy', 'University of Maryland']",['United States']
https://papers.nips.cc/paper_files/paper/2019/hash/75455e062929d32a333868084286bb68-Abstract.html,Security,Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks,"With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at https://github.com/kamwoh/DeepIPR",[],[],"['Lixin Fan', 'Kam Woh Ng', 'Chee Seng Chan']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/78211247db84d96acf4e00092a7fba80-Abstract.html,Security,Defending Neural Backdoors via Generative Distribution Modeling,"Neural backdoor attack is emerging as a severe security threat to deep learning, while the capability of existing defense methods is limited, especially for complex backdoor triggers. In the work, we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers, all of which can influence the backdoored model. Thus, previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution, e.g., via generative modeling, is a key of effective defense. However, existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work, we propose max-entropy staircase approximator (MESA) for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.",[],[],"['Ximing Qiao', 'Yukun Yang', 'Hai Li']","['ECE Department, Duke University, Durham, NC', 'ECE Department, Duke University, Durham, NC', 'ECE Department, Duke University, Durham, NC']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/7dd2ae7db7d18ee7c9425e38df1af5e2-Abstract.html,Security,Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness,"Ensemble approaches for uncertainty estimation have recently been applied to the tasks of misclassification detection, out-of-distribution input detection and adversarial attack detection. Prior Networks have been proposed as an approach to efficiently emulate an ensemble of models for classification by parameterising a Dirichlet prior distribution over output distributions. These models have been shown to outperform alternative ensemble approaches, such as Monte-Carlo Dropout, on the task of out-of-distribution input detection. However, scaling Prior Networks to complex datasets with many classes is difficult using the training criteria originally proposed. This paper makes two contributions. First, we show that the appropriate training criterion for Prior Networks is the reverse KL-divergence between Dirichlet distributions. This addresses issues in the nature of the training data target distributions, enabling prior networks to be successfully trained on classification tasks with arbitrarily many classes, as well as improving out-of-distribution detection performance. Second, taking advantage of this new training criterion, this paper investigates using Prior Networks to detect adversarial attacks and proposes a generalized form of adversarial training. It is shown that the construction of successful adaptive whitebox attacks, which affect the prediction and evade detection, against Prior Networks trained on CIFAR-10 and CIFAR-100 using the proposed approach requires a greater amount of computational effort than against networks defended using standard adversarial training or MC-dropout.",[],[],"['Andrey Malinin', 'Mark Gales']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8133415ea4647b6345849fb38311cf32-Abstract.html,Security,On the Hardness of Robust Classification,"It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks.  In this paper we study the feasibility of robust learning from the perspective of computational learning theory, considering both sample and computational complexity.  In particular, our definition of robust learnability requires polynomial sample complexity.  We start with two negative results.  We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit.  We show moreover that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb $\omega(\log n)$ input bits. However if the adversary is restricted to perturbing $O(\log n)$ bits, then the class of monotone conjunctions can be robustly learned with respect to a general class of distributions (that includes the uniform distribution). Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on another computational model (e.g. the statistical query model) nor on any hardness assumption other than the existence of a hard learning problem in the PAC framework.",[],[],"['Pascale Gourdeau', 'Varun Kanade', 'Marta Kwiatkowska', 'James Worrell']","['Department of Computer Science, University of Oxford, Oxford, UK', 'Department of Computer Science, University of Oxford, Oxford, UK', 'Department of Computer Science, University of Oxford, Oxford, UK', 'Department of Computer Science, University of Oxford, Oxford, UK']","['UK', 'UK', 'UK', 'UK']"
https://papers.nips.cc/paper_files/paper/2019/hash/8420d359404024567b5aefda1231af24-Abstract.html,Security,On Single Source Robustness in Deep Fusion Models,"Algorithms that fuse multiple input sources benefit from both complementary and shared information. Shared information may provide robustness against faulty or noisy inputs, which is indispensable for safety-critical applications like self-driving cars. We investigate learning fusion algorithms that are robust against noise added to a single source. We first demonstrate that robustness against single source noise is not guaranteed in a linear fusion model. Motivated by this discovery, two possible approaches are proposed to increase robustness: a carefully designed loss with corresponding training algorithms for deep fusion models, and a simple convolutional fusion layer that has a structural advantage in dealing with noise. Experimental results show that both training algorithms and our fusion layer make a deep fusion-based 3D object detector robust against noise applied to a single source, while preserving the original performance on clean data.",[],[],"['Taewan Kim', 'Joydeep Ghosh']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/873be0705c80679f2c71fbf4d872df59-Abstract.html,Security,Reinforcement Learning with Convex Constraints,"In standard reinforcement learning (RL), a learning agent seeks to optimize the overall reward. However, many key aspects of a desired behavior are more naturally expressed as constraints. For instance, the designer may want to limit the use of unsafe actions, increase the diversity of trajectories to enable exploration, or approximate expert trajectories when rewards are sparse. In this paper, we propose an algorithmic scheme that can handle a wide class of constraints in RL tasks: specifically, any constraints that require expected values of some vector measurements (such as the use of an action) to lie in a convex set. This captures previously studied constraints (such as safety and proximity to an expert), but also enables new classes of constraints (such as diversity). Our approach comes with rigorous theoretical guarantees and only relies on the ability to approximately solve standard RL tasks. As a result, it can be easily adapted to work with any model-free or model-based RL. In our experiments, we show that it matches previous algorithms that enforce safety via constraints, but can also enforce new properties that these algorithms do not incorporate, such as diversity.",[],[],"['Sobhan Miryoosefi', 'Kianté Brantley', 'Hal Daume III', 'Miro Dudik', 'Robert E. Schapire']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/885fe656777008c335ac96072a45be15-Abstract.html,Security,"Accurate, reliable and fast robustness evaluation","Throughout the past five years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is significantly impaired by the difficulty of evaluating the robustness of neural network models. Today's methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efficient than current state-of-the-art gradient-based attacks, (c) can be flexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These findings are carefully validated across a diverse set of six different models and hold for L0, L1, L2 and Linf in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.",[],[],"['Wieland Brendel', 'Jonas Rauber', 'Matthias Kümmerer', 'Ivan Ustyuzhaninov', 'Matthias Bethge']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8ba6c657b03fc7c8dd4dff8e45defcd2-Abstract.html,Security,Sequential Experimental Design for Transductive Linear Bandits,"In this paper we introduce the pure exploration transductive linear bandit problem: given a set of measurement vectors $\mathcal{X}\subset \mathbb{R}^d$, a set of items $\mathcal{Z}\subset \mathbb{R}^d$, a fixed confidence $\delta$, and an unknown vector $\theta^{\ast}\in \mathbb{R}^d$, the goal is to infer $\arg\max_{z\in \mathcal{Z}} z^\top\theta^\ast$ with probability $1-\delta$ by making as few sequentially chosen noisy measurements of the form $x^\top\theta^{\ast}$ as possible. When $\mathcal{X}=\mathcal{Z}$, this setting generalizes linear bandits, and when $\mathcal{X}$ is the standard basis vectors and $\mathcal{Z}\subset \{0,1\}^d$, combinatorial bandits. The transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages $\mathcal{X}$ a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages $\mathcal{Z}$ that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books $\mathcal{X}$ a user is queried about may be restricted to known best-sellers even though the goal might be to recommend more esoteric titles $\mathcal{Z}$. In this paper, we provide instance-dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we present the first non-asymptotic algorithm for linear bandits that nearly achieves the information-theoretic lower bound.",[],[],"['Tanner Fiez', 'Lalit Jain', 'Kevin G. Jamieson', 'Lillian Ratliff']","['Electrical & Computer Engineering, University of Washington', 'Allen School of Computer Science & Engineering, University of Washington', 'Allen School of Computer Science & Engineering, University of Washington', 'Electrical & Computer Engineering, University of Washington']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/99cd3843754d20ec3c5885d805db8a32-Abstract.html,Security,Cross-Domain Transferability of Adversarial Perturbations,"Adversarial examples reveal the blind spots of deep neural networks (DNNs) and represent a major concern for security-critical applications. The transferability of adversarial examples makes real-world attacks possible in black-box settings, where the attacker is forbidden to access the internal parameters of the model. The underlying assumption in most adversary generation methods, whether learning an instance-specific or an instance-agnostic perturbation, is the direct or indirect reliance on the original domain-specific data distribution. In this work, for the first time, we demonstrate the existence of domain-invariant adversaries, thereby showing common adversarial space among different datasets and models. To this end, we propose a framework capable of launching highly transferable attacks that crafts adversarial patterns to mislead networks trained on wholly different domains. For instance, an adversarial function learned on Paintings, Cartoons or Medical images can successfully perturb ImageNet samples to fool the classifier, with success rates as high as $\sim$99\% ($\ell_{\infty} \le 10$). The core of our proposed adversarial function is a generative network that is trained using a relativistic supervisory signal that enables domain-invariant perturbations. Our approach sets the new state-of-the-art for fooling rates, both under the white-box and black-box scenarios. Furthermore, despite being an instance-agnostic perturbation function, our attack outperforms the conventionally much stronger instance-specific attack methods.",[],[],"['Muhammad Muzammal Naseer', 'Salman H. Khan', 'Muhammad Haris Khan', 'Fahad Shahbaz Khan', 'Fatih Porikli']","['Australian National University, Canberra, Australia and Inception Institute of Artificial Intelligence, Abu Dhabi, UAE', 'Inception Institute of Artificial Intelligence, Abu Dhabi, UAE and Australian National University, Canberra, Australia', 'Inception Institute of Artificial Intelligence, Abu Dhabi, UAE', 'Inception Institute of Artificial Intelligence, Abu Dhabi, UAE and CVL, Department of Electrical Engineering, Linköping University, Sweden', 'Australian National University, Canberra, Australia']","['Australia', 'Australia', 'Sweden', 'Australia']"
https://papers.nips.cc/paper_files/paper/2019/hash/b20bb95ab626d93fd976af958fbc61ba-Abstract.html,Security,Controlling Neural Level Sets,"The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning.In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest.We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.",[],[],"['Matan Atzmon', 'Niv Haim', 'Lior Yariv', 'Ofer Israelov', 'Haggai Maron', 'Yaron Lipman']","['Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel']","['Israel', 'Israel', 'Israel', 'Israel', 'Israel', 'Israel']"
https://papers.nips.cc/paper_files/paper/2019/hash/b83aac23b9528732c23cc7352950e880-Abstract.html,Security,Adversarial Self-Defense for Cycle-Consistent GANs,"The goal of unsupervised image-to-image translation is to  map images from one domain to another without the ground truth correspondence between the two domains. State-of-art methods  learn the correspondence using large numbers of unpaired examples from both domains and are based on generative adversarial networks. In order to preserve the semantics of the input image, the adversarial objective is usually combined with a cycle-consistency loss that penalizes incorrect reconstruction of the input image from the translated one. However, if the target mapping is many-to-one, e.g. aerial photos to maps, such a restriction forces the generator to hide information in low-amplitude structured noise that is undetectable by human eye or by the discriminator. In this paper, we show how such self-attacking behavior of unsupervised translation methods affects their performance and provide two defense techniques. We perform a quantitative evaluation of the proposed techniques and show that making the translation model more robust to the self-adversarial attack increases its generation quality and reconstruction reliability and makes the model less sensitive to low-amplitude perturbations. Our project page can be found at ai.bu.edu/selfadv.",[],[],"['Dina Bashkirova', 'Ben Usman', 'Kate Saenko']","['Boston University', 'Boston University', 'Boston University and MIT-IBM Watson AI Lab']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/b994697479c5716eda77e8e9713e5f0f-Abstract.html,Security,Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs,"Robust MDPs (RMDPs) can be used to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution are determined by the ambiguity set---the set of plausible transition probabilities---which is usually constructed as a multi-dimensional confidence region. Existing methods construct ambiguity sets as confidence regions using concentration inequalities which leads to overly conservative solutions. This paper proposes a new paradigm that can achieve better solutions with the same robustness guarantees without using confidence regions as ambiguity sets. To incorporate prior knowledge, our algorithms optimize the size and position of ambiguity sets using Bayesian inference. Our theoretical analysis shows the safety of the proposed method, and the empirical results demonstrate its practical promise.",[],[],"['Marek Petrik', 'Reazul Hasan Russel']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/bc1ad6e8f86c42a371aff945535baebb-Abstract.html,Security,Attribution-Based Confidence Metric For Deep Neural Networks,"We propose a novel confidence metric, namely, attribution-based confidence (ABC) for deep neural networks (DNNs).  ABC metric characterizes whether the output of a DNN on an input can be trusted. DNNs are known to be brittle on inputs outside the training distribution and are, hence, susceptible to adversarial attacks. This fragility is compounded by a lack of effectively computable measures of model confidence that correlate well with the accuracy of DNNs. These factors have impeded the adoption of DNNs in high-assurance systems. The proposed ABC metric addresses these challenges. It does not require access to the training data, the use of ensembles, or the need to train a  calibration model on a held-out validation set. Hence, the new metric is usable even when only a trained model is available for inference. We mathematically motivate the proposed metric and evaluate its effectiveness with two sets of experiments.  First, we study the change in accuracy and the associated confidence over out-of-distribution inputs.  Second, we consider several digital and physically realizable attacks such as FGSM, CW, DeepFool, PGD, and adversarial patch generation methods. The ABC metric is low on out-of-distribution data and adversarial examples, where the accuracy of the model is also low. These experiments demonstrate the effectiveness of the ABC  metric to make DNNs more trustworthy and resilient.",[],[],"['Susmit Jha', 'Sunny Raj', 'Steven Fernandes', 'Sumit K. Jha', 'Somesh Jha', 'Brian Jalaian', 'Gunjan Verma', 'Ananthram Swami']","['Computer Science Laboratory, SRI International', 'Computer Science Department, University of Central Florida, Orlando', 'Computer Science Department, University of Central Florida, Orlando', 'Computer Science Department, University of Central Florida, Orlando', 'University of Wisconsin-Madison and Xaipient', 'US Army Research Laboratory, Adelphi', 'US Army Research Laboratory, Adelphi', 'US Army Research Laboratory, Adelphi']","['US', 'US', 'US']"
https://papers.nips.cc/paper_files/paper/2019/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html,Security,Metric Learning for Adversarial Robustness,"Deep networks are well-known to be fragile to adversarial attacks. We conduct an empirical analysis of deep representations under the state-of-the-art attack method called PGD, and find that the attack causes the internal representation to shift closer to the ``false'' class. Motivated by this observation, we propose to regularize the representation space under attack with metric learning to produce more robust classifiers. By carefully sampling examples for metric learning, our learned representation not only increases robustness, but also detects previously unseen adversarial samples. Quantitative experiments show improvement of robustness accuracy by up to 4% and detection efficiency by up to 6% according to Area Under Curve score over prior work. The code of our work is available at https://github.com/columbia/MetricLearningAdversarial_Robustness.",[],[],"['Chengzhi Mao', 'Ziyuan Zhong', 'Junfeng Yang', 'Carl Vondrick', 'Baishakhi Ray']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/c36b1132ac829ece87dda55d77ac06a4-Abstract.html,Security,Online Learning via the Differential Privacy Lens,"In this paper, we use differential privacy as a lens to examine online learning in both full and partial information settings. The differential privacy framework is, at heart, less about privacy and more about algorithmic stability, and thus has found application in domains well beyond those where information security is central. Here we develop an algorithmic property called one-step differential stability which facilitates a more refined regret analysis for online learning methods. We show that tools from the differential privacy literature can yield regret bounds for many interesting online learning problems including online convex optimization and online linear optimization. Our stability notion is particularly well-suited for deriving first-order regret bounds for follow-the-perturbed-leader algorithms, something that all previous analyses have struggled to achieve. We also generalize the standard max-divergence to obtain a broader class called Tsallis max-divergences. These define stronger notions of stability that are useful in deriving bounds in partial information settings such as multi-armed bandits and bandits with experts.",[],[],"['Jacob D. Abernethy', 'Young Hun Jung', 'Chansoo Lee', 'Audra McMillan', 'Ambuj Tewari']","['College of Computing, Georgia Institute of Technology', 'Department of Statistics, University of Michigan', 'Google Brain', 'Simons Inst. for the Theory of Computing, Department of Computer Science, Boston University, Khoury College of Computer Sciences, Northeastern University', 'Department of Statistics, Department of EECS, University of Michigan']",['Georgia']
https://papers.nips.cc/paper_files/paper/2019/hash/c46482dd5d39742f0bfd417b492d0e8e-Abstract.html,Security,Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning,"Generalized zero-shot learning (GZSL) is a challenging class of vision and knowledge transfer problems in which both seen and unseen classes appear during testing. Existing GZSL approaches either suffer from semantic loss and discard discriminative information at the embedding stage, or cannot guarantee the visual-semantic interactions. To address these limitations, we propose a Dual Adversarial Semantics-Consistent Network (referred to as DASCN), which learns both primal and dual Generative Adversarial Networks (GANs) in a unified framework for GZSL. In DASCN, the primal GAN learns to synthesize inter-class discriminative and semantics-preserving visual features from both the semantic representations of seen/unseen classes and the ones reconstructed by the dual GAN. The dual GAN enforces the synthetic visual features to represent prior semantic knowledge well via semantics-consistent adversarial learning. To the best of our knowledge, this is the first work that employs a novel dual-GAN mechanism for GZSL. Extensive experiments show that our approach achieves significant improvements over the state-of-the-art approaches.",[],[],"['Jian Ni', 'Shanghang Zhang', 'Haiyong Xie']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/c4819d06b0ca810d38506453cfaae9d8-Abstract.html,Security,Manipulating a Learning Defender and Ways to Counteract,"In Stackelberg security games when information about the attacker's payoffs is uncertain, algorithms have been proposed to learn the optimal defender commitment by interacting with the attacker and observing their best responses. In this paper, we show that, however, these algorithms can be easily manipulated if the attacker responds untruthfully. As a key finding, attacker manipulation normally leads to the defender learning a maximin strategy, which effectively renders the learning attempt meaningless as to compute a maximin strategy requires no additional information about the other player at all. We then apply a game-theoretic framework at a higher level to counteract such manipulation, in which the defender commits to a policy that specifies her strategy commitment according to the learned information. We provide a polynomial-time algorithm to compute the optimal such policy, and in addition, a heuristic approach that applies even when the attacker's payoff space is infinite or completely unknown. Empirical evaluation shows that our approaches can improve the defender's utility significantly as compared to the situation when attacker manipulation is ignored.",[],[],"['Jiarui Gan', 'Qingyu Guo', 'Long Tran-Thanh', 'Bo An', 'Michael Wooldridge']","['University of Oxford, Oxford, UK', 'Nanyang Technological University, Singapore', 'University of Southampton, Southampton, UK', 'Nanyang Technological University, Singapore', 'University of Oxford, Oxford, UK']","['UK', 'Singapore', 'UK', 'Singapore', 'UK']"
https://papers.nips.cc/paper_files/paper/2019/hash/cbb6a3b884f4f88b3a8e3d44c636cbd8-Abstract.html,Security,A New Defense Against Adversarial Images: Turning a Weakness into a Strength,"Natural images are virtually surrounded by low-density misclassified regions that can be efficiently discovered by gradient-guided search --- enabling the generation of adversarial images. While many techniques for detecting these attacks have been proposed, they are easily bypassed when the adversary has full knowledge of the detection mechanism and adapts the attack strategy accordingly. In this paper, we adopt a novel perspective and regard the omnipresence of adversarial perturbations as a strength rather than a weakness. We postulate that if an image has been tampered with, these adversarial directions either become harder to find with gradient methods or have substantially higher density than for natural images. We develop a practical test for this signature characteristic to successfully detect adversarial attacks, achieving unprecedented accuracy under the white-box setting where the adversary is given full knowledge of our detection mechanism.",[],[],"['Shengyuan Hu', 'Tao Yu', 'Chuan Guo', 'Wei-Lun Chao', 'Kilian Q. Weinberger']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/cd61a580392a70389e27b0bc2b439f49-Abstract.html,Security,Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks,"Modern machine learning systems are susceptible to adversarial examples; inputswhich clearly preserve the characteristic semantics of a given class, but whoseclassification is (usually confidently) incorrect. Existing approaches to adversarialdefense generally rely on modifying the input, e.g. quantization, or the learnedmodel parameters, e.g. via adversarial training. However, recent research hasshown that most such approaches succumb to adversarial examples when different norms or more sophisticated adaptive attacks are considered. In this paper, we propose a fundamentally different approach which instead changes the way the output is represented and decoded. This simple approach achieves state-of-the-art robustness to adversarial examples for L 2 and L ∞ based adversarial perturbations on MNIST and CIFAR10. In addition, even under strong white-box attacks, we find that our model often assigns adversarial examples a low probability; those with high probability are usually interpretable, i.e. perturbed towards the perceptual boundary between the original and adversarial class. Our approach has several advantages: it yields more meaningful probability estimates, is extremely fast during training and testing, requires essentially no architectural changes to existing discriminative learning pipelines, is wholly complementary to other defense approaches including adversarial training, and does not sacrifice benign test set performance",[],[],"['Gunjan Verma', 'Ananthram Swami']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d09bf41544a3365a46c9077ebb5e35c3-Abstract.html,Security,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks,"Predicting the future trajectories of multiple interacting pedestrians in a scene has become an increasingly important problem for many different applications ranging from control of autonomous vehicles and social robots to security and surveillance. This problem is compounded by the presence of social interactions between humans and their physical interactions with the scene. While the existing literature has explored some of these cues, they mainly ignored the multimodal nature of each human's future trajectory which is noticeably influenced by the intricate social interactions. In this paper, we present Social-BiGAT, a graph-based generative adversarial network that generates realistic, multimodal trajectory predictions for multiple pedestrians in a scene. Our method is based on a graph attention network (GAT) that learns feature representations that encode the social interactions between humans in the scene, and a recurrent encoder-decoder architecture that is trained adversarially to predict, based on the features, the humans' paths. We explicitly account for the multimodal nature of the prediction problem by forming a reversible transformation between each scene and its latent noise vector, as in Bicycle-GAN. We show that our framework achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks.",[],[],"['Vineet Kosaraju', 'Amir Sadeghian', 'Roberto Martín-Martín', 'Ian Reid', 'Hamid Rezatofighi', 'Silvio Savarese']","['Stanford University', 'Stanford University and Aibee Inc', 'Stanford University', 'University of Adelaide', 'Stanford University and University of Adelaide', 'Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d18f655c3fce66ca401d5f38b48c89af-Abstract.html,Security,"Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge","Text-to-image generation, i.e. generating an image given a text description, is a very challenging task due to the significant semantic gap between the two domains. Humans, however, tackle this problem intelligently. We learn from diverse objects to form a solid prior about semantics, textures, colors, shapes, and layouts. Given a text description, we immediately imagine an overall visual impression using this prior and, based on this, we draw a picture by progressively adding more and more details. In this paper, and inspired by this process, we propose a novel text-to-image method called LeicaGAN to combine the above three phases in a unified framework. First, we formulate the multiple priors learning phase as a textual-visual co-embedding (TVE) comprising a text-image encoder for learning semantic, texture, and color priors and a text-mask encoder for learning shape and layout priors. Then, we formulate the imagination phase as multiple priors aggregation (MPA) by combining these complementary priors and adding noise for diversity. Lastly, we formulate the creation phase by using a cascaded attentive generator (CAG) to progressively draw a picture from coarse to fine. We leverage adversarial learning for LeicaGAN to enforce semantic consistency and visual realism. Thorough experiments on two public benchmark datasets demonstrate LeicaGAN's superiority over the baseline method. Code has been made available at https://github.com/qiaott/LeicaGAN.",[],[],"['Tingting Qiao', 'Jing Zhang', 'Duanqing Xu', 'Dacheng Tao']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d384dec9f5f7a64a36b5c8f03b8a6d92-Abstract.html,Security,Cross-Modal Learning with Adversarial Samples,"With the rapid developments of deep neural networks, numerous deep cross-modal analysis methods have been presented and are being applied in widespread real-world applications, including healthcare and safety-critical environments. However, the recent studies on robustness and stability of deep neural networks show that a microscopic modification, known as adversarial sample, which is even imperceptible to humans, can easily fool a well-performed deep neural network and brings a new obstacle to deep cross-modal correlation exploring. In this paper, we propose a novel Cross-Modal correlation Learning with Adversarial samples, namely CMLA, which for the first time presents the existence of adversarial samples in cross-modal data. Moreover, we provide a simple yet effective adversarial sample learning method, where inter- and intra- modality similarity regularizations across different modalities are simultaneously integrated into the learning of adversarial samples. Finally, our proposed CMLA is demonstrated to be highly effective in cross-modal hashing based retrieval. Extensive experiments on two cross-modal benchmark datasets show that the adversarial examples produced by our CMLA are efficient in fooling a target deep cross-modal hashing network. On the other hand, such adversarial examples can significantly strengthen the robustness of the target network by conducting an adversarial training.",[],[],"['CHAO LI', 'Shangqian Gao', 'Cheng Deng', 'De Xie', 'Wei Liu']","[""School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, China and Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA"", ""School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, China"", 'Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA', ""School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, China"", 'Tencent AI Lab, China']","['China', 'China', 'China', 'China']"
https://papers.nips.cc/paper_files/paper/2019/hash/d8700cbd38cc9f30cecb34f0c195b137-Abstract.html,Security,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,"We introduce a feature scattering-based adversarial training approach for improving model robustness against adversarial attacks.Conventional adversarial training approaches leverage a supervised scheme (either targeted or non-targeted) in generating attacks for training, which typically suffer from issues such as label leaking as noted in recent works.Differently, the proposed approach generates adversarial images for training through feature scattering in the latent space, which is unsupervised in nature and avoids label leaking. More importantly, this new approach generates perturbed images in a collaborative fashion, taking the inter-sample relationships into consideration. We conduct analysis on model robustness and demonstrate the effectiveness of the proposed approach  through extensively experiments on different datasets compared with state-of-the-art approaches.",[],[],"['Haichao Zhang', 'Jianyu Wang']","['Horizon Robotics', 'Baidu Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/db29450c3f5e97f97846693611f98c15-Abstract.html,Security,Convergent Policy Optimization for Safe Reinforcement Learning,"We study the safe reinforcement learning problem with nonlinear function approximation, where policy optimization is formulated as a constrained optimization problem with both the objective and the constraint being nonconvex functions. For such a problem, we construct a sequence of surrogate convex constrained optimization problems by replacing the nonconvex functions locally with convex quadratic functions obtained from policy gradient estimators.  We prove that the solutions to these surrogate problems converge to a stationary point of the original nonconvex problem. Furthermore, to extend our theoretical results, we apply our algorithm to examples of optimal control and multi-agent reinforcement learning with safety constraints.",[],[],"['Ming Yu', 'Zhuoran Yang', 'Mladen Kolar', 'Zhaoran Wang']","['The University of Chicago Booth School of Business, Chicago, IL', 'Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ', 'The University of Chicago Booth School of Business, Chicago, IL', 'Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/dca5672ff3444c7e997aa9a2c4eb2094-Abstract.html,Security,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components,"Abstract Neural networks are state-of-the-art classification approaches but are generally difficult to interpret. This issue can be partly alleviated by constructing a precise decision process within the neural network. In this work, a network architecture, denoted as Classification-By-Components network (CBC), is proposed. It is restricted to follow an intuitive reasoning based decision process inspired by Biederman's recognition-by-components theory from cognitive psychology. The network is trained to learn and detect generic components that characterize objects. In parallel, a class-wise reasoning strategy based on these components is learned to solve the classification problem. In contrast to other work on reasoning, we propose three different types of reasoning: positive, negative, and indefinite. These three types together form a probability space to provide a probabilistic classifier. The decomposition of objects into generic components combined with the probabilistic reasoning provides by design a clear interpretation of the classification decision process. The evaluation of the approach on MNIST shows that CBCs are viable classifiers. Additionally, we demonstrate that the inherent interpretability offers a profound understanding of the classification behavior such that we can explain the success of an adversarial attack. The method's scalability is successfully tested using the ImageNet dataset.",[],[],"['Sascha Saralajew', 'Lars Holdijk', 'Maike Rees', 'Ebubekir Asan', 'Thomas Villmann']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e2f374c3418c50bc30d67d5f7454a5b4-Abstract.html,Security,Certifiable Robustness to Graph Perturbations,"Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.",[],[],"['Aleksandar Bojchevski', 'Stephan Günnemann']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e46bc064f8e92ac2c404b9871b2a4ef2-Abstract.html,Security,Self-Routing Capsule Networks,"Capsule networks have recently gained a great deal of interest as a new architecture of neural networks that can be more robust to input perturbations than similar-sized CNNs. Capsule networks have two major distinctions from the conventional CNNs: (i) each layer consists of a set of capsules that specialize in disjoint regions of the feature space and (ii) the routing-by-agreement coordinates connections between adjacent capsule layers. Although the routing-by-agreement is capable of filtering out noisy predictions of capsules by dynamically adjusting their influences, its unsupervised clustering nature causes two weaknesses: (i) high computational complexity and (ii) cluster assumption that may not hold in presence of heavy input noise. In this work, we propose a novel and surprisingly simple routing strategy called self-routing where each capsule is routed independently by its subordinate routing network. Therefore, the agreement between capsules is not required anymore but both poses and activations of upper-level capsules are obtained in a way similar to Mixture-of-Experts. Our experiments on CIFAR-10, SVHN and SmallNORB show that the self-routing performs more robustly against white-box adversarial attacks and affine transformations, requiring less computation.",[],[],"['Taeyoung Hahn', 'Myeongjang Pyeon', 'Gunhee Kim']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ebbdfea212e3a756a1fded7b35578525-Abstract.html,Security,Adversarial Music: Real world Audio Adversary against Wake-word Detection System,"Voice Assistants (VAs) such as Amazon Alexa or Google Assistant rely on wake-word detection to respond to people's commands, which could potentially be vulnerable to audio adversarial examples. In this work, we target our attack on the wake-word detection system. Our goal is to jam the model with some inconspicuous background music to deactivate the VAs while our audio adversary is present. We implemented an emulated wake-word detection system of Amazon Alexa based on recent publications. We validated our models against the real Alexa in terms of wake-word detection accuracy. Then we computed our audio adversaries with consideration of expectation over transform and we implemented our audio adversary with a differentiable synthesizer. Next we verified our audio adversaries digitally on hundreds of samples of utterances collected from the real world. Our experiments show that we can effectively reduce the recognition F1 score of our emulated model from 93.4% to 11.0%. Finally, we tested our audio adversary over the air, and verified it works effectively against Alexa, reducing its F1 score from 92.5% to 11.0%. To the best of our knowledge, this is the first real-world adversarial attack against a commercial grade VA wake-word detection system. Our demo video is included in the supplementary material.",[],[],"['Juncheng Li', 'Shuhui Qu', 'Xinjian Li', 'Joseph Szurley', 'J. Zico Kolter', 'Florian Metze']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ec1c59141046cd1866bbbcdfb6ae31d4-Abstract.html,Security,A Little Is Enough: Circumventing Defenses For Distributed Learning,"Distributed learning is central for large-scale training of deep-learning models. However, it is exposed to a security threat in which Byzantine participants can interrupt or control the learning process. Previous attack models assume that the rogue participants (a) are omniscient (know the data of all other participants), and (b) introduce large changes to the parameters. Accordingly, most defense mechanisms make a similar assumption and attempt to use statistically robust methods to identify and discard values whose reported gradients are far from the population mean. We observe that if the empirical variance between the gradients of workers is high enough, an attacker could take advantage of this and launch a non-omniscient attack that operates within the population variance. We show that the variance is indeed high enough even for simple datasets such as MNIST, allowing an attack that is not only undetected by existing defenses, but also uses their power against them, causing those defense mechanisms to consistently select the byzantine workers while discarding legitimate ones. We demonstrate our attack method works not only for preventing convergence but also for repurposing of the model behavior (``backdooring''). We show that less than 25\% of colluding workers are sufficient to degrade the accuracy of  models trained on MNIST, CIFAR10 and CIFAR100 by 50\%, as well as to introduce backdoors without hurting the accuracy for MNIST and CIFAR10 datasets, but with a degradation for CIFAR100.",[],[],"['Gilad Baruch', 'Moran Baruch', 'Yoav Goldberg']","['Dept. of Computer Science, Bar Ilan University, Israel', 'Dept. of Computer Science, Bar Ilan University, Israel', 'Dept. of Computer Science, Bar Ilan University, Israel and The Allen Institute for Artificial Intelligence']","['Israel', 'Israel', 'Israel']"
https://papers.nips.cc/paper_files/paper/2019/hash/f29e2360ef277f77595dfae0aab78138-Abstract.html,Security,Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning,"We investigate the theoretical limits of pipeline parallel learning of deep learning architectures, a distributed setup in which the computation is distributed per layer instead of per example. For smooth convex and non-convex objective functions, we provide matching lower and upper complexity bounds and show that a naive pipeline parallelization of Nesterov's accelerated gradient descent is optimal. For non-smooth convex functions, we provide a novel algorithm coined Pipeline Parallel Random Smoothing (PPRS) that is within a $d^{1/4}$ multiplicative factor of the optimal convergence rate, where $d$ is the underlying dimension. While the convergence rate still obeys a slow $\varepsilon^{-2}$ convergence rate, the depth-dependent part is accelerated, resulting in a near-linear speed-up and convergence time that only slightly depends on the depth of the deep learning architecture. Finally, we perform an empirical analysis of the non-smooth non-convex case and show that, for difficult and highly non-smooth problems, PPRS outperforms more traditional optimization algorithms such as gradient descent and Nesterov's accelerated gradient descent for problems where the sample size is limited, such as few-shot or adversarial learning.",[],[],"['Igor Colin', 'Ludovic DOS SANTOS', 'Kevin Scaman']","[""Huawei Noah's Ark Lab"", ""Huawei Noah's Ark Lab"", ""Huawei Noah's Ark Lab""]",[]
https://papers.nips.cc/paper_files/paper/2019/hash/f5aa4bd09c07d8b2f65bad6c7cd3358f-Abstract.html,Security,A Composable Specification Language for Reinforcement Learning Tasks,"Reinforcement learning is a promising approach for learning control policies for robot tasks. However, specifying complex tasks (e.g., with multiple objectives and safety constraints) can be challenging, since the user must design a reward function that encodes the entire task. Furthermore, the user often needs to manually shape the reward to ensure convergence of the learning algorithm. We propose a language for specifying complex control tasks, along with an algorithm that compiles specifications in our language into a reward function and automatically performs reward shaping. We implement our approach in a tool called SPECTRL, and show that it outperforms several state-of-the-art baselines.",[],[],"['Kishor Jothimurugan', 'Rajeev Alur', 'Osbert Bastani']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/f7fa6aca028e7ff4ef62d75ed025fe76-Abstract.html,Security,Certifying Geometric Robustness of Neural Networks,"The use of neural networks in safety-critical computer vision systems calls for theirrobustness certification against natural geometric transformations (e.g., rotation,scaling). However, current certification methods target mostly norm-based pixelperturbations and cannot certify robustness against geometric transformations. Inthis work, we propose a new method to compute sound and asymptotically optimallinear relaxations for any composition of transformations. Our method is based ona novel combination of sampling and optimization. We implemented the methodin a system called DeepG and demonstrated that it certifies significantly morecomplex geometric transformations than existing methods on both defended andundefended networks while scaling to large architectures.",[],[],"['Mislav Balunovic', 'Maximilian Baader', 'Gagandeep Singh', 'Timon Gehr', 'Martin Vechev']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/fe73f687e5bc5280214e0486b273a5f9-Abstract.html,Security,Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control,"Data-driven models are subject to model errors due to limited and noisy training data. Key to the application of such models in safety-critical domains is the quantification of their model error. Gaussian processes provide such a measure and uniform error bounds have been derived, which allow safe control based on these models. However, existing error bounds require restrictive assumptions. In this paper, we employ the Gaussian process distribution and continuity arguments to derive a novel uniform error bound under weaker assumptions. Furthermore, we demonstrate how this distribution can be used to derive probabilistic Lipschitz constants and analyze the asymptotic behavior of our bound. Finally, we derive safety conditions for the control of unknown dynamical systems based on Gaussian process models and evaluate them in simulations of a robotic manipulator.",[],[],"['Armin Lederer', 'Jonas Umlauft', 'Sandra Hirche']","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich']",[]