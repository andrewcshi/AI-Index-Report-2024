link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,affiliated_countries
https://nips.cc/virtual/2021/poster/27382,Transparency & Explainability,Cockpit: A Practical Debugging Tool for the Training of Deep Neural Networks,"When engineers train deep learning models, they are very much ""flying blind"". Commonly used methods for real-time training diagnostics, such as monitoring the train/test loss, are limited. Assessing a network's training process solely through these performance indicators is akin to debugging software without access to internal states through a debugger. To address this, we present Cockpit, a collection of instruments that enable a closer look into the inner workings of a learning machine, and a more informative and meaningful status report for practitioners. It facilitates the identification of learning phases and failure modes, like ill-chosen hyperparameters. These instruments leverage novel higher-order information about the gradient distribution and curvature, which has only recently become efficiently accessible. We believe that such a debugging tool, which we open-source for PyTorch, is a valuable help in troubleshooting the training process. By revealing new insights, it also more generally contributes to explainability and interpretability of deep nets.","['Deep Learning', 'Optimization', 'Interpretability']",[],"['Frank Schneider', 'Felix Dangel', 'Philipp Hennig']","['University of Tübingen', 'Vector Institute, Toronto', 'University of Tübingen']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26675,Transparency & Explainability,Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language,"In this work, we propose a unified framework, called Visual Reasoning with Differ-entiable Physics (VRDP), that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) from these object-centric representations based on the language, thus providing prior knowledge for the physics engine. The differentiable physics model, implemented as an impulse-based differentiable rigid-body simulator, performs differentiable physical simulation based on the grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. Consequently, these learned concepts and physical models can explain what we have seen and imagine what is about to happen in future and counterfactual scenarios. Integrating differentiable physics into the dynamic reasoning framework offers several appealing benefits.  More accurate dynamics prediction in learned physics models enables state-of-the-art performance on both synthetic and real-world benchmarks while still maintaining high transparency and interpretability; most notably, VRDP improves the accuracy of predictive and counterfactual questions by 4.5% and 11.5% compared to its best counterpart. VRDP is also highly data-efficient: physical parameters can be optimized from very few videos, and even a single video can be sufficient. Finally, with all physical parameters inferred, VRDP can quickly learn new concepts from a few examples.",['Interpretability'],[],"['Mingyu Ding', 'Zhenfang Chen', 'Tao Du', 'Ping Luo', 'Joshua B. Tenenbaum', 'Chuang Gan']","['University of California, Berkeley', 'MIT-IBM Watson AI lab', 'Tsinghua University', 'The University of', 'Massachusetts Institute of Technology', 'MIT-IBM Watson AI Lab']","[None, None, None, 'Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/26485,Transparency & Explainability,A Trainable Spectral-Spatial Sparse Coding Model for Hyperspectral Image Restoration,"Hyperspectral imaging offers new perspectives for diverse applications, ranging from the monitoring of the environment using airborne or satellite remote sensing, precision farming, food safety, planetary exploration, or astrophysics. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation,  and the lack of accurate ground-truth ""clean"" hyperspectral signals acquired on the spot makes restoration tasks challenging.  In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems where deep models tend to shine. In this paper, we advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data. We show on various denoising benchmarks that our method is computationally efficient and  significantly outperforms the state of the art.","['Deep Learning', 'Interpretability']",[],"['Theo Bodrito', 'Alexandre Zouaoui', 'Jocelyn Chanussot', 'Julien Mairal']","['INRIA', 'INRIA', 'INRIA', 'Inria']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27060,Transparency & Explainability,Regulating algorithmic filtering on social media,"By filtering the content that users see, social media platforms have the ability to influence users' perceptions and decisions, from their dining choices to their voting preferences. This influence has drawn scrutiny, with many calling for regulations on filtering algorithms, but designing and enforcing regulations remains challenging. In this work, we examine three questions. First, given a regulation, how would one design an audit to enforce it? Second, does the audit impose a performance cost on the platform? Third, how does the audit affect the content that the platform is incentivized to filter? In response to these questions, we propose a method such that, given a regulation, an auditor can test whether that regulation is met with only black-box access to the filtering algorithm. We then turn to the platform's perspective. The platform's goal is to maximize an objective function while meeting regulation. We find that there are conditions under which the regulation does not place a high performance cost on the platform and, notably, that content diversity can play a key role in aligning the interests of the platform and regulators.",[],[],"['Sarah Cen', 'Devavrat Shah']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/26454,Transparency & Explainability,Passive attention in artificial neural networks predicts human visual selectivity,"Developments in machine learning interpretability techniques over the past decade have provided new tools to observe the image regions that are most informative for classification and localization in artificial neural networks (ANNs). Are the same regions similarly informative to human observers? Using data from 79 new experiments and 7,810 participants, we show that passive attention techniques reveal a significant overlap with human visual selectivity estimates derived from 6 distinct behavioral tasks including visual discrimination, spatial localization, recognizability, free-viewing, cued-object search, and saliency search fixations. We find that input visualizations derived from relatively simple ANN architectures probed using guided backpropagation methods are the best predictors of a shared component in the joint variability of the human measures. We validate these correlational results with causal manipulations using recognition experiments. We show that images masked with ANN attention maps were easier for humans to classify than control masks in a speeded recognition experiment. Similarly, we find that recognition performance in the same ANN models was likewise influenced by masking input images using human visual selectivity maps. This work contributes a new approach to evaluating the biological and psychological validity of leading ANNs as models of human vision: by examining their similarities and differences in terms of their visual selectivity to the information contained in images.","['Deep Learning', 'Vision', 'Interpretability', 'Machine Learning']",[],"['Thomas A Langlois', 'Haicheng Charles Zhao', 'Erin Grant', 'Ishita Dasgupta', 'Thomas L. Griffiths', 'Nori Jacoby']","['Princeton University', 'Princeton University', 'University College London', 'DeepMind', 'Princeton University', 'Max-Planck Institute']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28837,Transparency & Explainability,Learning interaction rules from multi-animal trajectories via augmented behavioral models,"Extracting the interaction rules of biological agents from movement sequences pose challenges in various domains. Granger causality is a practical framework for analyzing the interactions from observed time-series data; however, this framework ignores the structures and assumptions of the generative process in animal behaviors, which may lead to interpretational problems and sometimes erroneous assessments of causality. In this paper, we propose a new framework for learning Granger causality from multi-animal trajectories via augmented theory-based behavioral models with interpretable data-driven models. We adopt an approach for augmenting incomplete multi-agent behavioral models described by time-varying dynamical systems with neural networks. For efficient and interpretable learning, our model leverages theory-based architectures separating navigation and motion processes, and the theory-guided regularization for reliable behavioral modeling. This can provide interpretable signs of Granger-causal effects over time, i.e., when specific others cause the approach or separation. In experiments using synthetic datasets, our method achieved better performance than various baselines. We then analyzed multi-animal datasets of mice, flies, birds, and bats, which verified our method and obtained novel biological insights.","['Theory', 'Deep Learning', 'Causality', 'Interpretability']",[],"['Keisuke Fujii', 'Naoya Takeishi', 'Kazushi Tsutsui', 'Emyo Fujioka', 'Nozomi Nishiumi', 'Ryooya Tanaka', 'Kaoru Ide', 'Hiroyoshi Kohno', 'Ken Yoda', 'Susumu Takahashi', 'Shizuko Hiryu', 'Yoshinobu Kawahara']","['Nagoya University', 'The University of Tokyo', 'Nagoya University', 'Doshisha University', 'National Institute for Basic Biology', 'Nagoya University', 'Doshisha University', 'Tokai University', 'Nagoya University', 'Doshisha University', 'Doshisha University', 'Osaka University']","[None, None, None, None, None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28813,Transparency & Explainability,Scalable Rule-Based Representation Learning for Interpretable Classification,"Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on nine small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: https://github.com/12wang3/rrl.","['Machine Learning', 'Optimization', 'Interpretability', 'Representation Learning']",[],"['Zhuo Wang', 'Wei Zhang', 'Ning Liu', 'Jianyong Wang']","['Tsinghua University, Tsinghua University', 'East  Normal University', 'Tsinghua University', 'Tsinghua University, Tsinghua University']","[None, 'China', None, None]"
https://nips.cc/virtual/2021/poster/28801,Transparency & Explainability,TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification,"Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, the current MIL methods are usually based on independent and identical distribution hypothesis, thus neglect the correlation among different instances. To address this problem, we proposed a new framework, called correlated MIL, and provided a proof for convergence. Based on this framework, we devised a Transformer based MIL (TransMIL), which explored both morphological and spatial information. The proposed TransMIL can effectively deal with unbalanced/balanced and binary/multiple classification with great visualization and interpretability. We conducted various experiments for three different computational pathology problems and achieved better performance and faster convergence compared with state-of-the-art methods. The test AUC for the binary tumor classification can be up to 93.09% over CAMELYON16 dataset. And the AUC over the cancer subtypes classification can be up to 96.03% and 98.82% over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is available at: https://github.com/szc19990412/TransMIL.","['Transformers', 'Vision', 'Interpretability', 'Machine Learning']",[],"['Hao Bian', 'Yang Chen', 'Yifeng Wang', 'Jian Zhang', 'Xiangyang Ji', 'Yongbing Zhang']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'national university of singaore, National University of', 'Peking University', 'Tsinghua University', 'Harbin Institute of Technology']","[None, None, 'Singapore', None, None, None]"
https://nips.cc/virtual/2021/poster/28602,Transparency & Explainability,"CROCS: Clustering and Retrieval of Cardiac Signals Based on Patient Disease Class, Sex, and Age","The process of manually searching for relevant instances in, and extracting information from, clinical databases underpin a multitude of clinical tasks. Such tasks include disease diagnosis, clinical trial recruitment, and continuing medical education. This manual search-and-extract process, however, has been hampered by the growth of large-scale clinical databases and the increased prevalence of unlabelled instances. To address this challenge, we propose a supervised contrastive learning framework, CROCS, where representations of cardiac signals associated with a set of patient-specific attributes (e.g., disease class, sex, age) are attracted to learnable embeddings entitled clinical prototypes. We exploit such prototypes for both the clustering and retrieval of unlabelled cardiac signals based on multiple patient attributes. We show that CROCS outperforms the state-of-the-art method, DTC, when clustering and also retrieves relevant cardiac signals from a large database. We also show that clinical prototypes adopt a semantically meaningful arrangement based on patient attributes and thus confer a high degree of interpretability.","['Clustering', 'Interpretability', 'Contrastive Learning']",[],"['Dani Kiyasseh', 'Tingting Zhu', 'David A. Clifton']","['California Institute of Technology', 'University of Oxford', 'University of Oxford']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28351,Transparency & Explainability,Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention,"Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin","['Transformers', 'Deep Learning', 'Graph Learning', 'Interpretability']",[],"['Byung-Hoon Kim', 'Jong Chul Ye', 'Jae-Jin Kim']","['Massachusetts General Hospital, Harvard University', 'Korea Advanced Institute of Science and Technology', 'Yonsei University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28229,Transparency & Explainability,Neural Additive Models: Interpretable Machine Learning with Neural Nets,"Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19.","['Deep Learning', 'Interpretability', 'Machine Learning']",[],"['Rishabh Agarwal', 'Levi Melnick', 'Nicholas Frosst', 'Xuezhou Zhang', 'Ben Lengerich', 'Rich Caruana', 'Geoffrey Hinton']","['Google Research, Brain Team', 'Microsoft', 'Google', 'Boston University, Boston University', 'Massachusetts Institute of Technology', 'School of Computer Science, Carnegie Mellon University', 'Google']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28215,Transparency & Explainability,Object-Aware Regularization for Addressing Causal Confusion in Imitation Learning,"Behavioral cloning has proven to be effective for learning sequential decision-making policies from expert demonstrations. However, behavioral cloning often suffers from the causal confusion problem where a policy relies on the noticeable effect of expert actions due to the strong correlation but not the cause we desire. This paper presents Object-aware REgularizatiOn (OREO), a simple technique that regularizes an imitation policy in an object-aware manner. Our main idea is to encourage a policy to uniformly attend to all semantic objects, in order to prevent the policy from exploiting nuisance variables strongly correlated with expert actions. To this end, we introduce a two-stage approach: (a) we extract semantic objects from images by utilizing discrete codes from a vector-quantized variational autoencoder, and (b) we randomly drop the units that share the same discrete code together, i.e., masking out semantic objects. Our experiments demonstrate that OREO significantly improves the performance of behavioral cloning, outperforming various other regularization and causality-based methods on a variety of Atari environments and a self-driving CARLA environment. We also show that our method even outperforms inverse reinforcement learning methods trained with a considerable amount of environment interaction.","['Reinforcement Learning and Planning', 'Causality']",[],"['Jongjin Park', 'Younggyo Seo', 'Chang Liu', 'Li Zhao', 'Tao Qin', 'Jinwoo Shin', 'Tie-Yan Liu']","['Korea Advanced Institute of Science and Technology', 'Dyson', 'Microsoft', 'Tsinghua University', 'Microsoft Research Asia', 'Korea Advanced Institute of Science and Technology', 'Microsoft']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28189,Transparency & Explainability,Curriculum Disentangled Recommendation with Noisy Multi-feedback,"Learning disentangled representations for user intentions from multi-feedback (i.e., positive and negative feedback) can enhance the accuracy and explainability of recommendation algorithms. However, learning such disentangled representations from multi-feedback data is challenging because  i) multi-feedback is complex: there exist complex relations among different types of feedback (e.g., click, unclick, and dislike, etc) as well as various user intentions, and ii) multi-feedback is noisy: there exists noisy (useless) information both in features and labels, which may deteriorate the recommendation performance.  Existing works on disentangled representation learning only focus on positive feedback, failing to handle the complex relations and noise hidden in multi-feedback data. To solve this problem, in this work we propose a Curriculum Disentangled Recommendation (CDR) model that is capable of efficiently learning disentangled representations from complex and noisy multi-feedback for better recommendation. Concretely, we design a co-filtering dynamic routing mechanism that simultaneously captures the complex relations among different behavioral feedback and user intentions as well as denoise the representations in the feature level. We then present an adjustable self-evaluating curriculum that is able to evaluate sample difficulties for better model training and conduct denoising in the label level via disregarding useless information. Our extensive experiments on several real-world datasets demonstrate that the proposed CDR model can significantly outperform several state-of-the-art methods in terms of recommendation accuracy.","['Interpretability', 'Representation Learning']",[],"['Hong Chen', 'Yudong Chen', 'Xin Wang', 'Ruobing Xie', 'Rui Wang', 'Feng Xia', 'Wenwu Zhu']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tencent', 'Tencent Wechat Group', 'Institute of Computing Technology, CAS', 'Tsinghua University, Tsinghua University']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28162,Transparency & Explainability,Explaining Latent Representations with a Corpus of Examples,"Modern machine learning models are complicated. Most of them rely on convoluted latent representations of their input to issue a prediction. To achieve greater transparency than a black-box that connects inputs to predictions, it is necessary to gain a deeper understanding of these latent representations. To that aim, we propose SimplEx: a user-centred method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user’s understanding of the latent space with post-hoc explanations answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example? SimplEx provides an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, we propose a novel approach, the integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Through experiments on tasks ranging from mortality prediction to image classification, we demonstrate that these decompositions are robust and accurate. With illustrative use cases in medicine, we show that SimplEx empowers the user by highlighting relevant patterns in the corpus that explain model representations. Moreover, we demonstrate how the freedom in choosing the corpus allows the user to have personalized explanations in terms of examples that are meaningful for them.","['Vision', 'Interpretability', 'Machine Learning']",[],"['Jonathan Crabbé', 'Zhaozhi Qian', 'Fergus Imrie', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge', 'University of California, Los Angeles', 'University of Cambridge']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28083,Transparency & Explainability,Counterfactual Maximum Likelihood Estimation for Training Deep Networks,"Although deep learning models have driven state-of-the-art performance on a wide array of tasks, they are prone to spurious correlations that should not be learned as predictive clues. To mitigate this problem, we propose a causality-based training framework to reduce the spurious correlations caused by observed confounders. We give theoretical analysis on the underlying general Structural Causal Model (SCM) and propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely Counterfactual Maximum Likelihood Estimation (CMLE). As the interventional distribution, in general, is hidden from the observational data, we then derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning models using observational data. We conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. The results show that CMLE methods outperform the regular MLE method in terms of out-of-domain generalization performance and reducing spurious correlations, while maintaining comparable performance on the regular evaluations.","['Deep Learning', 'Causality', 'Domain Adaptation', 'Language']",[],"['Xinyi Wang', 'Wenhu Chen', 'Michael Saxon', 'William Yang Wang']","['UC Santa Barbara', 'University of Waterloo', 'UC Santa Barbara', 'UC Santa Barbara']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28027,Transparency & Explainability,Adaptive wavelet distillation from neural networks through interpretations,"Recent deep-learning models have achieved impressive prediction performance, but often sacrifice interpretability and computational efficiency. Interpretability is crucial in many disciplines, such as science and medicine, where models must be carefully vetted or where interpretation is the goal itself. Moreover, interpretable models are concise and often yield computational efficiency. Here, we propose adaptive wavelet distillation (AWD), a method which aims to distill information from a trained neural network into a wavelet transform. Specifically, AWD penalizes feature attributions of a neural network in the wavelet domain to learn an effective multi-resolution wavelet transform. The resulting model is highly predictive, concise, computationally efficient, and has properties (such as a multi-scale structure) which make it easy to interpret. In close collaboration with domain experts, we showcase how AWD addresses challenges in two real-world settings: cosmological parameter inference and molecular-partner prediction. In both cases, AWD yields a scientifically interpretable and concise model which gives predictive performance better than state-of-the-art neural networks. Moreover, AWD identifies predictive features that are scientifically meaningful in the context of respective domains. All code and models are released in a full-fledged package available on Github.","['Deep Learning', 'Interpretability']",[],"['Wooseok Ha', 'Chandan Singh', 'Francois Lanusse', 'Srigokul Upadhyayula', 'Bin Yu']","['AWS AI Labs', 'Microsoft research', 'CNRS', 'University of California Berkeley', 'University of California - Berkeley']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28034,Transparency & Explainability,Functionally Regionalized Knowledge Transfer for Low-resource Drug Discovery,"More recently, there has been a surge of interest in employing machine learning approaches to expedite the drug discovery process where virtual screening for hit discovery and ADMET prediction for lead optimization play essential roles. One of the main obstacles to the wide success of machine learning approaches in these two tasks is that the number of compounds labeled with activities or ADMET properties is too small to build an effective predictive model. This paper seeks to remedy the problem by transferring the knowledge from previous assays, namely in-vivo experiments, by different laboratories and against various target proteins. To accommodate these wildly different assays and capture the similarity between assays, we propose a functional rationalized meta-learning algorithm FRML for such knowledge transfer. FRML constructs the predictive model with layers of neural sub-networks or so-called functional regions. Building on this, FRML shares an initialization for the weights of the predictive model across all assays, while customizes it to each assay with a region localization network choosing the pertinent regions. The compositionality of the model improves the capacity of generalization to various and even out-of-distribution tasks. Empirical results on both virtual screening and ADMET prediction validate the superiority of FRML over state-of-the-art baselines powered with interpretability in assay relationship.","['Meta Learning', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Huaxiu Yao', 'Ying Wei', 'Long-Kai Huang', 'Ding Xue', 'Junzhou Huang']","['Department of Computer Science, University of North Carolina at Chapel Hill', 'Nanyang Technological University', 'Tencent AI Lab', 'Tencent AI Lab', 'University of Texas, Arlington']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27987,Transparency & Explainability,Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression,"Models like LASSO and ridge regression are extensively used in practice due to their interpretability, ease of use, and strong theoretical guarantees. Cross-validation (CV) is widely used for hyperparameter tuning in these models, but do practical methods minimize the true out-of-sample loss? A recent line of research promises to show that the optimum of the CV loss matches the optimum of the out-of-sample loss (possibly after simple corrections). It remains to show how tractable it is to minimize the CV loss. In the present paper, we show that, in the case of ridge regression, the CV loss may fail to be quasiconvex and thus may have multiple local optima. We can guarantee that the CV loss is quasiconvex in at least one case: when the spectrum of the covariate matrix is nearly flat and the noise in the observed responses is not too high. More generally, we show that quasiconvexity status is independent of many properties of the observed data (response norm, covariate-matrix right singular vectors and singular-value scaling) and has a complex dependence on the few that remain. We empirically confirm our theory using simulated experiments.","['Theory', 'Optimization', 'Interpretability']",[],"['William T. Stephenson', 'Zachary Frangella', 'Madeleine Udell', 'Tamara Broderick']","['MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Stanford University', 'Stanford University', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27910,Transparency & Explainability,Discerning Decision-Making Process of Deep Neural Networks with Hierarchical Voting Transformation,"Neural network based deep learning techniques have shown great success for numerous applications. While it is expected to understand their intrinsic decision-making processes, these deep neural networks often work in a black-box way. To this end, in this paper, we aim to discern the decision-making processes of neural networks through a hierarchical voting strategy by developing an explainable deep learning model, namely Voting Transformation-based Explainable Neural Network (VOTEN). Specifically, instead of relying on massive feature combinations, VOTEN creatively models expressive single-valued voting functions between explicitly modeled latent concepts to achieve high fitting ability. Along this line, we first theoretically analyze the major components of VOTEN and prove the relationship and advantages of VOTEN compared with Multi-Layer Perceptron (MLP), the basic structure of deep neural networks. Moreover, we design efficient algorithms to improve the model usability by explicitly showing the decision processes of VOTEN. Finally, extensive experiments on multiple real-world datasets clearly validate the performances and explainability of VOTEN.","['Deep Learning', 'Interpretability']",[],"['Ying Sun', 'Hengshu Zhu', 'Chuan Qin', 'Fuzhen Zhuang', 'Qing He', 'Hui Xiong']","['University of Science and Technology (Guangzhou)', 'BOSS Zhipin', 'BOSS Zhipin', 'Beihang University', 'Institute of Computing Technology, CAS', 'University of Science and Technology']","['Hong Kong', None, None, None, None, 'Hong Kong']"
https://nips.cc/virtual/2021/poster/27871,Transparency & Explainability,Reliable Post hoc Explanations: Modeling Uncertainty in Explainability,"As black box explanations are increasingly being employed to establish model credibility in high stakes settings, it is important to ensure that these explanations are accurate and reliable. However, prior work demonstrates that explanations generated by state-of-the-art techniques are inconsistent, unstable, and provide very little insight into their correctness and reliability. In addition, these methods are also computationally inefficient, and require significant hyper-parameter tuning. In this paper, we address the aforementioned challenges by developing a novel Bayesian framework for generating local explanations along with their associated uncertainty. We instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP which  output credible intervals for the feature importances, capturing the associated uncertainty. The resulting explanations not only enable us to make concrete inferences about their quality (e.g., there is a 95% chance that the feature importance lies within the given range), but are also highly consistent and stable. We carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence.  This work makes the first attempt at addressing several critical issues with popular explanation methods in one shot, thereby generating consistent, stable, and reliable explanations with guarantees in a computationally efficient manner. Experimental evaluation with multiple real world datasets and user studies demonstrate that the efficacy of the proposed framework.","['Robustness', 'Interpretability']",[],"['Dylan Z Slack', 'Sophie Hilgard', 'Sameer Singh', 'Himabindu Lakkaraju']","['University of California, Irvine', 'Harvard University', 'University of California, Irvine', 'Harvard University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27847,Transparency & Explainability,Designing Counterfactual Generators using Deep Model Inversion,"Explanation techniques that synthesize small, interpretable changes to a given image while producing desired changes in the model prediction have become popular for introspecting black-box models. Commonly referred to as counterfactuals, the synthesized explanations are required to contain discernible changes (for easy interpretability) while also being realistic (consistency to the data manifold). In this paper, we focus on the case where we have access only to the trained deep classifier and not the actual training data. While the problem of inverting deep models to synthesize images from the training distribution has been explored, our goal is to develop a deep inversion approach to generate counterfactual explanations for a given query image. Despite their effectiveness in conditional image synthesis, we show that existing deep inversion methods are insufficient for producing meaningful counterfactuals. We propose DISC (Deep Inversion for Synthesizing Counterfactuals) that improves upon deep inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. We find that, in addition to producing visually meaningful explanations, the counterfactuals from DISC are effective at learning classifier decision boundaries and are robust to unknown test-time corruptions.","['Optimization', 'Interpretability', 'Representation Learning']",[],"['Jayaraman J. Thiagarajan', 'Vivek Narayanaswamy', 'Deepta Rajan', 'Jia Liang', 'Akshay Chaudhari']","['Lawrence Livermore National Labs', 'Lawrence Livermore National Labs', 'IBM, International Business Machines', 'Stanford University', 'Stanford University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27836,Transparency & Explainability,Emergent Communication of Generalizations,"To build agents that can collaborate effectively with others, recent research has trained artificial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication. We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overfitting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We find that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language.",['Interpretability'],[],"['Jesse Mu', 'Noah Goodman']","['Stanford University', 'Stanford University']","[None, None]"
https://nips.cc/virtual/2021/poster/27832,Transparency & Explainability,Sampling  with Trusthworthy Constraints:  A Variational Gradient Framework,"Sampling-based inference and learning techniques, especially Bayesian inference, provide an essential approach to handling uncertainty in machine learning (ML). As these techniques are increasingly used in daily life, it becomes essential to safeguard the ML systems with various trustworthy-related constraints, such as fairness, safety, interpretability. Mathematically, enforcing these constraints in probabilistic inference can be cast into sampling from intractable distributions subject to general nonlinear constraints, for which practical efficient algorithms are still largely missing. In this work, we propose a family of constrained sampling algorithms which generalize Langevin Dynamics (LD) and Stein Variational Gradient Descent (SVGD) to incorporate a moment constraint specified by a general nonlinear function. By exploiting the gradient flow structure of LD and SVGD, we derive two types of algorithms for handling constraints, including a primal-dual gradient approach and the constraint controlled gradient descent approach. We investigate the continuous-time mean-field limit of these algorithms and show that they have O(1/t) convergence under mild conditions. Moreover, the LD variant converges linearly assuming that a log Sobolev like inequality holds. Various numerical experiments are conducted to demonstrate the efficiency of our algorithms in trustworthy settings.","['Fairness', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Xingchao Liu', 'Xin Tong', 'qiang liu']","['University of Texas, Austin', 'National University of', 'Dartmouth College']","[None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/27764,Transparency & Explainability,Learning Riemannian metric for disease progression modeling,"Linear mixed-effect models provide a natural baseline for estimating disease progression using longitudinal data. They provide interpretable models at the cost of modeling assumptions on the progression profiles and their variability across subjects. A significant improvement is to embed the data in a Riemannian manifold and learn patient-specific trajectories distributed around a central geodesic. A few interpretable parameters characterize subject trajectories at the cost of a prior choice of the metric, which determines the shape of the trajectories. We extend this approach by learning the metric from the data allowing more flexibility while keeping the interpretability. Specifically, we learn the metric as the push-forward of the Euclidean metric by a diffeomorphism. This diffeomorphism is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The metric update allows us to improve the forecasting of imaging and clinical biomarkers in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) cohort. Our results compare favorably to the 56 methods benchmarked in the TADPOLE challenge.",['Interpretability'],[],"['Samuel Gruffaz', 'Pierre-Emmanuel Poulet', 'Etienne Maheux', 'Bruno Michel Jedynak', 'Stanley Durrleman']","['Ecole Normale Superieure', 'INRIA', 'INRIA', 'Portland State University', 'Inria']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26456,Transparency & Explainability,Foundations of Symbolic Languages for Model Interpretability,"Several queries and scores have recently been proposed to explain individual predictions over ML models. Examples include queries based on “anchors”, which are parts of an instance that are sufficient to justify its classification, and “feature-perturbation” scores such as SHAP. Given the need for flexible, reliable, and easy-to-apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We do this in a principled way by rooting such a language in a logic called FOIL, which allows for expressing many simple but important explainability queries, and might serve as a core for more expressive interpretability languages. We study the computational complexity of FOIL queries over two classes of ML models often deemed to be easily interpretable: decision trees and more general decision diagrams. Since the number of possible inputs for an ML model is exponential in its dimension, tractability of the FOIL evaluation problem is delicate but can be achieved by either restricting the structure of the models, or the fragment of FOIL being evaluated.  We also present a prototype implementation of FOIL wrapped in a high-level declarative language and perform experiments showing that such a language can be used in practice.","['Interpretability', 'Machine Learning']",[],"['Marcelo Arenas', 'Daniel Báez', 'Pablo Barcelo', 'Jorge Pérez', 'Bernardo Subercaseaux']","['RelationalAI', 'Universidad de', 'Pontificia Universidad Católica', 'Universidad de', 'Carnegie Mellon University']","[None, 'Chile', None, 'Chile', None]"
https://nips.cc/virtual/2021/poster/27651,Transparency & Explainability,Garment4D: Garment Reconstruction from Point Cloud Sequences,"Learning to reconstruct 3D garments is important for dressing 3D human bodies of different shapes in different poses. Previous works typically rely on 2D images as input, which however suffer from the scale and pose ambiguities. To circumvent the problems caused by 2D images, we propose a principled framework, Garment4D, that uses 3D point cloud sequences of dressed humans for garment reconstruction. Garment4D has three dedicated steps: sequential garments registration, canonical garment estimation, and posed garment reconstruction. The main challenges are two-fold: 1) effective 3D feature learning for fine details, and 2) capture of garment dynamics caused by the interaction between garments and the human body, especially for loose garments like skirts. To unravel these problems, we introduce a novel Proposal-Guided Hierarchical Feature Network and Iterative Graph Convolution Network, which integrate both high-level semantic features and low-level geometric features for fine details reconstruction. Furthermore, we propose a Temporal Transformer for smooth garment motions capture. Unlike non-parametric methods, the reconstructed garment meshes by our method are separable from the human body and have strong interpretability, which is desirable for downstream tasks. As the first attempt at this task, high-quality reconstruction results are qualitatively and quantitatively illustrated through extensive experiments. Codes are available at https://github.com/hongfz16/Garment4D.","['Transformers', 'Graph Learning', 'Interpretability']",[],"['Fangzhou Hong', 'Liang Pan', 'Zhongang Cai', 'Ziwei Liu']","['Nanyang Technological University', 'Shanghai AI Lab', 'Nanyang Technological University', 'Nanyang Technological University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27559,Transparency & Explainability,CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions,"In recent years there has been a considerable amount of research on local post hoc explanations for neural networks. However, work on building interpretable neural architectures has been relatively sparse. In this paper, we present a novel neural architecture, CoFrNet, inspired by the form of continued fractions which are known to have many attractive properties in number theory, such as fast convergence of approximations to real numbers. We show that CoFrNets can be efficiently trained as well as interpreted leveraging their particular functional form. Moreover, we prove that such architectures are universal approximators based on a proof strategy that is different than the typical strategy used to prove universal approximation results for neural networks based on infinite width (or depth), which is likely to be of independent interest. We experiment on nonlinear synthetic functions and are able to accurately model as well as estimate feature attributions and even higher order terms in some cases, which is a testament to the representational power as well as interpretability of such architectures. To further showcase the power of CoFrNets, we experiment on seven real datasets spanning tabular, text and image modalities, and show that they are either comparable or significantly better than other interpretable models and multilayer perceptrons, sometimes approaching the accuracies of state-of-the-art models.","['Theory', 'Deep Learning', 'Interpretability']",[],"['Isha Puri', 'Amit Dhurandhar', 'Tejaswini Pedapati', 'Karthikeyan Shanmugam', 'Dennis Wei', 'Kush R. Varshney']","['Harvard University', 'International Business Machines', 'International Business Machines', 'Google', 'International Business Machines', 'International Business Machines']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27493,Transparency & Explainability,Hard-Attention for Scalable Image Classification,"Can we leverage high-resolution information without the unsustainable quadratic complexity to input scale? We propose Traversal Network (TNet), a novel multi-scale hard-attention architecture, which traverses image scale-space in a top-down fashion, visiting only the most informative image regions along the way. TNet offers an adjustable trade-off between accuracy and complexity, by changing the number of attended image locations. We compare our model against hard-attention baselines on ImageNet, achieving higher accuracy with less resources (FLOPs, processing time and memory). We further test our model on fMoW dataset, where we process satellite images of size up to $896 \times 896$ px, getting up to $2.5$x faster processing compared to baselines operating on the same resolution, while achieving higher accuracy as well. TNet is modular, meaning that most classification models could be adopted as its backbone for feature extraction, making the reported performance gains orthogonal to benefits offered by existing optimized deep models. Finally, hard-attention guarantees a degree of interpretability to our model's predictions, without any extra cost beyond inference.","['Vision', 'Interpretability', 'Machine Learning']",[],"['Athanasios Papadopoulos', 'Pawel Korus', 'Nasir Memon']","['New York University', 'New York University', 'New York University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27794,Transparency & Explainability,Understanding Instance-based Interpretability of Variational Auto-Encoders,"Instance-based interpretation methods have been widely studied for supervised learning methods as they help explain how black box neural networks predict. However, instance-based interpretations remain ill-understood in the context of unsupervised learning. In this paper, we investigate influence functions [Koh and Liang, 2017], a popular instance-based interpretation method, for a class of deep generative models called variational auto-encoders (VAE). We formally frame the counter-factual question answered by influence functions in this setting, and through theoretical analysis, examine what they reveal about the impact of training samples on classical unsupervised learning methods. We then introduce VAE- TracIn, a computationally efficient and theoretically sound solution based on Pruthi et al. [2020], for VAEs. Finally, we evaluate VAE-TracIn on several real world datasets with extensive quantitative and qualitative analysis.","['Deep Learning', 'Self-Supervised Learning', 'Interpretability', 'Generative Model']",[],"['Zhifeng Kong', 'Kamalika Chaudhuri']","['NVIDIA', 'UC San Diego, University of California, San Diego']","[None, None]"
https://nips.cc/virtual/2021/poster/27327,Transparency & Explainability,Fair Sortition Made Transparent,"Sortition is an age-old democratic paradigm, widely manifested today through the random selection of citizens' assemblies. Recently-deployed algorithms select assemblies \textit{maximally fairly}, meaning that subject to demographic quotas, they give all potential participants as equal a chance as possible of being chosen.  While these fairness gains can bolster the legitimacy of citizens' assemblies and facilitate their uptake, existing algorithms remain limited by their lack of transparency. To overcome this hurdle, in this work we focus on panel selection by uniform lottery, which is easy to realize in an observable way. By this approach, the final assembly is selected by uniformly sampling some pre-selected set of $m$ possible assemblies. We provide theoretical guarantees on the fairness attainable via this type of uniform lottery, as compared to the existing maximally fair but opaque algorithms, for two different fairness objectives. We complement these results with experiments on real-world instances that demonstrate the viability of the uniform lottery approach as a method of selecting assemblies both fairly and transparently.","['Fairness', 'Graph Learning']",[],"['Bailey Flanigan', 'Gregory Kehne', 'Ariel D. Procaccia']","['CMU, Carnegie Mellon University', 'Harvard University', 'Harvard University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27250,Transparency & Explainability,Do Input Gradients Highlight Discriminative Features?,"Post-hoc gradient-based interpretability methods [Simonyan et al., 2013, Smilkov et al., 2017] that provide instance-specific explanations of model predictions are often based on assumption (A): magnitude of input gradients—gradients of logits with respect to input—noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach: 1. We develop an evaluation framework, DiffROAR, to test assumption (A) on four image classification benchmarks. Our results suggest that (i) input gradients of standard models (i.e., trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A). 2. We then introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models. 3. Finally, we theoretically prove that our empirical findings hold on a simplified version of the BlockMNIST dataset. Specifically, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-specific signal coordinates, thus grossly violating assumption (A). Our findings motivate the need to formalize and test common assumptions in interpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe that the DiffROAR evaluation framework and BlockMNIST-based datasets can serve as sanity checks to audit instance-specific interpretability methods; code and data available at https://github.com/harshays/inputgradients.","['Vision', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Interpretability']",[],"['Harshay Shah', 'Prateek Jain', 'Praneeth Netrapalli']","['Massachusetts Institute of Technology', 'Google', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27169,Transparency & Explainability,Explaining Hyperparameter Optimization via Partial Dependence Plots,"Automated hyperparameter optimization (HPO) can support practitioners to obtain peak performance in machine learning models. However, there is often a lack of valuable insights into the effects of different hyperparameters on the final model performance. This lack of explainability makes it difficult to trust and understand the automated HPO process and its results. We suggest using interpretable machine learning (IML) to gain insights from the experimental data obtained during HPO with Bayesian optimization (BO). BO tends to focus on promising regions with potential high-performance configurations and thus induces a sampling bias. Hence, many IML techniques, such as the partial dependence plot (PDP), carry the risk of generating biased interpretations. By leveraging the posterior uncertainty of the BO surrogate model, we introduce a variant of the PDP with estimated confidence bands. We propose to partition the hyperparameter space to obtain more confident and reliable PDPs in relevant sub-regions. In an experimental study, we provide quantitative evidence for the increased quality of the PDPs within sub-regions.","['Optimization', 'Interpretability', 'Machine Learning']",[],"['Julia Moosbauer', 'Julia Herbinger', 'Giuseppe Casalicchio', 'Marius Lindauer', 'Bernd Bischl']","['Department of Statistics', 'Institut für Statistik', 'Ludwig-Maximilians-Universität München', 'Leibniz Universität Hannover', 'LMU']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27172,Transparency & Explainability,Learning Causal Semantic Representation for Out-of-Distribution Prediction,"Conventional supervised learning methods, especially deep ones, are found to be sensitive to out-of-distribution (OOD) examples, largely because the learned representation mixes the semantic factor with the variation factor due to their domain-specific correlation, while only the semantic factor causes the output. To address the problem, we propose a Causal Semantic Generative model (CSG) based on a causal reasoning so that the two factors are modeled separately, and develop methods for OOD prediction from a single training domain, which is common and challenging. The methods are based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction. Theoretically, we prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic-identification guarantees the boundedness of OOD generalization error and the success of adaptation. Empirical study shows improved OOD performance over prevailing baselines.","['Generative Model', 'Representation Learning', 'Domain Adaptation']",[],"['Chang Liu', 'Xinwei Sun', 'Jindong Wang', 'Haoyue Tang', 'Tao Li', 'Tao Qin', 'Wei Chen', 'Tie-Yan Liu']","['Microsoft', 'Fudan University', 'Microsoft Research', 'Meta AI', 'Peking University', 'Microsoft Research Asia', ' Chinese Academy of Sciences', 'Microsoft']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27117,Transparency & Explainability,On Locality of Local Explanation Models,"Shapley values provide model agnostic feature attributions for model outcome at a particular instance by simulating feature absence under a global population distribution. The use of a global population can lead to potentially misleading results when local model behaviour is of interest. Hence we consider the formulation of  neighbourhood reference distributions that improve the local interpretability of Shapley values. By doing so, we find that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, we observe that Neighbourhood Shapley values identify meaningful sparse  feature relevance attributions that provide insight into local model behaviour, complimenting conventional Shapley analysis. They also increase on-manifold explainability and robustness to the construction of adversarial classifiers.","['Robustness', 'Interpretability']",[],"['Lucile Ter-Minassian', 'Karla DiazOrdaz', 'Christopher C. Holmes']","['University of Oxford', 'LSHTM', 'University of Oxford']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27104,Transparency & Explainability,Neural Population Geometry Reveals the Role of Stochasticity in Robust Perception,"Adversarial examples are often cited by neuroscientists and machine learning researchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired components to visual neural networks as a way to improve their adversarial robustness. One surprisingly effective component for reducing adversarial vulnerability is response stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations influence the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. We find distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrate that competing geometric effects of stochasticity mediate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneficial to machine and biological computation.","['Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning', 'Neuroscience']",[],"['Joel Dapello', 'Jenelle Feather', 'Hang Le', 'Tiago Marques', 'David Daniel Cox', 'Josh Mcdermott', 'James J. DiCarlo', 'SueYeon Chung']","['Altos Labs', 'Flatiron Institute', 'Massachusetts Institute of Technology', 'Champalimaud Foundation', 'International Business Machines', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'New York University']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27019,Transparency & Explainability,The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations,"Feature importance (FI) estimates are a popular form of explanation, and they are commonly created and evaluated by computing the change in model confidence caused by removing certain input features at test time. For example, in the standard Sufficiency metric, only the top-k most important tokens are kept. In this paper, we study several under-explored dimensions of FI explanations, providing conceptual and empirical improvements for this form of explanation. First, we advance a new argument for why it can be problematic to remove features from an input when creating or evaluating explanations: the fact that these counterfactual inputs are out-of-distribution (OOD) to models implies that the resulting explanations are socially misaligned. The crux of the problem is that the model prior and random weight initialization influence the explanations (and explanation metrics) in unintended ways. To resolve this issue, we propose a simple alteration to the model training process, which results in more socially aligned explanations and metrics. Second, we compare among five approaches for removing features from model inputs. We find that some methods produce more OOD counterfactuals than others, and we make recommendations for selecting a feature-replacement function. Finally, we introduce four search-based methods for identifying FI explanations and compare them to strong baselines, including LIME, Anchors, and Integrated Gradients. Through experiments with six diverse text classification datasets, we find that the only method that consistently outperforms random search is a Parallel Local Search (PLS) that we introduce. Improvements over the second best method are as large as 5.4 points for Sufficiency and 17 points for Comprehensiveness.","['Interpretability', 'Machine Learning']",[],"['Peter Hase', 'Harry Xie', 'Mohit Bansal']","['University of North Carolina, Chapel Hill', 'Department of Computer Science, University of North Carolina, Chapel Hill', 'University of North Carolina at Chapel Hill']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26995,Transparency & Explainability,The Utility of Explainable AI in Ad Hoc Human-Machine Teaming,"Recent advances in machine learning have led to growing interest in Explainable AI (xAI) to enable humans to gain insight into the decision-making of machine learning models. Despite this recent interest, the utility of xAI techniques has not yet been characterized in human-machine teaming. Importantly, xAI offers the promise of enhancing team situational awareness (SA) and shared mental model development, which are the key characteristics of effective human-machine teams. Rapidly developing such mental models is especially critical in ad hoc human-machine teaming, where agents do not have a priori knowledge of others' decision-making strategies. In this paper, we present two novel human-subject experiments quantifying the benefits of deploying xAI techniques within a human-machine teaming scenario. First, we show that xAI techniques can support SA ($p<0.05)$. Second, we examine how different SA levels induced via a collaborative AI policy abstraction affect ad hoc human-machine teaming performance. Importantly, we find that the benefits of xAI are not universal, as there is a strong dependence on the composition of the human-machine team. Novices benefit from xAI providing increased SA ($p<0.05$) but are susceptible to cognitive overhead ($p<0.05$). On the other hand, expert performance degrades with the addition of xAI-based support ($p<0.05$), indicating that the cost of paying attention to the xAI outweighs the benefits obtained from being provided additional information to enhance SA. Our results demonstrate that researchers must deliberately design and deploy the right xAI techniques in the right scenario by carefully considering human-machine team composition and how the xAI method augments SA.","['Interpretability', 'Machine Learning']",[],"['Rohan R Paleja', 'Muyleng Ghuy', 'Nadun Ranawaka Arachchige', 'Reed Jensen', 'Matthew Gombolay']","['Institute of Technology', 'Institute of Technology', 'Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'College of Computing,  Institute of Technology']","['Georgia', 'Georgia', 'Georgia', None, 'Georgia']"
https://nips.cc/virtual/2021/poster/26936,Transparency & Explainability,Sparsely Changing Latent States for Prediction and Planning in Partially Observable Domains,"A common approach to prediction and planning in partially observable domains is to use recurrent neural networks (RNNs), which ideally develop and maintain a latent memory about hidden, task-relevant factors. We hypothesize that many of these hidden factors in the physical world are constant over time, changing only sparsely. To study this hypothesis, we propose Gated $L_0$ Regularized Dynamics (GateL0RD), a novel recurrent architecture that incorporates the inductive bias to maintain stable, sparsely changing latent states.  The bias is implemented by means of a novel internal gating function and a penalty on the $L_0$ norm of latent state changes. We demonstrate that GateL0RD can compete with or outperform state-of-the-art RNNs in a variety of partially observable prediction and control tasks. GateL0RD tends to encode the underlying generative factors of the environment, ignores spurious temporal dependencies, and generalizes better, improving sampling efficiency and overall performance in model-based planning and reinforcement learning tasks. Moreover, we show that the developing latent states can be easily interpreted, which is a step towards better explainability in RNNs.","['Reinforcement Learning and Planning', 'Deep Learning', 'Interpretability']",[],"['Christian Gumbsch', 'Martin V. Butz', 'Georg Martius']","['University of Tuebingen', 'University of Tuebingen', 'Eberhard-Karls-Universität Tübingen']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28176,Transparency & Explainability,Reinforcement Learning Enhanced Explainer for Graph Neural Networks,"Graph neural networks (GNNs) have recently emerged as revolutionary technologies for machine learning tasks on graphs. In GNNs, the graph structure is generally incorporated with node representation via the message passing scheme, making the explanation much more challenging. Given a trained GNN model, a GNN explainer aims to identify a most influential subgraph to interpret the prediction of an instance (e.g., a node or a graph), which is essentially a combinatorial optimization problem over graph. The existing works solve this problem by continuous relaxation or search-based heuristics. But they suffer from key issues such as violation of message passing and hand-crafted heuristics, leading to inferior interpretability. To address these issues, we propose a RL-enhanced GNN explainer, RG-Explainer, which consists of three main components: starting point selection, iterative graph generation and stopping criteria learning. RG-Explainer could construct a connected explanatory subgraph by sequentially adding nodes from the boundary of the current generated graph, which is consistent with the message passing scheme. Further, we design an effective seed locator to select the starting point, and learn stopping criteria to generate superior explanations. Extensive experiments on both synthetic and real datasets show that RG-Explainer outperforms state-of-the-art GNN explainers. Moreover, RG-Explainer can be applied in the inductive setting, demonstrating its better generalization ability.","['Reinforcement Learning and Planning', 'Optimization', 'Machine Learning', 'Graph Learning', 'Deep Learning', 'Interpretability']",[],"['Caihua Shan', 'Yifei Shen', 'Yao Zhang', 'Xiang Li', 'Dongsheng Li']","['Microsoft', 'Microsoft Research Asia', 'eBay Inc.', 'East  Normal University', 'Microsoft Research Asia']","[None, None, None, 'China', None]"
https://nips.cc/virtual/2021/poster/26873,Transparency & Explainability,IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers,"The self-attention-based model, transformer, is recently becoming the leading backbone in the field of computer vision. In spite of the impressive success made by transformers in a variety of vision tasks, it still suffers from heavy computation and intensive memory costs. To address this limitation, this paper presents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$). We start by observing a large amount of redundant computation, mainly spent on uncorrelated input patches, and then introduce an interpretable module to dynamically and gracefully drop these redundant patches. This novel framework is then extended to a hierarchical structure, where uncorrelated tokens at different stages are gradually removed, resulting in a considerable shrinkage of computational cost. We include extensive experiments on both image and video tasks, where our method could deliver up to 1.4x speed-up for state-of-the-art models like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy. More importantly, contrary to other acceleration approaches, our method is inherently interpretable with substantial visual evidence, making vision transformer closer to a more human-understandable architecture while being lighter. We demonstrate that the interpretability that naturally emerged in our framework can outperform the raw attention learned by the original visual transformer, as well as those generated by off-the-shelf interpretation methods, with both qualitative and quantitative results. Project Page: http://people.csail.mit.edu/bpan/ia-red/.","['Transformers', 'Deep Learning', 'Vision', 'Interpretability']",[],"['Bowen Pan', 'Rameswar Panda', 'Yifan Jiang', 'Zhangyang Wang', 'Rogerio Feris']","['Massachusetts Institute of Technology', 'MIT-IBM Watson AI Lab', 'University of Texas, Austin', 'University of Texas at Austin', 'International Business Machines']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26687,Transparency & Explainability,Supercharging Imbalanced Data Learning With Energy-based Contrastive Representation Transfer,"Dealing with severe class imbalance poses a major challenge for many real-world applications, especially when the accurate classification and generalization of minority classes are of primary interest. In computer vision and NLP, learning from datasets with long-tail behavior is a recurring theme, especially for naturally occurring labels. Existing solutions mostly appeal to sampling or weighting adjustments to alleviate the extreme imbalance, or impose inductive bias to prioritize generalizable associations. Here we take a novel perspective to promote sample efficiency and model generalization based on the invariance principles of causality. Our contribution posits a meta-distributional scenario, where the causal generating mechanism for label-conditional features is invariant across different labels. Such causal assumption enables efficient knowledge transfer from the dominant classes to their under-represented counterparts, even if their feature distributions show apparent disparities. This allows us to leverage a causal data augmentation procedure to enlarge the representation of minority classes. Our development is orthogonal to the existing imbalanced data learning techniques thus can be seamlessly integrated. The proposed approach is validated on an extensive set of synthetic and real-world tasks against state-of-the-art solutions.","['Vision', 'Machine Learning', 'Contrastive Learning', 'Causality', 'Representation Learning']",[],"['Junya Chen', 'Zidi Xiu', 'Benjamin Goldstein', 'Ricardo Henao', 'Lawrence Carin', 'Chenyang Tao']","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'Duke University', 'Duke University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26633,Transparency & Explainability,Explicable Reward Design for Reinforcement Learning Agents,"We study the design of explicable reward functions for a reinforcement learning agent while guaranteeing that an optimal policy induced by the function belongs to a set of target policies. By being explicable, we seek to capture two properties: (a) informativeness so that the rewards speed up the agent's convergence, and (b) sparseness as a proxy for ease of interpretability of the rewards. The key challenge is that higher informativeness typically requires dense rewards for many learning tasks, and existing techniques do not allow one to balance these two properties appropriately. In this paper, we investigate the problem from the perspective of discrete optimization and introduce a novel framework, ExpRD, to design explicable reward functions. ExpRD builds upon an informativeness criterion that captures the (sub-)optimality of target policies at different time horizons in terms of actions taken from any given starting state. We provide a  mathematical analysis of ExpRD, and show its connections to existing reward design techniques, including potential-based reward shaping. Experimental results on two navigation tasks demonstrate the effectiveness of ExpRD in designing explicable reward functions.","['Reinforcement Learning and Planning', 'Optimization', 'Interpretability']",[],"['Rati Devidze', 'Goran Radanovic', 'Parameswaran Kamalaruban', 'Adish Singla']","['MPI-SWS', 'MPI-SWS', 'Alan Turing Institute', 'MPI-SWS']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26595,Transparency & Explainability,Differentiable Synthesis of Program Architectures,"Differentiable programs have recently attracted much interest due to their interpretability, compositionality, and their efficiency to leverage differentiable training. However, synthesizing differentiable programs requires optimizing over a combinatorial, rapidly exploded space of program architectures. Despite the development of effective pruning heuristics, previous works essentially enumerate the discrete search space of program architectures, which is inefficient. We propose to encode program architecture search as learning the probability distribution over all possible program derivations induced by a context-free grammar. This allows the search algorithm to efficiently prune away unlikely program derivations to synthesize optimal program architectures. To this end, an efficient gradient-descent based method is developed to conduct program architecture search in a continuous relaxation of the discrete space of grammar rules. Experiment results on four sequence classification tasks demonstrate that our program synthesizer excels in discovering program architectures that lead to differentiable programs with higher F1 scores, while being more efficient than state-of-the-art program synthesis methods.","['Optimization', 'Interpretability', 'Machine Learning']",[],"['Guofeng Cui', 'He Zhu']","['Rutgers University', 'Rutgers University']","[None, None]"
https://nips.cc/virtual/2021/poster/26541,Transparency & Explainability,Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization,"The invariance principle from causality is at the heart of notable approaches such as invariant risk minimization (IRM) that seek to address out-of-distribution (OOD) generalization failures. Despite the promising theory, invariance principle-based approaches fail in common classification tasks, where invariant (causal) features capture all the information about the label.  Are these failures due to the methods failing to capture the invariance? Or is the invariance principle itself insufficient? To answer these questions, we revisit the fundamental assumptions in linear regression tasks, where invariance-based approaches were shown to provably generalize OOD. In contrast to the linear regression tasks, we show that for linear classification tasks we need much stronger restrictions on the distribution shifts, or otherwise OOD generalization is impossible.  Furthermore, even with appropriate restrictions on distribution shifts in place, we show that the invariance principle alone is insufficient. We prove that a form of the information bottleneck constraint along with invariance helps address the key failures when invariant features capture all the information about the label and also retains the existing success when they do not. We propose an approach that incorporates both of these principles and demonstrate its effectiveness in several experiments.","['Theory', 'Causality', 'Machine Learning']",[],"['Kartik Ahuja', 'Ethan Caballero', 'Dinghuai Zhang', 'Jean-Christophe Gagnon-Audet', 'Yoshua Bengio', 'Ioannis Mitliagkas', 'Irina Rish']","['FAIR (Meta)', 'Mila', 'Mila, University of Montreal', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'University of Montreal', 'Université de Montréal', 'University of Montreal']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26496,Transparency & Explainability,Compositional Transformers for Scene Generation,"We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reflect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being refined, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a flat and monolithic latent space towards a transparent design that encourages efficiency, controllability and interpretability. We demonstrate GANformer2's strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model's disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects' depths and dependencies, and up to the final high-resolution depiction of vibrant and intricate real-world scenes. See https://github.com/dorarad/gansformer for model implementation.","['Transformers', 'Generative Model', 'Interpretability']",[],"['Drew Arad Hudson', 'C. Lawrence Zitnick']","['Google DeepMind', 'Meta']","[None, None]"
https://nips.cc/virtual/2021/poster/26410,Transparency & Explainability,Improving Deep Learning Interpretability by Saliency Guided Training,"Saliency methods have been widely used to highlight important input features in model predictions. Most existing methods use backpropagation on a modified gradient function to generate saliency maps. Thus, noisy gradients can result in unfaithful feature attributions. In this paper, we tackle this issue and introduce a {\it saliency guided training} procedure for neural networks to reduce noisy gradients used in predictions while retaining the predictive performance of the model. Our saliency guided training procedure iteratively masks features with small and potentially noisy gradients while maximizing the similarity of model outputs for both masked and unmasked inputs. We apply the saliency guided training procedure to various synthetic and real data sets from computer vision, natural language processing, and time series across diverse neural architectures, including Recurrent Neural Networks, Convolutional Networks, and Transformers. Through qualitative and quantitative evaluations, we show that saliency guided training procedure significantly improves model interpretability across various domains while preserving its predictive performance.","['Transformers', 'Vision', 'Language', 'Deep Learning', 'Interpretability']",[],"['Aya Abdelsalam Ismail', 'Hector Corrada Bravo', 'Soheil Feizi']","['Genentech', 'Genentech', 'University of Maryland, College Park']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26401,Transparency & Explainability,Multi-Agent Reinforcement Learning for Active Voltage Control on Power Distribution Networks,"This paper presents a problem in power networks that creates an exciting and yet challenging real-world scenario for application of multi-agent reinforcement learning (MARL). The emerging trend of decarbonisation is placing excessive stress on power distribution networks. Active voltage control is seen as a promising solution to relieve power congestion and improve voltage quality without extra hardware investment, taking advantage of the controllable apparatuses in the network, such as roof-top photovoltaics (PVs) and static var compensators (SVCs). These controllable apparatuses appear in a vast number and are distributed in a wide geographic area, making MARL a natural candidate. This paper formulates the active voltage control problem in the framework of Dec-POMDP and establishes an open-source environment. It aims to bridge the gap between the power community and the MARL community and be a drive force towards real-world applications of MARL algorithms. Finally, we analyse the special characteristics of the active voltage control problems that cause challenges (e.g. interpretability) for state-of-the-art MARL approaches, and summarise the potential directions.","['Reinforcement Learning and Planning', 'Graph Learning']",[],"['Jianhong Wang', 'Wangkun Xu', 'Wenbin Song']","['University of Manchester', 'Imperial College London', 'Shanghaitech University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26438,Transparency & Explainability,A Causal Lens for Controllable Text Generation,"Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of  rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.","['Causality', 'Language']",[],"['Zhiting Hu', 'Li Erran Li']","['University of California, San Diego', 'Amazon']","[None, None]"
https://nips.cc/virtual/2021/poster/25949,Transparency & Explainability,Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi,"Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI teammates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human's perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We find that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance.","['Reinforcement Learning and Planning', 'Deep Learning', 'Interpretability']",[],"['Ho Chit Siu', 'Jaime Daniel Pena', 'Edenna Chen', 'Yutai Zhou', 'Victor Lopez', 'Kyle Palko', 'Kimberlee Chestnut Chang', 'Ross Emerson Allen']","['MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25922,Transparency & Explainability,Contrastively Disentangled Sequential  Variational Autoencoder,"Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics.","['Self-Supervised Learning', 'Contrastive Learning', 'Generative Model', 'Interpretability', 'Representation Learning']",[],"['Junwen Bai', 'Weiran Wang', 'Carla P Gomes']","['Google', 'Google', 'Cornell University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26500,Transparency & Explainability,"Independent mechanism analysis, a new concept?","Independent component analysis provides a principled framework for unsupervised representation learning, with solid theory on the identifiability of the latent code that generated the data, given only observations of mixtures thereof. Unfortunately, when the mixing is nonlinear, the model is provably nonidentifiable, since statistical independence alone does not sufficiently constrain the problem. Identifiability can be recovered in settings where additional, typically observed variables are included in the generative process. We investigate an alternative path and consider instead including assumptions reflecting the principle of independent causal mechanisms exploited in the field of causality. Specifically, our approach is motivated by thinking of each source as independently influencing the mixing process. This gives rise to a framework which we term independent mechanism analysis. We provide theoretical and empirical evidence that our approach circumvents a number of nonidentifiability issues arising in nonlinear blind source separation.","['Theory', 'Causality', 'Representation Learning']",[],"['Luigi Gresele', 'Julius Von Kügelgen', 'Vincent Stimper', 'Michel Besserve']","['Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'MPI for Intelligent Systems']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27096,Transparency & Explainability,Auditing Black-Box Prediction Models for Data Minimization Compliance,"In this paper, we focus on auditing black-box prediction models for compliance with the GDPR’s data minimization principle. This principle restricts prediction models to use the minimal information that is necessary for performing the task at hand. Given the challenge of the black-box setting, our key idea is to check if each of the prediction model’s input features is individually necessary by assigning it some constant value (i.e., applying a simple imputation) across all prediction instances, and measuring the extent to which the model outcomes would change. We introduce a metric for data minimization that is based on model instability under simple imputations. We extend the applicability of this metric from a finite sample model to a distributional setting by introducing a probabilistic data minimization guarantee, which we derive using a Bayesian approach. Furthermore, we address the auditing problem under a constraint on the number of queries to the prediction system. We formulate the problem of allocating a budget of system queries to feasible simple imputations (for investigating model instability) as a multi-armed bandit framework with probabilistic success metrics. We define two bandit problems for providing a probabilistic data minimization guarantee at a given confidence level: a decision problem given a data minimization level, and a measurement problem given a fixed query budget. We design efficient algorithms for these auditing problems using novel exploration strategies that expand classical bandit strategies. Our experiments with real-world prediction systems show that our auditing algorithms significantly outperform simpler benchmarks in both measurement and decision problems.","['Reinforcement Learning and Planning', 'Bandits', 'Privacy']",[],"['Bashir Rastegarpanah', 'Krishna P. Gummadi', 'Mark Crovella']","['Boston University', 'MPI-SWS', 'Boston University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28445,Transparency & Explainability,Learning Tree Interpretation from Object Representation for Deep Reinforcement Learning,"Interpreting Deep Reinforcement Learning (DRL) models is important to enhance trust and comply with transparency regulations. Existing methods typically explain a DRL model by visualizing the importance of low-level input features with super-pixels, attentions, or saliency maps. Our approach provides an interpretation based on high-level latent object features derived from a disentangled representation. We propose a Represent And Mimic (RAMi) framework for training 1) an identifiable latent representation to capture the independent factors of variation for the objects and 2) a mimic tree that extracts the causal impact of the latent features on DRL action values. To jointly optimize both the fidelity and the simplicity of a mimic tree, we derive a novel Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. Based on this objective, we describe a Monte Carlo Regression Tree Search (MCRTS) algorithm that explores different splits to find the IB-optimal mimic tree. Experiments show that our mimic tree achieves strong approximation performance with significantly fewer nodes than baseline models. We demonstrate the interpretability of our mimic tree by showing latent traversals, decision rules, causal impacts, and human evaluation results.","['Reinforcement Learning and Planning', 'Interpretability']",[],"['Guiliang Liu', 'Oliver Schulte', 'Pascal Poupart']","['The Chinese University of , Shenzhen', 'Simon Fraser University', 'University of Waterloo']","['Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/28627,Transparency & Explainability,Multilingual Pre-training with Universal Dependency Learning,"The pre-trained language model (PrLM) demonstrates domination in downstream natural language processing tasks, in which multilingual PrLM takes advantage of language universality to alleviate the issue of limited resources for low-resource languages. Despite its successes, the performance of multilingual PrLM is still unsatisfactory, when multilingual PrLMs only focus on plain text and ignore obvious universal linguistic structure clues. Existing PrLMs have shown that monolingual linguistic structure knowledge may bring about better performance. Thus we propose a novel multilingual PrLM that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in our model, which brings unprecedented PrLM interpretability and convenience in downstream task use. Our model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets, demonstrating the effectiveness and stronger cross-lingual modeling capabilities of our approach.","['Interpretability', 'Language']",[],"['Kailai Sun', 'Zuchao Li', 'hai zhao']","['Shanghai Jiao Tong University, Shanghai Jiao Tong University', 'Wuhan University', 'Shanghai Jiao Tong University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27775,Transparency & Explainability,How Well do Feature Visualizations Support Causal Understanding of CNN Activations?,"A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated - an advantage over other alternatives like strongly activating dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog's head) changes a unit's activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit's activation. Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. (2017) indeed help humans on this task ($68 \pm 4$% accuracy; baseline performance without any visualizations is $60 \pm 3$%). However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance ($66\pm3$% to $67 \pm3$% accuracy). Taken together, we propose an objective psychophysical task to quantify the benefit of unit-level interpretability methods for humans, and find no evidence that a widely-used feature visualization method provides humans with better ""causal understanding"" of unit activations than simple alternative visualizations.",['Interpretability'],[],"['Roland Simon Zimmermann', 'Judy Borowski', 'Robert Geirhos', 'Matthias Bethge', 'Thomas S. A. Wallis', 'Wieland Brendel']","['Eberhard-Karls-Universität Tübingen', 'University of Tuebingen', 'Google DeepMind', 'University of Tuebingen', 'TU Darmstadt', 'ELLIS Institute Tübingen']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28825,Transparency & Explainability,Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models,"While probabilistic models are an important tool for studying causality, doing so suffers from the intractability of inference. As a step towards tractable causal models, we consider the problem of learning interventional distributions using sum-product networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. Providing an arbitrarily intervened causal graph as input, effectively subsuming Pearl's do-operator, the gate function predicts the parameters of the SPN. The resulting interventional SPNs are motivated and illustrated by a structural causal model themed around personal health. Our empirical evaluation against competing methods from both generative and causal modelling demonstrates that interventional SPNs indeed are both expressive and causally adequate.","['Deep Learning', 'Graph Learning', 'Generative Model', 'Causality']",[],"['Matej Zecevic', 'Devendra Singh Dhami', 'Athresh Karanam', 'Sriraam Natarajan', 'Kristian Kersting']","['TU Darmstadt', 'Eindhoven University of Technology', 'University of Texas, Dallas', 'University of Texas at Dallas', 'German Research Center for AI']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27533,Transparency & Explainability,Self-Interpretable Model with Transformation Equivariant Interpretation,"With the proliferation of machine learning applications in the real world, the demand for explaining machine learning predictions continues to grow especially in high-stakes fields. Recent studies have found that interpretation methods can be sensitive and unreliable, where the interpretations can be disturbed by perturbations or transformations of input data. To address this issue, we propose to learn robust interpretation through transformation equivariant regularization in a self-interpretable model. The resulting model is capable of capturing valid interpretation that is equivariant to geometric transformations. Moreover, since our model is self-interpretable, it enables faithful interpretations that reflect the true predictive mechanism. Unlike existing self-interpretable models, which usually sacrifice expressive power for the sake of interpretation quality, our model preserves the high expressive capability comparable to the state-of-the-art deep learning models in complex tasks, while providing visualizable and faithful high-quality interpretation. We compare with various related methods and validate the interpretation quality and consistency of our model.","['Deep Learning', 'Vision', 'Machine Learning']",[],"['Yipei Wang', 'Xiaoqian Wang']","['Purdue University', 'Purdue University']","[None, None]"
https://nips.cc/virtual/2021/poster/26692,Transparency & Explainability,Deep Synoptic Monte-Carlo Planning in Reconnaissance Blind Chess,"This paper introduces deep synoptic Monte Carlo planning (DSMCP) for large imperfect information games. The algorithm constructs a belief state with an unweighted particle filter and plans via playouts that start at samples drawn from the belief state. The algorithm accounts for uncertainty by performing inference on ""synopses,"" a novel stochastic abstraction of information states. DSMCP is the basis of the program Penumbra, which won the official 2020 reconnaissance blind chess competition versus 33 other programs. This paper also evaluates algorithm variants that incorporate caution, paranoia, and a novel bandit algorithm. Furthermore, it audits the synopsis features used in Penumbra with per-bit saliency statistics.","['Reinforcement Learning and Planning', 'Deep Learning', 'Bandits']",[],['Gregory Clark'],['Google'],[None]
https://nips.cc/virtual/2021/poster/27050,Transparency & Explainability,Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling,"Integrating physics models within machine learning models holds considerable promise toward learning robust models with improved interpretability and abilities to extrapolate. In this work, we focus on the integration of incomplete physics models into deep generative models. In particular, we introduce an architecture of variational autoencoders (VAEs) in which a part of the latent space is grounded by physics. A key technical challenge is to strike a balance between the incomplete physics and trainable components such as neural networks for ensuring that the physics part is used in a meaningful manner. To this end, we propose a regularized learning method that controls the effect of the trainable components and preserves the semantics of the physics-based latent variables as intended. We not only demonstrate generative performance improvements over a set of synthetic and real-world datasets, but we also show that we learn robust models that can consistently extrapolate beyond the training distribution in a meaningful manner. Moreover, we show that we can control the generative process in an interpretable manner.","['Deep Learning', 'Generative Model', 'Interpretability', 'Machine Learning']",[],"['Naoya Takeishi', 'Alexandros Kalousis']","['The University of Tokyo', 'University Of Geneva,']","[None, 'Switzerland']"
https://nips.cc/virtual/2021/poster/28517,Transparency & Explainability,Towards Multi-Grained Explainability for Graph Neural Networks,"When a graph neural network (GNN) made a prediction, one raises question about explainability: “Which fraction of the input graph is most inﬂuential to the model’s decision?” Producing an answer requires understanding the model’s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the ﬂexibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and ﬁne-tuning idea to develop our explainer and generate multi-grained explanations. Speciﬁcally, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the ﬁne-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classiﬁcation over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.","['Deep Learning', 'Graph Learning', 'Interpretability']",[],"['Xiang Wang', 'Yingxin Wu', 'An Zhang', 'Xiangnan He', 'Tat-seng Chua']","['University of Science and Technology of', 'Computer Science Department, Stanford University', 'National University of', 'University of Science and Technology of', 'National University of']","['China', None, 'Singapore', 'China', 'Singapore']"
https://nips.cc/virtual/2021/poster/27877,Transparency & Explainability,Counterfactual Explanations Can Be Manipulated,"Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions.  As they are deployed in critical applications (e.g. law enforcement, financial lending), it becomes important to ensure that we clearly understand the vulnerabilties of these methods and find ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the first framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More specifically, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust.  Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations find much lower cost recourse under a slight perturbation.  We describe how these models can unfairly provide low-cost recourse for specific subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.",['Robustness'],[],"['Dylan Z Slack', 'Sophie Hilgard', 'Himabindu Lakkaraju', 'Sameer Singh']","['University of California, Irvine', 'Harvard University', 'Harvard University', 'University of California, Irvine']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27763,Transparency & Explainability,Nonparametric estimation of continuous DPPs with kernel methods,"Determinantal Point Process (DPPs) are statistical models for repulsive point patterns. Both sampling and inference are tractable for DPPs, a rare feature among models with negative dependence that explains their popularity in machine learning and spatial statistics. Parametric and nonparametric inference methods have been proposed in the finite case, i.e. when the point patterns live in a finite ground set. In the continuous case, only parametric methods have been investigated, while nonparametric maximum likelihood for DPPs -- an optimization problem over trace-class operators -- has remained an open question. In this paper, we show that a restricted version of this maximum likelihood (MLE) problem falls within the scope of a recent representer theorem for nonnegative functions in an RKHS. This leads to a finite-dimensional problem, with strong statistical ties to the original MLE. Moreover, we propose, analyze, and demonstrate a fixed point algorithm to solve this finite-dimensional problem. Finally, we also provide a controlled estimate of the correlation kernel of the DPP, thus providing more interpretability.","['Optimization', 'Interpretability', 'Machine Learning', 'Kernel Methods']",[],"['Michaël Fanuel', 'Rémi Bardenet']","['CNRS, Univ. de Lille', 'CNRS & Univ. Lille']","[None, None]"
https://nips.cc/virtual/2021/poster/27702,Transparency & Explainability,Topic Modeling Revisited: A Document Graph-based Neural Network Perspective,"Most topic modeling approaches are based on the bag-of-words assumption, where each word is required to be conditionally independent in the same document. As a result, both of the generative story and the topic formulation have totally ignored the semantic dependency among words, which is important for improving the semantic comprehension and model interpretability. To this end, in this paper, we revisit the task of topic modeling by transforming each document into a directed graph with word dependency as edges between word nodes, and develop a novel approach, namely Graph Neural Topic Model (GNTM). Specifically, in GNTM, a well-defined probabilistic generative story is designed to model both the graph structure and word sets with multinomial distributions on the vocabulary and word dependency edge set as the topics. Meanwhile, a Neural Variational Inference (NVI) approach is proposed to learn our model with graph neural networks to encode the document graphs. Besides, we theoretically demonstrate that Latent Dirichlet Allocation (LDA) can be derived from GNTM as a special case with similar objective functions. Finally, extensive experiments on four benchmark datasets have clearly demonstrated the effectiveness and interpretability of GNTM compared with state-of-the-art baselines.","['Deep Learning', 'Graph Learning', 'Interpretability', 'Generative Model']",[],"['Dazhong Shen', 'Chuan Qin', 'Chao Wang', 'Zheng Dong', 'Hengshu Zhu', 'Hui Xiong']","['Shanghai Artificial Intelligence Laboratory', 'BOSS Zhipin', 'HKUST Fok Ying Tung Research Institute, The  University of Science and Technology\xa0(Guangzhou)', 'University of British Columbia', 'BOSS Zhipin', 'University of Science and Technology']","[None, None, 'Hong Kong', None, None, 'Hong Kong']"
https://nips.cc/virtual/2021/poster/27625,Fairness & Bias,A Gaussian Process-Bayesian Bernoulli Mixture Model for Multi-Label Active Learning,"Multi-label classification (MLC) allows complex dependencies among labels, making it more suitable to model many real-world problems. However, data annotation for training MLC models becomes much more labor-intensive due to the correlated (hence non-exclusive) labels and a potential large and sparse label space.  We propose to conduct multi-label active learning (ML-AL) through a novel integrated Gaussian Process-Bayesian Bernoulli Mixture model (GP-B$^2$M) to accurately quantify a data sample's overall contribution to a correlated label space and choose the most informative samples for cost-effective annotation. In particular, the B$^2$M encodes label correlations using a Bayesian Bernoulli mixture of label clusters, where each mixture component corresponds to a global pattern of label correlations. To tackle highly sparse labels under AL, the B$^2$M is further integrated with a predictive GP to connect data features as an effective inductive bias and achieve a feature-component-label mapping. The GP predicts coefficients of mixture components that help to recover the final set of labels of a data sample. A novel auxiliary variable based variational inference algorithm is developed to tackle the non-conjugacy introduced along with the mapping process for efficient end-to-end posterior inference.  The model also outputs a predictive  distribution that provides both the label prediction and their correlations in the form of a label covariance matrix. A principled sampling function is designed accordingly to naturally capture both the feature uncertainty (through GP) and label covariance (through B$^2$M) for effective data sampling. Experiments on real-world multi-label datasets demonstrate the state-of-the-art AL performance of the proposed GP-B$^2$M model.","['Active Learning', 'Generative Model', 'Machine Learning', 'Kernel Methods']",[],"['Weishi Shi', 'Dayou Yu', 'Qi Yu']","['University of North Texas', 'Rochester Institute of Technology', 'Rochester Institute of Technology']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27457,Fairness & Bias,"On Plasticity, Invariance, and Mutually Frozen Weights in Sequential Task Learning","Plastic neural networks have the ability to adapt to new tasks. However, in a continual learning setting, the configuration of parameters learned in previous tasks can severely reduce the adaptability to future tasks. In particular, we show that, when using weight decay, weights in successive layers of a deep network may become ""mutually frozen"". This has a double effect: on the one hand, it makes the network updates more invariant to nuisance factors, providing a useful bias for future tasks. On the other hand, it can prevent the network from learning new tasks that require significantly different features. In this context, we find that the local input sensitivity of a deep model is correlated with its ability to adapt, thus leading to an intriguing trade-off between adaptability and invariance when training a deep model more than once. We then show that a simple intervention that ""resets"" the mutually frozen connections can improve transfer learning on a variety of visual classification tasks. The efficacy of ""resetting"" itself depends on the size of the target dataset and the difference of the pre-training and target domains, allowing us to achieve state-of-the-art results on some datasets.","['Transfer Learning', 'Deep Learning', 'Machine Learning', 'Continual Learning']",[],"['Julian G. Zilly', 'Alessandro Achille']","['Swiss Federal Institute of Technology', 'California Institute of Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/27208,Fairness & Bias,Container: Context Aggregation Networks,"Convolutional neural networks (CNNs) are ubiquitous in computer vision, with a myriad of effective and efficient variations. Recently, Transformers -- originally introduced in natural language processing -- have been increasingly adopted in computer vision. While early adopters continued to employ CNN backbones, the latest networks are end-to-end CNN-free Transformer solutions. A recent surprising finding now shows that a simple MLP based solution without any traditional convolutional or Transformer components can produce effective visual representations. While CNNs, Transformers and MLP-Mixers may be considered as completely disparate architectures, we provide a unified view showing that they are in fact special cases of a more general method to aggregate spatial context in a neural network stack. We present the \model (CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head context aggregation that can exploit long-range interactions \emph{a la} Transformers while still exploiting the inductive bias of the local convolution operation leading to faster convergence speeds, often seen in CNNs. Our \model architecture achieves 82.7 \% Top-1 accuracy on ImageNet using 22M parameters, +2.8 improvement compared with DeiT-Small, and can converge to 79.9 \% Top-1 accuracy in just 200 epochs. In contrast to Transformer-based methods that do not scale well to downstream tasks that rely on larger input image resolutions, our efficient network, named \modellight, can be employed in object detection and instance segmentation networks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50 backbone with a comparable compute and parameter size. Our method also achieves promising results on self-supervised learning compared to DeiT on the DINO framework. Code is released at https://github.com/allenai/container.","['Transformers', 'Vision', 'Self-Supervised Learning', 'Language', 'Deep Learning']",[],"['Gao Peng', 'Jiasen Lu', 'Hongsheng Li', 'Roozbeh Mottaghi', 'Aniruddha Kembhavi']","['shanghai ai lab ', 'Allen Institute for Artificial Intelligence', 'The Chinese University of', 'Meta', 'Allen Institute for Artificial Intelligence']","[None, None, 'Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/26492,Fairness & Bias,On Model Calibration for Long-Tailed Object Detection and Instance Segmentation,"Vanilla models for object detection and instance segmentation suffer from the heavy bias toward detecting frequent objects in the long-tailed setting. Existing methods address this issue mostly during training, e.g., by re-sampling or re-weighting. In this paper, we investigate a largely overlooked approach --- post-processing calibration of confidence scores. We propose NorCal, Normalized Calibration for long-tailed object detection and instance segmentation, a simple and straightforward recipe that reweighs the predicted scores of each class by its training sample size. We show that separately handling the background class and normalizing the scores over classes for each proposal are keys to achieving superior performance. On the LVIS dataset, NorCal can effectively improve nearly all the baseline models not only on rare classes but also on common and frequent classes.  Finally, we conduct extensive analysis and ablation studies to offer insights into various modeling choices and mechanisms of our approach. Our code is publicly available at https://github.com/tydpan/NorCal.","['Vision', 'Machine Learning']",[],"['Tai-Yu Pan', 'Cheng Zhang', 'YANDONG LI', 'Soravit Changpinyo', 'Boqing Gong', 'Wei-Lun Chao']","['Ohio State University', 'Carnegie Mellon University', 'Google Research', 'Google Research', 'University of Central Florida', 'Ohio State University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28333,Fairness & Bias,Towards robust vision by multi-task learning on monkey visual cortex,"Deep neural networks set the state-of-the-art across many tasks in computer vision, but their generalization ability to simple image distortions is surprisingly fragile. In contrast, the mammalian visual system is robust to a wide range of perturbations. Recent work suggests that this generalization ability can be explained by useful inductive biases encoded in the representations of visual stimuli throughout the visual cortex. Here, we successfully leveraged these inductive biases with a multi-task learning approach: we jointly trained a deep network to perform image classification and to predict neural activity in macaque primary visual cortex (V1) in response to the same natural stimuli. We measured the out-of-distribution generalization abilities of our resulting network by testing its robustness to common image distortions. We found that co-training on monkey V1 data indeed leads to increased robustness despite the absence of those distortions during training. Additionally, we showed that our network's robustness is often very close to that of an Oracle network where parts of the architecture are directly trained on noisy images. Our results also demonstrated that the network's representations become more brain-like as their robustness improves. Using a novel constrained reconstruction analysis, we investigated what makes our brain-regularized network more robust. We found that our monkey co-trained network is more sensitive to content than noise when compared to a Baseline network that we trained for image classification alone. Using DeepGaze-predicted saliency maps for ImageNet images, we found that the monkey co-trained network tends to be more sensitive to salient regions in a scene, reminiscent of existing theories on the role of V1 in the detection of object borders and bottom-up saliency. Overall, our work expands the promising research avenue of transferring inductive biases from biological to artificial neural networks on the representational level, and provides a novel analysis of the effects of our transfer.","['Vision', 'Robustness', 'Machine Learning', 'Theory', 'Deep Learning', 'Neuroscience']",[],"['Shahd Safarani', 'Arne Nix', 'Konstantin Friedrich Willeke', 'Santiago A Cadena', 'Kelli Restivo', 'George Denfield', 'Andreas S. Tolias', 'Fabian H. Sinz']","['University of Tuebingen', 'University of Tuebingen', 'University of Tuebingen', 'University of Tuebingen', 'Baylor College of Medicine', 'Baylor College of Medicine', 'Baylor College of Medicine', 'University Göttingen']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28689,Fairness & Bias,Analysis of one-hidden-layer neural networks via the resolvent method,"In this work, we investigate the asymptotic spectral density of the random feature matrix $M = Y Y^*$ with $Y = f(WX)$ generated by a single-hidden-layer neural network, where $W$ and $X$ are random rectangular matrices with i.i.d. centred entries and $f$ is a non-linear smooth function which is applied entry-wise. We prove that the Stieltjes transform of the limiting spectral distribution approximately satisfies a quartic self-consistent equation, which is exactly the equation obtained by [Pennington, Worah 2017] and [Benigni, Péché 2019] with the moment method. We extend the previous results to the case of additive bias $Y=f(WX+B)$ with $B$ being an independent rank-one Gaussian random matrix, closer modelling the neural network infrastructures encountered in practice. Our key finding is that in the case of additive bias it is impossible to choose an activation function preserving the layer-to-layer singular value distribution, in sharp contrast to the bias-free case where a simple integral constraint is sufficient to achieve isospectrality. To obtain the asymptotics for the empirical spectral density we follow the resolvent method from random matrix theory via the cumulant expansion. We find that this approach is more robust and less combinatorial than the moment method and expect that it will apply also for models where the combinatorics of the former become intractable. The resolvent method has been widely employed, but compared to previous works, it is applied here to non-linear random matrices.","['Theory', 'Deep Learning']",[],"['Vanessa Piccolo', 'Dominik Schröder']","['Ecole Normale Supérieure de Lyon', 'Swiss Federal Institute of Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/26975,Fairness & Bias,Marginalised Gaussian Processes with Nested Sampling,"Gaussian Process models are a rich distribution over functions with inductive biases controlled by a kernel function. Learning occurs through optimisation of the kernel hyperparameters using the marginal likelihood as the objective. This work proposes nested sampling as a means of marginalising kernel hyperparameters,  because it is a technique that is well-suited to exploring complex, multi-modal distributions. We benchmark against Hamiltonian Monte Carlo on time-series and two-dimensional regression tasks, finding that a principled approach to quantifying hyperparameter uncertainty substantially improves the quality of prediction intervals.",['Kernel Methods'],[],"['Fergus Simpson', 'Vidhi Lalchand', 'Carl Edward Rasmussen']","['Secondmind', 'University of Cambridge', 'University of Cambridge']","[None, None, None]"
https://nips.cc/virtual/2021/poster/25996,Fairness & Bias,Grounding Spatio-Temporal Language with Transformers,"Language is an interface to the outside world. In order for embodied agents to use it, language must be grounded in other, sensorimotor modalities. While there is an extended literature studying how machines can learn grounded language, the topic of how to learn spatio-temporal linguistic concepts is still largely uncharted. To make progress in this direction, we here introduce a novel spatio-temporal language grounding task where the goal is to learn the meaning of spatio-temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, we train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. We test models on two classes of generalization: 1) generalization to new sentences, 2) generalization to grammar primitives. We observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance. We then discuss how this opens new perspectives for language-guided autonomous embodied agents.",['Transformers'],[],"['Tristan Karch', 'Laetitia Teodorescu', 'Katja Hofmann', 'Clément Moulin-Frier', 'Pierre-Yves Oudeyer']","['INRIA', 'INRIA', 'Microsoft', 'Inria', 'Inria']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28852,Fairness & Bias,Efficient constrained sampling via the mirror-Langevin algorithm,"We propose a new discretization of the mirror-Langevin diffusion and give a crisp proof of its convergence. Our analysis uses relative convexity/smoothness and self-concordance, ideas which originated in convex optimization, together with a new result in optimal transport that generalizes the displacement convexity of the entropy. Unlike prior works, our result both (1) requires much weaker assumptions on the mirror map and the target distribution, and (2) has vanishing bias as the step size tends to zero. In particular, for the task of sampling from a log-concave distribution supported on a compact set, our theoretical results are significantly better than the existing guarantees.","['Optimization', 'Generative Model', 'Optimal Transport']",[],"['Kwangjun Ahn', 'Sinho Chewi']","['Massachusetts Institute of Technology', 'Institue for Advanced Study, Princeton']","[None, None]"
https://nips.cc/virtual/2021/poster/26733,Fairness & Bias,Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Making by Reinforcement Learning,"In the predict-then-optimize framework, the objective is to train a predictive model, mapping from environment features to parameters of an optimization problem, which maximizes decision quality when the optimization is subsequently solved. Recent work on decision-focused learning shows that embedding the optimization problem in the training pipeline can improve decision quality and help generalize better to unseen tasks compared to relying on an intermediate loss function for evaluating prediction quality. We study the predict-then-optimize framework in the context of sequential decision problems (formulated as MDPs) that are solved via reinforcement learning. In particular, we are given environment features and a set of trajectories from training MDPs, which we use to train a predictive model that generalizes to unseen test MDPs without trajectories. Two significant computational challenges arise in applying decision-focused learning to MDPs: (i) large state and action spaces make it infeasible for existing techniques to differentiate through MDP problems, and (ii) the high-dimensional policy space, as parameterized by a neural network, makes differentiating through a policy expensive. We resolve the first challenge by sampling provably unbiased derivatives to approximate and differentiate through optimality conditions, and the second challenge by using a low-rank approximation to the high-dimensional sample-based derivatives. We implement both Bellman-based and policy gradient-based decision-focused learning on three different MDP problems with missing parameters, and show that decision-focused learning performs better in generalization to unseen tasks.","['Reinforcement Learning and Planning', 'Deep Learning', 'Optimization']",[],"['Kai Wang', 'Sanket Shah', 'Haipeng Chen', 'Andrew Perrault', 'Finale Doshi-Velez', 'Milind Tambe']","['Institute of Technology', 'Harvard University', 'College of William and Mary', 'Ohio State University', 'Harvard University', 'Harvard University']","['Georgia', None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27844,Fairness & Bias,Near-Optimal No-Regret Learning in General Games,"We show that Optimistic Hedge -- a common variant of multiplicative-weights-updates with recency bias -- attains ${\rm poly}(\log T)$ regret in multi-player general-sum games. In particular, when every player of the game uses Optimistic Hedge to iteratively update her action in response to the history of play so far, then after $T$ rounds of interaction, each player experiences total regret that is ${\rm poly}(\log T)$. Our bound improves, exponentially, the $O(T^{1/2})$ regret attainable  by standard no-regret learners in games, the $O(T^{1/4})$ regret attainable by no-regret learners with recency bias (Syrgkanis et al., NeurIPS 2015), and the $O(T^{1/6})$ bound that was recently shown for Optimistic Hedge in the special case of two-player games (Chen & Peng, NeurIPS 2020). A direct corollary of our bound is that Optimistic Hedge converges to coarse correlated equilibrium in general games at a rate of $\tilde{O}(1/T)$.",[],[],"['Constantinos Costis Daskalakis', 'Maxwell Fishelson', 'Noah Golowich']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26838,Fairness & Bias,Neural Production Systems,"Visual environments are structured, consisting of distinct  objects or entities. These entities have properties---visible or latent---that determine the manner in which they interact with one another. To partition images into entities, deep-learning researchers have proposed structural inductive biases such as slot-based architectures. To model interactions among entities, equivariant graph neural nets (GNNs) are used, but these are not particularly well suited to the task for two reasons. First, GNNs do not predispose interactions to be sparse, as relationships among independent entities are likely to be.  Second, GNNs do not factorize knowledge about  interactions in an entity-conditional manner. As an alternative, we take inspiration from cognitive science and resurrect a classic approach, production systems, which consist of a set of rule templates that are applied by binding placeholder  variables in the rules to specific entities. Rules are scored on their match to entities, and the best fitting rules are applied to update entity properties. In a series of experiments, we demonstrate that this architecture achieves a flexible, dynamic flow of control and serves to factorize entity-specific and rule-based information. This disentangling of knowledge achieves robust future-state prediction in rich visual environments, outperforming state-of-the-art methods using GNNs, and allows for the extrapolation from simple (few object) environments to more complex environments.","['Deep Learning', 'Graph Learning']",[],"['Anirudh Goyal', 'Aniket Rajiv Didolkar', 'Nan Rosemary Ke', 'Charles Blundell', 'Philippe Beaudoin', 'Nicolas Heess', 'Michael Curtis Mozer', 'Yoshua Bengio']","['Google DeepMind', 'Université de Montréal', 'DeepMind', 'DeepMind', 'University of British Columbia', 'Google', 'Google Research', 'University of Montreal']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26841,Fairness & Bias,Equivariant Manifold Flows,"Tractably modelling distributions over manifolds has long been an important goal in the natural sciences. Recent work has focused on developing general machine learning models to learn such distributions. However, for many applications these distributions must respect manifold symmetries—a trait which most previous models disregard. In this paper, we lay the theoretical foundations for learning symmetry-invariant distributions on arbitrary manifolds via equivariant manifold flows. We demonstrate the utility of our approach by learning quantum field theory-motivated invariant SU(n) densities and by correcting meteor impact dataset bias.","['Theory', 'Generative Model', 'Machine Learning']",[],"['Isay Katsman', 'Aaron Lou', 'Derek Lim', 'Qingxuan Jiang', 'Ser-Nam Lim', 'Christopher De Sa']","['Yale University', 'Stanford University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Central Florida', 'Cornell University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27437,Fairness & Bias,Fairness in Ranking under Uncertainty,"Fairness has emerged as an important consideration in algorithmic decision making. Unfairness occurs when an agent with higher merit obtains a worse outcome than an agent with lower merit. Our central point is that a primary cause of unfairness is uncertainty. A principal or algorithm making decisions never has access to the agents' true merit, and instead uses proxy features that only imperfectly predict merit (e.g., GPA, star ratings, recommendation letters). None of these ever fully capture an agent's merit; yet existing approaches have mostly been defining fairness notions directly based on observed features and outcomes. Our primary point is that it is more principled to acknowledge and model the uncertainty explicitly. The role of observed features is to give rise to a posterior distribution of the agents' merits. We use this viewpoint to define a notion of approximate fairness in ranking. We call an algorithm $\phi$-fair (for $\phi \in [0,1]$) if it has the following property for all agents $x$ and all $k$: if agent $x$ is among the top $k$ agents with respect to merit with probability at least $\rho$ (according to the posterior merit distribution), then the algorithm places the agent among the top $k$ agents in its ranking with probability at least $\phi \rho$. We show how to compute rankings that optimally trade off approximate fairness against utility to the principal. In addition to the theoretical characterization, we present an empirical analysis of the potential impact of the approach in simulation studies. For real-world validation, we applied the approach in the context of a paper recommendation system that we built and fielded at the KDD 2020 conference.",['Fairness'],[],"['Ashudeep Singh', 'David Kempe', 'Thorsten Joachims']","['Pinterest, Inc.', 'University of Southern California', 'Amazon']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27234,Fairness & Bias,Identifiable Generative models for Missing Not at Random Data Imputation,"Real-world datasets often have missing values associated with complex generative processes, where the cause of the missingness may not be fully observed. This is known as missing not at random (MNAR) data. However, many imputation methods do not take into account the missingness mechanism, resulting in biased imputation values when MNAR data is present. Although there are a few methods that have considered the MNAR scenario, their model's identifiability under MNAR is generally not guaranteed. That is, model parameters can not be uniquely determined even with infinite data samples, hence the imputation results given by such models can still be biased. This issue is especially overlooked by many modern deep generative models. In this work, we fill in this gap by systematically analyzing the identifiability of generative models under MNAR. Furthermore, we propose a practical deep generative model which can provide identifiability guarantees under mild assumptions, for a wide range of MNAR mechanisms. Our method demonstrates a clear advantage for tasks on both synthetic data and multiple real-world scenarios with MNAR data.",['Generative Model'],[],"['Chao Ma', 'Cheng Zhang']","['Microsoft', 'Microsoft']","[None, None]"
https://nips.cc/virtual/2021/poster/27339,Fairness & Bias,Fair Exploration via Axiomatic Bargaining,"Motivated by the consideration of fairly sharing the cost of exploration between multiple groups in learning problems, we develop the Nash bargaining solution in the context of multi-armed bandits. Specifically, the 'grouped' bandit associated with any multi-armed bandit problem associates, with each time step, a single group from some finite set of groups. The utility gained by a given group under some learning policy is naturally viewed as the reduction in that group's regret relative to the regret that group would have incurred 'on its own'. We derive policies that yield the Nash bargaining solution relative to the set of incremental utilities possible under any policy. We show that on the one hand, the 'price of fairness' under such policies is limited, while on the other hand, regret optimal policies are arbitrarily unfair under generic conditions. Our theoretical development is complemented by a case study on contextual bandits for warfarin dosing where we are concerned with the cost of exploration across multiple races and age groups.","['Reinforcement Learning and Planning', 'Fairness', 'Bandits']",[],"['Jackie Baek', 'Vivek Farias']","['New York University', 'Massachusetts Institute of Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/27879,Fairness & Bias,Bellman-consistent Pessimism for Offline Reinforcement Learning,"The use of pessimism, when reasoning about datasets lacking exhaustive exploration has recently gained prominence in offline reinforcement learning. Despite the robustness it adds to the algorithm, overly pessimistic reasoning can be equally damaging in precluding the discovery of good policies, which is an issue for the popular bonus-based pessimism. In this paper, we introduce the notion of Bellman-consistent pessimism for general function approximation: instead of calculating a point-wise lower bound for the value function, we implement pessimism at the initial state over the set of functions consistent with the Bellman equations. Our theoretical guarantees only require Bellman closedness as standard in the exploratory setting, in which case bonus-based pessimism fails to provide guarantees.  Even in the special case of linear function approximation where stronger expressivity assumptions hold, our result improves upon a recent bonus-based approach by $\mathcal O(d)$ in its sample complexity (when the action space is finite). Remarkably, our algorithms automatically adapt to the best bias-variance tradeoff in the hindsight, whereas most prior approaches require tuning extra hyperparameters a priori.","['Reinforcement Learning and Planning', 'Robustness', 'Theory']",[],"['Tengyang Xie', 'Ching-An Cheng', 'Nan Jiang', 'Paul Mineiro', 'Alekh Agarwal']","['Department of Computer Science, University of Wisconsin - Madison', 'Microsoft Research', 'University of Illinois, Urbana Champaign', 'University of California, San Diego', 'Google']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26487,Fairness & Bias,Recursive Causal Structure Learning in the Presence of Latent Variables and Selection Bias,"We consider the problem of learning the causal MAG of a system from observational data in the presence of latent variables and selection bias. Constraint-based methods are one of the main approaches for solving this problem, but the existing methods are either computationally impractical when dealing with large graphs or lacking completeness guarantees. We propose a novel computationally efficient recursive constraint-based method that is sound and complete. The key idea of our approach is that at each iteration a specific type of variable is identified and removed. This allows us to learn the structure efficiently and recursively, as this technique reduces both the number of required conditional independence (CI) tests and the size of the conditioning sets. The former substantially reduces the computational complexity, while the latter results in more reliable CI tests. We provide an upper bound on the number of required CI tests in the worst case. To the best of our knowledge, this is the tightest bound in the literature. We further provide a lower bound on the number of CI tests required by any constraint-based method.  The upper bound of our proposed approach and the lower bound at most differ by a factor equal to the number of variables in the worst case. We provide experimental results to compare the proposed approach with the state of the art on both synthetic and real-world structures.","['Graph Learning', 'Causality']",[],"['Sina Akbari', 'Ehsan Mokhtarian', 'AmirEmad Ghassami', 'Negar Kiyavash']","['Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'Boston University', 'Swiss Federal Institute of Technology Lausanne']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26516,Fairness & Bias,"SGD: The Role of Implicit Regularization, Batch-size and Multiple-epochs","Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the method of choice for learning with large over-parameterized models. A popular theory for explaining why SGD works well in practice is that the algorithm has an implicit regularization that biases its output towards a good solution. Perhaps the theoretically most well understood learning setting for SGD is that of Stochastic Convex Optimization (SCO), where it is well known that SGD learns at a rate of $O(1/\sqrt{n})$, where $n$ is the number of samples. In this paper, we consider the problem of SCO and explore the role of implicit regularization, batch size and multiple epochs for SGD. Our main contributions are threefold: * We show that for any regularizer, there is an SCO problem for which Regularized Empirical Risk Minimzation fails to learn. This automatically rules out any implicit regularization based explanation for the success of SGD.  * We provide a separation between SGD and learning via Gradient Descent on empirical loss (GD) in terms of sample complexity. We show that there is an SCO problem such that GD with any step size and number of iterations can only learn at a suboptimal rate: at least $\widetilde{\Omega}(1/n^{5/12})$.  *  We present a multi-epoch variant of SGD commonly used in practice. We prove that this algorithm is at least as good as single pass SGD in the worst case. However, for certain SCO problems, taking multiple passes over the dataset can significantly outperform single pass SGD. We extend our results to the general learning setting by showing a problem which is learnable for any data distribution, and for this problem, SGD is strictly better than RERM for any regularization function. We conclude by discussing the implications of our results for deep learning, and show a separation between SGD and ERM for two layer diagonal neural networks.","['Theory', 'Deep Learning', 'Optimization']",[],"['Ayush Sekhari', 'Karthik Sridharan', 'Satyen Kale']","['Massachusetts Institute of Technology', 'Cornell University', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26537,Fairness & Bias,IQ-Learn: Inverse soft-Q Learning for Imitation,"In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment’s dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, significantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x.","['Reinforcement Learning and Planning', 'Optimization', 'Adversarial Robustness and Security']",[],"['Divyansh Garg', 'Shuvam Chakraborty', 'Chris Cundy', 'Jiaming Song', 'Stefano Ermon']","['Stanford University', 'Stanford University', 'Stanford University', 'NVIDIA', 'Stanford University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25958,Fairness & Bias,ParK: Sound and Efficient Kernel Ridge Regression by Feature Space Partitions,"We introduce ParK, a new large-scale solver for kernel ridge regression. Our approach combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. In particular, constructing suitable partitions directly in the feature space rather than in the input space, we promote orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. We characterize the statistical-computational tradeoff of our model, and demonstrate the effectiveness of our method by numerical experiments on large-scale datasets.","['Optimization', 'Kernel Methods']",[],"['Luigi Carratino', 'Stefano Vigogna', 'Daniele Calandriello', 'Lorenzo Rosasco']","['Università degli Studi di Genova', 'Università degli Studi di Roma Tor Vergata', 'DeepMind', 'Università degli Studi di Genova']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27458,Fairness & Bias,Visualizing the Emergence of Intermediate Visual Patterns in DNNs,"This paper proposes a method to visualize the discrimination power of intermediate-layer visual patterns encoded by a DNN. Specifically, we visualize (1) how the DNN gradually learns regional visual patterns in each intermediate layer during the training process, and (2) the effects of the DNN using non-discriminative patterns in low layers to construct disciminative patterns in middle/high layers through the forward propagation. Based on our visualization method, we can quantify knowledge points (i.e. the number of discriminative visual patterns) learned by the DNN to evaluate the representation capacity of the DNN. Furthermore, this method also provides new insights into signal-processing behaviors of existing deep-learning techniques, such as adversarial attacks and knowledge distillation.","['Adversarial Robustness and Security', 'Deep Learning']",[],"['Mingjie Li', 'Shaobo Wang', 'Quanshi Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27856,Fairness & Bias,CBP: backpropagation with constraint on weight precision using a pseudo-Lagrange multiplier method,"Backward propagation of errors (backpropagation) is a method to minimize objective functions (e.g., loss functions) of deep neural networks by identifying optimal sets of weights and biases. Imposing constraints on weight precision is often required to alleviate prohibitive workloads on hardware. Despite the remarkable success of backpropagation, the algorithm itself is not capable of considering such constraints unless additional algorithms are applied simultaneously. To address this issue, we propose the constrained backpropagation (CBP) algorithm based on the pseudo-Lagrange multiplier method to obtain the optimal set of weights that satisfy a given set of constraints. The defining characteristic of the proposed CBP algorithm is the utilization of a Lagrangian function (loss function plus constraint function) as its objective function. We considered various types of constraints — binary, ternary, one-bit shift, and two-bit shift weight constraints. As a post-training method, CBP applied to AlexNet, ResNet-18, ResNet-50, and GoogLeNet on ImageNet, which were pre-trained using the conventional backpropagation. For most cases, the proposed algorithm outperforms the state-of-the-art methods on ImageNet, e.g., 66.6\%, 74.4\%, and 64.0\% top-1 accuracy for ResNet-18, ResNet-50, and GoogLeNet with binary weights, respectively. This highlights CBP as a learning algorithm to address diverse constraints with the minimal performance loss by employing appropriate constraint functions. The code for CBP is publicly available at \url{https://github.com/dooseokjeong/CBP}.",['Deep Learning'],[],"['Guhyun Kim', 'Doo Seok Jeong']","['Hanyang University', 'Hanyang University']","[None, None]"
https://nips.cc/virtual/2021/poster/26454,Fairness & Bias,Passive attention in artificial neural networks predicts human visual selectivity,"Developments in machine learning interpretability techniques over the past decade have provided new tools to observe the image regions that are most informative for classification and localization in artificial neural networks (ANNs). Are the same regions similarly informative to human observers? Using data from 79 new experiments and 7,810 participants, we show that passive attention techniques reveal a significant overlap with human visual selectivity estimates derived from 6 distinct behavioral tasks including visual discrimination, spatial localization, recognizability, free-viewing, cued-object search, and saliency search fixations. We find that input visualizations derived from relatively simple ANN architectures probed using guided backpropagation methods are the best predictors of a shared component in the joint variability of the human measures. We validate these correlational results with causal manipulations using recognition experiments. We show that images masked with ANN attention maps were easier for humans to classify than control masks in a speeded recognition experiment. Similarly, we find that recognition performance in the same ANN models was likewise influenced by masking input images using human visual selectivity maps. This work contributes a new approach to evaluating the biological and psychological validity of leading ANNs as models of human vision: by examining their similarities and differences in terms of their visual selectivity to the information contained in images.","['Deep Learning', 'Vision', 'Interpretability', 'Machine Learning']",[],"['Thomas A Langlois', 'Haicheng Charles Zhao', 'Erin Grant', 'Ishita Dasgupta', 'Thomas L. Griffiths', 'Nori Jacoby']","['Princeton University', 'Princeton University', 'University College London', 'DeepMind', 'Princeton University', 'Max-Planck Institute']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27454,Fairness & Bias,NEO: Non Equilibrium Sampling on the Orbits of a Deterministic Transform,"Sampling from a complex distribution $\pi$ and approximating its intractable normalizing constant $\mathrm{Z}$ are challenging problems.  In this paper, a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) samplers is derived.  Given an invertible map $\mathrm{T}$, these schemes combine (with weights) elements from the forward and backward Orbits   through points sampled from a proposal distribution $\rho$. The map $\mathrm{T}$ does not leave the target $\pi$ invariant, hence the name NEO, standing for Non-Equilibrium Orbits. NEO-IS provides unbiased estimators of the normalizing constant and self-normalized IS estimators of expectations under $\pi$ while NEO-MCMC combines multiple NEO-IS estimates of the normalizing constant and an iterated sampling-importance resampling mechanism to sample from $\pi$.  For $\mathrm{T}$ chosen as a discrete-time integrator of a conformal Hamiltonian system, NEO-IS achieves state-of-the art performance on difficult benchmarks and NEO-MCMC is able to explore highly multimodal targets. Additionally, we provide detailed theoretical results for both methods. In particular, we show that NEO-MCMC is uniformly geometrically ergodic and establish explicit mixing time estimates under mild conditions.",['Generative Model'],[],"['Achille Thin', 'Yazid Janati El Idrissi', 'Sylvain Le Corff', 'Charles Ollion', 'Eric Moulines', 'Arnaud Doucet', 'Alain Durmus', 'Christian P Robert']","['Ecole polytechnique', 'Telecom SudParis', 'Sorbonne Université, LPSM', 'Ecole polytechnique', 'Ecole polytechnique', 'Google DeepMind', 'École Polytechnique', 'The university of Warwick']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28412,Fairness & Bias,Noisy Recurrent Neural Networks,"We provide a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Specifically, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We find that, under reasonable assumptions, this implicit regularization promotes flatter minima; it biases towards models with more stable dynamics; and, in classification tasks, it favors models with larger classification margin. Sufficient conditions for global stability are obtained, highlighting the phenomenon of stochastic stabilization, where noise injection can improve stability during training. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations.","['Theory', 'Deep Learning', 'Machine Learning', 'Robustness']",[],"['Soon Hoe Lim', 'N. Benjamin Erichson', 'Liam Hodgkinson', 'Michael W. Mahoney']","['Nordic Institute for Theoretical Physics', 'Lawrence Berkeley National Lab', 'University of Melbourne', 'University of California Berkeley']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26740,Fairness & Bias,How Modular should Neural Module Networks Be for Systematic Generalization?,"Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via composition of modules that tackle a sub-task. NMNs are a promising strategy to achieve systematic generalization, i.e., overcoming biasing factors in the training distribution. However, the aspects of NMNs that facilitate systematic generalization are not fully understood. In this paper, we demonstrate that the degree of modularity of the NMN have large influence on systematic generalization. In a series of experiments on three VQA datasets (VQA-MNIST, SQOOP, and CLEVR-CoGenT), our results reveal that tuning the degree of modularity, especially at the image encoder stage, reaches substantially higher systematic generalization. These findings lead to new NMN architectures that outperform previous ones in terms of systematic generalization.","['Deep Learning', 'Vision']",[],"[""Vanessa D'Amario""]",['Nova Southeastern University'],[None]
https://nips.cc/virtual/2021/poster/28734,Fairness & Bias,An Analysis of Constant Step Size SGD in the Non-convex Regime: Asymptotic Normality and Bias,"Structured non-convex learning problems, for which critical points have favorable statistical properties, arise frequently in statistical machine learning. Algorithmic convergence and statistical estimation rates are well-understood for such problems. However, quantifying the uncertainty associated with the underlying training algorithm is not well-studied in the non-convex setting. In order to address this shortcoming, in this work, we establish an asymptotic normality result for the constant step size stochastic gradient descent (SGD)  algorithm---a widely used algorithm in practice. Specifically, based on the relationship between SGD and Markov Chains  [DDB19], we show that the average of SGD iterates is asymptotically normally distributed around the expected value of their unique invariant distribution, as long as the non-convex and non-smooth objective function satisfies a dissipativity property. We also characterize the bias between this expected value and the critical points of the objective function under various local regularity conditions. Together, the above two results could be leveraged to construct confidence intervals for non-convex problems that are trained using the SGD algorithm.","['Optimization', 'Machine Learning']",[],"['Lu Yu', 'Krishna Balasubramanian', 'Stanislav Volgushev', 'Murat A Erdogdu']","[""Ecole Nationale de la Statistique et de l'Administration Economique"", 'University of California, Davis', 'Toronto University', 'University of Toronto']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28715,Fairness & Bias,A Theory-Driven Self-Labeling Refinement Method for Contrastive Representation Learning,"For an image  query, unsupervised contrastive learning  labels crops of  the same image as positives,  and other image crops as  negatives. Although intuitive, such a native label assignment strategy cannot reveal the underlying semantic similarity between a  query and  its positives and negatives, and impairs performance,  since some negatives are  semantically similar to  the query or even share the same semantic class as the query.  In this work, we first  prove that for  contrastive learning,  inaccurate label assignment heavily  impairs its generalization for semantic instance discrimination, while accurate labels  benefit its generalization.  Inspired by this theory, we  propose   a novel self-labeling refinement approach for contrastive learning. It improves the label quality via two complementary  modules:  (i)  self-labeling refinery (SLR) to  generate accurate labels and (ii)  momentum mixup (MM)  to enhance similarity between query and its positive. SLR uses a positive of a query to estimate  semantic similarity between  a query and its positive and negatives, and  combines estimated similarity with  vanilla label assignment in contrastive learning to  iteratively generate  more accurate and informative soft labels. We theoretically show that our SLR can exactly recover the true semantic  labels of  label-corrupted  data, and  supervises   networks to achieve zero prediction  error on classification tasks.  MM randomly  combines   queries and  positives to increase  semantic similarity between the generated virtual queries and their positives so as to improves label accuracy.  Experimental results on CIFAR10,  ImageNet, VOC and COCO show the effectiveness of our method.","['Self-Supervised Learning', 'Machine Learning', 'Contrastive Learning', 'Theory', 'Representation Learning']",[],"['Caiming Xiong', 'Xiaotong Yuan', 'Steven Hoi']","['Salesforce Research', 'NUIST', 'Salesforce Research Asia']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27065,Fairness & Bias,Characterizing the risk of fairwashing,"Fairwashing refers to the risk that an unfair black-box model can be explained by a fairer model through post-hoc explanation manipulation. In this paper, we investigate the capability of fairwashing attacks by analyzing their fidelity-unfairness trade-offs. In particular, we show that fairwashed explanation models can generalize beyond the suing group (i.e., data points that are being explained), meaning that a fairwashed explainer can be used to rationalize subsequent unfair decisions of a black-box model. We also demonstrate that fairwashing attacks can transfer across black-box models, meaning that other black-box models can perform fairwashing without explicitly using their predictions. This generalization and transferability of fairwashing attacks imply that their detection will be difficult in practice. Finally, we propose an approach to quantify the risk of fairwashing, which is based on the computation of the range of the unfairness of high-fidelity explainers.",['Fairness'],[],"['Ulrich Aïvodji', 'Hiromi Arai', 'Sébastien Gambs', 'Satoshi Hara']","['École de technologie supérieure, Université du Québec', 'RIKEN', 'Université du Québec à Montréal', 'Osaka University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26782,Fairness & Bias,Perceptual Score: What Data Modalities Does Your Model Perceive?,"Machine learning advances in the last decade have relied significantly on large-scale datasets that continue to grow in size. Increasingly, those datasets also contain different data modalities. However, large multi-modal datasets are hard to annotate, and annotations may contain biases that we are often unaware of. Deep-net-based classifiers, in turn, are prone to exploit those biases and to find shortcuts. To study and quantify this concern, we introduce the perceptual score, a metric that assesses the degree to which a model relies on the different subsets of the input features, i.e., modalities. Using the perceptual score, we find a surprisingly consistent trend across four popular datasets: recent, more accurate state-of-the-art multi-modal models for visual question-answering or visual dialog tend to perceive the visual data less than their predecessors. This is concerning as answers are hence increasingly inferred from textual cues only. Using the perceptual score also helps to analyze model biases by decomposing the score into data subset contributions. We hope to spur a discussion on the perceptiveness of multi-modal models and also hope to encourage the community working on multi-modal classifiers to start quantifying perceptiveness via the proposed perceptual score.","['Vision', 'Machine Learning']",[],"['Itai Gat', 'Idan Schwartz', 'Alex Schwing']","['Meta', 'Technion', 'University of Illinois, Urbana Champaign']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26727,Fairness & Bias,Unifying Gradient Estimators for Meta-Reinforcement Learning  via Off-Policy Evaluation,"Model-agnostic meta-reinforcement learning requires estimating the Hessian matrix of value functions. This is challenging from an implementation perspective, as repeatedly differentiating policy gradient estimates may lead to biased Hessian estimates. In this work, we provide a unifying framework for estimating higher-order derivatives of value functions, based on off-policy evaluation. Our framework interprets a number of prior approaches as special cases and elucidates the bias and variance trade-off of Hessian estimates. This framework also opens the door to a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice.","['Reinforcement Learning and Planning', 'Meta Learning']",[],"['Yunhao Tang', 'Tadashi Kozuno', 'Mark Rowland', 'Remi Munos', 'Michal Valko']","['DeepMind', 'Omron Sinic X', 'DeepMind', 'DeepMind', 'Google DeepMind']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26918,Fairness & Bias,Fairness via Representation Neutralization,"Existing bias mitigation methods for DNN models primarily work on learning debiased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs? To this end, we propose a new mitigation technique, namely, Representation Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The key idea of RNF is to discourage the classification head from capturing spurious correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination of DNN models with minimal degradation in task-specific performance.","['Fairness', 'Interpretability', 'Machine Learning']",[],"['Mengnan Du', 'Subhabrata Mukherjee', 'Guanchu Wang', 'Ruixiang Tang', 'Ahmed Hassan Awadallah', 'Xia Hu']","['New  Institute of Technology', 'Hippocratic AI', 'Rice University', 'Rice University', 'Microsoft Research', 'Rice University']","['Jersey', None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28848,Fairness & Bias,Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent Biases,"Visual search is a ubiquitous and often challenging daily task, exemplified by looking for the car keys at home or a friend in a crowd. An intriguing property of some classical search tasks is an asymmetry such that finding a target A among distractors B can be easier than finding B among A. To elucidate the mechanisms responsible for asymmetry in visual search, we propose a computational model that takes a target and a search image as inputs and produces a sequence of eye movements until the target is found. The model integrates eccentricity-dependent visual recognition with target-dependent top-down cues. We compared the model against human behavior in six paradigmatic search tasks that show asymmetry in humans. Without prior exposure to the stimuli or task-specific training, the model provides a plausible mechanism for search asymmetry. We hypothesized that the polarity of search asymmetry arises from experience with the natural environment. We tested this hypothesis by training the model on augmented versions of ImageNet where the biases of natural images were either removed or reversed. The polarity of search asymmetry disappeared or was altered depending on the training protocol. This study highlights how classical perceptual properties can emerge in neural network models, without the need for task-specific training, but rather as a consequence of the statistical properties of the developmental diet fed to the model. All source code and data are publicly available at https://github.com/kreimanlab/VisualSearchAsymmetry.","['Deep Learning', 'Neuroscience']",[],"['Shashi Kant Gupta', 'Mengmi Zhang', 'Chia-Chien Wu', 'Jeremy M. Wolfe', 'Gabriel Kreiman']","['IIT Kanpur', 'Nanyang Technological University,', 'Harvard Medical School', ""Brigham & Women's Hospital / Harvard Med School"", 'Harvard Medical School']","[None, 'Singapore', None, None, None]"
https://nips.cc/virtual/2021/poster/28771,Fairness & Bias,A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models,"We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk.  We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice.","['Deep Learning', 'Machine Learning', 'Fairness']",[],"['Ibrahim Alabdulmohsin', 'Mario Lucic']","['Google', 'Google']","[None, None]"
https://nips.cc/virtual/2021/poster/28732,Fairness & Bias,Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets,"Language models can generate harmful and biased outputs and exhibit undesirable behavior according to a given cultural context. We propose a Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to significantly change model behavior by crafting and fine-tuning on a dataset that reflects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs significantly better on all metrics compared to baseline and control models for a broad range of GPT-3 language model sizes without compromising capability integrity. We find that the effectiveness of PALMS increases with model size. We show that significantly adjusting language model behavior is feasible with a small, hand-curated dataset.",[],[],"['Irene Solaiman', 'Christy Dennison']","['Hugging Face', 'Massachusetts Institute of Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/28717,Fairness & Bias,Bias and variance of the Bayesian-mean decoder,"Perception, in theoretical neuroscience, has been modeled as the encoding of external stimuli into internal signals, which are then decoded. The Bayesian mean is an important decoder, as it is optimal for purposes of both estimation and discrimination. We present widely-applicable approximations to the bias and to the variance of the Bayesian mean, obtained under the minimal and biologically-relevant assumption that the encoding results from a series of independent, though not necessarily identically-distributed, signals. Simulations substantiate the accuracy of our approximations in the small-noise regime. The bias of the Bayesian mean comprises two components: one driven by the prior, and one driven by the precision of the encoding. If the encoding is 'efficient', the two components have opposite effects; their relative strengths are determined by the objective that the encoding optimizes. The experimental literature on perception reports both 'Bayesian' biases directed towards prior expectations, and opposite, 'anti-Bayesian' biases. We show that different tasks are indeed predicted to yield such contradictory biases, under a consistently-optimal encoding-decoding model. Moreover, we recover Wei and Stocker's ""law of human perception"", a relation between the bias of the Bayesian mean and the derivative of its variance, and show how the coefficient of proportionality in this law depends on the task at hand. Our results provide a parsimonious theory of optimal perception under constraints, in which encoding and decoding are adapted both to the prior and to the task faced by the observer.","['Theory', 'Neuroscience']",[],"['Arthur Prat-Carrabin', 'Michael Woodford']","['Columbia University', 'Columbia University']","[None, None]"
https://nips.cc/virtual/2021/poster/28710,Fairness & Bias,Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations,"Contrastive self-supervised learning has outperformed supervised pretraining on many downstream tasks like segmentation and object detection. However, current methods are still primarily applied to curated datasets like ImageNet. In this paper, we first study how biases in the dataset affect existing methods. Our results show that an approach like MoCo works surprisingly well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed and (iii) general versus domain-specific datasets. Second, given the generality of the approach, we try to realize further gains with minor modifications. We show that learning additional invariances - through the use of multi-scale cropping, stronger augmentations and nearest neighbors - improves the representations. Finally, we observe that MoCo learns spatially structured representations when trained with a multi-crop strategy. The representations can be used for semantic segment retrieval and video instance segmentation without finetuning. Moreover, the results are on par with specialized models. We hope this work will serve as a useful study for other researchers.","['Vision', 'Self-Supervised Learning', 'Contrastive Learning', 'Representation Learning']",[],"['Wouter Van Gansbeke', 'Simon Vandenhende', 'Stamatios Georgoulis', 'Luc Van Gool']","['INSAIT', 'Meta AI', 'Huawei Technologies Ltd.', 'INSAIT - Sofia Un.']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28695,Fairness & Bias,Self-Instantiated Recurrent Units with Dynamic Soft Recursion,"While standard recurrent neural networks explicitly impose a chain structure on different forms of data, they do not have an explicit bias towards recursive self-instantiation where the extent of recursion is dynamic.  Given diverse and even growing data modalities (e.g., logic, algorithmic input and output, music, code, images, and language) that can be expressed in sequences and may benefit from more architectural flexibility, we propose the self-instantiated recurrent unit (Self-IRU) with a novel inductive bias towards dynamic soft recursion. On one hand, theSelf-IRU is characterized by recursive self-instantiation via its gating functions, i.e., gating mechanisms of the Self-IRU are controlled by instances of the Self-IRU itself, which are repeatedly invoked in a recursive fashion. On the other hand, the extent of the Self-IRU recursion is controlled by gates whose values are between 0 and 1 and may vary across the temporal dimension of sequences,  enabling dynamic soft recursion depth at each time step. The architectural flexibility and effectiveness of our proposed approach are demonstrated across multiple data modalities. For example, the Self-IRU achieves state-of-the-art performance on the logical inference dataset [Bowman et al., 2014] even when comparing with competitive models that have access to ground-truth syntactic information.",['Deep Learning'],[],"['Aston Zhang', 'Yi Tay', 'Yikang Shen', 'Alvin Chan', 'Shuai Zhang']","['AWS', 'Google', 'University of Montreal', 'Massachusetts Institute of Technology', 'Amazon']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28668,Fairness & Bias,How Data Augmentation affects Optimization for Linear Regression,"Though data augmentation has rapidly emerged as a key tool for optimization in modern machine learning, a clear picture of how augmentation schedules affect optimization and interact with optimization hyperparameters such as learning rate is nascent. In the spirit of classical convex optimization and recent work on implicit bias, the present work analyzes the effect of augmentation on optimization in the simple convex setting of linear regression with MSE loss. We find joint schedules for learning rate and data augmentation scheme under which augmented gradient descent provably converges and characterize the resulting minimum. Our results apply to arbitrary augmentation schemes, revealing complex interactions between learning rates and augmentations even in the convex setting. Our approach interprets augmented (S)GD as a stochastic optimization method for a time-varying sequence of proxy losses. This gives a unified way to analyze learning rate, batch size, and augmentations ranging from additive noise to random projections. From this perspective, our results, which also give rates of convergence, can be viewed as Monro-Robbins type conditions for augmented (S)GD.","['Optimization', 'Machine Learning']",[],"['Boris Hanin', 'Yi Sun']","['Princeton University', 'University of Chicago']","[None, None]"
https://nips.cc/virtual/2021/poster/28655,Fairness & Bias,Spot the Difference: Detection of Topological Changes via Geometric Alignment,"Geometric alignment appears in a variety of applications, ranging from domain adaptation, optimal transport, and normalizing flows in machine learning; optical flow and learned augmentation in computer vision and deformable registration within biomedical imaging. A recurring challenge is the alignment of domains whose topology is not the same; a problem that is routinely ignored, potentially introducing bias in downstream analysis. As a first step towards solving such alignment problems, we propose an unsupervised algorithm for the detection of changes in image topology. The model is based on a conditional variational auto-encoder and detects topological changes between two images during the registration step. We account for both topological changes in the image under spatial variation and unexpected transformations. Our approach is validated on two tasks and datasets: detection of topological changes in microscopy images of cells, and unsupervised anomaly detection brain imaging.","['Vision', 'Domain Adaptation', 'Machine Learning', 'Optimal Transport', 'Generative Model']",[],"['Steffen Czolbe', 'Aasa Feragen', 'Oswin Krause']","['University of Copenhagen', 'Technical University of', 'University of Copenhagen']","[None, 'Denmark', None]"
https://nips.cc/virtual/2021/poster/28615,Fairness & Bias,Class-Incremental Learning via Dual Augmentation,"Deep learning systems typically suffer from catastrophic forgetting of past knowledge when acquiring new skills continually. In this paper, we emphasize two dilemmas, representation bias and classifier bias in class-incremental learning, and present a simple and novel approach that employs explicit class augmentation (classAug) and implicit semantic augmentation (semanAug) to address the two biases, respectively. On the one hand, we propose to address the representation bias by learning transferable and diverse representations. Specifically, we investigate the feature representations in incremental learning based on spectral analysis and present a simple technique called classAug, to let the model see more classes during training for learning representations transferable across classes. On the other hand, to overcome the classifier bias, semanAug implicitly involves the simultaneous generating of an infinite number of instances of old classes in the deep feature space, which poses tighter constraints to maintain the decision boundary of previously learned classes. Without storing any old samples, our method can perform comparably with representative data replay based approaches.","['Deep Learning', 'Continual Learning']",[],"['Fei Zhu', 'Zhen Cheng', 'Cheng-lin Liu']","['Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28576,Fairness & Bias,To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs,"Due to the discrete nature of words, language GANs require to be optimized from rewards provided by discriminator networks, via reinforcement learning methods. This is a much harder setting than for continuous tasks, which enjoy gradient flows from discriminators to generators, usually leading to dramatic learning instabilities.   However, we claim that this can be solved by making discriminator and generator networks cooperate to produce output sequences during training. These cooperative outputs, inherently built to obtain higher discrimination scores, not only provide denser rewards for training but also form a more compact artificial set for discriminator training, hence improving its accuracy and stability. In this paper, we show that our SelfGAN framework, built on this cooperative principle, outperforms Teacher Forcing and obtains state-of-the-art results on two challenging tasks, Summarization and Question Generation.","['Reinforcement Learning and Planning', 'Generative Model']",[],"['Thomas Scialom', 'Paul-Alexis Dray', 'Jacopo Staiano', 'sylvain lamprier', 'Benjamin Piwowarski']","['LIP6', 'reciTAL', 'University of Trento', ""Université d'Angers"", 'CNRS / ISIR, Sorbonne Université']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28554,Fairness & Bias,Causal Inference for Event Pairs in Multivariate Point Processes,"Causal inference and discovery from observational data has been extensively studied across multiple fields. However, most prior work has focused on independent and identically distributed (i.i.d.) data. In this paper, we propose a formalization for causal inference between pairs of event variables in multivariate recurrent event streams by extending Rubin's framework for the average treatment effect (ATE) and propensity scores to multivariate point processes. Analogous to a joint probability distribution representing i.i.d. data, a multivariate point process represents data involving asynchronous and irregularly spaced occurrences of various types of events over a common timeline. We theoretically justify our point process causal framework and show how to obtain unbiased estimates of the proposed measure. We conduct an experimental investigation using synthetic and real-world event datasets, where our proposed causal inference framework is shown to exhibit superior performance against a set of baseline pairwise causal association scores.",['Causality'],[],"['Tian Gao', 'Dharmashankar Subramanian', 'Debarun Bhattacharjya', 'Xiao Shou', 'Nicholas Mattei', 'Kristin Bennett']","['International Business Machines', 'International Business Machines', 'Stanford University', 'IBM, International Business Machines', 'Tulane University', 'Rensselaer Polytechnic Institute']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28556,Fairness & Bias,Identity testing for Mallows model,"In this paper, we devise identity tests for ranking data that is generated from Mallows model both in the \emph{asymptotic} and \emph{non-asymptotic} settings. First we consider the case when the central ranking is known, and devise two algorithms for testing the spread parameter of the Mallows model. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test in the asymptotic setting and then converting it into a sample-optimal non-asymptotic identity test. The resulting test is, however, impractical even for medium sized data, because it requires computing the distribution of the sufficient statistic. The second non-asymptotic test is derived from an optimal learning algorithm for the Mallows model. This test is both easy to compute and is sample-optimal for a wide range of parameters. Next, we consider testing Mallows models for the unknown central ranking case. This case can be tackled in the asymptotic setting by introducing a bias that exponentially decays with the sample size. We support all our findings with extensive numerical experiments and show that the proposed tests scale gracefully with the number of items to be ranked.",[],[],"['Robert Istvan Busa-Fekete', 'Dimitris Fotakis', 'Balazs Szorenyi', 'Manolis Zampetakis']","['Google Research', 'Archimedes/Athena RC ', 'Yahoo', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28529,Fairness & Bias,Generalized and Discriminative Few-Shot Object Detection via SVD-Dictionary Enhancement,"Few-shot object detection (FSOD) aims to detect new objects based on few annotated samples. To alleviate the impact of few samples, enhancing the generalization and discrimination abilities of detectors on new objects plays an important role. In this paper, we explore employing Singular Value Decomposition (SVD) to boost both the generalization and discrimination abilities. In specific, we propose a novel method, namely, SVD-Dictionary enhancement, to build two separated spaces based on the sorted singular values. Concretely, the eigenvectors corresponding to larger singular values are used to build the generalization space in which localization is performed, as these eigenvectors generally suppress certain variations (e.g., the variation of styles) and contain intrinsical characteristics of objects. Meanwhile, since the eigenvectors corresponding to relatively smaller singular values may contain richer category-related information, we can utilize them to build the discrimination space in which classification is performed. Dictionary learning is further leveraged to capture high-level discriminative information from the discrimination space, which is beneficial for improving detection accuracy. In the experiments, we separately verify the effectiveness of our method on PASCAL VOC and COCO benchmarks. Particularly, for the 2-shot case in VOC split1, our method significantly outperforms the baseline by 6.2\%. Moreover, visualization analysis shows that our method is instrumental in doing FSOD.","['Vision', 'Machine Learning']",[],"['Aming WU', 'Suqi Zhao', 'Cheng Deng', 'Wei Liu']","['Xidian University', 'Xidian University', 'Xidian University', 'Tencent']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28526,Fairness & Bias,Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem,"In this paper, we study the problem of fair sparse regression on a biased dataset where bias depends upon a hidden binary attribute. The presence of a hidden attribute adds an extra layer of complexity to the problem by combining sparse regression and clustering with unknown binary labels. The corresponding optimization problem is combinatorial, but we propose a novel relaxation of it as an invex optimization problem. To the best of our knowledge, this is the first invex relaxation for a combinatorial problem. We show that the inclusion of the debiasing/fairness constraint in our model has no adverse effect on the performance. Rather, it enables the recovery of the hidden attribute. The support of our recovered regression parameter vector matches exactly with the true parameter vector. Moreover, we simultaneously solve the clustering problem by recovering the exact value of the hidden attribute for each sample. Our method uses carefully constructed primal dual witnesses to provide theoretical guarantees for the combinatorial problem. To that end, we show that the sample complexity of our method is logarithmic in terms of the dimension of the regression parameter vector.","['Theory', 'Fairness', 'Optimization', 'Clustering']",[],['Jean Honorio'],['University of Melbourne'],[None]
https://nips.cc/virtual/2021/poster/28502,Fairness & Bias,Online Multi-Armed Bandits with Adaptive Inference,"During online decision making in Multi-Armed Bandits (MAB), one needs to conduct inference on the true mean reward of each arm based on data collected so far at each step. However, since the arms are adaptively selected--thereby yielding non-iid data--conducting inference accurately is not straightforward. In particular, sample averaging, which is used in the family of UCB and Thompson sampling (TS) algorithms, does not provide a good choice as it suffers from bias and a lack of good statistical properties (e.g.  asymptotic normality). Our thesis in this paper is that more sophisticated inference schemes that take into account the adaptive nature of the sequentially collected data can unlock further performance gains, even though both UCB and TS type algorithms are optimal in the worst case. In particular, we propose a variant of TS-style algorithms--which we call doubly adaptive TS--that leverages recent advances in causal inference and adaptively reweights the terms of a doubly robust estimator on the true mean reward of each arm. Through 20 synthetic domain experiments and a semi-synthetic experiment based on data from an A/B test of a web service, we demonstrate that using an adaptive inferential scheme (while still retaining the exploration efficacy of TS) provides clear benefits in online decision making: the proposed DATS algorithm has superior empirical performance to existing baselines (UCB and TS) in terms of regret and sample complexity in identifying the best arm. In addition, we also provide a finite-time regret bound of doubly adaptive TS that matches (up to log factors) those of UCB and TS algorithms, thereby establishing that its improved practical benefits do not come at the expense of worst-case suboptimality.","['Reinforcement Learning and Planning', 'Theory', 'Online Learning', 'Causality', 'Bandits']",[],"['Maria Dimakopoulou', 'Zhimei Ren', 'Zhengyuan Zhou']","['Netflix', 'The Wharton School, University of Pennsylvania', 'New York University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28468,Fairness & Bias,Test-time Collective Prediction,"An increasingly common setting in machine learning involves multiple parties, each with their own data, who want to jointly make predictions on future test points. Agents wish to benefit from the collective expertise of the full set of agents to make better predictions than they would individually, but may not be willing to release labeled data or model parameters. In this work, we explore a decentralized mechanism to make collective predictions at test time, that is inspired by the literature in social science on human consensus-making. Building on a query model to facilitate information exchange among agents, our approach leverages each agent’s pre-trained model without relying on external validation, model retraining, or data pooling. A theoretical analysis shows that our approach recovers inverse mean-squared-error (MSE) weighting in the large-sample limit which is known to be the optimal way to combine independent, unbiased estimators. Empirically, we demonstrate that our scheme effectively combines models with differing quality across the input space: the proposed consensus prediction achieves significant gains over classical model averaging, and even outperforms weighted averaging schemes that have access to additional validation data. Finally, we propose a decentralized Jackknife procedure as a tool to evaluate the sensitivity of the collective predictions with respect to a single agent's opinion.","['Federated Learning', 'Machine Learning']",[],"['Celestine Mendler-Dünner', 'Wenshuo Guo', 'Stephen Bates', 'Michael Jordan']","['Max Planck Institute for Intelligent Systems', 'University of California Berkeley', 'University of California Berkeley', 'University of California, Berkeley']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28481,Fairness & Bias,Are Transformers more robust than CNNs?,"Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks,  recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at: https://github.com/ytongbai/ViTs-vs-CNNs.","['Transformers', 'Deep Learning', 'Adversarial Robustness and Security', 'Robustness']",[],"['Yutong Bai', 'Jieru Mei', 'Alan Yuille', 'Cihang Xie']","['Facebook', 'Johns Hopkins University', 'Johns Hopkins University', 'University of California, Santa Cruz']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28431,Fairness & Bias,Implicit Regularization in Matrix Sensing via Mirror Descent,"We study discrete-time mirror descent applied to the unregularized empirical risk in matrix sensing. In both the general case of rectangular matrices and the particular case of positive semidefinite matrices, a simple potential-based analysis in terms of the Bregman divergence allows us to establish convergence of mirror descent---with different choices of the mirror maps---to a matrix that, among all global minimizers of the empirical risk, minimizes a quantity explicitly related to the nuclear norm, the Frobenius norm, and the von Neumann entropy. In both cases, this characterization implies that mirror descent, a first-order algorithm minimizing the unregularized empirical risk, recovers low-rank matrices under the same set of assumptions that are sufficient to guarantee recovery for nuclear-norm minimization. When the sensing matrices are symmetric and commute, we show that gradient descent with full-rank factorized parametrization is a first-order approximation to mirror descent, in which case we obtain an explicit characterization of the implicit bias of gradient flow as a by-product.",['Optimization'],[],"['Fan Wu', 'Patrick Rebeschini']","['University of Oxford', 'University of Oxford']","[None, None]"
https://nips.cc/virtual/2021/poster/28432,Fairness & Bias,Generalized DataWeighting via Class-Level Gradient Manipulation,"Label noise and class imbalance are two major issues coexisting in real-world datasets. To alleviate the two issues, state-of-the-art methods reweight each instance by leveraging a small amount of clean and unbiased data. Yet, these methods overlook class-level information within each instance, which can be further utilized to improve performance. To this end, in this paper, we propose Generalized Data Weighting (GDW) to simultaneously mitigate label noise and class imbalance by manipulating gradients at the class level. To be specific, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. In this way, GDW achieves remarkable performance improvement on both issues. Aside from the performance gain, GDW efficiently obtains class-level weights without introducing any extra computational cost compared with instance weighting methods. Specifically, GDW performs a gradient descent step on class-level weights, which only relies on intermediate gradients. Extensive experiments in various settings verify the effectiveness of GDW. For example, GDW outperforms state-of-the-art methods by $2.56\%$ under the $60\%$ uniform noise setting in CIFAR10. Our code is available at https://github.com/GGchen1997/GDW-NIPS2021.","['Meta Learning', 'Optimization', 'Machine Learning']",[],"['Can Chen', 'Shuhao Zheng', 'Xi Chen', 'Erqun Dong', 'Xue Liu', 'Hao Liu', 'Dejing Dou']","['Mila - Quebec AI Institute', 'McGill University', 'Huawei Technologies Ltd.', 'University of British Columbia', 'McGill University', 'The  University of Science and Technology (Guangzhou)', 'University of Oregon Eugene']","[None, None, None, None, None, 'Hong Kong', None]"
https://nips.cc/virtual/2021/poster/25965,Fairness & Bias,Residual2Vec: Debiasing graph embedding with random graphs,"Graph embedding maps a graph into a convenient vector-space representation for graph analysis and machine learning applications. Many graph embedding methods hinge on a sampling of context nodes based on random walks. However, random walks can be a biased sampler due to the structural properties of graphs. Most notably, random walks are biased by the degree of each node, where a node is sampled proportionally to its degree. The implication of such biases has not been clear, particularly in the context of graph representation learning. Here, we investigate the impact of the random walks' bias on graph embedding and propose residual2vec, a general graph embedding method that can debias various structural biases in graphs by using random graphs. We demonstrate that this debiasing not only improves link prediction and clustering performance but also allows us to explicitly model salient structural properties in graph embedding.","['Clustering', 'Graph Learning', 'Machine Learning', 'Representation Learning']",[],"['Sadamori Kojaku', 'Jisung Yoon', 'Isabel Constantino', 'Yong-Yeol Ahn']","['na University, Bloomington', 'Northwestern University', 'na University, Bloomington', 'na University']","['India', None, 'India', 'India']"
https://nips.cc/virtual/2021/poster/28408,Fairness & Bias,Towards Context-Agnostic Learning Using Synthetic Data,"We propose a novel setting for learning, where the input domain is the image of a map defined on the product of two sets, one of which completely determines the labels. We derive a new risk bound for this setting that decomposes into a bias and an error term, and exhibits a surprisingly weak dependence on the true labels. Inspired by these results, we present an algorithm aimed at minimizing the bias term by exploiting the ability to sample from each set independently. We apply our setting to visual classification tasks, where our approach enables us to train classifiers on datasets that consist entirely of a single synthetic example of each class. On several standard benchmarks for real-world image classification, we achieve robust performance in the context-agnostic setting, with good generalization to real world domains, whereas training directly on real world data without our techniques yields classifiers that are brittle to perturbations of the background.","['Vision', 'Machine Learning']",[],"['Charles Jin', 'Martin Rinard']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/28395,Fairness & Bias,Implicit Sparse Regularization: The Impact of Depth and Early Stopping,"In this paper, we study the implicit bias of gradient descent for sparse regression. We extend results on regression with quadratic parametrization, which amounts to depth-2 diagonal linear networks, to more general depth-$N$ networks, under more realistic settings of noise and correlated designs. We show that early stopping is crucial for gradient descent to converge to a sparse model, a phenomenon that we call \emph{implicit sparse regularization}. This result is in sharp contrast to known results for noiseless and uncorrelated-design cases. We characterize the impact of depth and early stopping and show that for a general depth parameter $N$, gradient descent with early stopping achieves minimax optimal sparse recovery with sufficiently small initialization $w_0$ and step size $\eta$. In particular, we show that increasing depth enlarges the scale of working initialization and the early-stopping window so that this implicit sparse regularization effect is more likely to take place.",['Optimization'],[],"['Jiangyuan Li', 'Thanh V Nguyen', 'Chinmay Hegde', 'Raymond K. W. Wong']","['Texas A&M University', 'Amazon', 'New York University', 'Texas A&M University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28383,Fairness & Bias,A Biased Graph Neural Network Sampler with Near-Optimal Regret,"Graph neural networks (GNN) have recently emerged as a vehicle for applying deep network architectures to graph and relational data.  However, given the increasing size of industrial datasets, in many practical situations, the message passing computations required for sharing information across GNN layers are no longer scalable. Although various sampling methods have been introduced to approximate full-graph training within a tractable budget, there remain unresolved complications such as high variances and limited theoretical guarantees.  To address these issues, we build upon existing work and treat GNN neighbor sampling as a multi-armed bandit problem but with a newly-designed reward function that introduces some degree of bias designed to reduce variance and avoid unstable, possibly-unbounded pay outs.  And unlike prior bandit-GNN use cases, the resulting policy leads to near-optimal regret while accounting for the GNN training dynamics introduced by SGD. From a practical standpoint, this translates into lower variance estimates and competitive or superior test accuracy across several benchmarks.","['Deep Learning', 'Graph Learning', 'Bandits']",[],"['Qingru Zhang', 'David Wipf', 'Quan Gan', 'Le Song']","['Institute of Technology', 'Amazon AI Research Lab', 'Amazon', 'College of Computing,  Institute of Technology']","['Georgia', None, None, 'Georgia']"
https://nips.cc/virtual/2021/poster/28389,Fairness & Bias,Combining Latent Space and Structured Kernels for Bayesian Optimization over Combinatorial Spaces,"We consider the problem of optimizing combinatorial spaces (e.g., sequences, trees, and graphs) using expensive black-box function evaluations. For example, optimizing molecules for drug design using physical lab experiments. Bayesian optimization (BO) is an efficient framework for solving such problems by intelligently selecting the inputs with high utility guided by a learned surrogate model. A recent BO approach for combinatorial spaces is through a reduction to BO over continuous spaces by learning a latent representation of structures using deep generative models (DGMs). The selected input from the continuous space is decoded into a discrete structure for performing function evaluation. However, the surrogate model over the latent space only uses the information learned by the DGM, which may not have the desired inductive bias to approximate the target black-box function. To overcome this drawback, this paper proposes a principled approach referred as LADDER. The key idea is to define a novel structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. Our experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method, and performs better or similar to state-of-the-art methods.","['Graph Learning', 'Optimization', 'Generative Model', 'Kernel Methods']",[],"['Aryan Deshwal', 'Jana Doppa']","['Washington State University, Pullman', 'Washington State University, Pullman']","[None, None]"
https://nips.cc/virtual/2021/poster/28373,Fairness & Bias,ROI Maximization in Stochastic Online Decision-Making,"We introduce a novel theoretical framework for Return On Investment (ROI) maximization in repeated decision-making. Our setting is motivated by the use case of companies that regularly receive proposals for technological innovations and want to quickly decide whether they are worth implementing. We design an algorithm for learning ROI-maximizing decision-making policies over a sequence of innovation proposals. Our algorithm provably converges to an optimal policy in class $\Pi$ at a rate of order $\min\big\{1/(N\Delta^2),N^{-1/3}\}$, where $N$ is the number of innovations and $\Delta$ is the suboptimality gap in $\Pi$. A significant hurdle of our formulation, which sets it aside from other online learning problems such as bandits, is that running a policy does not provide an unbiased estimate of its performance.","['Online Learning', 'Bandits']",[],"['Nicolò Cesa-Bianchi', 'Tommaso Cesari', 'Yishay Mansour', 'Vianney Perchet']","['University of Milan', 'University of Ottawa', 'School of Computer Science, Tel Aviv University', 'Ensae ParisTech']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28357,Fairness & Bias,Are My Deep Learning Systems Fair? An Empirical Study of Fixed-Seed Training,"Deep learning (DL) systems have been gaining popularity in critical tasks such as credit evaluation and crime prediction. Such systems demand fairness. Recent work shows that DL software implementations introduce variance: identical DL training runs (i.e., identical network, data, configuration, software, and hardware) with a fixed seed produce different models. Such variance could make DL models and networks violate fairness compliance laws, resulting in negative social impact. In this paper, we conduct the first empirical study to quantify the impact of software implementation on the fairness and its variance of DL systems. Our study of 22 mitigation techniques and five baselines reveals up to 12.6% fairness variance across identical training runs with identical seeds. In addition, most debiasing algorithms have a negative impact on the model such as reducing model accuracy, increasing fairness variance, or increasing accuracy variance. Our literature survey shows that while fairness is gaining popularity in artificial intelligence (AI) related conferences, only 34.4% of the papers use multiple identical training runs to evaluate their approach, raising concerns about their results’ validity. We call for better fairness evaluation and testing protocols to improve fairness and fairness variance of DL systems as well as DL research validity and reproducibility at large.","['Deep Learning', 'Fairness']",[],"['Shangshu Qian', 'Hung Viet Pham', 'Thibaud Lutellier', 'Zeou Hu', 'Jungwon Kim', 'Yaoliang Yu', 'Jiahao Chen', 'Sameena Shah']","['Purdue University', 'University of Waterloo', 'University of Waterloo', 'University of Waterloo', 'Purdue University', 'University of Waterloo', 'Responsible AI LLC', 'J.P. Morgan Chase']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28331,Fairness & Bias,Disentangled Contrastive Learning on Graphs,"Recently, self-supervised learning for graph neural networks (GNNs) has attracted considerable attention because of their notable successes in learning the representation of graph-structure data. However, the formation of a real-world graph typically arises from the highly complex interaction of many latent factors. The existing self-supervised learning methods for GNNs are inherently holistic and neglect the entanglement of the latent factors, resulting in the learned representations suboptimal for downstream tasks and difficult to be interpreted. Learning disentangled graph representations with self-supervised learning poses great challenges and remains largely ignored by the existing literature. In this paper, we introduce the Disentangled Graph Contrastive Learning (DGCL) method, which is able to learn disentangled graph-level representations with self-supervision. In particular, we first identify the latent factors of the input graph and derive its factorized representations. Each of the factorized representations describes a latent and disentangled aspect pertinent to a specific latent factor of the graph. Then we propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the factorized representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our method against several state-of-the-art baselines.","['Graph Learning', 'Self-Supervised Learning', 'Contrastive Learning', 'Deep Learning', 'Representation Learning']",[],"['Haoyang Li', 'Xin Wang', 'Ziwei Zhang', 'Zehuan Yuan', 'Hang Li', 'Wenwu Zhu']","['Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tsinghua University, Tsinghua University', 'Nanjing University', 'ByteDance Technology', 'Tsinghua University, Tsinghua University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28320,Fairness & Bias,Few-Shot Object Detection via Association and DIscrimination,"Object detection has achieved substantial progress in the last decade. However, detecting novel classes with only few samples remains challenging, since deep learning under low data regime usually leads to a degraded feature space. Existing works employ a holistic fine-tuning paradigm to tackle this problem, where the model is first pre-trained on all base classes with abundant samples, and then it is used to carve the novel class feature space. Nonetheless, this paradigm is still imperfect. Durning fine-tuning, a novel class may implicitly leverage the knowledge of multiple base classes to construct its feature space, which induces a scattered feature space, hence violating the inter-class separability. To overcome these obstacles, we propose a two-step fine-tuning framework, Few-shot object detection via Association and DIscrimination (FADI), which builds up a discriminative feature space for each novel class with two integral steps. 1) In the association step, in contrast to implicitly leveraging multiple base classes, we construct a compact novel class feature space via explicitly imitating a specific base class feature space. Specifically, we associate each novel class with a base class according to their semantic similarity. After that, the feature space of a novel class can readily imitate the well-trained feature space of the associated base class. 2) In the discrimination step, to ensure the separability between the novel classes and associated base classes, we disentangle the classification branches for base and novel classes. To further enlarge the inter-class separability between all classes, a set-specialized margin loss is imposed. Extensive experiments on standard Pascal VOC and MS-COCO datasets demonstrate that FADI achieves new state-of-the-art performance, significantly improving the baseline in any shot/split by +18.7. Notably, the advantage of FADI is most announced on extremely few-shot scenarios (e.g. 1- and 3- shot).","['Deep Learning', 'Vision', 'Machine Learning']",[],"['Yuhang Cao', 'Jiaqi Wang', 'Ying Jin', 'Tong Wu', 'Kai Chen', 'Ziwei Liu', 'Dahua Lin']","['The Chinese University of', 'Shanghai AI Laboratory', 'The Chinese University of', 'The Chinese University of', 'Shanghai AI Laboratory', 'Nanyang Technological University', 'The Chinese University of']","['Hong Kong', None, 'Hong Kong', 'Hong Kong', None, None, 'Hong Kong']"
https://nips.cc/virtual/2021/poster/28287,Fairness & Bias,IRM—when it works and when it doesn't: A test case of natural language inference,"Invariant Risk Minimization (IRM) is a recently proposed framework for out-of-distribution (o.o.d) generalization.  Most of the studies on IRM so far have focused on theoretical results, toy problems, and simple models. In this work, we investigate the applicability of IRM to bias mitigation-a special case of o.o.d generalization-in increasingly naturalistic settings and deep models. Using natural language inference (NLI) as a test case, we start with a setting where both the dataset and the bias are synthetic, continue with a natural dataset and synthetic bias, and end with a fully realistic setting with natural datasets and bias. Our results show that in naturalistic settings, learning complex features in place of the bias proves to be difficult, leading to a rather small improvement over empirical risk minimization. Moreover, we find that in addition to being sensitive to random seeds, the performance of IRM also depends on several critical factors, notably dataset size, bias prevalence, and bias strength, thus limiting IRM's advantage in practical scenarios. Our results  highlight key challenges in applying IRM to real-world scenarios, calling for a more naturalistic characterization of  the problem setup for o.o.d generalization.","['Robustness', 'Causality', 'Language']",[],"['Yana Dranker', 'He He', 'Yonatan Belinkov']","['Technion, Technion', 'New York University', 'Technion, Technion']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28268,Fairness & Bias,Robust Generalization despite Distribution Shift via Minimum Discriminating Information,"Training models that perform well under distribution shifts is a central challenge in machine learning. In this paper, we introduce a modeling framework where, in addition to training data, we have partial structural knowledge of the shifted test distribution. We employ the principle of minimum discriminating information to embed the available prior knowledge, and use distributionally robust optimization to account for uncertainty due to the limited samples. By leveraging large deviation results, we obtain explicit generalization bounds with respect to the unknown shifted distribution. Lastly, we demonstrate the versatility of our framework by demonstrating it on two rather distinct applications: (1) training classifiers on systematically biased data and (2) off-policy evaluation in Markov Decision Processes.","['Optimization', 'Machine Learning']",[],"['Tobias Sutter', 'Andreas Krause', 'Daniel Kuhn']","['Universität Konstanz', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology Lausanne']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26507,Fairness & Bias,Heuristic-Guided Reinforcement Learning,"We provide a framework to accelerate reinforcement learning (RL) algorithms by heuristics that are constructed by domain knowledge or offline data.  Tabula rasa RL algorithms require environment interactions or computation that scales with the horizon of the sequential decision-making task.  Using our framework, we show how heuristic-guided RL induces a much shorter horizon sub-problem that provably solves the original task. Our framework can be viewed as a horizon-based regularization for controlling bias and variance in RL under a finite interaction budget.  In theory, we characterize the properties of a good heuristic and the resulting impact on RL acceleration. In particular, we introduce the novel concept of an improvable heuristic that can allow any RL agent to conservatively extrapolate beyond its prior knowledge.  In practice, we instantiate our framework to accelerate several state-of-the-art algorithms in simulated robotic control tasks and procedurally generated games. Our framework complements the rich literature on warm-starting RL using expert demonstrations or exploratory data-sets, and creates a unified channel to inject prior knowledge into RL.","['Reinforcement Learning and Planning', 'Theory']",[],"['Ching-An Cheng', 'Adith Swaminathan']","['Microsoft Research', 'Microsoft']","[None, None]"
https://nips.cc/virtual/2021/poster/28232,Fairness & Bias,Mirror Langevin Monte Carlo: the Case Under Isoperimetry,"Motivated by the connection between sampling and optimization, we study a mirror descent analogue of Langevin dynamics and analyze three different discretization schemes, giving nonasymptotic convergence rate under functional inequalities such as Log-Sobolev in the corresponding metric. Compared to the Euclidean setting, the result reveals intricate relationship between the underlying geometry and the target distribution and suggests that care might need to be taken in order for the discretized algorithm to achieve vanishing bias with diminishing stepsize for sampling from potentials under weaker smoothness/convexity regularity conditions.",['Optimization'],[],[],[],[]
https://nips.cc/virtual/2021/poster/28197,Fairness & Bias,Noise2Score: Tweedie’s Approach to Self-Supervised Image Denoising without Clean Images,"Recently, there has  been extensive research interest in training  deep networks to denoise images without clean reference. However, the representative approaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator (SURE), etc.  seem to differ from one another and it is difficult to find the coherent mathematical structure. To address this, here we present a novel approach, called Noise2Score, which reveals a missing link in order to unite these seemingly different approaches. Specifically, we  show that   image denoising  problems  without clean images can be addressed by finding the mode of the posterior distribution and that the Tweedie's formula offers an explicit solution through the score function (i.e. the gradient of loglikelihood). Our method then uses the  recent finding that  the score function  can be stably estimated from the noisy images using the amortized residual denoising autoencoder, the method of which is closely related to Noise2Noise or Nose2Void. Our Noise2Score approach is so universal  that the same network training can be used to remove noises from images that are corrupted by any exponential family distributions and noise parameters. Using extensive  experiments with Gaussian, Poisson, and Gamma noises, we show  that  Noise2Score significantly outperforms the state-of-the-art self-supervised denoising methods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc.",[],[],"['Kwanyoung Kim', 'Jong Chul Ye']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/28173,Fairness & Bias,Mixture weights optimisation for Alpha-Divergence Variational Inference,"This paper focuses on $\alpha$-divergence minimisation methods for Variational Inference. More precisely, we are interested in algorithms optimising the mixture weights of any given mixture model, without any information on the underlying distribution of its mixture components parameters. The Power Descent, defined for all $\alpha \neq 1$, is one such algorithm and we establish in our work the full proof of its convergence towards the optimal mixture weights when $\alpha <1$. Since the $\alpha$-divergence recovers the widely-used forward Kullback-Leibler when $\alpha \to 1$, we then extend the Power Descent to the case $\alpha = 1$ and show that we obtain an Entropic Mirror Descent. This leads us to investigate the link between Power Descent and Entropic Mirror Descent: first-order approximations allow us to introduce the R\'{e}nyi Descent, a novel algorithm for which we prove an $O(1/N)$ convergence rate. Lastly, we compare numerically the behavior of the unbiased Power Descent and of the biased R\'{e}nyi Descent and we discuss the potential advantages of one algorithm over the other.",['Generative Model'],[],"['Kamelia Daudel', 'randal douc']","['Oxford, University of Oxford', 'Telecom Sudparis']","[None, None]"
https://nips.cc/virtual/2021/poster/28153,Fairness & Bias,Addressing Algorithmic Disparity and Performance Inconsistency in Federated Learning,"Federated learning (FL) has gain growing interests for its capability of learning from distributed data sources collectively without the need of accessing the raw data samples across different sources. So far FL research has mostly focused on improving the performance, how the algorithmic disparity will be impacted for the model learned from FL and the impact of algorithmic disparity on the utility inconsistency are largely unexplored. In this paper, we propose an FL framework to jointly consider performance consistency and algorithmic fairness across different local clients (data sources). We derive our framework from a constrained multi-objective optimization perspective, in which we learn a model satisfying fairness constraints on all clients with consistent performance. Specifically, we treat the algorithm prediction loss at each local client as an objective and maximize the worst-performing client with fairness constraints through optimizing a surrogate maximum function with all objectives involved. A gradient-based procedure is employed to achieve the Pareto optimality of this optimization problem. Theoretical analysis is provided to prove that our method can converge to a Pareto solution that achieves the min-max performance with fairness constraints on all clients. Comprehensive experiments on synthetic and real-world datasets demonstrate the superiority that our approach over baselines and its effectiveness in achieving both fairness and consistency across all local clients.","['Federated Learning', 'Fairness', 'Optimization']",[],"['Sen Cui', 'Weishen Pan', 'Jian Liang', 'Changshui Zhang', 'Fei Wang']","['Tsinghua University, Tsinghua University', 'Weill Cornell Medicine, Cornell University', 'Alibaba Group', 'Tsinghua University', 'Cornell University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28140,Fairness & Bias,ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning,"Existing semi-supervised learning (SSL) algorithms typically assume class-balanced datasets, although the class distributions of many real world datasets are imbalanced. In general, classifiers trained on a class-imbalanced dataset are biased toward the majority classes. This issue becomes more problematic for SSL algorithms because they utilize the biased prediction of unlabeled data for training. However, traditional class-imbalanced learning techniques, which are designed for labeled data, cannot be readily combined with SSL algorithms. We propose a scalable class-imbalanced SSL algorithm that can effectively use unlabeled data, while mitigating class imbalance by introducing an auxiliary balanced classifier (ABC) of a single layer, which is attached to a representation layer of an existing SSL algorithm. The ABC is trained with a class-balanced loss of a minibatch, while using high-quality representations learned from all data points in the minibatch using the backbone SSL algorithm to avoid overfitting and information loss. Moreover, we use consistency regularization, a recent SSL technique for utilizing unlabeled data in a modified way, to train the ABC to be balanced among the classes by selecting unlabeled data with the same probability for each class. The proposed algorithm achieves state-of-the-art performance in various class-imbalanced SSL experiments using four benchmark datasets.",['Semi-Supervised Learning'],[],"['Hyuck Lee', 'Seungjae Shin']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/28139,Fairness & Bias,Boosted CVaR Classification,"Many modern machine learning tasks require models with high tail performance, i.e. high performance over the worst-off samples in the dataset. This problem has been widely studied in fields such as algorithmic fairness, class imbalance, and risk-sensitive decision making. A popular approach to maximize the model's tail performance is to minimize the CVaR (Conditional Value at Risk) loss, which computes the average risk over the tails of the loss. However, for classification tasks where models are evaluated by the zero-one loss, we show that if the classifiers are deterministic, then the minimizer of the average zero-one loss also minimizes the CVaR zero-one loss, suggesting that CVaR loss minimization is not helpful without additional assumptions. We circumvent this negative result by minimizing the CVaR loss over randomized classifiers, for which the minimizers of the average zero-one loss and the CVaR zero-one loss are no longer the same, so minimizing the latter can lead to better tail performance. To learn such randomized classifiers, we propose the Boosted CVaR Classification framework which is motivated by a direct relationship between CVaR and a classical boosting algorithm called LPBoost. Based on this framework, we design an algorithm called $\alpha$-AdaLPBoost. We empirically evaluate our proposed algorithm on four benchmark datasets and show that it achieves higher tail performance than deterministic model training methods.","['Fairness', 'Machine Learning']",[],"['Runtian Zhai', 'Chen Dan', 'Arun Suggala', 'J Zico Kolter', 'Pradeep Kumar Ravikumar']","['Carnegie Mellon University', 'Toyota Technological Institute at Chicago', 'Google', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28125,Fairness & Bias,Assessing Fairness in the Presence of Missing Data,"Missing data are prevalent and present daunting challenges in real data analysis. While there is a growing body of literature on fairness in analysis of fully observed data, there has been little theoretical work on investigating fairness in analysis of incomplete data. In practice, a popular analytical approach for dealing with missing data is to use only the set of complete cases, i.e., observations with all features fully observed to train a prediction algorithm. However, depending on the missing data mechanism, the distribution of complete cases and the distribution of the complete data may be substantially different. When the goal is to develop a fair algorithm in the complete data domain where there are no missing values, an algorithm that is fair in the complete case domain may show disproportionate bias towards some marginalized groups in the complete data domain. To fill this significant gap, we study the problem of estimating fairness in the complete data domain for an arbitrary model evaluated merely using complete cases. We provide upper and lower bounds on the fairness estimation error and conduct numerical experiments to assess our theoretical results. Our work provides the first known theoretical results on fairness guarantee in analysis of incomplete data.","['Fairness', 'Domain Adaptation']",[],"['Yiliang Zhang', 'Qi Long']","['University of Pennsylvania', 'University of Pennsylvania']","[None, None]"
https://nips.cc/virtual/2021/poster/28081,Fairness & Bias,Learning to Draw: Emergent Communication through Sketching,"Evidence that visual communication preceded written language and provided a basis for it goes back to prehistory, in forms such as cave and rock paintings depicting traces of our distant ancestors. Emergent communication research has sought to explore how agents can learn to communicate in order to collaboratively solve tasks. Existing research has focused on language, with a learned communication channel transmitting sequences of discrete tokens between the agents. In this work, we explore a visual communication channel between agents that are allowed to draw with simple strokes. Our agents are parameterised by deep neural networks, and the drawing procedure is differentiable, allowing for end-to-end training. In the framework of a referential communication game, we demonstrate that agents can not only successfully learn to communicate by drawing, but with appropriate inductive biases, can do so in a fashion that humans can interpret. We hope to encourage future research to consider visual communication as a more flexible and directly interpretable alternative of training collaborative agents.","['Deep Learning', 'Interpretability']",[],"['Daniela Mihai', 'Jonathon Hare']","['University of Southampton', 'University of Southampton']","[None, None]"
https://nips.cc/virtual/2021/poster/28075,Fairness & Bias,Can contrastive learning avoid shortcut solutions?,"The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via “shortcuts"", i.e., by inadvertently suppressing important predictive features.  We find that feature extraction is influenced by  the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks.","['Self-Supervised Learning', 'Contrastive Learning']",[],"['Joshua David Robinson', 'Li Sun', 'Ke Yu', 'kayhan Batmanghelich', 'Stefanie Jegelka', 'Suvrit Sra']","['Stanford University', 'Boston University', 'University of Pittsburgh', 'Boston University, Boston University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28044,Fairness & Bias,Learning a Single Neuron with Bias Using Gradient Descent,"We theoretically study the fundamental problem of learning a single neuron with a bias term ($\mathbf{x}\mapsto \sigma(\langle\mathbf{w},\mathbf{x}\rangle + b)$) in the realizable setting with the ReLU activation, using gradient descent. Perhaps surprisingly, we show that this is a significantly different and more challenging problem than the bias-less case (which was the focus of previous works on single neurons), both in terms of the optimization geometry as well as the ability of gradient methods to succeed in some scenarios. We provide a detailed study of this problem, characterizing the critical points of the objective, demonstrating failure cases, and providing positive convergence guarantees under different sets of assumptions. To prove our results, we develop some tools which may be of independent interest, and improve previous results on learning single neurons.",['Optimization'],[],"['Gal Vardi', 'Gilad Yehudai', 'Ohad Shamir']","['Toyota Technological Institute at Chicago', 'Weizmann Institute', 'Weizmann Institute']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27957,Fairness & Bias,Finite-Sample Analysis of Off-Policy TD-Learning via Generalized Bellman Operators,"In TD-learning, off-policy sampling is known to be more practical than on-policy sampling, and by decoupling learning from data collection, it enables data reuse. It is known that policy evaluation has the interpretation of solving a generalized Bellman equation. In this paper, we derive finite-sample bounds for any general off-policy TD-like stochastic approximation algorithm that solves for the fixed-point of this generalized Bellman operator. Our key step is to show that the generalized Bellman operator is simultaneously a contraction mapping with respect to a weighted $\ell_p$-norm for each $p$ in $[1,\infty)$, with a common contraction factor. Off-policy TD-learning is known to suffer from  high variance due to the product of importance sampling ratios. A number of algorithms (e.g. $Q^\pi(\lambda)$, Tree-Backup$(\lambda)$, Retrace$(\lambda)$, and $Q$-trace) have been proposed in the literature to address this issue. Our results immediately imply finite-sample bounds of these algorithms. In particular, we provide first-known finite-sample guarantees for $Q^\pi(\lambda)$, Tree-Backup$(\lambda)$, and Retrace$(\lambda)$, and improve the best known bounds of $Q$-trace in \citep{chen2021finite}. Moreover, we show the bias-variance trade-offs in each of these algorithms.",[],[],"['Siva Theja Maguluri', 'Sanjay Shakkottai', 'Karthikeyan Shanmugam']","['Institute of Technology', 'University of Texas, Austin', 'Google']","['Georgia', None, None]"
https://nips.cc/virtual/2021/poster/27946,Fairness & Bias,Debiased Visual Question Answering from Feature and Sample Perspectives,"Visual question answering (VQA) is designed to examine the visual-textual reasoning ability of an intelligent agent. However, recent observations show that many VQA models may only capture the biases between questions and answers in a dataset rather than showing real reasoning abilities. For example, given a question, some VQA models tend to output the answer that occurs frequently in the dataset and ignore the images. To reduce this tendency, existing methods focus on weakening the language bias. Meanwhile, only a few works also consider vision bias implicitly. However, these methods introduce additional annotations or show unsatisfactory performance. Moreover, not all biases are harmful to the models. Some “biases” learnt from datasets represent natural rules of the world and can help limit the range of answers. Thus, how to filter and remove the true negative biases in language and vision modalities remain a major challenge. In this paper, we propose a method named D-VQA to alleviate the above challenges from the feature and sample perspectives. Specifically, from the feature perspective, we build a question-to-answer and vision-to-answer branch to capture the language and vision biases, respectively. Next, we apply two unimodal bias detection modules to explicitly recognise and remove the negative biases. From the sample perspective, we construct two types of negative samples to assist the training of the models, without introducing additional annotations. Extensive experiments on the VQA-CP v2 and VQA v2 datasets demonstrate the effectiveness of our D-VQA method.",['Vision'],[],"['Zhiquan Wen', 'Guanghui Xu', 'Mingkui Tan', 'Qingyao Wu', 'Qi Wu']","['South  University of Technology', 'South  University of Technology, Tsinghua University', 'South  University of Technology', 'South  University of Technology', 'The University of Adelaide']","['China', 'China', 'China', 'China', None]"
https://nips.cc/virtual/2021/poster/27944,Fairness & Bias,Capacity and Bias of Learned Geometric Embeddings for Directed Graphs,"A wide variety of machine learning tasks such as knowledge base completion, ontology alignment, and multi-label classification can benefit from incorporating into learning differentiable representations of graphs or taxonomies.  While vectors in Euclidean space can theoretically represent any graph, much recent work shows that alternatives such as complex, hyperbolic, order, or box embeddings have geometric properties better suited to modeling real-world graphs. Experimentally these gains are seen only in lower dimensions, however, with performance benefits diminishing in higher dimensions. In this work, we introduce a novel variant of box embeddings that uses a learned smoothing parameter to achieve better representational capacity than vector models in low dimensions, while also avoiding performance saturation common to other geometric models in high dimensions. Further, we present theoretical results that prove box embeddings can represent any DAG. We perform rigorous empirical evaluations of vector, hyperbolic, and region-based geometric representations on several families of synthetic and real-world directed graphs. Analysis of these results exposes correlations between different families of graphs, graph characteristics, model size, and embedding geometry, providing useful insights into the inductive biases of various differentiable graph representations.","['Graph Learning', 'Machine Learning', 'Representation Learning']",[],"['Michael Boratko', 'Dongxu Zhang', 'Nicholas Monath', 'Luke Vilnis', 'Kenneth L. Clarkson', 'Andrew McCallum']","['Google', 'ASAPP, Inc.', 'Google', 'Google', 'International Business Machines', 'Department of Computer Science, University of Massachusetts, Amherst']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27930,Fairness & Bias,Fast Abductive Learning by Similarity-based Consistency Optimization,"To utilize the raw inputs and symbolic knowledge simultaneously, some recent neuro-symbolic learning methods use abduction, i.e., abductive reasoning, to integrate sub-symbolic perception and logical inference. While the perception model, e.g., a neural network, outputs some facts that are inconsistent with the symbolic background knowledge base, abduction can help revise the incorrect perceived facts by minimizing the inconsistency between them and the background knowledge. However, to enable effective abduction, previous approaches need an initialized perception model that discriminates the input raw instances. This limits the application of these methods, as the discrimination ability is usually acquired from a thorough pre-training when the raw inputs are difficult to classify. In this paper, we propose a novel abduction strategy, which leverages the similarity between samples, rather than the output information by the perceptual neural network, to guide the search in abduction. Based on this principle, we further present ABductive Learning with Similarity (ABLSim) and apply it to some difficult neuro-symbolic learning tasks. Experiments show that the efficiency of ABLSim is significantly higher than the state-of-the-art neuro-symbolic methods, allowing it to achieve better performance with less labeled data and weaker domain knowledge.","['Deep Learning', 'Optimization']",[],"['Yu-Xuan Huang', 'Wang-Zhou Dai', 'Le-Wen Cai', 'Yuan Jiang']","['Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27908,Fairness & Bias,Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial,"In this paper, we answer the question of when inserting label noise (less informative labels) can instead return us more accurate and fair models. We are primarily inspired by three observations: 1) In contrast to reducing label noise rates, increasing the noise rates is easy to implement; 2) Increasing a certain class of instances' label noise to balance the noise rates (increasing-to-balancing) results in an easier learning problem; 3) Increasing-to-balancing improves fairness guarantees against label bias. In this paper, we first quantify the trade-offs introduced by increasing a certain group of instances' label noise rate w.r.t. the loss of label informativeness and the lowered learning difficulties. We analytically demonstrate when such an increase is beneficial, in terms of either improved generalization power or the fairness guarantees. Then we present a method to insert label noise properly for the task of learning with noisy labels, either without or with a fairness constraint. The primary technical challenge we face is due to the fact that we would not know which data instances are suffering from higher noise, and we would not have the ground truth labels to verify any possible hypothesis. We propose a detection method that informs us which group of labels might suffer from higher noise without using ground truth labels. We formally establish the effectiveness of the proposed solution and demonstrate it with extensive experiments.","['Fairness', 'Machine Learning']",[],"['Yang Liu', 'Jialu Wang']","['University of California, Santa Cruz', 'University of California, Santa Cruz']","[None, None]"
https://nips.cc/virtual/2021/poster/27881,Fairness & Bias,A mechanistic multi-area recurrent network model of decision-making,"Recurrent neural networks (RNNs) trained on neuroscience-based tasks have been widely used as models for cortical areas performing analogous tasks. However, very few tasks involve a single cortical area, and instead require the coordination of multiple brain areas. Despite the importance of multi-area computation, there is a limited understanding of the principles underlying such computation. We propose to use multi-area RNNs with neuroscience-inspired architecture constraints to derive key features of multi-area computation. In particular, we show that incorporating multiple areas and Dale's Law is critical for biasing the networks to learn biologically plausible solutions. Additionally, we leverage the full observability of the RNNs to show that output-relevant information is preferentially propagated between areas. These results suggest that cortex uses modular computation to generate minimal sufficient representations of task information. More broadly, our results suggest that constrained multi-area RNNs can produce experimentally testable hypotheses for computations that occur within and across multiple brain areas, enabling new insights into distributed computation in neural systems.","['Deep Learning', 'Interpretability', 'Neuroscience']",[],"['Michael Kleinman', 'Chandramouli Chandrasekaran', 'Jonathan Kao']","['Stanford University', 'Boston University', 'University of California, Los Angeles']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27858,Fairness & Bias,The Inductive Bias of Quantum Kernels,"It has been hypothesized that quantum computers may lend themselves well to applications in machine learning. In the present work, we analyze function classes defined via quantum kernels. Quantum computers offer the possibility to efficiently compute inner products of exponentially large density operators that are classically hard to compute. However, having an exponentially large feature space renders the problem of generalization hard. Furthermore, being able to evaluate inner products in high dimensional spaces efficiently by itself does not guarantee a quantum advantage, as already classically tractable kernels can correspond to high- or infinite-dimensional reproducing kernel Hilbert spaces (RKHS). We analyze the spectral properties of quantum kernels and find that we can expect an advantage if their RKHS is low dimensional and contains functions that are hard to compute classically. If the target function is known to lie in this class, this implies a quantum advantage, as the quantum computer can encode this inductive bias, whereas there is no classically efficient way to constrain the function class in the same way. However, we show that finding suitable quantum kernels is not easy because the kernel evaluation might require exponentially many measurements. In conclusion, our message is a somewhat sobering one: we conjecture that quantum machine learning models can offer speed-ups only if we manage to encode knowledge about the problem at hand into quantum circuits, while encoding the same bias into a classical model would be hard. These situations may plausibly occur when learning on data generated by a quantum process, however, they appear to be harder to come by for classical datasets.","['Theory', 'Machine Learning', 'Kernel Methods']",[],"['Jonas M. Kübler', 'Simon Buchholz']","['Amazon', 'Max-Planck Institute']","[None, None]"
https://nips.cc/virtual/2021/poster/27850,Fairness & Bias,Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data,"Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efficiency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adaptation without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA. HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target samples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Specifically, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups.","['Domain Adaptation', 'Privacy', 'Transfer Learning', 'Machine Learning', 'Contrastive Learning']",[],"['Jiaxing Huang', 'Aoran Xiao', 'Shijian Lu']","['Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27827,Fairness & Bias,Control Variates for Slate Off-Policy Evaluation,"We study the problem of off-policy evaluation from batched contextual bandit data with multidimensional actions, often termed slates. The problem is common to recommender systems and user-interface optimization, and it is particularly challenging because of the combinatorially-sized action space. Swaminathan et al. (2017) have proposed the pseudoinverse (PI) estimator under the assumption that the conditional mean rewards are additive in actions. Using control variates, we consider a large class of unbiased estimators that includes as specific cases the PI estimator and (asymptotically) its self-normalized variant. By optimizing over this class, we obtain new estimators with risk improvement guarantees over both the PI and the self-normalized PI estimators. Experiments with real-world recommender data as well as synthetic data validate these improvements in practice.","['Optimization', 'Bandits']",[],"['Nikos Vlassis', 'Ashok Chandrashekar', 'Fernando Amat', 'Nathan Kallus']","['Adobe Systems', 'Warner Media', 'Google', 'Cornell University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27810,Fairness & Bias,Understanding the Under-Coverage Bias in Uncertainty Estimation,"Estimating the data uncertainty in regression tasks is often done by learning a quantile function or a prediction interval of the true label conditioned on the input. It is frequently observed that quantile regression---a vanilla algorithm for learning quantiles with asymptotic guarantees---tends to *under-cover* than the desired coverage level in reality. While various fixes have been proposed, a more fundamental understanding of why this under-coverage bias happens in the first place remains elusive. In this paper, we present a rigorous theoretical study on the coverage of uncertainty estimation algorithms in learning quantiles. We prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for $\alpha>0.5$ and small $d/n$, the $\alpha$-quantile learned by quantile regression roughly achieves coverage $\alpha - (\alpha-1/2)\cdot d/n$ regardless of the noise distribution, where $d$ is the input dimension and $n$ is the number of training data. Our theory reveals that this under-coverage bias stems from a certain high-dimensional parameter estimation error that is not implied by existing theories on quantile regression. Experiments on simulated and real data verify our theory and further illustrate the effect of various factors such as sample size and model capacity on the under-coverage bias in more practical setups.",['Theory'],[],"['Yu Bai', 'Song Mei', 'Huan Wang', 'Caiming Xiong']","['Salesforce Research', 'University of California Berkeley', 'SalesForce.com', 'Salesforce Research']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27791,Fairness & Bias,Iterative Causal Discovery in the Possible Presence of Latent Confounders and Selection Bias,"We present a sound and complete algorithm, called iterative causal discovery (ICD), for recovering causal graphs in the presence of latent confounders and selection bias. ICD relies on the causal Markov and faithfulness assumptions and recovers the equivalence class of the underlying causal graph. It starts with a complete graph, and consists of a single iterative stage that gradually refines this graph by identifying conditional independence (CI) between connected nodes. Independence and causal relations entailed after any iteration are correct, rendering ICD anytime. Essentially, we tie the size of the CI conditioning set to its distance on the graph from the tested nodes, and increase this value in the successive iteration. Thus, each iteration refines a graph that was recovered by previous iterations having smaller conditioning sets---a higher statistical power---which contributes to stability. We demonstrate empirically that ICD requires significantly fewer CI tests and learns more accurate causal graphs compared to FCI, FCI+, and RFCI algorithms.","['Graph Learning', 'Causality']",[],"['Raanan Yehezkel Rohekar', 'Shami Nisimov', 'Yaniv Gurwicz', 'Gal Novik']","['Intel Corporation', 'Intel corporation', 'Intel', 'Intel Corporation']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27784,Fairness & Bias,ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias,"Transformers have shown great potential in various computer vision tasks owing to their strong capability in modeling long-range dependency using the self-attention mechanism. Nevertheless, vision transformers treat an image as 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance. Alternatively, they require large-scale training data and longer training schedules to learn the IB implicitly. In this paper, we propose a new Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of ViTAE over the baseline transformer and concurrent works. Source code and pretrained models will be available at https://github.com/Annbless/ViTAE.","['Transformers', 'Vision', 'Machine Learning']",[],"['Yufei Xu', 'Qiming ZHANG', 'Jing Zhang', 'Dacheng Tao']","['The University of Sydney, University of Sydney', 'University of Sydney, University of Sydney', 'The University of Sydney', 'University of Sydney']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27740,Fairness & Bias,Supervising the Transfer of Reasoning Patterns in VQA,"Methods for Visual Question Anwering (VQA) are notorious for leveraging dataset biases rather than performing reasoning, hindering generalization. It has been recently shown that better reasoning patterns emerge in attention layers of a state-of-the-art VQA model when they are trained on perfect (oracle) visual inputs. This provides evidence that deep neural networks can learn to reason when training conditions are favorable enough. However, transferring this learned knowledge to deployable models is a challenge, as much of it is lost during the transfer. We propose a method for knowledge transfer based on a regularization term in our loss function, supervising the sequence of required reasoning operations. We provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. We also demonstrate the effectiveness of this approach experimentally on the GQA dataset and show its complementarity to BERT-like self-supervised pre-training.","['Theory', 'Deep Learning', 'Vision']",[],"['Corentin Kervadec', 'Christian Wolf', 'Grigory Antipov', 'Moez Baccouche', 'Madiha Nadri']","['Universitat Pompeu Fabra', 'Naver Labs Europe', 'Orange-labs', 'French Ministry of the Armed Forces', 'Université Claude Bernard Lyon1 - LAGEPP']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27732,Fairness & Bias,The Implicit Bias of Minima Stability: A View from Function Space,"The loss terrains of over-parameterized neural networks have multiple global minima. However, it is well known that stochastic gradient descent (SGD) can stably converge only to minima that are sufficiently flat w.r.t. SGD's step size. In this paper we study the effect that this mechanism has on the function implemented by the trained model. First, we extend the existing knowledge on minima stability to non-differentiable minima, which are common in ReLU nets. We then use our stability results to study a single hidden layer univariate ReLU network. In this setting, we show that SGD is biased towards functions whose second derivative (w.r.t the input) has a bounded weighted $L_1$ norm, and this is regardless of the initialization. In particular, we show that the function implemented by the network upon convergence gets smoother as the learning rate increases. The weight multiplying the second derivative is larger around the center of the support of the training distribution, and smaller towards its boundaries, suggesting that a trained model tends to be smoother at the center of the training distribution.","['Deep Learning', 'Optimization']",[],"['Rotem Mulayoff', 'Tomer Michaeli', 'Daniel Soudry']","['Technion', 'Technion, Technion', 'Technion -  Institute of Technology, Technion']","[None, None, 'Israel']"
https://nips.cc/virtual/2021/poster/27721,Fairness & Bias,What can linearized neural networks actually say about generalization?,"For certain infinitely-wide neural networks, the neural tangent kernel (NTK) theory fully characterizes generalization, but for the networks used in practice, the empirical NTK only provides a rough first-order approximation. Still, a growing body of work keeps leveraging this approximation to successfully analyze important deep learning phenomena and design algorithms for new applications. In our work, we provide strong empirical evidence to determine the practical validity of such approximation by conducting a systematic comparison of the behavior of different neural networks and their linear approximations on different tasks. We show that the linear approximations can indeed rank the learning complexity of certain tasks for neural networks, even when they achieve very different performances. However, in contrast to what was previously reported, we discover that neural networks do not always perform better than their kernel approximations, and reveal that the performance gap heavily depends on architecture, dataset size and training task. We discover that networks overfit to these tasks mostly due to the evolution of their kernel during training, thus, revealing a new type of implicit bias.","['Theory', 'Deep Learning']",[],"['Guillermo Ortiz-Jimenez', 'Seyed-Mohsen Moosavi-Dezfooli', 'Pascal Frossard']","['Google DeepMind', 'Imperial College London, Imperial College London', 'EPFL']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27710,Fairness & Bias,Variational Multi-Task Learning with Gumbel-Softmax Priors,"Multi-task learning aims to explore task relatedness to improve individual tasks, which is of particular significance in the challenging scenario that only limited data is available for each task. To tackle this challenge, we propose variational multi-task learning (VMTL), a general probabilistic inference framework for learning multiple related tasks. We cast multi-task learning as a variational Bayesian inference problem, in which task relatedness is explored in a unified manner by specifying priors. To incorporate shared knowledge into each task, we design the prior of a task to be a learnable mixture of the variational posteriors of other related tasks, which is learned by the Gumbel-Softmax technique. In contrast to previous methods, our VMTL can exploit task relatedness for both representations and classifiers in a principled way by jointly inferring their posteriors. This enables individual tasks to fully leverage inductive biases provided by related tasks, therefore improving the overall performance of all tasks. Experimental results demonstrate that the proposed VMTL is able to effectively tackle a variety of challenging multi-task learning settings with limited training data for both classification and regression. Our method consistently surpasses previous methods, including strong Bayesian approaches, and achieves state-of-the-art performance on five benchmark datasets.","['Generative Model', 'Machine Learning']",[],"['Jiayi Shen', 'Xiantong Zhen', 'Marcel Worring', 'Ling Shao']","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam', 'Terminus Group']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27688,Fairness & Bias,Wisdom of the Crowd Voting: Truthful Aggregation of Voter Information and Preferences,"We consider two-alternative elections where voters' preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. As a special case, our model captures the common scenario where voters can be categorized into three types: those who always prefer one alternative, those who always prefer the other, and those contingent voters whose preferences depends on the state.  In this setting, even if every voter is a contingent voter, agents voting according to their private information need not result in the adoption of the universally preferred alternative, because the signals can be systematically biased. We present a mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority.  In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium (where no coalition of voters can deviate and receive a better outcome).",[],[],"['Grant Schoenebeck', 'Biaoshuai Tao']","['University of Michigan', 'Shanghai Jiao Tong University']","[None, None]"
https://nips.cc/virtual/2021/poster/27665,Fairness & Bias,Fast Federated Learning in the Presence of Arbitrary Device Unavailability,"Federated learning (FL) coordinates with numerous heterogeneous devices to collaboratively train a shared model while preserving user privacy. Despite its multiple advantages, FL faces new challenges. One challenge arises when devices drop out of the training process. In this case, the convergence of popular FL algorithms such as FedAvg is severely influenced by the straggling devices. To tackle this challenge, we study federated learning algorithms in the presence of arbitrary device unavailability and propose an algorithm named Memory-augmented Impatient Federated Averaging (MIFA). Our algorithm efficiently avoids excessive latency induced by inactive devices, and corrects the gradient bias using the memorized latest updates from them. We prove that MIFA achieves minimax optimal convergence rates on non-i.i.d. data for both strongly convex and non-convex smooth functions. We also provide an explicit characterization of the improvement over baseline algorithms through a case study, and validate the results by numerical experiments on real-world datasets.","['Federated Learning', 'Optimization', 'Privacy']",[],"['Xinran Gu', 'Kaixuan Huang', 'Jingzhao Zhang', 'Longbo Huang']","['Tsinghua University, Tsinghua University', 'Princeton University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27659,Fairness & Bias,Estimating Multi-cause Treatment Effects via Single-cause Perturbation,"Most existing methods for conditional average treatment effect estimation are designed to estimate the effect of a single cause - only one variable can be intervened on at one time. However, many applications involve simultaneous intervention on multiple variables, which leads to multi-cause treatment effect problems. The multi-cause problem is challenging because one needs to overcome the confounding bias for a large number of treatment groups, each with a different cause combination. The combinatorial nature of the problem also leads to severe data scarcity - we only observe one factual outcome out of many potential outcomes. In this work, we propose Single-cause Perturbation (SCP), a novel two-step procedure to estimate the multi-cause treatment effect. SCP starts by augmenting the observational dataset with the estimated potential outcomes under single-cause interventions. It then performs covariate adjustment on the augmented dataset to obtain the estimator. SCP is agnostic to the exact choice of algorithm in either step. We show formally that the procedure is valid under standard assumptions in causal inference. We demonstrate the performance gain of SCP on extensive synthetic and semi-synthetic experiments.",['Causality'],[],"['Zhaozhi Qian', 'Alicia Curth', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27614,Fairness & Bias,Limiting fluctuation and trajectorial stability of multilayer neural networks with mean field training,"The mean field theory of multilayer neural networks centers around a particular infinite-width scaling, in which the learning dynamics is shown to be closely tracked by the mean field limit. A random fluctuation around this infinite-width limit is expected from a large-width expansion to the next order. This fluctuation has been studied only in the case of shallow networks, where previous works employ heavily technical notions or additional formulation ideas amenable only to that case. Treatment of the multilayer case has been missing, with the chief difficulty in finding a formulation that must capture the stochastic dependency across not only time but also depth. In this work, we initiate the study of the fluctuation in the case of multilayer networks, at any network depth. Leveraging on the neuronal embedding framework recently introduced by Nguyen and Pham, we systematically derive a system of dynamical equations, called the second-order mean field limit, that captures the limiting fluctuation distribution. We demonstrate through the framework the complex interaction among neurons in this second-order mean field limit, the stochasticity with cross-layer dependency and the nonlinear time evolution inherent in the limiting fluctuation. A limit theorem is proven to relate quantitatively this limit to the fluctuation realized by large-width networks. We apply the result to show a stability property of gradient descent mean field training: in the large-width regime, along the training trajectory, it progressively biases towards a solution with ""minimal fluctuation"" (in fact, vanishing fluctuation) in the learned output function, even after the network has been initialized at or has converged (sufficiently fast) to a global optimum. This extends a similar phenomenon previously shown only for shallow networks with a squared loss in the empirical risk minimization setting, to multilayer networks with a loss function that is not necessarily convex in a more general setting.","['Theory', 'Deep Learning', 'Optimization']",[],"['Huy Tuan Pham', 'Phan-Minh Nguyen']","['Stanford University', 'The Voleon Group']","[None, None]"
https://nips.cc/virtual/2021/poster/27604,Fairness & Bias,An Information-theoretic Approach to Distribution Shifts,"Safely deploying machine learning models to the real world is often a challenging process. For example, models trained with data obtained from a specific geographic location tend to fail when queried with data obtained elsewhere, agents trained in a simulation can struggle to adapt when deployed in the real world or novel environments, and neural networks that are fit to a subset of the population might carry some selection bias into their decision process. In this work, we describe the problem of data shift from an information-theoretic perspective by (i) identifying and describing the different sources of error, (ii) comparing some of the most promising objectives explored in the recent domain generalization and fair classification literature. From our theoretical analysis and empirical evaluation, we conclude that the model selection procedure needs to be guided by careful considerations regarding the observed data, the factors used for correction, and the structure of the data-generating process.","['Graph Learning', 'Domain Adaptation', 'Machine Learning', 'Theory', 'Deep Learning', 'Representation Learning']",[],"['Marco Federici', 'Ryota Tomioka']","['University of Amsterdam', 'Microsoft Research Cambridge']","[None, None]"
https://nips.cc/virtual/2021/poster/27602,Fairness & Bias,Offline Reinforcement Learning with Reverse Model-based Imagination,"In offline reinforcement learning (offline RL), one of the main challenges is to deal with the distributional shift between the learning policy and the given dataset. To address this problem,  recent offline RL methods attempt to introduce conservatism bias to encourage learning in high-confidence areas. Model-free approaches directly encode such bias into policy or value function learning using conservative regularizations or special network structures, but their constrained policy search limits the generalization beyond the offline dataset. Model-based approaches learn forward dynamics models with conservatism quantifications and then generate imaginary trajectories to extend the offline datasets. However, due to limited samples in offline datasets, conservatism quantifications often suffer from overgeneralization in out-of-support regions. The unreliable conservative measures will mislead forward model-based imaginations to undesired areas, leading to overaggressive behaviors. To encourage more conservatism, we propose a novel model-based offline RL framework, called Reverse Offline Model-based Imagination (ROMI). We learn a reverse dynamics model in conjunction with a novel reverse policy,  which can generate rollouts leading to the target goal states within the offline dataset. These reverse imaginations provide informed data augmentation for model-free policy learning and enable conservative generalization beyond the offline dataset. ROMI can effectively combine with off-the-shelf model-free algorithms to enable model-based generalization with proper conservatism. Empirical results show that our method can generate more conservative behaviors and achieve state-of-the-art performance on offline RL benchmark tasks.",['Reinforcement Learning and Planning'],[],"['Jianhao Wang', 'Wenzhe Li', 'Haozhe Jiang', 'Guangxiang Zhu', 'Siyuan Li', 'Chongjie Zhang']","['Tsinghua University', 'Princeton University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Harbin Institute of Technology', 'Washington University, Saint Louis']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27610,Fairness & Bias,Periodic Activation Functions Induce Stationarity,"Neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. We seek to build models that `know what they do not know' by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection.","['Deep Learning', 'Kernel Methods']",[],"['Lassi Meronen', 'Martin Trapp', 'Arno Solin']","['Aalto University', 'Aalto University', 'Aalto University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27600,Fairness & Bias,On the Estimation Bias in Double Q-Learning,"Double Q-learning is a classical method for reducing overestimation bias, which is caused by taking maximum estimated values in the Bellman operation. Its variants in the deep Q-learning paradigm have shown great promise in producing reliable value prediction and improving learning performance. However, as shown by prior work, double Q-learning is not fully unbiased and suffers from underestimation bias. In this paper, we show that such underestimation bias may lead to multiple non-optimal fixed points under an approximate Bellman operator. To address the concerns of converging to non-optimal stationary solutions, we propose a simple but effective approach as a partial fix for the underestimation bias in double Q-learning. This approach leverages an approximate dynamic programming to bound the target value. We extensively evaluate our proposed method in the Atari benchmark tasks and demonstrate its significant improvement over baseline algorithms.",[],[],"['Zhizhou Ren', 'Guangxiang Zhu', 'Hao Hu', 'Beining Han', 'Jianglun Chen', 'Chongjie Zhang']","['University of Illinois, Urbana Champaign', 'Tsinghua University', 'Tsinghua University', 'Department of Computer Science, Princeton University', 'Tsinghua University, Tsinghua University', 'Washington University, Saint Louis']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27584,Fairness & Bias,Few-Shot Segmentation via Cycle-Consistent Transformer,"Few-shot segmentation aims to train a segmentation model that can fast adapt to novel classes with few exemplars. The conventional training paradigm is to learn to make predictions on query images conditioned on the features from support images. Previous methods only utilized the semantic-level prototypes of support images as the conditional information. These methods cannot utilize all pixel-wise support information for the query predictions, which is however critical for the segmentation task. In this paper, we focus on utilizing pixel-wise relationships between support and target images to facilitate the few-shot semantic segmentation task. We design a novel Cycle-Consistent Transformer (CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR performs cross-attention between features from different images, i.e. support and query images. We observe that there may exist unexpected irrelevant pixel-level support features. Directly performing cross-attention may aggregate these features from support to query and bias the query features. Thus, we propose using a novel cycle-consistent attention mechanism to filter out possible harmful support features and encourage query features to attend to the most informative pixels from support images. Experiments on all few-shot segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable improvement compared to previous state-of-the-art methods. Specifically, on Pascal-5^i and COCO-20^i datasets, we achieve 66.6% and 45.6% mIoU for 5-shot segmentation, outperforming previous state-of-the-art by 4.6% and 7.1% respectively.","['Transformers', 'Few Shot Learning', 'Vision']",[],"['Gengwei Zhang', 'Guoliang Kang', 'Yi Yang', 'Yunchao Wei']","['University of Technology Sydney', 'Beihang University', 'Zhejiang University', 'Beijing Jiaotong University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27575,Fairness & Bias,Towards Enabling Meta-Learning from Target Models,"Meta-learning can extract an inductive bias from previous learning experience and assist the training of new tasks. It is often realized through optimizing a meta-model with the evaluation loss of task-specific solvers. Most existing algorithms sample non-overlapping $\mathit{support}$ sets and $\mathit{query}$ sets to train and evaluate the solvers respectively due to simplicity ($\mathcal{S}$/$\mathcal{Q}$ protocol). Different from $\mathcal{S}$/$\mathcal{Q}$ protocol, we can also evaluate a task-specific solver by comparing it to a target model $\mathcal{T}$, which is the optimal model for this task or a model that behaves well enough on this task ($\mathcal{S}$/$\mathcal{T}$ protocol). Although being short of research, $\mathcal{S}$/$\mathcal{T}$ protocol has unique advantages such as offering more informative supervision, but it is computationally expensive. This paper looks into this special evaluation method and takes a step towards putting it into practice. We find that with a small ratio of tasks armed with target models, classic meta-learning algorithms can be improved a lot without consuming many resources. We empirically verify the effectiveness of $\mathcal{S}$/$\mathcal{T}$ protocol in a typical application of meta-learning, $\mathit{i.e.}$, few-shot learning. In detail, after constructing target models by fine-tuning the pre-trained network on those hard tasks, we match the task-specific solvers and target models via knowledge distillation.","['Meta Learning', 'Few Shot Learning']",[],"['Su Lu', 'Han-Jia Ye', 'Le Gan', 'De-Chuan Zhan']","['Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27562,Fairness & Bias,Unsupervised Foreground Extraction via Deep Region Competition,"We present Deep Region Competition (DRC), an algorithm designed to extract foreground objects from images in a fully unsupervised manner. Foreground extraction can be viewed as a special case of generic image segmentation that focuses on identifying and disentangling objects from the background. In this work, we rethink the foreground extraction by reconciling energy-based prior with generative image modeling in the form of Mixture of Experts (MoE), where we further introduce the learned pixel re-assignment as the essential inductive bias to capture the regularities of background regions. With this modeling, the foreground-background partition can be naturally found through Expectation-Maximization (EM). We show that the proposed method effectively exploits the interaction between the mixture components during the partitioning process, which closely connects to region competition, a seminal approach for generic image segmentation. Experiments demonstrate that DRC exhibits more competitive performances on complex real-world data and challenging multi-object scenes compared with prior methods. Moreover, we show empirically that DRC can potentially generalize to novel foreground objects even from categories unseen during training.","['Self-Supervised Learning', 'Generative Model']",[],"['Peiyu Yu', 'Sirui Xie', 'Xiaojian Ma', 'Yixin Zhu', 'Ying Nian Wu', 'Song-Chun Zhu']","['UCLA Department of Statistics', 'University of California, Los Angeles', 'University of California, Los Angeles', 'Peking University', 'UCLA', 'Beijing Institute for General Artificial Intelligence']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27553,Fairness & Bias,Factored Policy Gradients: Leveraging Structure for Efficient Learning in MOMDPs,"Policy gradient methods can solve complex tasks but often fail when the dimensionality of the action-space or objective multiplicity grow very large. This occurs, in part, because the variance on score-based gradient estimators scales quadratically. In this paper, we address this problem through a factor baseline which exploits independence structure encoded in a novel action-target influence network. Factored policy gradients (FPGs), which follow, provide a common framework for analysing key state-of-the-art algorithms, are shown to generalise traditional policy gradients, and yield a principled way of incorporating prior knowledge of a problem domain's generative processes. We provide an analysis of the proposed estimator and identify the conditions under which variance is reduced. The algorithmic aspects of FPGs are discussed, including optimal policy factorisation, as characterised by minimum biclique coverings, and the implications for the bias variance trade-off of incorrectly specifying the network. Finally, we demonstrate the performance advantages of our algorithm on large-scale bandit and traffic intersection problems,  providing a novel contribution to the latter in the form of a spatial approximation.",['Bandits'],[],"['Thomas Spooner', 'Nelson Vadori', 'Sumitra Ganesh']","['University of Liverpool', 'J.P. Morgan AI Research', 'J.P. Morgan Chase']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27548,Fairness & Bias,From Canonical Correlation Analysis to Self-supervised Graph Neural Networks,"We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.","['Deep Learning', 'Graph Learning', 'Representation Learning', 'Self-Supervised Learning']",[],"['Hengrui Zhang', 'Junchi Yan', 'David Wipf', 'Philip S. Yu']","['University of Illinois, Chicago', 'Shanghai Jiao Tong University', 'Amazon AI Research Lab', 'University of Illinois, Chicago']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27564,Fairness & Bias,Towards a Unified Game-Theoretic View of Adversarial Perturbations and Robustness,"This paper provides a unified view to explain different adversarial attacks and defense methods, i.e. the view of multi-order interactions between input variables of DNNs. Based on the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide a potential method to unify adversarial perturbations and robustness, which can explain the existing robustness-boosting methods in a principle way. Besides, our findings also make a revision of previous inaccurate understanding of the shape bias of adversarially learned features. Our code is available online at https://github.com/Jie-Ren/A-Unified-Game-Theoretic-Interpretation-of-Adversarial-Robustness.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Jie Ren', 'Die Zhang', 'Yisen Wang', 'Lu Chen', 'Zhanpeng Zhou', 'Yiting Chen', 'Xu Cheng', 'Xin Wang', 'Meng Zhou', 'Jie Shi', 'Quanshi Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Peking University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Nanjing University of Science and Technology', 'Shanghai Jiao Tong University', 'CMU, Carnegie Mellon University', 'Huawei International.', 'Shanghai Jiao Tong University']","[None, None, None, None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27530,Fairness & Bias,An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning,"We address an inherent difficulty in welfare-theoretic fair machine learning (ML), by proposing an equivalently-axiomatically justified alternative setting, and studying the resulting computational and statistical learning questions. Welfare metrics quantify overall wellbeing across a population of groups, and welfare-based objectives and constraints have recently been proposed to incentivize fair ML methods to satisfy their diverse needs. However, many ML problems are cast as loss minimization tasks, rather than utility maximization, and thus require nontrivial modeling to construct utility functions. We define a complementary metric, termed malfare, measuring overall societal harm, with axiomatic justification via the standard axioms of cardinal welfare, and cast fair ML as malfare minimization over the risk values (expected losses) of each group. Surprisingly, the axioms of cardinal welfare (malfare) dictate that this is not equivalent to simply defining utility as negative loss and maximizing welfare. Building upon these concepts, we define fair-PAC learning, where a fair-PAC learner is an algorithm that learns an ε-δ malfare-optimal model with bounded sample complexity, for any data distribution and (axiomatically justified) malfare concept. Finally, we show conditions under which many standard PAC-learners may be converted to fair-PAC learners, which places fair-PAC learning on firm theoretical ground, as it yields statistical — and in some cases computational — efficiency guarantees for many well-studied ML models. Fair-PAC learning is also practically relevant, as it democratizes fair ML by providing concrete training algorithms with rigorous generalization guarantees.","['Theory', 'Machine Learning']",[],['Cyrus Cousins'],['Brown University'],[None]
https://nips.cc/virtual/2021/poster/27490,Fairness & Bias,On the Bias-Variance-Cost Tradeoff of Stochastic Optimization,"We consider stochastic optimization when one only has access to biased stochastic oracles of the objective, and obtaining stochastic gradients with low biases comes at high costs. This setting captures a variety of optimization paradigms widely used in machine learning, such as conditional stochastic optimization, bilevel optimization, and distributionally robust optimization. We examine a family of multi-level Monte Carlo (MLMC) gradient methods that exploit a delicate trade-off among the bias, the variance, and the oracle cost. We provide a systematic study of their convergences and total computation complexities for strongly convex, convex, and nonconvex objectives, and demonstrate their superiority over the naive biased stochastic gradient method. Moreover, when applied to conditional stochastic optimization, the MLMC gradient methods significantly improve the best-known sample complexity in the literature.","['Theory', 'Optimization', 'Machine Learning']",[],"['Yifan Hu', 'Xin Chen', 'Niao He']","['EPFL - EPF Lausanne', 'University of Illinois, Urbana Champaign', 'Swiss Federal Institute of Technology']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27491,Fairness & Bias,Dynamic COVID risk assessment accounting for community virus exposure from a spatial-temporal transmission model,"COVID-19 pandemic has caused unprecedented negative impacts on our society, including further exposing inequity and disparity in public health. To study the impact of socioeconomic factors on COVID transmission, we first propose a spatial-temporal model to examine the socioeconomic heterogeneity and spatial correlation of COVID-19 transmission at the community level. Second, to assess the individual risk of severe COVID-19 outcomes after a positive diagnosis, we propose a dynamic, varying-coefficient model that integrates individual-level risk factors from electronic health records (EHRs) with community-level risk factors. The underlying neighborhood prevalence of infections (both symptomatic and pre-symptomatic) predicted from the previous spatial-temporal model is included in the individual risk assessment so as to better capture the background risk of virus exposure for each individual. We design a weighting scheme to mitigate multiple selection biases inherited in EHRs of COVID patients. We analyze COVID transmission data in New York City (NYC, the epicenter of the first surge in the United States) and EHRs from NYC hospitals, where time-varying effects of community risk factors and significant interactions between individual- and community-level risk factors are detected. By examining the socioeconomic disparity of infection risks and interaction among the risk factors, our methods can assist public health decision-making and facilitate better clinical management of COVID patients.",['Kernel Methods'],[],"['Yuan Chen', 'Wenbo Fei', 'Qinxia Wang', 'Donglin Zeng', 'Yuanjia Wang']","['Memorial Sloan Kettering Cancer Centre', 'Columbia University', 'Columbia University', 'University of North Carolina, Chapel Hill', 'Columbia University Irving Medical Center']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27463,Fairness & Bias,On Inductive Biases for Heterogeneous Treatment Effect Estimation,"We investigate how to exploit structural similarities of an individual's potential outcomes (POs) under different treatments to obtain better estimates of conditional average treatment effects in finite samples. Especially when it is unknown whether a treatment has an effect at all, it is natural to hypothesize that the POs are similar -- yet, some existing strategies for treatment effect estimation employ regularization schemes that implicitly encourage heterogeneity even when it does not exist and fail to fully make use of shared structure. In this paper, we investigate and compare three end-to-end learning strategies to overcome this problem -- based on regularization, reparametrization and a flexible multi-task architecture -- each encoding inductive bias favoring shared behavior across POs. To build understanding of their relative strengths, we implement all strategies using neural networks and conduct a wide range of semi-synthetic experiments. We observe that all three approaches can lead to substantial improvements upon numerous baselines and gain insight into performance differences across various experimental settings.","['Deep Learning', 'Causality']",[],"['Alicia Curth', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge']","[None, None]"
https://nips.cc/virtual/2021/poster/27415,Fairness & Bias,Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence,"Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common metrics for evaluating classification performance for imbalanced problems. Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced datasets. While stochastic optimization of AUROC has been studied extensively, principled stochastic optimization of AUPRC has been rarely explored. In this work, we propose a principled technical method to optimize AUPRC for deep learning. Our approach is based on maximizing the averaged precision (AP), which is an unbiased point estimator of AUPRC. We cast the objective into a sum of dependent compositional functions with inner functions dependent on random variables of the outer level. We propose efficient adaptive and non-adaptive stochastic algorithms named SOAP with provable convergence guarantee under mild conditions by leveraging recent advances in stochastic compositional optimization. Extensive experimental results on image and graph datasets demonstrate that our proposed method outperforms prior methods on imbalanced problems in terms of AUPRC. To the best of our knowledge, our work represents the first attempt to optimize AUPRC with provable convergence. The SOAP has been implemented in the libAUC library at https://libauc.org/.","['Vision', 'Machine Learning', 'Optimization', 'Graph Learning', 'Deep Learning']",[],"['Qi Qi', 'Youzhi Luo', 'Zhao Xu', 'Shuiwang Ji', 'Tianbao Yang']","['University of Iowa', 'Texas A&M University', 'Texas A&M University', 'Washington State University', 'University of Iowa']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27406,Fairness & Bias,Generalized Shape Metrics on Neural Representations,"Understanding the operation of biological and artificial networks remains a difficult and important challenge. To identify general principles, researchers are increasingly interested in surveying large collections of networks that are trained on, or biologically adapted to, similar tasks. A standardized set of analysis tools is now needed to identify how network-level covariates---such as architecture, anatomical brain region, and model organism---impact neural representations (hidden layer activations). Here, we provide a rigorous foundation for these analyses by defining a broad family of metric spaces that quantify representational dissimilarity. Using this framework, we modify existing representational similarity measures based on canonical correlation analysis and centered kernel alignment to satisfy the triangle inequality, formulate a novel metric that respects the inductive biases in convolutional layers, and identify approximate Euclidean embeddings that enable network representations to be incorporated into essentially any off-the-shelf machine learning method. We demonstrate these methods on large-scale datasets from biology (Allen Institute Brain Observatory) and deep learning (NAS-Bench-101). In doing so, we identify relationships between neural representations that are interpretable in terms of anatomical features and model performance.","['Deep Learning', 'Generative Model', 'Machine Learning', 'Representation Learning']",[],"['Alex H Williams', 'Erin Kunz', 'Simon Kornblith', 'Scott Linderman']","['New York University', 'Stanford University', 'Anthropic', 'Stanford University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27372,Fairness & Bias,Improving Self-supervised Learning with Automated Unsupervised Outlier Arbitration,"Our work reveals a structured shortcoming of the existing mainstream self-supervised learning methods. Whereas self-supervised learning frameworks usually take the prevailing perfect instance level invariance hypothesis for granted, we carefully investigate the pitfalls behind. Particularly, we argue that the existing augmentation pipeline for generating multiple positive views naturally introduces out-of-distribution (OOD) samples that undermine the learning of the downstream tasks. Generating diverse positive augmentations on the input does not always pay off in benefiting downstream tasks. To overcome this inherent deficiency, we introduce a lightweight latent variable model UOTA, targeting the view sampling issue for self-supervised learning. UOTA adaptively searches for the most important sampling region to produce views, and provides viable choice for outlier-robust self-supervised learning approaches. Our method directly generalizes to many mainstream self-supervised learning approaches, regardless of the loss's nature contrastive or not. We empirically show UOTA's advantage over the state-of-the-art self-supervised paradigms with evident margin, which well justifies the existence of the OOD sample issue embedded in the existing approaches. Especially, we theoretically prove that the merits of the proposal boil down to guaranteed estimator variance and bias reduction. Code is available: https://github.com/ssl-codelab/uota.","['Self-Supervised Learning', 'Contrastive Learning']",[],"['Yu Wang', 'Jingyang Lin', 'Jingjing Zou', 'Yingwei Pan', 'Ting Yao', 'Tao Mei']","['Tsinghua University, Tsinghua University', 'University of Rochester', 'University of California, San Diego', 'JD.com', 'JD AI Research', 'JD Explore Academy']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27354,Fairness & Bias,Determinantal point processes based on orthogonal polynomials for sampling minibatches in SGD,"Stochastic gradient descent (SGD) is a cornerstone of machine learning. When the number $N$ of data items is large, SGD relies on constructing an unbiased estimator of the gradient of the empirical risk using a small subset of the original dataset, called a minibatch. Default minibatch construction involves uniformly sampling a subset of the desired size, but alternatives have been explored for variance reduction. In particular, experimental evidence suggests drawing minibatches from determinantal point processes (DPPs), tractable distributions over minibatches that favour diversity among selected items. However, like in recent work on DPPs for coresets, providing a systematic and principled understanding of how and why DPPs help has been difficult. In this work, we contribute an orthogonal polynomial-based determinantal point process paradigm for performing minibatch sampling in SGD. Our approach leverages the specific data distribution at hand, which endows it with greater sensitivity and power over existing data-agnostic methods. We substantiate our method via a detailed theoretical analysis of its convergence properties, interweaving between the discrete data set and the underlying continuous domain. In particular, we show how specific DPPs and a string of controlled approximations can lead to gradient estimators with a variance that decays faster with the batchsize than under uniform sampling. Coupled with existing finite-time guarantees for SGD on convex objectives, this entails that, for a large enough batchsize and a fixed budget of item-level gradients to evaluate, DPP minibatches lead to a smaller bound on the mean square approximation error than uniform minibatches. Moreover, our estimators are amenable to a recent algorithm that directly samples linear statistics of DPPs (i.e., the gradient estimator) without sampling the underlying DPP (i.e., the minibatch), thereby reducing computational overhead. We provide detailed synthetic as well as real data experiments to substantiate our theoretical claims.","['Optimization', 'Machine Learning']",[],"['Rémi Bardenet', 'Subhroshekhar Ghosh', 'Meixia LIN']","['CNRS & Univ. Lille', 'National University of', 'National University of']","[None, 'Singapore', 'Singapore']"
https://nips.cc/virtual/2021/poster/27348,Fairness & Bias,A sampling-based circuit for optimal decision making,"Many features of human and animal behavior can be understood in the framework of Bayesian inference and optimal decision making, but the biological substrate of such processes is not fully understood. Neural sampling provides a flexible code for probabilistic inference in high dimensions and explains key features of sensory responses under experimental manipulations of uncertainty. However, since it encodes uncertainty implicitly, across time and neurons, it remains unclear how such representations can be used for decision making. Here we propose a spiking network model that maps neural samples of a task-specific marginal distribution into an instantaneous representation of uncertainty via a procedure inspired by online  kernel density estimation, so that its output can be readily used for decision making. Our model is consistent with experimental results at the level of single neurons and populations, and makes predictions for how neural responses and decisions could be modulated by uncertainty and prior biases. More generally, our work brings together conflicting perspectives on probabilistic brain computation.",['Neuroscience'],[],"['Camille E. Rullán Buxó', 'Cristina Savin']","['New York University', 'New York University']","[None, None]"
https://nips.cc/virtual/2021/poster/27325,Fairness & Bias,Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems,"We consider the setting of vector valued non-linear dynamical systems $X_{t+1} = \phi(A^{*} X_t) + \eta_t$, where $\eta_t$ is unbiased noise and $\phi : \mathbb{R} \to \mathbb{R}$ is a known link function that satisfies certain {\em expansivity property}. The goal is to learn $A^{*}$ from a single trajectory $X_1,\cdots , X_T$ of {\em dependent or correlated} samples.	While the problem is well-studied in the linear case, where $\phi$ is identity, with optimal error rates even for non-mixing systems, existing results in the non-linear case hold only for mixing systems. In this work, we improve existing results for learning nonlinear systems in a number of ways: a) we provide the first offline algorithm that can learn non-linear dynamical systems without the mixing assumption, b) we significantly improve upon the sample complexity of existing results for mixing systems, c) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay (SGD-RER) method, and demonstrate that for mixing systems, it achieves the same sample complexity as our offline algorithm, d) we justify the expansivity assumption by showing that for the popular ReLU  link function --- a non-expansive but easy to learn link function with i.i.d. samples --- any method would require exponentially many samples (with respect to dimension of $X_t$) from the dynamical system. We validate our results via. simulations and  demonstrate that a naive application of SGD can be highly sub-optimal. Indeed, our work demonstrates that for correlated data, specialized  methods designed for the dependency structure in data can  significantly outperform  standard SGD based methods.",['Theory'],[],"['Suhas S Kowshik', 'Dheeraj Mysore Nagaraj', 'Prateek Jain', 'Praneeth Netrapalli']","['Massachusetts Institute of Technology', 'Google', 'Google', 'Google']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27319,Fairness & Bias,TopicNet: Semantic Graph-Guided Topic Discovery,"Existing deep hierarchical topic models are able to extract semantically meaningful topics from a text corpus  in an unsupervised manner and automatically organize them into a topic hierarchy.  However, it is unclear how to incorporate prior belief such as knowledge graph to guide the learning of the topic hierarchy. To address this issue, we introduce TopicNet as a deep hierarchical topic model that can inject prior structural knowledge as inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. With a variational auto-encoding inference network,  the model parameters are optimized by minimizing the evidence lower bound and supervised loss via stochastic gradient descent. Experiments on widely used benchmark show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations.","['Graph Learning', 'Optimization', 'Generative Model']",[],"['Zhibin Duan', 'Yi.shi Xu', 'Bo Chen', 'dongsheng wang', 'Chaojie Wang', 'Mingyuan Zhou']","['Xidian University', 'Xidian University', 'Xidian University', 'Xidian University', 'Nanyang Technological University', 'The University of Texas at Austin']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27281,Fairness & Bias,Global Filter Networks for Image Classification,"Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity trade-offs of our models on both ImageNet and downstream tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet","['Transformers', 'Robustness', 'Vision', 'Machine Learning']",[],"['Yongming Rao', 'Wenliang Zhao', 'Zheng Zhu', 'Jiwen Lu', 'Jie Zhou']","['Tsinghua University', 'Automation, Tsinghua University, Tsinghua University', 'PhiGent Robotics', 'Tsinghua University', 'Tsinghua University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27280,Fairness & Bias,No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data,"A central challenge in training classification models in the real-world federated system is learning with non-IID data. To cope with this, most of the existing works involve enforcing regularization in local optimization or improving the model aggregation scheme at the server. Other works also share public datasets or synthesized samples to supplement the training of under-represented classes or introduce a certain level of personalization. Though effective, they lack a deep understanding of how the data heterogeneity affects each layer of a deep classification model. In this paper, we bridge this gap by performing an experimental analysis of the representations learned by different layers. Our observations are surprising: (1) there exists a greater bias in the classifier than other layers, and (2) the classification performance can be significantly improved by post-calibrating the classifier after federated training. Motivated by the above findings, we propose a novel and simple algorithm called Classifier Calibration with Virtual Representations (CCVR), which adjusts the classifier using virtual representations sampled from an approximated gaussian mixture model. Experimental results demonstrate that CCVR achieves state-of-the-art performance on popular federated learning benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple yet effective method can shed some light on the future research of federated learning with non-IID data.","['Federated Learning', 'Optimization', 'Machine Learning']",[],"['Mi Luo', 'Fei Chen', 'Dapeng Hu', 'Yifan Zhang', 'Jian Liang', 'Jiashi Feng']","['University of Texas at Austin', ""Huawei Noah's Ark Lab"", 'National University of', 'National University of', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'ByteDance']","[None, None, 'Singapore', 'Singapore', None, None]"
https://nips.cc/virtual/2021/poster/27416,Fairness & Bias,Adaptive Online Packing-guided Search for POMDPs,"The partially observable Markov decision process (POMDP) provides a general framework for modeling an agent's decision process with state uncertainty, and online planning plays a pivotal role in solving it. A belief is a distribution of states representing state uncertainty. Methods for large-scale POMDP problems rely on the same idea of sampling both states and observations. That is, instead of exact belief updating, a collection of sampled states is used to approximate the belief; instead of considering all possible observations, only a set of sampled observations are considered. Inspired by this, we take one step further and propose an online planning algorithm, Adaptive Online Packing-guided Search (AdaOPS), to better approximate beliefs with adaptive particle filter technique and balance estimation bias and variance by fusing similar observation branches. Theoretically, our algorithm is guaranteed to find an $\epsilon$-optimal policy with a high probability given enough planning time under some mild assumptions. We evaluate our algorithm on several tricky POMDP domains, and it outperforms the state-of-the-art in all of them.",[],[],"['Chenyang Wu', 'Guoyu Yang', 'Zongzhang Zhang', 'Yang Yu', 'Dong Li', 'Wulong Liu', 'Jianye HAO']","['Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University', 'Huawei Technologies Ltd.', ""Huawei Noah's Ark Lab"", 'Tianjin University']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27928,Fairness & Bias,Multi-Label Learning with Pairwise Relevance Ordering,"Precisely annotating objects with multiple labels is costly and has become a critical bottleneck in real-world multi-label classification tasks. Instead, deciding the relative order of label pairs is obviously less laborious than collecting exact labels. However, the supervised information of pairwise relevance ordering is less informative than exact labels. It is thus an important challenge to effectively learn with such weak supervision. In this paper, we formalize this problem as a novel learning framework, called multi-label learning with pairwise relevance ordering (PRO). We show that the unbiased estimator of classification risk can be derived with a cost-sensitive loss only from PRO examples. Theoretically, we provide the estimation error bound for the proposed estimator and further prove that it is consistent with respective to the commonly used ranking loss. Empirical studies on multiple datasets and metrics validate the effectiveness of the proposed method.",['Machine Learning'],[],"['Ming-Kun Xie', 'Sheng-Jun Huang']","['Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics']","[None, None]"
https://nips.cc/virtual/2021/poster/27277,Fairness & Bias,Volume Rendering of Neural Implicit Surfaces,"Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images.  So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.","['Vision', 'Representation Learning']",[],"['Lior Yariv', 'Jiatao Gu', 'Yoni Kasten', 'Yaron Lipman']","['Weizmann Institute of Science', 'Apple (MLR)', 'NVIDIA', 'Facebook']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27268,Fairness & Bias,Don’t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence,"Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efficient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for controlling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images.","['Generative Model', 'Machine Learning', 'Optimal Transport', 'Privacy']",[],"['Tianshi Cao', 'Alex Bie', 'Arash Vahdat', 'Sanja Fidler', 'Karsten Kreis']","['University of Toronto', 'University of Waterloo', 'NVIDIA', 'University of Toronto', 'NVIDIA']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27217,Fairness & Bias,Learning to Learn Dense Gaussian Processes for Few-Shot Learning,"Gaussian processes with deep neural networks demonstrate to be a strong learner for few-shot learning since they combine the strength of deep learning and kernels while being able to well capture uncertainty. However, it remains an open problem to leverage the shared knowledge provided by related tasks. In this paper, we propose to learn Gaussian processes with dense inducing variables by meta-learning for few-shot learning. In contrast to sparse Gaussian processes, we define a set of dense inducing variables to be of a much larger size than the support set in each task, which collects prior knowledge from experienced tasks. The dense inducing variables specify a shared Gaussian process prior over prediction functions of all tasks, which are learned in a variational inference framework and offer a strong inductive bias for learning new tasks. To achieve task-specific prediction functions, we propose to adapt the inducing variables to each task by efficient gradient descent. We conduct extensive experiments on common benchmark datasets for a variety of few-shot learning tasks. Our dense Gaussian processes present significant improvements over vanilla Gaussian processes and comparable or even better performance with state-of-the-art methods.","['Optimization', 'Kernel Methods', 'Meta Learning', 'Few Shot Learning', 'Deep Learning', 'Generative Model']",[],"['Ze Wang', 'Zichen Miao', 'Xiantong Zhen', 'Qiang Qiu']","['Purdue University', 'Purdue University', 'University of Amsterdam', 'Purdue University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27215,Fairness & Bias,"Efficient First-Order Contextual Bandits: Prediction, Allocation, and Triangular Discrimination","A recurring theme in statistical learning, online learning, and beyond is that faster convergence rates are possible for problems with low noise, often quantified by the performance of the best hypothesis; such results are known as first-order or small-loss guarantees. While first-order guarantees are relatively well understood in statistical and online learning, adapting to low noise in contextual bandits (and more broadly, decision making) presents major algorithmic challenges. In a COLT 2017 open problem, Agarwal, Krishnamurthy, Langford, Luo, and Schapire asked whether first-order guarantees are even possible for contextual bandits and---if so---whether they can be attained by efficient algorithms. We give a resolution to this question by providing an optimal and efficient reduction from contextual bandits to online regression with the logarithmic (or, cross-entropy) loss. Our algorithm is simple and practical, readily accommodates rich function classes, and requires no distributional assumptions beyond realizability. In a large-scale empirical evaluation, we find that our approach typically outperforms  comparable non-first-order methods. On the technical side, we show that the logarithmic loss and an information-theoretic quantity called the triangular discrimination play a fundamental role in obtaining first-order guarantees, and we combine this observation with new refinements to the regression oracle reduction framework of Foster and Rakhlin (2020). The use of triangular discrimination yields novel results even for the classical statistical learning model, and we anticipate that it will find broader use.","['Reinforcement Learning and Planning', 'Theory', 'Online Learning', 'Bandits']",[],"['Dylan J Foster', 'Akshay Krishnamurthy']","['Microsoft Research', 'Microsoft Research']","[None, None]"
https://nips.cc/virtual/2021/poster/27199,Fairness & Bias,Residual Pathway Priors for Soft Equivariance Constraints,"Models such as convolutional neural networks restrict the hypothesis space to a set of functions satisfying equivariance constraints, and improve generalization in problems by capturing relevant symmetries. However, symmetries are often only partially respected, preventing models with restriction biases from fitting the data. We introduce Residual Pathway Priors (RPPs) as a method for converting hard architectural constraints into soft priors, guiding models towards structured solutions while retaining the ability to capture additional complexity. RPPs are resilient to approximate or misspecified symmetries, and are as effective as fully constrained models even when symmetries are exact. We show that RPPs provide compelling performance on both model-free and model-based reinforcement learning problems, where contact forces and directional rewards violate the assumptions of equivariant networks. Finally, we demonstrate that RPPs have broad applicability, including dynamical systems, regression, and classification.","['Reinforcement Learning and Planning', 'Deep Learning', 'Machine Learning']",[],"['Marc Anton Finzi', 'Gregory Benton', 'Andrew Gordon Wilson']","['Carnegie Mellon University', 'New York University', 'Cornell University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27169,Fairness & Bias,Explaining Hyperparameter Optimization via Partial Dependence Plots,"Automated hyperparameter optimization (HPO) can support practitioners to obtain peak performance in machine learning models. However, there is often a lack of valuable insights into the effects of different hyperparameters on the final model performance. This lack of explainability makes it difficult to trust and understand the automated HPO process and its results. We suggest using interpretable machine learning (IML) to gain insights from the experimental data obtained during HPO with Bayesian optimization (BO). BO tends to focus on promising regions with potential high-performance configurations and thus induces a sampling bias. Hence, many IML techniques, such as the partial dependence plot (PDP), carry the risk of generating biased interpretations. By leveraging the posterior uncertainty of the BO surrogate model, we introduce a variant of the PDP with estimated confidence bands. We propose to partition the hyperparameter space to obtain more confident and reliable PDPs in relevant sub-regions. In an experimental study, we provide quantitative evidence for the increased quality of the PDPs within sub-regions.","['Optimization', 'Interpretability', 'Machine Learning']",[],"['Julia Moosbauer', 'Julia Herbinger', 'Giuseppe Casalicchio', 'Marius Lindauer', 'Bernd Bischl']","['Department of Statistics', 'Institut für Statistik', 'Ludwig-Maximilians-Universität München', 'Leibniz Universität Hannover', 'LMU']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27168,Fairness & Bias,Charting and Navigating the Space of Solutions for Recurrent Neural Networks,"In recent years Recurrent Neural Networks (RNNs) were successfully used to model the way neural activity drives task-related behavior in animals, operating under the implicit assumption that the obtained solutions are universal. Observations in both neuroscience and machine learning challenge this assumption. Animals can approach a given task with a variety of strategies, and training machine learning algorithms introduces the phenomenon of underspecification. These observations imply that every task is associated with a space of solutions. To date, the structure of this space is not understood, limiting the approach of comparing RNNs with neural data. Here, we characterize the space of solutions associated with various tasks. We first study a simple two-neuron network on a task that leads to multiple solutions. We trace the nature of the final solution back to the network’s initial connectivity and identify discrete dynamical regimes that underlie this diversity. We then examine three neuroscience-inspired tasks: Delayed discrimination, Interval discrimination, and Time reproduction. For each task, we find a rich set of solutions. One layer of variability can be found directly in the neural activity of the networks. An additional layer is uncovered by testing the trained networks' ability to extrapolate, as a perturbation to a system often reveals hidden structure. Furthermore, we relate extrapolation patterns to specific dynamical objects and effective algorithms found by the networks. We introduce a tool to derive the reduced dynamics of networks by generating a compact directed graph describing the essence of the dynamics with regards to behavioral inputs and outputs. Using this representation, we can partition the solutions to each task into a handful of types and show that neural features can partially predict them. Taken together, our results shed light on the concept of the space of solutions and its uses both in Machine learning and in Neuroscience.","['Deep Learning', 'Graph Learning', 'Neuroscience', 'Machine Learning']",[],"['Elia Turner', 'Kabir Vinay Dabholkar', 'Omri Barak']","['Technion, Technion', 'n Institute of Science Education and Research (IISER) Pune', 'Technion']","[None, 'India', None]"
https://nips.cc/virtual/2021/poster/27143,Fairness & Bias,Coupled Gradient Estimators for Discrete Latent Variables,"Training models with discrete latent variables is challenging due to the high variance of unbiased gradient estimators. While low-variance reparameterization gradients of a continuous relaxation can provide an effective solution, a continuous relaxation is not always available or tractable. Dong et al. (2020) and Yin et al. (2020) introduced a performant estimator that does not rely on continuous relaxations; however, it is limited to binary random variables. We introduce a novel derivation of their estimator based on importance sampling and statistical couplings, which we extend to the categorical setting. Motivated by the construction of a stick-breaking coupling, we introduce gradient estimators based on reparameterizing categorical variables as sequences of binary variables and Rao-Blackwellization. In systematic experiments, we show that our proposed categorical gradient estimators provide state-of-the-art performance, whereas even with additional Rao-Blackwellization previous estimators (Yin et al., 2019) underperform a simpler REINFORCE with a leave-one-out-baseline estimator (Kool et al., 2019).",[],[],"['Zhe Dong', 'Andriy Mnih', 'George Tucker']","['Google', 'DeepMind', 'Google Brain']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27130,Fairness & Bias,Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective,"Real-world data universally confronts a severe class-imbalance problem and exhibits a long-tailed distribution, i.e., most labels are associated with limited instances. The naïve models supervised by such datasets would prefer dominant labels, encounter a serious generalization challenge and become poorly calibrated. We propose two novel methods from the prior perspective to alleviate this dilemma. First, we deduce a balance-oriented data augmentation named Uniform Mixup (UniMix) to promote mixup in long-tailed scenarios, which adopts advanced mixing factor and sampler in favor of the minority. Second, motivated by the Bayesian theory, we figure out the Bayes Bias (Bayias), an inherent bias caused by the inconsistency of prior, and compensate it as a modification on standard cross-entropy loss. We further prove that both the proposed methods ensure the classification calibration theoretically and empirically. Extensive experiments verify that our strategies contribute to a better-calibrated model, and their combination achieves state-of-the-art performance on CIFAR-LT, ImageNet-LT, and iNaturalist 2018.","['Theory', 'Machine Learning']",[],"['Zhengzhuo Xu', 'Zenghao Chai', 'Chun Yuan']","['Tsinghua University, Tsinghua University', 'National University of', 'Tsinghua University, Tsinghua University']","[None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/27120,Fairness & Bias,On the Convergence Theory of Debiased Model-Agnostic Meta-Reinforcement Learning,"We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement Learning (RL) problems, where the goal is to find a policy using data from several tasks represented by Markov Decision Processes (MDPs) that can be updated by one step of \textit{stochastic} policy gradient for the realized MDP. In particular, using stochastic gradients in MAML update steps is crucial for RL problems since computation of exact gradients requires access to a large number of possible trajectories. For this formulation, we propose a variant of the MAML method, named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and study its convergence properties. We derive the iteration and sample complexity of SG-MRL to find an $\epsilon$-first-order stationary point, which, to the best of our knowledge, provides the first convergence guarantee for model-agnostic meta-reinforcement learning algorithms. We further show how our results extend to the case where more than one step of stochastic policy gradient method is used at test time. Finally, we empirically compare SG-MRL and MAML in several deep RL environments.","['Reinforcement Learning and Planning', 'Meta Learning', 'Optimization', 'Theory']",[],"['Alireza Fallah', 'Kristian Georgiev', 'Aryan Mokhtari', 'Asuman E. Ozdaglar']","['University of California, Berkeley', 'Massachusetts Institute of Technology', 'University of Texas, Austin', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27094,Fairness & Bias,Uncertainty Calibration for Ensemble-Based Debiasing Methods,"Ensemble-based debiasing methods have been shown effective in mitigating the reliance of classifiers on specific dataset bias, by exploiting the output of a bias-only model to adjust the learning target. In this paper, we focus on the bias-only model in these ensemble-based methods, which plays an important role but has not gained much attention in the existing literature. Theoretically, we prove that the debiasing performance can be damaged by inaccurate uncertainty estimations of the bias-only model. Empirically, we show that existing bias-only models fall short in producing accurate uncertainty estimations. Motivated by these findings, we propose to conduct calibration on the bias-only model, thus achieving a three-stage ensemble-based debiasing framework, including bias modeling, model calibrating, and debiasing. Experimental results on NLI and fact verification tasks show that our proposed three-stage debiasing framework consistently outperforms the traditional two-stage one in out-of-distribution accuracy.",['Machine Learning'],[],"['Ruibin Xiong', 'Yimeng Chen', 'Liang Pang', 'Xueqi Cheng', 'Zhi-Ming Ma', 'Yanyan Lan']","['Institude of Computing Technology, Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', ', Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Tsinghua University, Tsinghua University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27090,Fairness & Bias,Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication,"Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication.  Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel.  We experimentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence.","['Deep Learning', 'Graph Learning']",[],"['Łukasz Kuciński', 'Tomasz Korbak', 'Paweł Kołodziej', 'Piotr Miłoś']","['IDEAS NCBR', 'Anthropic', 'Google', 'IDEAS NCBR']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27043,Fairness & Bias,SurvITE: Learning Heterogeneous Treatment Effects from Time-to-Event Data,"We study the problem of inferring heterogeneous treatment effects from time-to-event data. While both the related problems of (i) estimating treatment effects for binary or continuous outcomes and (ii) predicting survival outcomes have been well studied in the recent machine learning literature, their combination -- albeit of high practical relevance -- has received considerably less attention. With the ultimate goal of reliably estimating the effects of treatments on instantaneous risk and survival probabilities, we focus on the problem of learning (discrete-time) treatment-specific conditional hazard functions. We find that unique challenges arise in this context due to a variety of covariate shift issues that go beyond a mere combination of well-studied confounding and censoring biases. We theoretically analyse their effects by adapting recent generalization bounds from domain adaptation and treatment effect estimation to our setting and discuss implications for model design. We use the resulting insights to propose a novel deep learning method for treatment-specific hazard estimation based on balancing representations. We investigate performance across a range of experimental settings and empirically confirm that our method outperforms baselines by addressing covariate shifts from various sources.","['Deep Learning', 'Causality', 'Machine Learning', 'Domain Adaptation']",[],"['Alicia Curth', 'Changhee Lee', 'Mihaela van der Schaar']","['University of Cambridge', 'ChungAng University', 'University of Cambridge']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27033,Fairness & Bias,Memory Efficient Meta-Learning with Large Images,"Meta learning approaches to few-shot classification are computationally efficient at test time, requiring just a few optimization steps or single forward pass to learn a new task, but they remain highly memory-intensive to train. This limitation arises because a task's entire support set, which can contain up to 1000 images, must be processed before an optimization step can be taken. Harnessing the performance gains offered by large images thus requires either parallelizing the meta-learner across multiple GPUs, which may not be available, or trade-offs between task and image size when memory constraints apply. We improve on both options by proposing LITE, a general and memory efficient episodic training scheme that enables meta-training on large tasks composed of large images on a single GPU. We achieve this by observing that the gradients for a task can be decomposed into a sum of gradients over the task's training images. This enables us to perform a forward pass on a task's entire training set but realize significant memory savings by back-propagating only a random subset of these images which we show is an unbiased approximation of the full gradient. We use LITE to train meta-learners and demonstrate new state-of-the-art accuracy on the real-world ORBIT benchmark and 3 of the 4 parts of the challenging VTAB+MD benchmark relative to leading meta-learners. LITE also enables meta-learners to be competitive with transfer learning approaches but at a fraction of the test-time computational cost, thus serving as a counterpoint to the recent narrative that transfer learning is all you need for few-shot classification.","['Vision', 'Transfer Learning', 'Machine Learning', 'Meta Learning', 'Optimization', 'Few Shot Learning']",[],"['John F Bronskill', 'Daniela Massiceti', 'Massimiliano Patacchiola', 'Katja Hofmann', 'Sebastian Nowozin', 'Richard E Turner']","['University of Cambridge', 'Research, Microsoft', 'University of Cambridge', 'Microsoft', 'Microsoft', 'University of Cambridge']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27020,Fairness & Bias,CARMS: Categorical-Antithetic-REINFORCE Multi-Sample Gradient Estimator,"Accurately backpropagating the gradient through categorical variables is a challenging task that arises in various domains, such as training discrete latent variable models. To this end, we propose CARMS, an unbiased estimator for categorical random variables based on multiple mutually negatively correlated (jointly antithetic) samples. CARMS combines REINFORCE with copula based sampling to avoid duplicate samples and reduce its variance, while keeping the estimator unbiased using importance sampling. It generalizes both the ARMS antithetic estimator for binary variables, which is CARMS for two categories, as well as LOORF/VarGrad, the leave-one-out REINFORCE estimator, which is CARMS with independent samples.  We evaluate CARMS on several benchmark datasets on a generative modeling task, as well as a structured output prediction task, and find it to outperform competing methods including a strong self-control baseline. The code is publicly available.",['Generative Model'],[],"['Alek Dimitriev', 'Mingyuan Zhou']","['University of Texas, Austin', 'The University of Texas at Austin']","[None, None]"
https://nips.cc/virtual/2021/poster/27009,Fairness & Bias,Efficient Training of Visual Transformers with Small Datasets,"Visual Transformers (VTs) are emerging as an architectural paradigm alternative to Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose an auxiliary self-supervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn  spatial relations within an image and makes the VT training much more robust when training data is scarce. Our task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different VTs and datasets, we show that our method can improve (sometimes dramatically) the final accuracy of the VTs. Our code is available at: https://github.com/yhlleo/VTs-Drloc.","['Transformers', 'Robustness', 'Vision']",[],"['Yahui Liu', 'Enver Sangineto', 'Wei Bi', 'Nicu Sebe', 'Bruno Lepri', 'Marco De Nadai']","['Huawei Technologies Ltd.', 'University of Trento', 'Tencent AI Lab', 'University of Trento', 'Fondazione Bruno Kessler', 'Spotify']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27011,Fairness & Bias,On the Frequency Bias of Generative Models,"The key objective of Generative Adversarial Networks (GANs) is to generate new data with the same statistics as the provided training data. However, multiple recent works show that state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an elevated amount of high frequencies in the spectral statistics which makes it straightforward to distinguish real and generated images. Explanations for this phenomenon are controversial: While most works attribute the artifacts to the generator, other works point to the discriminator.  We take a sober look at those explanations and provide insights on what makes proposed measures against high-frequency artifacts effective. To achieve this, we first independently assess the architectures of both the generator and discriminator and investigate if they exhibit a frequency bias that makes learning the distribution of high-frequency content particularly problematic. Based on these experiments, we make the following four observations: 1) Different upsampling operations bias the generator towards different spectral properties. 2) Checkerboard artifacts introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able to compensate for these artifacts. 3) The discriminator does not struggle with detecting high frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling operations in the discriminator can impair the quality of the training signal it provides. In light of these findings, we analyze proposed measures against high-frequency artifacts in state-of-the-art GAN training but find that none of the existing approaches can fully resolve spectral artifacts yet. Our results suggest that there is great potential in improving the discriminator and that this could be key to match the distribution of the training data more closely.",['Generative Model'],[],"['Katja Schwarz', 'Yiyi Liao', 'Andreas Geiger']","['University of Tuebingen', 'Zhejiang University', 'University of Tuebingen']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27005,Fairness & Bias,Discrete-Valued Neural Communication,"Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. The nature of structured models is that communication among the components has a bottleneck, typically achieved by restricted connectivity and attention. In this work, we further tighten the bottleneck via discreteness of the representations transmitted between components. We hypothesize that this constraint serves as a useful form of inductive bias. Our hypothesis is motivated by past empirical work showing the benefits of discretization in non-structured architectures as well as our own theoretical results showing that discretization increases noise robustness and reduces the underlying dimensionality of the model. Building on an existing technique for discretization from the VQ-VAE, we consider multi-headed discretization with shared codebooks as the output of each architectural component. One motivating intuition is human language in which communication occurs through multiple discrete symbols. This form of communication is hypothesized to facilitate transmission of information between functional components of the brain by providing a common interlingua, just as it does for human-to-human communication. Our experiments show that discrete-valued neural communication (DVNC) substantially improves systematic generalization in a variety of architectures—transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method useful in practice.","['Transformers', 'Graph Learning', 'Robustness', 'Deep Learning', 'Generative Model']",[],"['Dianbo Liu', 'Alex Lamb', 'Kenji Kawaguchi', 'Anirudh Goyal', 'Chen Sun', 'Michael Curtis Mozer', 'Yoshua Bengio']","['Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'Microsoft Research NYC', 'National University of', 'Google DeepMind', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'Google Research', 'University of Montreal']","[None, None, 'Singapore', None, None, None, None]"
https://nips.cc/virtual/2021/poster/27014,Fairness & Bias,Robust Contrastive Learning Using Negative Samples with Diminished Semantics,"Unsupervised learning has recently made exceptional progress because of the development of more effective contrastive learning methods. However, CNNs are prone to depend on low-level features that humans deem non-semantic. This dependency has been conjectured to induce a lack of robustness to image perturbations or domain shift. In this paper, we show that by generating carefully designed negative samples, contrastive learning can learn more robust representations with less dependence on such features. Contrastive learning utilizes positive pairs which preserve semantic information while perturbing superficial features in the training images. Similarly, we propose to generate negative samples in a reversed way, where only the superfluous instead of the semantic features are preserved. We develop two methods, texture-based and patch-based augmentations, to generate negative samples.  These samples achieve better generalization, especially under out-of-domain settings. We also analyze our method and the generated texture-based samples, showing that texture features are indispensable in classifying particular ImageNet classes and especially finer classes. We also show that the model bias between texture and shape features favors them differently under different test settings.","['Robustness', 'Self-Supervised Learning', 'Contrastive Learning']",[],"['Songwei Ge', 'Shlok Kumar Mishra', 'Chun-Liang Li', 'Haohan Wang', 'David Jacobs']","['University of Maryland, College Park', 'University of Maryland, College Park', 'Google', 'University of Illinois at Urbana-Champaign', 'University of Maryland']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26974,Fairness & Bias,Unbiased Classification through Bias-Contrastive and Bias-Balanced Learning,"Datasets for training machine learning models tend to be biased unless the data is collected with complete care. In such a biased dataset, models are susceptible to making predictions based on the biased features of the data. The biased model fails to generalize to the case where correlations between biases and targets are shifted. To mitigate this, we propose Bias-Contrastive (BiasCon) loss based on the contrastive learning framework, which effectively leverages the knowledge of bias labels. We further suggest Bias-Balanced (BiasBal) regression which trains the classification model toward the data distribution with balanced target-bias correlation. Furthermore, we propose Soft Bias-Contrastive (SoftCon) loss which handles the dataset without bias labels by softening the pair assignment of the BiasCon loss based on the distance in the feature space of the bias-capturing model. Our experiments show that our proposed methods significantly improve previous debiasing methods in various realistic datasets.","['Fairness', 'Contrastive Learning', 'Machine Learning']",[],"['Youngkyu Hong', 'Eunho Yang']","['Korea Advanced Institute of Science and Technology', 'KAIST']","[None, None]"
https://nips.cc/virtual/2021/poster/26969,Fairness & Bias,Local Explanation of Dialogue Response Generation,"In comparison to the interpretation of classification models, the explanation of sequence generation models is also an important problem, however it has seen little attention. In this work, we study model-agnostic explanations of a representative text generation task -- dialogue response generation. Dialog response generation is challenging with its open-ended sentences and multiple acceptable responses. To gain insights into the reasoning process of a generation model, we propose a new method, local explanation of response generation (LERG) that regards the explanations as the mutual interaction of segments in input and output sentences. LERG views the sequence prediction as uncertainty estimation of a human response and then creates explanations by perturbing the input and calculating the certainty change over the human response. We show that LERG adheres to desired properties of explanations for text generation including unbiased approximation, consistency and cause identification. Empirically, our results show that our method consistently improves other widely used methods on proposed automatic- and human- evaluation metrics for this new task by $4.4$-$12.8$\%. Our analysis demonstrates that LERG can extract both explicit and implicit relations between input and output segments.",['Machine Learning'],[],"['Connor Pryor', 'Wenhu Chen', 'Lise Getoor', 'William Yang Wang']","['University of California, Santa Cruz', 'University of Waterloo', 'University of California, Santa Cruz', 'UC Santa Barbara']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26951,Fairness & Bias,Comprehensive Knowledge Distillation with Causal Intervention,"Knowledge distillation (KD) addresses model compression by distilling knowledge from a large model (teacher) to a smaller one (student). The existing distillation approaches mainly focus on using different criteria to align the sample representations learned by the student and the teacher, while they fail to transfer the class representations. Good class representations can benefit the sample representation learning by shaping the sample representation distribution. On the other hand, the existing approaches enforce the student to fully imitate the teacher while ignoring the fact that the teacher is typically not perfect. Although the teacher has learned rich and powerful representations, it also contains unignorable bias knowledge which is usually induced by the context prior (e.g., background) in the training data. To address these two issues, in this paper, we propose comprehensive, interventional distillation (CID) that captures both sample and class representations from the teacher while removing the bias with causal intervention. Different from the existing literature that uses the softened logits of the teacher as the training targets, CID considers the softened logits as the context information of an image, which is further used to remove the biased knowledge based on causal inference. Keeping the good representations while removing the bad bias enables CID to have a better generalization ability on test data and a better transferability across different datasets against the existing state-of-the-art approaches, which is demonstrated by extensive experiments on several benchmark datasets.","['Causality', 'Representation Learning']",[],['Zhongfei Zhang'],"['State University of New York, Binghamton']",[None]
https://nips.cc/virtual/2021/poster/26936,Fairness & Bias,Sparsely Changing Latent States for Prediction and Planning in Partially Observable Domains,"A common approach to prediction and planning in partially observable domains is to use recurrent neural networks (RNNs), which ideally develop and maintain a latent memory about hidden, task-relevant factors. We hypothesize that many of these hidden factors in the physical world are constant over time, changing only sparsely. To study this hypothesis, we propose Gated $L_0$ Regularized Dynamics (GateL0RD), a novel recurrent architecture that incorporates the inductive bias to maintain stable, sparsely changing latent states.  The bias is implemented by means of a novel internal gating function and a penalty on the $L_0$ norm of latent state changes. We demonstrate that GateL0RD can compete with or outperform state-of-the-art RNNs in a variety of partially observable prediction and control tasks. GateL0RD tends to encode the underlying generative factors of the environment, ignores spurious temporal dependencies, and generalizes better, improving sampling efficiency and overall performance in model-based planning and reinforcement learning tasks. Moreover, we show that the developing latent states can be easily interpreted, which is a step towards better explainability in RNNs.","['Reinforcement Learning and Planning', 'Deep Learning', 'Interpretability']",[],"['Christian Gumbsch', 'Martin V. Butz', 'Georg Martius']","['University of Tuebingen', 'University of Tuebingen', 'Eberhard-Karls-Universität Tübingen']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26937,Fairness & Bias,Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning,"Large natural language models (LMs) (such as GPT-3 or T5) demonstrate impressive abilities across a range of general NLP tasks. Here, we show that the knowledge embedded in such models provides a useful inductive bias, not just on traditional NLP tasks, but also in the nontraditional task of training a symbolic reasoning engine. We observe that these engines learn quickly and generalize in a natural way that reflects human intuition. For example, training such a system to model block-stacking might naturally generalize to stacking other types of objects because of structure in the real world that has been partially captured by the language describing it. We study several abstract textual reasoning tasks, such as object manipulation and navigation, and demonstrate multiple types of generalization to novel scenarios and the symbols that comprise them. We also demonstrate the surprising utility of $\textit{compositional learning}$, where a learner dedicated to mastering a complicated task gains an advantage by training on relevant simpler tasks instead of jumping straight to the complicated task.","['Transfer Learning', 'Language']",[],"['Christopher Michael Rytting', 'David Wingate']","['Brigham Young University', 'Brigham Young University']","[None, None]"
https://nips.cc/virtual/2021/poster/26942,Fairness & Bias,Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction,"Recently there has been significant theoretical progress on understanding the convergence and generalization of gradient-based methods on nonconvex losses with overparameterized models. Nevertheless, many aspects of optimization and generalization and in particular the critical role of small random initialization are not fully understood. In this paper, we take a step towards demystifying this role by proving that small random initialization followed by a few iterations of gradient descent behaves akin to popular spectral methods. We also show that this implicit spectral bias from small random initialization, which is provably more prominent for overparameterized models, also puts the gradient descent iterations on a particular trajectory towards solutions that are not only globally optimal but also generalize well. Concretely, we focus on the problem of reconstructing a low-rank matrix from a few measurements via a natural nonconvex formulation. In this setting, we show that the trajectory of the gradient descent iterations from small random initialization can be approximately decomposed into three phases: (I) a spectral or alignment phase where we show that that the iterates have an implicit spectral bias akin to spectral initialization allowing us to show that at the end of this phase the column space of the iterates and the underlying low-rank matrix are sufficiently aligned, (II) a saddle avoidance/refinement phase where we show that the trajectory of the gradient iterates moves away from certain degenerate saddle points, and (III) a local refinement phase where we show that after avoiding the saddles the iterates converge quickly to the underlying low-rank matrix. Underlying our analysis are insights for the analysis of overparameterized nonconvex optimization schemes that may have implications for computational problems beyond low-rank reconstruction.",['Optimization'],[],"['Dominik Stöger', 'Mahdi Soltanolkotabi']","['Catholic University of Eichstätt-Ingolstadt', 'University of Southern California']","[None, None]"
https://nips.cc/virtual/2021/poster/26929,Fairness & Bias,Discovering Dynamic Salient Regions for Spatio-Temporal Graph Neural Networks,"Graph Neural Networks are perfectly suited to capture latent interactions between various entities in the spatio-temporal domain (e.g. videos). However, when an explicit structure is not available, it is not obvious what atomic elements should be represented as nodes. Current works generally use pre-trained object detectors or fixed, predefined regions to extract graph nodes. Improving upon this, our proposed model learns nodes that dynamically attach to well-delimited salient regions, which are relevant for a higher-level task, without using any object-level supervision. Constructing these localized, adaptive nodes gives our model inductive bias towards object-centric representations and we show that it discovers regions that are well correlated with objects in the video. In extensive ablation studies and experiments on two challenging datasets, we show superior performance to previous graph neural networks models for video classification.","['Deep Learning', 'Graph Learning', 'Machine Learning']",[],"['Iulia Duta', 'Andrei Liviu Nicolicioiu', 'Marius Leordeanu']","['University of Cambridge', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'Norwegian Research Center (NORCE)']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26910,Fairness & Bias,CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks,"We present a continual learning approach for generative adversarial networks (GANs), by designing and leveraging parameter-efficient feature map transformations. Our approach is based on learning a set of global and task-specific parameters. The global parameters are fixed across tasks whereas the task-specific parameters act as local adapters for each task, and help in efficiently obtaining task-specific feature maps. Moreover, we propose an element-wise addition of residual bias in the transformed feature space, which further helps stabilize GAN training in such settings. Our approach also leverages task similarities based on the Fisher information matrix. Leveraging this knowledge from previous tasks significantly improves the model performance. In addition, the similarity measure also helps reduce the parameter growth in continual adaptation and helps to learn a compact model. In contrast to the recent approaches for continually-learned GANs, the proposed approach provides a memory-efficient way to perform effective continual data generation. Through extensive experiments on challenging and diverse datasets, we show that the feature-map-transformation approach outperforms state-of-the-art methods for continually-learned GANs, with substantially fewer parameters. The proposed method generates high-quality samples that can also improve the generative-replay-based continual learning for discriminative tasks.","['Generative Model', 'Representation Learning', 'Continual Learning']",[],"['Sakshi Varshney', 'Vinay Kumar Verma', 'Srijith P K', 'Lawrence Carin', 'Piyush Rai']","['Aalto University', 'Amazon', 'n Institute of Technology Hyderabad', 'Duke University', 'IIT Kanpur, IIT Kanpur']","[None, None, 'India', None, None]"
https://nips.cc/virtual/2021/poster/26882,Fairness & Bias,Data-Efficient Instance Generation from Instance Discrimination,"Generative Adversarial Networks (GANs) have significantly advanced image synthesis, however, the synthesis quality drops significantly given a limited amount of training data. To improve the data efficiency of GAN training, prior work typically employs data augmentation to mitigate the overfitting of the discriminator yet still learn the discriminator with a bi-classification ($\textit{i.e.}$, real $\textit{vs.}$ fake) task. In this work, we propose a data-efficient Instance Generation ($\textit{InsGen}$) method based on instance discrimination. Concretely, besides differentiating the real domain from the fake domain, the discriminator is required to distinguish every individual image, no matter it comes from the training set or from the generator. In this way, the discriminator can benefit from the infinite synthesized samples for training, alleviating the overfitting problem caused by insufficient training data. A noise perturbation strategy is further introduced to improve its discriminative power. Meanwhile, the learned instance discrimination capability from the discriminator is in turn exploited to encourage the generator for diverse generation. Extensive experiments demonstrate the effectiveness of our method on a variety of datasets and training settings. Noticeably, on the setting of $2K$ training images from the FFHQ dataset, we outperform the state-of-the-art approach with 23.5\% FID improvement.","['Generative Model', 'Machine Learning']",[],"['Ceyuan Yang', 'Yujun Shen', 'Yinghao Xu', 'Bolei Zhou']","['The Chinese University of', 'Ant Group', 'Chinese University of', 'University of California, Los Angeles']","['Hong Kong', None, 'Hong Kong', None]"
https://nips.cc/virtual/2021/poster/26879,Fairness & Bias,High Probability Complexity Bounds for Line Search Based on Stochastic Oracles,"We consider a line-search method for continuous optimization under a stochastic setting where the function values and gradients are available only through inexact probabilistic zeroth and first-order oracles. These oracles capture multiple standard settings including expected loss minimization and zeroth-order optimization. Moreover, our framework is very general and allows the function and gradient estimates to be biased.  The proposed algorithm is simple to describe, easy to implement, and uses these oracles in a similar way as the standard deterministic line search uses exact function and gradient values.  Under fairly general conditions on the oracles, we derive a high probability tail bound on the iteration complexity of the algorithm when applied to non-convex smooth functions. These results are stronger than those for other existing stochastic line search methods and apply in more general settings.",['Optimization'],[],"['Billy Jin', 'Katya Scheinberg', 'Miaolan Xie']","['Cornell University', 'Cornell University', 'Cornell University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26865,Fairness & Bias,An Image is Worth More Than a Thousand Words: Towards Disentanglement in The Wild,"Unsupervised disentanglement has been shown to be theoretically impossible without inductive biases on the models and the data. As an alternative approach, recent methods rely on limited supervision to disentangle the factors of variation and allow their identifiability. While annotating the true generative factors is only required for a limited number of observations, we argue that it is infeasible to enumerate all the factors of variation that describe a real-world image distribution. To this end, we propose a method for disentangling a set of factors which are only partially labeled, as well as separating the complementary set of residual factors that are never explicitly specified. Our success in this challenging setting, demonstrated on synthetic benchmarks, gives rise to leveraging off-the-shelf image descriptors to partially annotate a subset of attributes in real image domains (e.g. of human faces) with minimal manual effort. Specifically, we use a recent language-image embedding model (CLIP) to annotate a set of attributes of interest in a zero-shot manner and demonstrate state-of-the-art disentangled image manipulation results.",[],[],"['Aviv Gabbay', 'Niv Cohen', 'Yedid Hoshen']","['Hebrew University of Jerusalem', 'Hebrew University of Jerusalem', 'Hebrew University of Jerusalem']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26870,Fairness & Bias,Error Compensated Distributed SGD Can Be Accelerated,"Gradient compression is a recent and increasingly popular technique for reducing the communication cost in distributed training of large-scale machine learning models. In this work we focus on developing efficient distributed methods that can work for any compressor satisfying a certain contraction property, which includes both unbiased (after appropriate scaling) and biased compressors such as RandK and TopK. Applied naively, gradient compression introduces errors that either slow down convergence or lead to divergence. A popular technique designed to tackle this issue is error compensation/error feedback. Due to the difficulties associated with analyzing biased compressors, it is not known whether gradient compression with error compensation can be combined with acceleration. In this work, we show for the first time that error compensated gradient compression methods can be accelerated. In particular, we propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. We show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms.",['Machine Learning'],[],"['Xun Qian', 'Peter Richtárik', 'Tong Zhang']","['KAT', 'King Abdullah University of Science and Technology (KAT)', 'UIUC']","['US', 'US', None]"
https://nips.cc/virtual/2021/poster/26854,Fairness & Bias,Retiring Adult: New Datasets for Fair Machine Learning,"Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.","['Fairness', 'Graph Learning', 'Machine Learning']",[],"['Frances Ding', 'Moritz Hardt', 'John Miller', 'Ludwig Schmidt']","['University of California Berkeley', 'Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'University of California Berkeley', 'University of Washington']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26776,Fairness & Bias,Evaluating Efficient Performance Estimators of Neural Architectures,"Conducting efficient performance estimations of neural architectures is a major challenge in neural architecture search (NAS). To reduce the architecture training costs in NAS, one-shot estimators (OSEs) amortize the architecture training costs by sharing the parameters of one supernet between all architectures. Recently, zero-shot estimators (ZSEs) that involve no training are proposed to further reduce the architecture evaluation cost. Despite the high efficiency of these estimators, the quality of such estimations has not been thoroughly studied. In this paper, we conduct an extensive and organized assessment of OSEs and ZSEs on five NAS benchmarks: NAS-Bench-101/201/301, and NDS ResNet/ResNeXt-A. Specifically, we employ a set of NAS-oriented criteria to study the behavior of OSEs and ZSEs, and reveal their biases and variances. After analyzing how and why the OSE estimations are unsatisfying, we explore how to mitigate the correlation gap of OSEs from three perspectives. Through our analysis, we give out suggestions for future application and development of efficient architecture performance estimators. Furthermore, the analysis framework proposed in our work could be utilized in future research to give a more comprehensive understanding of newly designed architecture performance estimators. The code is available at https://github.com/walkerning/aw_nas.","['Deep Learning', 'Generative Model']",[],"['Xuefei Ning', 'Changcheng Tang', 'Wenshuo Li', 'Zixuan Zhou', 'Huazhong Yang', 'Yu Wang']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26762,Fairness & Bias,Best-case lower bounds in online learning,"Much of the work in online learning focuses on the study of sublinear upper bounds on the regret. In this work, we initiate the study of best-case lower bounds in online convex optimization, wherein we bound the largest \emph{improvement} an algorithm can obtain relative to the single best action in hindsight. This problem is motivated by the goal of better understanding the adaptivity of a learning algorithm. Another motivation comes from fairness: it is known that best-case lower bounds are instrumental in obtaining algorithms for decision-theoretic online learning (DTOL) that satisfy a notion of group fairness. Our contributions are a general method to provide best-case lower bounds in Follow The Regularized Leader (FTRL) algorithms with time-varying regularizers, which we use to show that best-case lower bounds are of the same order as existing upper regret bounds: this includes situations with a fixed learning rate, decreasing learning rates, timeless methods, and adaptive gradient methods. In stark contrast, we show that the linearized version of FTRL can attain negative linear regret. Finally, in DTOL with two experts and binary losses, we fully characterize the best-case sequences, which provides a finer understanding of the best-case lower bounds.","['Theory', 'Fairness', 'Optimization', 'Online Learning']",[],"['Cristóbal A Guzmán', 'Nishant A Mehta', 'Ali Mortazavi']","['Pontificia Universidad Católica', 'University of Victoria', 'University of Victoria']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26739,Fairness & Bias,Fast Axiomatic Attribution for Neural Networks,"Mitigating the dependence on spurious correlations present in the training dataset is a quickly emerging and important topic of deep learning. Recent approaches include priors on the feature attribution of a deep neural network (DNN) into the training process to reduce the dependence on unwanted features. However, until now one needed to trade off high-quality attributions, satisfying desirable axioms, against the time required to compute them. This in turn either led to long training times or ineffective attribution priors. In this work, we break this trade-off by considering a special class of efficiently axiomatically attributable DNNs for which an axiomatic feature attribution can be computed with only a single forward/backward pass. We formally prove that nonnegatively homogeneous DNNs, here termed $\mathcal{X}$-DNNs, are efficiently axiomatically attributable and show that they can be effortlessly constructed from a wide range of regular DNNs by simply removing the bias term of each layer. Various experiments demonstrate the advantages of $\mathcal{X}$-DNNs, beating state-of-the-art generic attribution methods on regular DNNs for training with attribution priors.","['Deep Learning', 'Interpretability']",[],"['Robin Hesse', 'Simone Schaub-Meyer', 'Stefan Roth']","['TU Darmstadt', 'Technische Universität Darmstadt', 'TU Darmstadt']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26730,Fairness & Bias,Stochastic Bias-Reduced Gradient Methods,"We develop a new primitive for stochastic optimization: a low-bias, low-cost  estimator of the minimizer $x_\star$ of any Lipschitz strongly-convex function $f$. In particular, we use a multilevel Monte-Carlo approach due to Blanchet and Glynn to turn any optimal stochastic gradient method into an estimator of $x_\star$ with bias $\delta$, variance $O(\log(1/\delta))$, and an expected sampling cost of $O(\log(1/\delta))$ stochastic gradient evaluations. As an immediate consequence, we obtain cheap and nearly unbiased gradient estimators for the Moreau envelope of any Lipschitz convex function. We demonstrate the potential of our estimator through four applications. First, we develop a method for minimizing the maximum of $N$ functions, improving on recent results and matching a lower bound up to logarithmic factors. Second and third, we recover state-of-the-art rates for projection-efficient and gradient-efficient optimization using simple algorithms with a transparent analysis. Finally, we show that an improved version of our estimator would yield a nearly linear-time, optimal-utility, differentially-private non-smooth stochastic optimization method.","['Theory', 'Optimization', 'Privacy']",[],"['Hilal Asi', 'Yair Carmon', 'Arun Jambulapati', 'Yujia Jin', 'Aaron Sidford']","['Apple', 'Tel Aviv University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26721,Fairness & Bias,Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training data,"There has been a recent surge of interest in designing Graph Neural Networks (GNNs) for semi-supervised learning tasks. Unfortunately this work has assumed that the nodes labeled for use in training were selected uniformly at random (i.e. are an IID sample). However in many real world scenarios gathering labels for graph nodes is both expensive and inherently biased -- so this assumption can not be met. GNNs can suffer poor generalization when this occurs, by overfitting to superfluous regularities present in the training data. In this work we present a method, Shift-Robust GNN (SR-GNN), designed to account for distributional differences between biased training data and the graph's true inference distribution. SR-GNN adapts GNN models for the presence of distributional shifts between the nodes which have had labels provided for training and the rest of the dataset. We illustrate the effectiveness of SR-GNN in a variety of experiments with biased training datasets on common GNN benchmark datasets for semi-supervised learning, where we see that SR-GNN outperforms other GNN baselines by accuracy, eliminating at least (~40%) of the negative effects introduced by biased training data. On the largest dataset we consider, ogb-arxiv, we observe an 2% absolute improvement over the baseline and reduce 30% of the negative effects.","['Transfer Learning', 'Deep Learning', 'Graph Learning', 'Semi-Supervised Learning']",[],"['Qi Zhu', 'Natalia Ponomareva', 'Jiawei Han', 'Bryan Perozzi']","['Amazon', 'Google', 'University of Illinois, Urbana-Champaign', 'Google']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26710,Fairness & Bias,Kernel Identification Through Transformers,"Kernel selection plays a central role in determining the performance of Gaussian Process (GP) models, as the chosen kernel determines both the inductive biases and prior support of functions under the GP prior. This work addresses the challenge of constructing custom kernel functions for high-dimensional GP regression models. Drawing inspiration from recent progress in deep learning, we introduce a novel approach named KITT: Kernel Identification Through Transformers. KITT exploits a transformer-based architecture to generate kernel recommendations in under 0.1 seconds, which is several orders of magnitude faster than conventional kernel search algorithms. We train our model using synthetic data generated from priors over a vocabulary of known kernels. By exploiting the nature of the self-attention mechanism, KITT is able to process datasets with inputs of arbitrary dimension. We demonstrate that kernels chosen by KITT yield strong performance over a diverse collection of regression benchmarks.","['Transformers', 'Deep Learning', 'Kernel Methods']",[],"['Fergus Simpson', 'Ian Davies', 'Vidhi Lalchand', 'Alessandro Vullo', 'Nicolas Durrande', 'Carl Edward Rasmussen']","['Secondmind', 'University College London', 'University of Cambridge', 'University College Dublin', 'Prowler.io', 'University of Cambridge']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26686,Fairness & Bias,Towards Sample-efficient Overparameterized Meta-learning,"An overarching goal in machine learning is to build a generalizable model with few samples. To this end, overparameterization has been the subject of immense interest to explain the generalization ability of deep nets even when the size of the dataset is smaller than that of the model. While the prior literature focuses on the classical supervised setting, this paper aims to demystify overparameterization for meta-learning. Here we have a sequence of linear-regression tasks and we ask: (1) Given earlier tasks, what is the optimal linear representation of features for a new downstream task? and (2) How many samples do we need to build this representation? This work shows that surprisingly, overparameterization arises as a natural answer to these fundamental meta-learning questions. Specifically, for (1), we first show that learning the optimal representation coincides with the problem of designing a task-aware regularization to promote inductive bias. We leverage this inductive bias to explain how the downstream task actually benefits from overparameterization, in contrast to prior works on few-shot learning. For (2), we develop a theory to explain how feature covariance can implicitly help reduce the sample complexity well below the degrees of freedom and lead to small estimation error. We then integrate these findings to obtain an overall performance guarantee for our meta-learning algorithm. Numerical experiments on real and synthetic data verify our insights on overparameterized meta-learning.","['Machine Learning', 'Meta Learning', 'Theory', 'Few Shot Learning', 'Representation Learning']",[],"['Yue Sun', 'Adhyyan Narang', 'Halil Ibrahim Gulluk', 'Samet Oymak', 'Maryam Fazel']","['Microsoft', 'University of Washington, Seattle', 'Stanford University', 'University of Michigan - Ann Arbor', 'University of Washington, Seattle']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26687,Fairness & Bias,Supercharging Imbalanced Data Learning With Energy-based Contrastive Representation Transfer,"Dealing with severe class imbalance poses a major challenge for many real-world applications, especially when the accurate classification and generalization of minority classes are of primary interest. In computer vision and NLP, learning from datasets with long-tail behavior is a recurring theme, especially for naturally occurring labels. Existing solutions mostly appeal to sampling or weighting adjustments to alleviate the extreme imbalance, or impose inductive bias to prioritize generalizable associations. Here we take a novel perspective to promote sample efficiency and model generalization based on the invariance principles of causality. Our contribution posits a meta-distributional scenario, where the causal generating mechanism for label-conditional features is invariant across different labels. Such causal assumption enables efficient knowledge transfer from the dominant classes to their under-represented counterparts, even if their feature distributions show apparent disparities. This allows us to leverage a causal data augmentation procedure to enlarge the representation of minority classes. Our development is orthogonal to the existing imbalanced data learning techniques thus can be seamlessly integrated. The proposed approach is validated on an extensive set of synthetic and real-world tasks against state-of-the-art solutions.","['Vision', 'Machine Learning', 'Contrastive Learning', 'Causality', 'Representation Learning']",[],"['Junya Chen', 'Zidi Xiu', 'Benjamin Goldstein', 'Ricardo Henao', 'Lawrence Carin', 'Chenyang Tao']","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'Duke University', 'Duke University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26664,Fairness & Bias,Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time,"From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Adding auxiliary losses to the main objective function is a general way of encoding biases that can help networks learn better representations. However, since auxiliary losses are minimized only on training data, they suffer from the same generalization gap as regular task losses. Moreover, by adding a term to the loss function, the model optimizes a different objective than the one we care about. In this work we address both problems: first, we take inspiration from transductive learning and note that after receiving an input but before making a prediction, we can fine-tune our networks on any unsupervised loss. We call this process tailoring, because we customize the model to each input to ensure our prediction satisfies the inductive bias. Second, we formulate meta-tailoring, a nested optimization similar to that in meta-learning, and train our models to perform well on the task objective after adapting them using an unsupervised loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on a diverse set of examples.","['Optimization', 'Self-Supervised Learning', 'Machine Learning', 'Meta Learning', 'Deep Learning']",[],"['Ferran Alet', 'Maria Bauza Villalonga', 'Kenji Kawaguchi', 'Nurullah Giray Kuru', 'Tomas Perez', 'Leslie Pack Kaelbling']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'National University of', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, 'Singapore', None, None, None]"
https://nips.cc/virtual/2021/poster/26650,Fairness & Bias,Double Machine Learning Density Estimation for Local Treatment Effects with Instruments,"Local treatment effects are a common quantity found throughout the empirical sciences that measure the treatment effect among those who comply with what they are assigned. Most of the literature is focused on estimating the average of such quantity, which is called the ``local average treatment effect (LATE)'' [Imbens and Angrist, 1994]). In this work, we study how to estimate the density of the local treatment effect, which is naturally more informative than its average. Specifically, we develop two families of methods for this task, namely, kernel-smoothing and model-based approaches. The kernel-smoothing-based approach estimates the density through some smooth kernel functions. The model-based approach estimates the density by projecting it onto a finite-dimensional density class. For both approaches, we derive the corresponding double/debiased machine learning-based estimators [Chernozhukov et al., 2018]. We further study the asymptotic convergence rates of the estimators and show that they are robust to the biases in nuisance function estimation. The use of the proposed methods is illustrated through both synthetic and a real dataset called 401(k).","['Causality', 'Machine Learning']",[],"['Yonghan Jung', 'Jin Tian', 'Elias Bareinboim']","['Purdue University', 'Iowa State University', 'Purdue University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26620,Fairness & Bias,Better Algorithms for Individually Fair $k$-Clustering,"We study data clustering problems with $\ell_p$-norm objectives (e.g. \textsc{$k$-Median} and \textsc{$k$-Means}) in the context of individual fairness. The dataset consists of $n$ points, and we want to find $k$ centers such that (a) the objective is minimized, while (b) respecting the individual fairness constraint that every point $v$ has a center within a distance at most $r(v)$, where $r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz [FORC 2020] introduced this concept and designed a clustering algorithm with provable (approximate) fairness and objective guarantees for the $\ell_\infty$ or \textsc{$k$-Center} objective.  Mahabadi and Vakilian [ICML 2020] revisited this problem to give a local-search algorithm for all $\ell_p$-norms. Empirically, their algorithms outperform Jung et. al.'s by a large margin in terms of cost (for \textsc{$k$-Median} and \textsc{$k$-Means}), but they incur a reasonable loss in fairness. In this paper, our main contribution is to use Linear Programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. We prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal.  Furthermore, our theoretical fairness guarantees are comparable with MV20 in theory, and empirically, we obtain noticeably fairer solutions. Although solving the LP {\em exactly} might be prohibitive, we demonstrate that in practice, a simple sparsification technique drastically improves the run-time of our algorithm.","['Theory', 'Fairness', 'Self-Supervised Learning', 'Clustering']",[],"['Maryam Negahbani', 'Deeparnab Chakrabarty']","['Dartmouth College', 'Dartmouth College']","[None, None]"
https://nips.cc/virtual/2021/poster/26598,Fairness & Bias,A Gang of Adversarial Bandits,"We consider running multiple instances of multi-armed bandit (MAB) problems in parallel. A main motivation for this study are online recommendation systems, in which each of $N$ users is associated with a MAB problem and the goal is to exploit users' similarity in order to learn users' preferences to $K$ items more efficiently. We consider the adversarial MAB setting, whereby an adversary is free to choose which user and which loss to present to the learner during the learning process. Users are in a social network and the learner is aided by a-priori knowledge of the strengths of the social links between all pairs of users. It is assumed that if the social link between two users is strong then they tend to share the same action. The regret is measured relative to an arbitrary function which maps users to actions. The smoothness of the function is captured by a resistance-based dispersion measure $\Psi$. We present two learning algorithms, GABA-I and GABA-II, which exploit the network structure to bias towards functions of low $\Psi$  values. We show that GABA-I has an expected regret bound of $\mathcal{O}(\sqrt{\ln(NK/\Psi)\Psi KT})$ and per-trial time complexity of $\mathcal{O}(K\ln(N))$, whilst GABA-II has a weaker $\mathcal{O}(\sqrt{\ln(N/\Psi)\ln(NK/\Psi)\Psi KT})$ regret, but a better $\mathcal{O}(\ln(K)\ln(N))$ per-trial time complexity. We highlight improvements of both algorithms over running independent standard MABs across users.","['Generative Model', 'Online Learning', 'Bandits']",[],"['Mark Herbster', 'Stephen Pasteris', 'massimiliano pontil']","['University College London', 'Alan Turing Institute', 'Istituto Italiano di Tecnologia']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26591,Fairness & Bias,Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models,"The incorporation of appropriate inductive bias plays a critical role in learning dynamics from data. A growing body of work has been exploring ways to enforce energy conservation in the learned dynamics by encoding Lagrangian or Hamiltonian dynamics into the neural network architecture. These existing approaches are based on differential equations, which do not allow discontinuity in the states and thereby limit the class of systems one can learn. However, in reality, most physical systems, such as legged robots and robotic manipulators, involve contacts and collisions, which introduce discontinuities in the states. In this paper, we introduce a differentiable contact model, which can capture contact mechanics: frictionless/frictional, as well as elastic/inelastic. This model can also accommodate inequality constraints, such as limits on the joint angles. The proposed contact model extends the scope of Lagrangian and Hamiltonian neural networks by allowing simultaneous learning of contact and system properties. We demonstrate this framework on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction. The learned dynamics can be used as a differentiable physics simulator for downstream gradient-based optimization tasks, such as planning and control.","['Deep Learning', 'Optimization']",[],"['Yaofeng Desmond Zhong', 'Biswadip Dey', 'Amit Chakraborty']","['Siemens', 'Siemens Corporate Research', 'Siemens Corporate Research']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26584,Fairness & Bias,Double/Debiased Machine Learning for Dynamic Treatment Effects,"We consider the estimation of treatment effects in settings when multiple treatments are assigned over time and treatments can have a causal effect on future outcomes. We propose an extension of the double/debiased machine learning framework to estimate the dynamic effects of treatments and apply it to a concrete linear Markovian high-dimensional state space model and to general structural nested mean models. Our method allows the use of arbitrary machine learning methods to control for the high dimensional state, subject to a mean square error guarantee, while still allowing parametric estimation and construction of confidence intervals for the dynamic treatment effect parameters of interest.  Our method is based on a sequential regression peeling process, which we show can be equivalently interpreted as a Neyman orthogonal moment estimator. This allows us to show root-n asymptotic normality of the estimated causal effects.","['Causality', 'Machine Learning']",[],"['Greg Lewis', 'Vasilis Syrgkanis']","['Microsoft Research', 'Microsoft']","[None, None]"
https://nips.cc/virtual/2021/poster/26576,Fairness & Bias,Delayed Propagation Transformer: A Universal Computation Engine towards Practical Control in Cyber-Physical Systems,"Multi-agent control is a central theme in the Cyber-Physical Systems (CPS). However, current control methods either receive non-Markovian states due to insufficient sensing and decentralized design, or suffer from poor convergence. This paper presents the Delayed Propagation Transformer (DePT), a new transformer-based model that specializes in the global modeling of CPS while taking into account the immutable constraints from the physical world. DePT induces a cone-shaped spatial-temporal attention prior, which injects the information propagation and aggregation principles and enables a global view. With physical constraint inductive bias baked into its design, our DePT is ready to plug and play for a broad class of multi-agent systems. The experimental results on one of the most challenging CPS -- network-scale traffic signal control system in the open world -- show that our model outperformed the state-of-the-art expert methods on synthetic and real-world datasets. Our codes are released at: https://github.com/VITA-Group/DePT.","['Reinforcement Learning and Planning', 'Transformers']",[],"['Wenqing Zheng', 'Qiangqiang Guo', 'Hao Frank Yang', 'Peihao Wang', 'Zhangyang Wang']","['University of Texas, Austin', 'University of Washington, Seattle', 'University of Washington, Seattle', 'University of Texas, Austin', 'University of Texas at Austin']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27900,Fairness & Bias,Robust Regression Revisited: Acceleration and Improved Estimation Rates,"We study fast algorithms for statistical regression problems under the strong contamination model, where the goal is to approximately optimize a generalized linear model (GLM) given adversarially corrupted samples. Prior works in this line of research were based on the \emph{robust gradient descent} framework of \cite{PrasadSBR20}, a first-order method using biased gradient queries, or the \emph{Sever} framework of \cite{DiakonikolasKK019}, an iterative outlier-removal method calling a stationary point finder. We present nearly-linear time algorithms for robust regression problems with improved runtime or estimation guarantees compared to the state-of-the-art. For the general case of smooth GLMs (e.g.\ logistic regression), we show that the robust gradient descent framework of \cite{PrasadSBR20} can be \emph{accelerated}, and show our algorithm extends to optimizing the Moreau envelopes of Lipschitz GLMs (e.g.\ support vector machines), answering several open questions in the literature. For the well-studied case of robust linear regression, we present an alternative approach obtaining improved estimation rates over prior nearly-linear time algorithms. Interestingly, our algorithm starts with an identifiability proof introduced in the context of the sum-of-squares algorithm of \cite{BakshiP21}, which achieved optimal error rates while requiring large polynomial runtime and sample complexity. We reinterpret their proof within the Sever framework and obtain a dramatically faster and more sample-efficient algorithm under fewer distributional assumptions.","['Theory', 'Optimization']",[],"['Arun Jambulapati', 'Jerry Li', 'Tselil Schramm', 'Kevin Tian']","['Stanford University', 'Microsoft', 'Stanford University', 'Stanford University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26547,Fairness & Bias,Learning to See by Looking at Noise,"Current vision systems are trained on huge datasets, and these datasets come with costs: curation is expensive, they inherit human biases, and there are concerns over privacy and usage rights. To counter these costs, interest has surged in learning from cheaper data sources, such as unlabeled images. In this paper we go a step further and ask if we can do away with real image datasets entirely, instead learning from procedural noise processes. We investigate a suite of image generation models that produce images from simple random processes. These are then used as training data for a visual representation learner with a contrastive loss. In particular, we study statistical image models, randomly initialized deep generative models, and procedural graphics models. Our findings show that it is important for the noise to capture certain structural properties of real data but that good performance can be achieved even with processes that are far from realistic. We also find that diversity is a key property to learn good representations.","['Graph Learning', 'Generative Model', 'Representation Learning', 'Privacy']",[],"['Manel Baradad', 'Jonas Wulff', 'Tongzhou Wang', 'Phillip Isola', 'Antonio Torralba']","['Massachusetts Institute of Technology', 'Xyla Inc', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26531,Fairness & Bias,Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update,"In second-order optimization, a potential bottleneck can be computing the Hessian matrix of the optimized function at every iteration. Randomized sketching has emerged as a powerful technique for constructing estimates of the Hessian which can be used to perform approximate Newton steps. This involves multiplication by a random sketching matrix, which introduces a trade-off between the computational cost of sketching and the convergence rate of the optimization. A theoretically desirable but practically much too expensive choice is to use a dense Gaussian sketching matrix, which produces unbiased estimates of the exact Newton step and offers strong problem-independent convergence guarantees. We show that the Gaussian matrix can be drastically sparsified, substantially reducing the computational cost, without affecting its convergence properties in any way. This approach, called Newton-LESS, is based on a recently introduced sketching technique: LEverage Score Sparsified (LESS) embeddings. We prove that Newton-LESS enjoys nearly the same problem-independent local convergence rate as Gaussian embeddings for a large class of functions. In particular, this leads to a new state-of-the-art convergence result for an iterative least squares solver. Finally, we substantially extend LESS embeddings to include uniformly sparsified random sign matrices which can be implemented efficiently and perform well in numerical experiments.","['Theory', 'Optimization']",[],"['Michal Derezinski', 'Jonathan Lacotte', 'Mert Pilanci', 'Michael W. Mahoney']","['University of Michigan - Ann Arbor', 'Stanford University', 'University of Michigan', 'University of California Berkeley']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26490,Fairness & Bias,Can Information Flows Suggest Targets for Interventions in Neural Circuits?,"Motivated by neuroscientific and clinical applications, we empirically examine whether observational measures of information flow can suggest interventions. We do so by performing experiments on artificial neural networks in the context of fairness in machine learning, where the goal is to induce fairness in the system through interventions. Using our recently developed M-information flow framework, we measure the flow of information about the true label (responsible for accuracy, and hence desirable), and separately, the flow of information about a protected attribute (responsible for bias, and hence undesirable) on the edges of a trained neural network. We then compare the flow magnitudes against the effect of intervening on those edges by pruning. We show that pruning edges that carry larger information flows about the protected attribute reduces bias at the output to a greater extent. This demonstrates that M-information flow can meaningfully suggest targets for interventions, answering the title's question in the affirmative. We also evaluate bias-accuracy tradeoffs for different intervention strategies, to analyze how one might use estimates of desirable and undesirable information flows (here, accuracy and bias flows) to inform interventions that preserve the former while reducing the latter.","['Machine Learning', 'Fairness', 'Theory', 'Deep Learning', 'Interpretability', 'Neuroscience']",[],"['Praveen Venkatesh', 'Sanghamitra Dutta', 'Neil Mehta', 'Pulkit Grover']","['Allen Institute', 'University of Maryland, College Park', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26483,Fairness & Bias,Fair Clustering Under a Bounded Cost,"Clustering is a fundamental unsupervised learning problem where a dataset is partitioned into clusters that consist of nearby points in a metric space. A recent variant, fair clustering, associates a color with each point representing its group membership and requires that each color has (approximately) equal representation in each cluster to satisfy group fairness. In this model, the cost of the clustering objective increases due to enforcing fairness in the algorithm. The relative increase in the cost, the ```````''price of fairness,'' can indeed be unbounded. Therefore, in this paper we propose to treat an upper bound on the clustering objective as a constraint on the clustering problem, and to maximize equality of representation subject to it. We consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective which generalizes the group egalitarian objective. We derive fundamental lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. For the leximin objective we introduce an effective heuristic algorithm. We further derive impossibility results for other natural fairness objectives. We conclude with experimental results on real-world datasets that demonstrate the validity of our algorithms.","['Clustering', 'Fairness', 'Self-Supervised Learning']",[],"['Seyed A. Esmaeili', 'Brian Brubach', 'Aravind Srinivasan', 'John P Dickerson']","['Simons Laufer Mathematical Sciences Institute', 'Wellesley College', 'Amazon', 'University of Maryland, College Park']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26436,Fairness & Bias,Generalization Bounds for Graph Embedding Using Negative Sampling: Linear vs Hyperbolic,"Graph embedding, which represents real-world entities in a mathematical space, has enabled numerous applications such as analyzing natural languages, social networks, biochemical networks, and knowledge bases. It has been experimentally shown that graph embedding in hyperbolic space can represent hierarchical tree-like data more effectively than embedding in linear space, owing to hyperbolic space's exponential growth property. However, since the theoretical comparison has been limited to ideal noiseless settings, the potential for the hyperbolic space's property to worsen the generalization error for practical data has not been analyzed. In this paper, we provide a generalization error bound applicable for graph embedding both in linear and hyperbolic spaces under various negative sampling settings that appear in graph embedding. Our bound states that error is polynomial and exponential with respect to the embedding space's radius in linear and hyperbolic spaces, respectively, which implies that hyperbolic space's exponential growth property worsens the error. Using our bound, we clarify the data size condition on which graph embedding in hyperbolic space can represent a tree better than in Euclidean space by discussing the bias-variance trade-off. Our bound also shows that imbalanced data distribution, which often appears in graph embedding, can worsen the error.","['Theory', 'Graph Learning']",[],"['Atsushi Suzuki', 'Atsushi Nitanda', 'jing wang', 'Linchuan Xu', 'Kenji Yamanishi', 'Marc Cavazza']","[""King's College London, University of London"", 'A*STAR', 'University of Greenwich', 'The  Polytechnic University', 'The University of Tokyo', 'University of Stirling']","[None, None, None, 'Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/26424,Fairness & Bias,NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction,"We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR [Niemeyer et al., 2020] and IDR [Yariv et al., 2020], require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF [Mildenhall et al., 2020] and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.","['Robustness', 'Optimization']",[],"['Peng Wang', 'Lingjie Liu', 'Yuan Liu', 'Christian Theobalt', 'Taku Komura', 'Wenping Wang']","['The University of', 'University of Pennsylvania, University of Pennsylvania', 'The University of', 'Max-Planck-Institute for Informatics, Saarland Informatics Campus', 'the University of , University of', 'Texas A&M University - College Station']","['Hong Kong', None, 'Hong Kong', None, 'Hong Kong', None]"
https://nips.cc/virtual/2021/poster/26405,Fairness & Bias,Analytic Insights into Structure and Rank of Neural Network Hessian Maps,"The Hessian of a neural network captures parameter interactions through second-order derivatives of the loss. It is a fundamental object of study, closely tied to various problems in deep learning, including model design, optimization, and generalization. Most prior work has been empirical, typically focusing on low-rank approximations and heuristics that are blind to the network structure.  In contrast, we develop theoretical tools to analyze the range of the Hessian map, which provide us with a precise understanding of its rank deficiency and the structural reasons behind it. This yields exact formulas and tight upper bounds for the Hessian rank of deep linear networks --- allowing for an elegant interpretation in terms of rank deficiency. Moreover, we demonstrate that our bounds remain faithful as an estimate of the numerical Hessian rank, for a larger class of models such as rectified and hyperbolic tangent networks. Further, we also investigate the implications of model architecture (e.g.~width, depth, bias) on the rank deficiency. Overall, our work provides novel insights into the source and extent of redundancy in overparameterized neural networks.","['Deep Learning', 'Optimization', 'Generative Model']",[],"['Sidak Pal Singh', 'Gregor Bachmann', 'Thomas Hofmann']","['Swiss Federal Institute of Technology Zurich', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26402,Fairness & Bias,LEADS: Learning Dynamical Systems that Generalize Across Environments,"When modeling dynamical systems from real-world data samples, the distribution of data often changes according to the environment in which they are captured, and the dynamics of the system itself vary from one environment to another. Generalizing across environments thus challenges the conventional frameworks. The classical settings suggest either considering data as i.i.d and learning a single model to cover all situations or learning environment-specific models. Both are sub-optimal: the former disregards the discrepancies between environments leading to biased solutions, while the latter does not exploit their potential commonalities and is prone to scarcity problems. We propose LEADS, a novel framework that leverages the commonalities and discrepancies among known environments to improve model generalization. This is achieved with a tailored training formulation aiming at capturing common dynamics within a shared model while additional terms capture environment-specific dynamics. We ground our approach in theory, exhibiting a decrease in sample complexity w.r.t classical alternatives.  We show how theory and practice coincides on the simplified case of linear dynamics. Moreover, we instantiate this framework for neural networks and evaluate it experimentally on representative families of nonlinear dynamics. We show that this new setting can exploit knowledge extracted from environment-dependent data and improves generalization for both known and novel environments.","['Theory', 'Deep Learning']",[],"['Yuan Yin', 'Ibrahim Ayed', 'Emmanuel de Bezenac', 'Nicolas Baskiotis', 'patrick gallinari']","['Sorbonne Université, CNRS, ISIR', 'LIP6', 'ETHZ - ETH Zurich', 'ISIR, UMR 7222 Sorbonne Universite', 'Criteo AI Lab']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26406,Fairness & Bias,Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations,"We present polynomial time and sample efficient algorithms for learning an unknown depth-2 feedforward neural network with general ReLU activations, under mild non-degeneracy assumptions. In particular, we consider learning an unknown network of the form $f(x) = {a}^{\mathsf{T}}\sigma({W}^\mathsf{T}x+b)$, where $x$ is drawn from the Gaussian distribution, and $\sigma(t) = \max(t,0)$ is the ReLU activation. Prior works for learning networks with ReLU activations assume that the bias ($b$) is zero. In order to deal with the presence of the bias terms, our proposed algorithm consists of robustly decomposing multiple higher order tensors arising from the Hermite expansion of the function $f(x)$. Using these ideas we also establish identifiability of the network parameters under very mild assumptions.","['Theory', 'Deep Learning']",[],"['Pranjal Awasthi', 'Alex Tang', 'Aravindan Vijayaraghavan']","['Google', 'Northwestern University', 'Northwestern University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26378,Fairness & Bias,Overparameterization Improves Robustness to Covariate Shift in High Dimensions,"A significant obstacle in the development of robust machine learning models is \emph{covariate shift}, a form of distribution shift that occurs when the input distributions of the training and test sets differ while the conditional label distributions remain the same. Despite the prevalence of covariate shift in real-world applications, a theoretical understanding in the context of modern machine learning has remained lacking. In this work, we examine the exact high-dimensional asymptotics of random feature regression under covariate shift and present a precise characterization of the limiting test error, bias, and variance in this setting. Our results motivate a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. We find that overparameterized models exhibit enhanced robustness to covariate shift, providing one of the first theoretical explanations for this ubiquitous empirical phenomenon. Additionally, our analysis reveals an exact linear relationship between the in-distribution and out-of-distribution generalization performance, offering an explanation for this surprising recent observation.","['Theory', 'Deep Learning', 'Machine Learning', 'Robustness']",[],"['Nilesh Tripuraneni', 'Ben Adlam', 'Jeffrey Pennington']","['University of California Berkeley', 'Google', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26383,Fairness & Bias,Causal Effect Inference for Structured Treatments,"We address the estimation of conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). Given a weak condition on the effect, we propose the generalized Robinson decomposition, which (i) isolates the causal estimand (reducing regularization bias), (ii) allows one to plug in arbitrary models for learning, and (iii) possesses a quasi-oracle convergence guarantee under mild assumptions. In experiments with small-world and molecular graphs we demonstrate that our approach outperforms prior work in CATE estimation.","['Graph Learning', 'Causality']",[],"['Jean Kaddour', 'Yuchen Zhu', 'Qi Liu', 'Matt Kusner', 'Ricardo Silva']","['University College London', 'University College London', 'University of', 'University College London', 'University College London']","[None, None, 'Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/26346,Fairness & Bias,Two-sided fairness in rankings via Lorenz dominance,"We consider the problem of generating rankings that are fair towards both users and item producers in recommender systems. We address both usual recommendation (e.g., of music or movies) and reciprocal recommendation (e.g., dating). Following concepts of distributive justice in welfare economics, our notion of fairness aims at increasing the utility of the worse-off individuals, which we formalize using the criterion of Lorenz efficiency. It guarantees that rankings are Pareto efficient, and that they maximally redistribute utility from better-off to worse-off, at a given level of overall utility. We propose to generate rankings by maximizing concave welfare functions, and develop an efficient inference procedure based on the Frank-Wolfe algorithm. We prove that unlike existing approaches based on fairness constraints, our approach always produces fair rankings. Our experiments also show that it increases the utility of the worse-off at lower costs in terms of overall utility.",['Fairness'],[],"['Virginie Do', 'Sam Corbett-Davies', 'Jamal Atif', 'Nicolas Usunier']","['Université Paris Dauphine - PSL', 'Facebook', 'Université Paris-Dauphine', 'Facebook']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26342,Fairness & Bias,Refining Language Models with Compositional Explanations,"Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences in a new domain. Prior work reveals such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated. However, such a regularization technique lacks flexibility and coverage, since only importance scores towards a pre-defined list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to refine a learned language model for a target domain by collecting human-provided compositional explanations regarding observed biases. By parsing these explanations into executable logic rules, the human-specified refinement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regularization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classification tasks by showing improved performance in target domain as well as improved model fairness after refinement.","['Fairness', 'Machine Learning', 'Language']",[],"['Huihan Yao', 'Ying Chen', 'Qinyuan Ye', 'Xisen Jin', 'Xiang Ren']","['Peking University', 'Tsinghua University', 'University of Southern California', 'University of Southern California', 'University of Southern California']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26348,Fairness & Bias,Noether Networks: meta-learning useful conserved quantities,"Progress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether's theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.","['Meta Learning', 'Vision', 'Machine Learning']",[],"['Ferran Alet', 'Dylan Doblar', 'Allan Zhou', 'Joshua B. Tenenbaum', 'Kenji Kawaguchi', 'Chelsea Finn']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Stanford University', 'Massachusetts Institute of Technology', 'National University of', 'Stanford University']","[None, None, None, None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/26321,Fairness & Bias,"EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback","Error feedback (EF), also known as error compensation, is an immensely popular convergence stabilization mechanism in the context of distributed training of supervised machine learning models enhanced by the use of contractive communication compression mechanisms, such as Top-$k$. First proposed by Seide et al [2014] as a heuristic, EF resisted any theoretical understanding until recently [Stich et al., 2018, Alistarh et al., 2018]. While these early breakthroughs were followed by a steady stream of works offering various improvements and generalizations, the current theoretical understanding of EF is still very limited. Indeed, to the best of our knowledge, all existing analyses either i) apply to the single node setting only, ii) rely on very strong and often unreasonable assumptions, such as global boundedness of the gradients, or iterate-dependent assumptions that cannot be checked a-priori and may not hold in practice, or iii) circumvent these issues via the introduction of additional unbiased compressors, which increase the communication cost. In this work we fix all these deficiencies by proposing and analyzing a new EF mechanism, which we call EF21, which consistently and substantially outperforms EF in practice. Moreover, our theoretical analysis relies on standard assumptions only, works in the distributed heterogeneous data setting, and leads to better and more meaningful rates. In particular, we prove that EF21 enjoys a fast $\mathcal{O}(1/T)$  convergence rate for smooth nonconvex problems, beating the previous bound of $\mathcal{O}(1/T^{2/3})$, which was shown under a strong bounded gradients assumption. We further improve this to a fast linear rate for Polyak-Lojasiewicz functions, which is the first linear convergence result for an error feedback method not relying on unbiased compressors. Since EF has a large number of applications where it reigns supreme, we believe that our 2021 variant, EF21, will have a large impact on the practice of communication efficient distributed learning.","['Optimization', 'Machine Learning']",[],"['Peter Richtárik', 'Igor Sokolov', 'Ilyas Fatkhullin']","['King Abdullah University of Science and Technology (KAT)', 'King Abdullah University of Science and Technology', 'ETHZ - ETH Zurich']","['US', None, None]"
https://nips.cc/virtual/2021/poster/26317,Fairness & Bias,"The Causal-Neural Connection: Expressiveness, Learnability, and Inference","One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal  identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.","['Deep Learning', 'Causality']",[],"['Kevin Muyuan Xia', 'Kai-Zhan Lee', 'Yoshua Bengio', 'Elias Bareinboim']","['Columbia University', 'Columbia University', 'University of Montreal', 'Purdue University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26311,Fairness & Bias,Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models,"The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models \textit{should} learn - whether they should reflect or correct for existing inequalities.",['Language'],[],"['Hannah Rose Kirk', 'Yennie Jun', 'Filippo Volpin', 'Haider Iqbal', 'Elias Benussi', 'Aleksandar Shtedritski', 'Yuki Asano']","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Amsterdam']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26284,Fairness & Bias,Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias,"The generalization mystery of overparametrized deep nets has motivated efforts to understand how gradient descent (GD) converges to low-loss solutions that generalize well. Real-life neural networks are initialized from small random values and trained with cross-entropy loss for classification (unlike the ""lazy"" or ""NTK"" regime of training where analysis was more successful), and a recent sequence of results (Lyu and Li, 2020; Chizat and Bach, 2020; Ji and Telgarsky, 2020) provide theoretical evidence that GD may converge to the ""max-margin"" solution with zero loss, which presumably generalizes well. However, the global optimality of margin is proved only in some settings where neural nets are infinitely or exponentially wide. The current paper is able to establish this global optimality for two-layer Leaky ReLU nets trained with gradient flow on linearly separable and symmetric data, regardless of the width. The analysis also gives some theoretical justification for recent empirical findings (Kalimeris et al., 2019) on the so-called simplicity bias of GD towards linear or other ""simple"" classes of solutions, especially early in training. On the pessimistic side, the paper suggests that such results are fragile. A simple data manipulation can make gradient flow converge to a linear classifier with suboptimal margin.","['Deep Learning', 'Optimization', 'Machine Learning']",[],"['Kaifeng Lyu', 'Zhiyuan Li', 'Runzhe Wang', 'Sanjeev Arora']","['Princeton University', 'Toyota Technological Institute at Chicago', 'Princeton University', 'Princeton University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27220,Fairness & Bias,You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection,"Can Transformer perform $2\mathrm{D}$ object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the $2\mathrm{D}$ spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-$1k$ dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain $42.0$ box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.","['Transformers', 'Vision', 'Transfer Learning', 'Language']",[],"['Bencheng Liao', 'Xinggang Wang', 'Jiemin Fang', 'Jiyang Qi', 'Rui Wu', 'Jianwei Niu', 'Wenyu Liu']","['Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Huawei Technologies Ltd.', 'Huazhong University of Science and Technology', 'Horizon Robotics', 'Northwestern Polytechnical University, Tsinghua University', 'Huazhong University of Science and Technology']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26265,Fairness & Bias,Learning Debiased Representation via Disentangled Feature Augmentation,"Image classification models tend to make decisions based on peripheral attributes of data items that have strong correlation with a target variable (i.e., dataset bias). These biased models suffer from the poor generalization capability when evaluated on unbiased datasets. Existing approaches for debiasing often identify and emphasize those samples with no such correlation (i.e., bias-conflicting) without defining the bias type in advance. However, such bias-conflicting samples are significantly scarce in biased datasets, limiting the debiasing capability of these approaches. This paper first presents an empirical analysis revealing that training with ""diverse"" bias-conflicting samples beyond a given training set is crucial for debiasing as well as the generalization capability. Based on this observation, we propose a novel feature-level data augmentation technique in order to synthesize diverse bias-conflicting samples.  To this end, our method learns the disentangled representation of (1) the intrinsic attributes (i.e., those inherently defining a certain class) and (2) bias attributes (i.e., peripheral attributes causing the bias), from a large number of bias-aligned samples, the bias attributes of which have strong correlation with the target variable.  Using the disentangled representation, we synthesize bias-conflicting samples that contain the diverse intrinsic attributes of bias-aligned samples by swapping their latent features. By utilizing these diversified bias-conflicting features during the training, our approach achieves superior classification accuracy and debiasing results against the existing baselines on both synthetic and real-world datasets.","['Vision', 'Machine Learning']",[],"['Jungsoo Lee', 'Eungyeup Kim', 'Jihyeon Lee', 'Jaegul Choo']","['Korea Advanced Institute of Science & Technology', 'CMU, Carnegie Mellon University', 'KakaoBrain', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26261,Fairness & Bias,ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis,"Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively compresses and removes information to coarsen an image, we train a Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments demonstrate the gain over current autoregressive models, continuous diffusion probabilistic models, and latent variable models. Moreover, the approach enables to control the synthesis process and to trade compression rate against reconstruction accuracy, while still guaranteeing visually plausible results.","['Transformers', 'Generative Model']",[],"['Patrick Esser', 'Robin Rombach', 'Andreas Blattmann', 'Björn Ommer']","['Heidelberg University', 'Stability AI', 'StabilityAI', 'Ludwig-Maximilians-Universität München']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26987,Fairness & Bias,Post-processing for Individual Fairness,"Post-processing in algorithmic fairness is a versatile approach for correcting bias in ML systems that are already used in production. The main appeal of post-processing is that it avoids expensive retraining. In this work, we propose general post-processing algorithms for individual fairness (IF). We consider a setting where the learner only has access to the predictions of the original model and a similarity graph between individuals, guiding the desired fairness constraints. We cast the IF post-processing problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired ""treat similar individuals similarly"" interpretation. Our theoretical results demonstrate the connection of the new objective function to a local relaxation of the original individual fairness. Empirically, our post-processing algorithms correct individual biases in large-scale NLP models such as BERT, while preserving accuracy.","['Fairness', 'Graph Learning']",[],"['Felix Petersen', 'Debarghya Mukherjee', 'Yuekai Sun', 'Mikhail Yurochkin']","['Stanford University', 'Boston University, Boston University', 'University of Michigan', 'International Business Machines']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26438,Fairness & Bias,A Causal Lens for Controllable Text Generation,"Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of  rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.","['Causality', 'Language']",[],"['Zhiting Hu', 'Li Erran Li']","['University of California, San Diego', 'Amazon']","[None, None]"
https://nips.cc/virtual/2021/poster/26222,Fairness & Bias,Edge Representation Learning with Hypergraphs,"Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message-passing.","['Deep Learning', 'Graph Learning', 'Machine Learning', 'Representation Learning']",[],"['Jaehyeong Jo', 'Jinheon Baek', 'Seul Lee', 'Dongki Kim', 'Minki Kang', 'Sung Ju Hwang']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26129,Fairness & Bias,Sample Selection for Fair and Robust Training,"Fairness and robustness are critical elements of Trustworthy AI that need to be addressed together. Fairness is about learning an unbiased model while robustness is about learning from corrupted data, and it is known that addressing only one of them may have an adverse affect on the other. In this work, we propose a sample selection-based algorithm for fair and robust training. To this end, we formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption. Observing that solving this optimization problem is strongly NP-hard, we propose a greedy algorithm that is efficient and effective in practice. Experiments show that our method obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets. Moreover, unlike other fair and robust training baselines, our algorithm can be used by only modifying the sampling step in batch selection without changing the training algorithm or leveraging additional clean data.","['Robustness', 'Optimization', 'Fairness']",[],"['Yuji Roh', 'Kangwook Lee', 'Steven Euijong Whang', 'Changho Suh']","['Korea Advanced Institute of Science and Technology', 'University of Wisconsin, Madison', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/25976,Fairness & Bias,Learning where to learn: Gradient sparsity in meta and continual learning,"Finding neural network weights that generalize well from small datasets is difficult. A promising approach is to learn a weight initialization such that a small number of weight changes results in low generalization error. We show that this form of meta-learning can be improved by letting the learning algorithm decide which weights to change, i.e., by learning where to learn. We find that patterned sparsity emerges from this process, with the pattern of sparsity varying on a problem-by-problem basis. This selective sparsity results in better generalization and less interference in a range of few-shot and continual learning problems. Moreover, we find that sparse learning also emerges in a more expressive model where learning rates are meta-learned. Our results shed light on an ongoing debate on whether meta-learning can discover adaptable features and suggest that learning by sparse gradient descent is a powerful inductive bias for meta-learning systems.","['Optimization', 'Meta Learning', 'Continual Learning', 'Deep Learning', 'Few Shot Learning']",[],"['Johannes Von Oswald', 'Dominic Zhao', 'Simon Schug', 'Massimo Caccia', 'Nicolas Zucchet', 'Joao Sacramento']","['Research, Google', 'Common Sense Machines', 'Swiss Federal Institute of Technology', 'University of Montreal', 'ETHZ - ETH Zurich', 'Department of Computer Science, ETHZ - ETH Zurich']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25978,Fairness & Bias,What training reveals about neural network complexity,"This work explores the Benevolent Training Hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. Our analysis provides evidence for BTH by relating the NN's Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure.  We first observe that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. We then show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point. Finally, we find that steady training with Dropout implies a training- and data-dependent generalization bound that grows poly-logarithmically with the number of parameters. Overall, our results support the intuition that good training behavior can be a useful bias towards good generalization.",['Deep Learning'],[],"['Andreas Loukas', 'Marinos Poiitis', 'Stefanie Jegelka']","['Roche / Genentech', 'Aristotle University of Thessaloniki', 'Massachusetts Institute of Technology']","[None, None, None]"
https://nips.cc/virtual/2021/poster/25991,Fairness & Bias,Leveraging Recursive Gumbel-Max Trick for Approximate Inference in Combinatorial Spaces,"Structured latent variables allow incorporating meaningful prior knowledge into deep learning models. However, learning with such variables remains challenging because of their discrete nature. Nowadays, the standard learning approach is to define a latent variable as a perturbed algorithm output and to use a differentiable surrogate for training. In general, the surrogate puts additional constraints on the model and inevitably leads to biased gradients. To alleviate these shortcomings, we extend the Gumbel-Max trick to define distributions over structured domains. We avoid the differentiable surrogates by leveraging the score function estimators for optimization. In particular, we highlight a family of recursive algorithms with a common feature we call stochastic invariant. The feature allows us to construct reliable gradient estimates and control variates without additional constraints on the model. In our experiments, we consider various structured latent variable models and achieve results competitive with relaxation-based counterparts.","['Deep Learning', 'Optimization']",[],"['Kirill Struminsky', 'Artyom Gadetsky', 'Denis Rakitin', 'Danil Karpushkin', 'Dmitry P. Vetrov']","['Higher School of Economics', 'EPFL - EPF Lausanne', 'HSE University', 'Artificial Intelligence Research Institute', 'Constructor University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25938,Fairness & Bias,Constrained Two-step Look-Ahead Bayesian Optimization,"Recent advances in computationally efficient non-myopic Bayesian optimization offer improved query efficiency over traditional myopic methods like expected improvement, with only a modest increase in computational cost. These advances have been largely limited to unconstrained BO methods with only a few exceptions which require heavy computation. For instance, one existing multi-step lookahead constrained BO method (Lam & Willcox, 2017) relies on computationally expensive unreliable brute force derivative-free optimization of a Monte Carlo rollout acquisition function. Methods that use the reparameterization trick for more efficient derivative-based optimization of non-myopic acquisition functions in the unconstrained setting, like sample average approximation and infinitesimal perturbation analysis, do not extend: constraints introduce discontinuities in the sampled acquisition function surface. Moreover, we argue here that being non-myopic is even more important in constrained problems because fear of violating constraints pushes myopic methods away from sampling the boundary between feasible and infeasible regions, slowing the discovery of optimal solutions with tight constraints. In this paper, we propose a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) supporting both sequential and batch settings. To enable fast acquisition function optimization, we develop a novel likelihood ratio-based unbiased estimator of the gradient of the two-step optimal acquisition function that does not use the reparameterization trick. In numerical experiments, 2-OPT-C typically improves query efficiency by 2x or more over previous methods, and in some cases by 10x or more.","['Optimization', 'Kernel Methods']",[],"['Duke Zhang', 'Xiangyu Zhang', 'Peter I. Frazier']","['Cornell University', 'Cornell University', 'Cornell University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/25936,Fairness & Bias,A Stochastic Newton Algorithm for Distributed Convex Optimization,"We propose and analyze a stochastic Newton algorithm for homogeneous distributed stochastic convex optimization, where each machine can calculate stochastic gradients of the same population objective, as well as stochastic Hessian-vector products (products of an independent unbiased estimator of the Hessian of the population objective with arbitrary vectors), with many such stochastic computations performed between rounds of communication.  We show that our method can reduce the number, and frequency, of required communication rounds, compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression), alongside empirical evidence.","['Federated Learning', 'Optimization']",[],"['Brian Bullins', 'Kumar Kshitij Patel', 'Ohad Shamir', 'Nathan Srebro', 'Blake Woodworth']","['Purdue University', 'Toyota Technological Institute at Chicago', 'Weizmann Institute', 'Toyota Technological Institute at Chicago', 'INRIA']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25919,Fairness & Bias,Object-aware Contrastive Learning for Debiased Scene Representation,"Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images. Code is available at https://github.com/alinlab/object-aware-contrastive.","['Self-Supervised Learning', 'Contrastive Learning', 'Representation Learning']",[],"['Sangwoo Mo', 'Hyunwoo Kang', 'Kihyuk Sohn', 'Chun-Liang Li', 'Jinwoo Shin']","['University of Michigan - Ann Arbor', 'KLleon AI Research', 'Google', 'Google', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25922,Fairness & Bias,Contrastively Disentangled Sequential  Variational Autoencoder,"Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics.","['Self-Supervised Learning', 'Contrastive Learning', 'Generative Model', 'Interpretability', 'Representation Learning']",[],"['Junwen Bai', 'Weiran Wang', 'Carla P Gomes']","['Google', 'Google', 'Cornell University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28757,Fairness & Bias,Machine Learning for Variance Reduction in Online Experiments,"We consider the problem of variance reduction in randomized controlled trials, through the use of covariates correlated with the outcome but independent of the treatment. We propose a machine learning regression-adjusted treatment effect estimator, which we call MLRATE. MLRATE uses machine learning predictors of the outcome to reduce estimator variance. It employs cross-fitting to avoid overfitting biases, and we prove consistency and asymptotic normality under general conditions. MLRATE is robust to poor predictions from the machine learning step: if the predictions are uncorrelated with the outcomes, the estimator performs asymptotically no worse than the standard difference-in-means estimator, while if predictions are highly correlated with outcomes, the efficiency gains are large. In A/A tests, for a set of 48 outcome metrics commonly monitored in Facebook experiments, the estimator has over $70\%$ lower variance than the simple difference-in-means estimator, and about $19\%$ lower variance than the common univariate procedure which adjusts only for pre-experiment values of the outcome.",['Machine Learning'],[],"['Yongyi Guo', 'Dominic Coey', 'Mikael Konutgan']","['Harvard University, Harvard University', 'Facebook', 'Facebook']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28550,Fairness & Bias,Machine learning structure preserving brackets for forecasting irreversible processes,"Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with \emph{reversible} dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning \emph{irreversible} dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either ""black-box"" or penalty-based approaches.","['Deep Learning', 'Machine Learning']",[],"['Kookjin Lee', 'Nathaniel Trask', 'Panos Stinis']","['Arizona State University', 'University of Pennsylvania, University of Pennsylvania', 'Pacific Northwest National Laboratory']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27552,Fairness & Bias,DECAF:  Generating Fair Synthetic Data Using Causally-Aware Generative Networks,"Machine learning models have been criticized for reflecting unfair biases in the training data.  Instead of solving for this by introducing fair learning algorithms directly, we focus on generating fair synthetic data, such that any downstream learner is fair. Generating fair synthetic data from unfair data - while remaining truthful to the underlying data-generating process (DGP) - is non-trivial. In this paper, we introduce DECAF: a GAN-based fair synthetic data generator for tabular data.  With DECAF we embed the DGP explicitly as a structural causal model in the input layers of the generator, allowing each variable to be reconstructed conditioned on its causal parents.  This procedure enables inference time debiasing, where biased edges can be strategically removed for satisfying user-defined fairness requirements. The DECAF framework is versatile and compatible with several popular definitions of fairness. In our experiments, we show that DECAF successfully removes undesired bias and - in contrast to existing methods - is capable of generating high-quality synthetic data. Furthermore, we provide theoretical guarantees on the generator's convergence and the fairness of downstream models.","['Fairness', 'Generative Model', 'Machine Learning', 'Causality']",[],"['Boris van Breugel', 'Trent Kyono', 'Jeroen Berrevoets', 'Mihaela van der Schaar']","['University of Cambridge', 'University of California, Los Angeles', 'University of Cambridge', 'University of Cambridge']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27018,Fairness & Bias,Learning to Assimilate in Chaotic Dynamical Systems,"The accuracy of simulation-based forecasting in chaotic systems is heavily dependent on high-quality estimates of the system state at the beginning of the forecast. Data assimilation methods are used to infer these initial conditions by systematically combining noisy, incomplete observations and numerical models of system dynamics to produce highly effective estimation schemes. We introduce a self-supervised framework, which we call \textit{amortized assimilation}, for learning to assimilate in dynamical systems. Amortized assimilation combines deep learning-based denoising with differentiable simulation, using independent neural networks to assimilate specific observation types while connecting the gradient flow between these sub-tasks with differentiable simulation and shared recurrent memory. This hybrid architecture admits a self-supervised training objective which is minimized by an unbiased estimator of the true system state even in the presence of only noisy training data. Numerical experiments across several chaotic benchmark systems highlight the improved effectiveness of our approach compared to widely-used data assimilation methods.",['Deep Learning'],[],"['Michael McCabe', 'Jed Brown']","['University of Colorado, Boulder', 'University of Colorado, Boulder']","[None, None]"
https://nips.cc/virtual/2021/poster/28101,Fairness & Bias,Grounding inductive biases in natural images: invariance stems from variations in data,"To perform well on unseen and potentially out-of-distribution samples, it is desirable for machine learning models to have a predictable response with respect to transformations affecting the factors of variation of the input. Here, we study the relative importance of several types of inductive biases towards such predictable behavior: the choice of data, their augmentations, and model architectures. Invariance is commonly achieved through hand-engineered data augmentation, but do standard data augmentations address transformations that explain variations in real data? While prior work has focused on synthetic data, we attempt here to characterize the factors of variation in a real dataset, ImageNet, and study the invariance of both standard residual networks and the recently proposed vision transformer with respect to changes in these factors. We show standard augmentation relies on a precise combination of translation and scale, with translation recapturing most of the performance improvement---despite the (approximate) translation invariance built in to convolutional architectures, such as residual networks. In fact, we found that scale and translation invariance was similar across residual networks and vision transformer models despite their markedly different architectural inductive biases. We show the training data itself is the main source of invariance, and that data augmentation only further increases the learned invariances. Notably, the invariances learned during training align with the ImageNet factors of variation we found. Finally, we find that the main factors of variation in ImageNet mostly relate to appearance and are specific to each class.","['Transformers', 'Machine Learning']",[],"['Diane Bouchacourt', 'Mark Ibrahim', 'Ari S. Morcos']","['Facebook AI Research', 'Facebook AI Research (FAIR)', 'Meta AI (FAIR)']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27831,Fairness & Bias,On the Variance of the Fisher Information for Deep Learning,"In the realm of deep learning, the Fisher information matrix (FIM) gives novel insights and useful tools to characterize the loss landscape, perform second-order optimization, and build geometric learning theories. The exact FIM is either unavailable in closed form or too expensive to compute. In practice, it is almost always estimated based on empirical samples.  We investigate two such estimators based on two equivalent representations of the FIM --- both unbiased and consistent. Their estimation quality is naturally gauged by their variance given in closed form. We analyze how the parametric structure of a deep neural network can affect the variance. The meaning of this variance measure and its upper bounds are then discussed in the context of deep learning.","['Theory', 'Deep Learning', 'Optimization']",[],"['Alexander Soen', 'Ke Sun']","['RIKEN', ""CSIRO's Data61 & n National University""]","[None, 'Australia']"
https://nips.cc/virtual/2021/poster/27473,Fairness & Bias,Regularized Softmax Deep Multi-Agent Q-Learning,"Tackling overestimation in $Q$-learning is an important problem that has been extensively studied in single-agent reinforcement learning, but has received comparatively little attention in the multi-agent setting. In this work, we empirically demonstrate that QMIX, a popular $Q$-learning algorithm for cooperative multi-agent reinforcement learning (MARL), suffers from a more severe overestimation in practice than previously acknowledged, and is not mitigated by existing approaches. We rectify this with a novel regularization-based update scheme that penalizes large joint action-values that deviate from a baseline and demonstrate its effectiveness in stabilizing learning. Furthermore, we propose to employ a softmax operator, which we efficiently approximate in a novel way in the multi-agent setting, to further reduce the potential overestimation bias. Our approach, Regularized Softmax (RES) Deep Multi-Agent $Q$-Learning, is general and can be applied to any $Q$-learning based MARL algorithm. We demonstrate that, when applied to QMIX, RES avoids severe overestimation and significantly improves performance, yielding state-of-the-art results in a variety of cooperative multi-agent tasks, including the challenging StarCraft II micromanagement benchmarks.",['Reinforcement Learning and Planning'],[],"['Ling Pan', 'Tabish Rashid', 'Bei Peng', 'Longbo Huang', 'Shimon Whiteson']","['University of Science and Technology', 'Microsoft', 'University of Liverpool', 'Tsinghua University, Tsinghua University', 'University of Oxford']","['Hong Kong', None, None, None, None]"
https://nips.cc/virtual/2021/poster/26622,Fairness & Bias,Learning Debiased and Disentangled Representations for Semantic Segmentation,"Deep neural networks are susceptible to learn biased models with entangled feature representations, which may lead to subpar performances on various downstream tasks. This is particularly true for under-represented classes, where a lack of diversity in the data exacerbates the tendency. This limitation has been addressed mostly in classification tasks, but there is little study on additional challenges that may appear in more complex dense prediction problems including semantic segmentation. To this end, we propose a model-agnostic and stochastic training scheme for semantic segmentation, which facilitates the learning of debiased and disentangled representations. For each class, we first extract class-specific information from the highly entangled feature map. Then, information related to a randomly sampled class is suppressed by a feature selection process in the feature space. By randomly eliminating certain class information in each training iteration, we effectively reduce feature dependencies among classes, and the model is able to learn more debiased and disentangled feature representations. Models trained with our approach demonstrate strong results on multiple semantic segmentation benchmarks, with especially notable performance gains on under-represented classes.","['Deep Learning', 'Vision', 'Machine Learning']",[],"['Sanghyeok Chu', 'Dongwan Kim', 'Bohyung Han']","['Seoul National University', 'Seoul National University', 'POSTECH']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26338,Fairness & Bias,Differentially Private Sampling from Distributions,"We initiate an investigation of  private sampling from distributions. Given a dataset with $n$ independent observations from an unknown distribution $P$, a sampling algorithm must output a single observation from a distribution that is close in total variation distance to $P$ while satisfying differential privacy. Sampling abstracts the goal of generating small amounts of realistic-looking data. We provide tight upper and lower bounds for the dataset size needed for this task for three natural families of distributions: arbitrary distributions on $\{1,\ldots ,k\}$, arbitrary product distributions on $\{0,1\}^d$, and product distributions on on $\{0,1\}^d$ with bias in each coordinate bounded away from 0 and 1. We demonstrate that, in some parameter regimes, private sampling requires asymptotically fewer observations than learning a description of $P$ nonprivately; in other regimes, however, private sampling proves to be as difficult as private learning. Notably, for some classes of distributions, the overhead in the number of observations needed for private learning compared to non-private learning is completely captured by the number of observations needed for private sampling.",['Privacy'],[],"['Sofya Raskhodnikova', 'Satchit Sivakumar', 'Adam Smith', 'Marika Swanberg']","['Boston University, Boston University', 'Boston University', 'Boston University', 'Boston University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28687,Fairness & Bias,Storchastic: A Framework for General Stochastic Automatic Differentiation,"Modelers use automatic differentiation (AD) of computation graphs to implement complex Deep Learning models without defining gradient computations. Stochastic AD extends AD to stochastic computation graphs with sampling steps, which arise when modelers handle the intractable expectations common in Reinforcement Learning and Variational Inference. However, current methods for stochastic AD are limited: They are either only applicable to continuous random variables and differentiable functions, or can only use simple but high variance score-function estimators. To overcome these limitations, we introduce Storchastic, a new framework for AD of stochastic computation graphs. Storchastic allows the modeler to choose from a wide variety of gradient estimation methods at each sampling step, to optimally reduce the variance of the gradient estimates. Furthermore, Storchastic is provably unbiased for estimation of any-order gradients, and generalizes variance reduction techniques to higher-order gradient estimates. Finally, we implement Storchastic as a PyTorch library at github.com/HEmile/storchastic.","['Reinforcement Learning and Planning', 'Optimization', 'Graph Learning', 'Deep Learning', 'Generative Model']",[],"['Emile van Krieken', 'Jakub Mikolaj Tomczak', 'Annette Ten Teije']","['Edinburgh University, University of Edinburgh', 'Eindhoven University of Technology', 'Vrije Universiteit Amsterdam']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26895,Fairness & Bias,Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer Treatment-Effects from Observational Data,"Estimating personalized treatment effects from high-dimensional observational data is essential in situations where experimental designs are infeasible, unethical, or expensive. Existing approaches rely on fitting deep models on outcomes observed for treated and control populations. However, when measuring individual outcomes is costly, as is the case of a tumor biopsy, a sample-efficient strategy for acquiring each result is required. Deep Bayesian active learning provides a framework for efficient data acquisition by selecting points with high uncertainty. However, existing methods bias training data acquisition towards regions of non-overlapping support between the treated and control populations. These are not sample-efficient because the treatment effect is not identifiable in such regions. We introduce causal, Bayesian acquisition functions grounded in information theory that bias data acquisition towards regions with overlapping support to maximize sample efficiency for learning personalized treatment effects. We demonstrate the performance of the proposed acquisition strategies on synthetic and semi-synthetic datasets IHDP and CMNIST and their extensions, which aim to simulate common dataset biases and pathologies.","['Theory', 'Deep Learning', 'Causality', 'Active Learning']",[],"['Andrew Jesson', 'Panagiotis Tigas', 'Joost van Amersfoort', 'Andreas Kirsch', 'Uri Shalit', 'Yarin Gal']","['Columbia University', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'Technion', 'University of Oxford']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26947,Fairness & Bias,Subgraph Federated Learning with Missing Neighbor Generation,"Graphs have been widely used in data mining and machine learning due to their unique representation of real-world objects and their interactions. As graphs are getting bigger and bigger nowadays, it is common to see their subgraphs separately collected and stored in multiple local systems. Therefore, it is natural to consider the subgraph federated learning setting, where each local system holds a small subgraph that may be biased from the distribution of the whole graph. Hence, the subgraph federated learning aims to collaboratively train a powerful and generalizable graph mining model without directly sharing their graph data. In this work, towards the novel yet realistic setting of subgraph federated learning, we propose two major techniques: (1) FedSage, which trains a GraphSage model based on FedAvg to integrate node features, link structures, and task labels on multiple local subgraphs; (2) FedSage+, which trains a missing neighbor generator along FedSage to deal with missing links across local subgraphs. Empirical results on four real-world graph datasets with synthesized subgraph federated learning settings demonstrate the effectiveness and efficiency of our proposed techniques. At the same time, consistent theoretical implications are made towards their generalization ability on the global graphs.","['Federated Learning', 'Graph Learning', 'Machine Learning', 'Generative Model']",[],"['Ke ZHANG', 'Carl Yang', 'Xiaoxiao Li', 'Lichao Sun', 'Siu Ming Yiu']","['The University of', 'Emory University', 'University of British Columbia', 'Lehigh University', 'The University of']","['Hong Kong', None, None, None, 'Hong Kong']"
https://nips.cc/virtual/2021/poster/28819,Fairness & Bias,Provably Efficient Causal Reinforcement Learning with Confounded Observational Data,"Empowered by neural networks, deep reinforcement learning (DRL) achieves tremendous empirical success. However, DRL requires a large dataset by interacting with the environment, which is unrealistic in critical scenarios such as autonomous driving and personalized medicine. In this paper, we study how to incorporate the dataset collected in the offline setting to improve the sample efficiency in the online setting. To incorporate the observational data, we face two challenges. (a) The behavior policy that generates the observational data may depend on unobserved random variables (confounders), which affect the received rewards and transition dynamics. (b) Exploration in the online setting requires quantifying the uncertainty given both the observational and interventional data. To tackle such challenges, we propose the deconfounded optimistic value iteration (DOVI) algorithm, which incorporates the confounded observational data in a provably efficient manner. DOVI explicitly adjusts for the confounding bias in the observational data, where the confounders are partially observed or unobserved. In both cases, such adjustments allow us to construct the bonus based on a notion of information gain, which takes into account the amount of information acquired from the offline setting. In particular, we prove that the regret of DOVI is smaller than the optimal regret achievable in the pure online setting when the confounded observational data are informative upon the adjustments.","['Reinforcement Learning and Planning', 'Deep Learning', 'Causality']",[],"['Lingxiao Wang', 'Zhuoran Yang', 'Zhaoran Wang']","['Northwestern University', 'Yale University', 'Northwestern University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28457,Fairness & Bias,Fair Sequential Selection Using Supervised Learning Models,"We consider a selection problem where sequentially arrived applicants apply for a limited number of positions/jobs. At each time step, a decision maker accepts or rejects the given applicant using a pre-trained supervised learning model until all the vacant positions are filled. In this paper, we discuss whether the fairness notions (e.g., equal opportunity, statistical parity, etc.) that are commonly used in classification problems are suitable for the sequential selection problems. In particular, we show that even with a pre-trained model that satisfies the common fairness notions, the selection outcomes may still be biased against certain demographic groups. This observation implies that the fairness notions used in classification problems are not suitable for a selection problem where the applicants compete for a limited number of positions.  We introduce a new fairness notion, ``Equal Selection (ES),'' suitable for sequential selection problems and propose a post-processing approach to satisfy the ES fairness notion. We also consider a setting where the applicants have privacy concerns, and the decision maker only has access to the noisy version of sensitive attributes. In this setting, we can show that the \textit{perfect} ES fairness can still be attained under certain conditions.","['Fairness', 'Graph Learning', 'Machine Learning', 'Privacy']",[],"['Mohammad Mahdi Khalili', 'Xueru Zhang', 'Mahed Abroshan']","['Yahoo! Research', 'Ohio State University', 'Optum AI']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27872,Fairness & Bias,Rethinking Neural Operations for Diverse Tasks,"An important goal of AutoML is to automate-away the design of neural networks on new tasks in under-explored domains. Motivated by this goal, we study the problem of enabling users to discover the right neural operations given data from their specific domain. We introduce a search space of operations called XD-Operations that mimic the inductive bias of standard multi-channel convolutions while being much more expressive: we prove that it includes many named operations across multiple application areas. Starting with any standard backbone such as ResNet, we show how to transform it into a search space over XD-operations and how to traverse the space using a simple weight sharing scheme. On a diverse set of tasks—solving PDEs, distance prediction for protein folding, and music modeling—our approach consistently yields models with lower error than baseline networks and often even lower error than expert-designed domain-specific approaches.","['Deep Learning', 'Machine Learning']",[],"['Nicholas Carl Roberts', 'Tri Dao', 'Liam Li', 'Ameet Talwalkar']","['University of Wisconsin-Madison', 'Princeton University', 'Carnegie Mellon University', 'University of California-Los Angeles']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27936,Fairness & Bias,ErrorCompensatedX: error compensation for variance reduced algorithms,"Communication cost is one major bottleneck for the scalability for distributed learning. One approach to reduce the communication cost is to compress the gradient during communication. However, directly compressing the gradient decelerates the convergence speed, and the resulting algorithm may diverge for biased compression. Recent work addressed this problem for stochastic gradient descent by adding back the compression error from the previous step. This idea was further extended to one class of variance reduced algorithms, where the variance of the stochastic gradient is reduced by taking a moving average over all history gradients. However, our analysis shows that just adding the previous step's compression error, as done in existing work, does not fully compensate the compression error. So, we propose ErrorCompensateX, which uses the compression error from the previous two steps. We show that ErrorCompensateX can achieve the same asymptotic convergence rate with the training without compression. Moreover, we  provide a unified theoretical analysis framework for this class of variance reduced algorithms, with or without error compensation.",['Optimization'],[],"['Hanlin Tang', 'Yao Li', 'Ji Liu', 'Ming Yan']","['University of Rochester', 'Michigan State University', 'Facebook', 'Michigan State University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26725,Fairness & Bias,SOPE: Spectrum of Off-Policy Estimators,"Many sequential decision making problems are high-stakes and require off-policy evaluation (OPE) of a new policy using historical data collected using some other policy. One of the most common OPE techniques that provides unbiased estimates is trajectory based importance sampling (IS). However, due to the high variance of trajectory IS estimates, importance sampling methods based on state-action visitation distributions (SIS) have recently been adopted. Unfortunately, while SIS often provides lower variance estimates for long horizons, estimating the state-action distribution ratios can be challenging and lead to biased estimates. In this paper, we present a new perspective on this bias-variance trade-off and show the existence of a spectrum of estimators whose endpoints are SIS and IS. Additionally, we also establish a spectrum for doubly-robust and weighted version of these estimators. We provide empirical evidence that estimators in this spectrum can be used to trade-off between the bias and variance of IS and SIS and can achieve lower mean-squared error than both IS and SIS.",['Reinforcement Learning and Planning'],[],"['Christina Yuan', 'Yash Chandak', 'Stephen Giguere', 'Philip S. Thomas', 'Scott Niekum']","['University of Texas, Austin', 'Computer Science Department, Stanford University', 'University of Texas, Austin', 'College of Information and Computer Science, University of Massachusetts, Amherst', 'University of Massachusetts at Amherst']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26952,Fairness & Bias,Does enforcing fairness mitigate biases caused by subpopulation shift?,"Many instances of algorithmic bias are caused by subpopulation shifts. For example, ML models often perform worse on demographic groups that are underrepresented in the training data. In this paper, we study whether enforcing algorithmic fairness during training improves the performance of the trained model in the \emph{target domain}. On one hand, we conceive scenarios in which enforcing fairness does not improve performance in the target domain. In fact, it may even harm performance. On the other hand, we derive necessary and sufficient conditions under which enforcing algorithmic fairness leads to the Bayes model in the target domain. We also illustrate the practical implications of our theoretical results in simulations and on real data.","['Fairness', 'Graph Learning', 'Domain Adaptation']",[],"['Subha Maity', 'Debarghya Mukherjee', 'Mikhail Yurochkin', 'Yuekai Sun']","['University of Michigan, Ann Arbor', 'Boston University, Boston University', 'International Business Machines', 'University of Michigan']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28746,Fairness & Bias,CoAtNet: Marrying Convolution and Attention for All Data Sizes,"Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced ""coat"" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.","['Transformers', 'Vision']",[],"['Zihang Dai', 'Hanxiao Liu', 'Quoc V Le', 'Mingxing Tan']","['Google', 'Google Brain', 'Google', 'Google']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28501,Fairness & Bias,Multiple Descent: Design Your Own Generalization Curve,"This paper explores the generalization loss of linear regression in variably parameterized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, the locations of those peaks can be explicitly controlled. Our results highlight the fact that both the classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms.",[],[],"['Yifei Min', 'Misha Belkin', 'amin karbasi']","['Yale University', 'University of California, San Diego', 'Yale University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26283,Fairness & Bias,On the Representation Power of Set Pooling Networks,"Point clouds and sets are input data-types which pose unique problems to deep learning. Since sets can have variable cardinality and are unchanged by permutation, the input space for these problems naturally form infinite-dimensional non-Euclidean spaces. Despite these mathematical difficulties, PointNet (Qi et al. 2017) and Deep Sets (Zaheer et al. 2017) introduced foundational neural network architectures to address these problems. In this paper we present a unified framework to study the expressive power of such networks as well as their extensions beyond point clouds (partially addressing a conjecture on the extendibility of DeepSets along the way). To this end, we demonstrate the crucial role that the Hausdorff and Wasserstein metrics play and prove new cardinality-agnostic universality results to characterize exactly which functions can be approximated by these models. In particular, these results imply that PointNet generally cannot approximate averages of continuous functions over sets (e.g. center-of-mass or higher moments) implying that DeepSets is strictly more expressive than PointNet in the constant cardinality setting. Moreover, we obtain explicit lower-bounds on the approximation error and present a simple method to produce arbitrarily many examples of this failure-mode. Counterintuitively, we also prove that in the unbounded cardinality setting that any function which can be uniformly approximated by both PointNet and normalized-DeepSets must be constant. Finally, we also prove theorems on the Lipschitz properties of PointNet and normalized-DeepSets which shed insight into exploitable inductive bias in these networks.","['Deep Learning', 'Machine Learning']",[],['Alan Hylton'],['Glenn Research Center'],[None]
https://nips.cc/virtual/2021/poster/26550,Fairness & Bias,Attention over Learned Object Embeddings Enables Complex Visual Reasoning,"Neural networks have achieved success in a wide array of perceptual tasks but often fail at tasks involving both perception and higher-level reasoning. On these more challenging tasks, bespoke approaches (such as modular symbolic components, independent dynamics models or semantic parsers) targeted towards that specific type of task have typically performed better. The downside to these targeted approaches, however, is that they can be more brittle than general-purpose neural networks, requiring significant modification or even redesign according to the particular task at hand. Here, we propose a more general neural-network-based approach to dynamic visual reasoning problems that obtains state-of-the-art performance on three different domains, in each case outperforming bespoke modular approaches tailored specifically to the task. Our method relies on learned object-centric representations, self-attention and self-supervised dynamics learning, and all three elements together are required for strong performance to emerge. The success of this combination suggests that there may be no need to trade off flexibility for performance on problems involving spatio-temporal or causal-style reasoning. With the right soft biases and learning objectives in a neural network we may be able to attain the best of both worlds.","['Transformers', 'Deep Learning', 'Vision']",[],"['David Ding', 'Felix Hill', 'Adam Santoro', 'Malcolm Reynolds', 'Matthew Botvinick']","['DeepMind', 'Google', 'Google', 'DeepMind', 'Google DeepMind']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27802,Fairness & Bias,Intriguing Properties of Vision Transformers,"Vision transformers (ViT) have demonstrated impressive performance across numerous machine vision tasks. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a)Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b)The robustness towards occlusions is not due to texture bias, instead we show that ViTs are significantly less biased towards local textures, compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c)Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d)Off-the-shelf features from a single ViT model can be combined to create a feature ensemble,  leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms.  We show effective features of ViTs are due to flexible and dynamic receptive fields possible via self-attention mechanisms. Our code will be publicly released.","['Transformers', 'Vision', 'Robustness', 'Machine Learning', 'Deep Learning', 'Few Shot Learning']",[],"['Muzammal Naseer', 'Kanchana Ranasinghe', 'Salman Khan', 'Munawar Hayat', 'Fahad Khan', 'Ming-Hsuan Yang']","['Mohamed bin Zayed University of Artificial Intelligence', 'State University of New York, Stony Brook', 'Mohamed bin Zayed University of Artificial Intelligence', 'Monash University', 'Inception Institute of Artificial Intelligence', 'Google']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26110,Fairness & Bias,Arbitrary Conditional Distributions with Energy,"Modeling distributions of covariates, or density estimation, is a core challenge in unsupervised learning. However, the majority of work only considers the joint distribution, which has limited relevance to practical situations. A more general and useful problem is arbitrary conditional density estimation, which aims to model any possible conditional distribution over a set of covariates, reflecting the more realistic setting of inference based on prior knowledge. We propose a novel method, Arbitrary Conditioning with Energy (ACE), that can simultaneously estimate the distribution $p(\mathbf{x}_u \mid \mathbf{x}_o)$ for all possible subsets of unobserved features $\mathbf{x}_u$ and observed features $\mathbf{x}_o$. ACE is designed to avoid unnecessary bias and complexity --- we specify densities with a highly expressive energy function and reduce the problem to only learning one-dimensional conditionals (from which more complex distributions can be recovered during inference). This results in an approach that is both simpler and higher-performing than prior methods. We show that ACE achieves state-of-the-art for arbitrary conditional likelihood estimation and data imputation on standard benchmarks.",['Self-Supervised Learning'],[],"['Ryan Strauss', 'Junier Oliva']","['Department of Computer Science, University of North Carolina, Chapel Hill', 'University of North Carolina, Chapel Hill']","[None, None]"
https://nips.cc/virtual/2021/poster/28112,Fairness & Bias,A Theoretical Analysis of Fine-tuning with Linear Teachers,"Fine-tuning is a common practice in deep learning, achieving excellent generalization results on downstream tasks using relatively little training data. Although widely used in practice, it is not well understood theoretically. Here we analyze the sample complexity of this scheme for regression with linear teachers in several settings. Intuitively, the success of fine-tuning depends on the similarity between the source tasks and the target task. But what is the right way of measuring this similarity? We show that the relevant measure has to do with the relation between the source task, the target task and the covariance structure of the target data. In the setting of linear regression, we show that under realistic settings there can be substantial sample complexity reduction when the above measure is low. For deep linear regression, we propose a novel result regarding the inductive bias of gradient-based training when the network is initialized with pretrained weights. Using this result we show that the similarity measure for this setting is also affected by the depth of the network. We conclude with results on shallow ReLU models, and analyze the dependence of sample complexity there on source and target tasks. We empirically demonstrate our results for both synthetic and realistic data.","['Transfer Learning', 'Deep Learning', 'Theory']",[],"['Gal Shachaf', 'Alon Brutzkus', 'Amir Globerson']","['Tel Aviv University', 'Electrical Engineering Department, Technion –  Institute of Technology, Technion -  Institute of Technology', 'Tel Aviv University']","[None, 'Israel', None]"
https://nips.cc/virtual/2021/poster/26447,Fairness & Bias,Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefit of Stochasticity,"Understanding the implicit bias of training algorithms is of crucial importance in order to explain the success of overparametrised neural networks. In this paper, we study the dynamics of stochastic gradient descent over diagonal linear networks through its continuous time version, namely stochastic gradient flow. We explicitly characterise the solution chosen by the stochastic flow and prove that it always enjoys better generalisation properties than that of gradient flow.Quite surprisingly, we show that the convergence speed of the training loss controls the magnitude of the biasing effect: the slower the convergence, the better the bias. To fully complete our analysis, we provide convergence guarantees for the dynamics. We also give experimental results which support our theoretical claims. Our findings highlight the fact that structured noise can induce better generalisation and they help explain the greater performances of stochastic gradient  descent over gradient descent observed in practice.","['Deep Learning', 'Optimization']",[],"['Scott Pesme', 'Loucas Pillaud-Vivien', 'Nicolas Flammarion']","['Swiss Federal Institute of Technology Lausanne', 'New York University', 'Swiss Federal Institute of Technology Lausanne']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27462,Fairness & Bias,Reconstruction for Powerful Graph Representations,"Graph neural networks (GNNs) have limited expressive power, failing to represent many graph classes correctly. While more expressive graph representation learning (GRL) alternatives can distinguish some of these classes, they are significantly harder to implement, may not scale well, and have not been shown to outperform well-tuned GNNs in real-world tasks. Thus, devising simple, scalable, and expressive GRL architectures that also achieve real-world improvements remains an open challenge. In this work, we show the extent to which graph reconstruction---reconstructing a graph from its subgraphs---can mitigate the theoretical and practical problems currently faced by GRL architectures. First, we leverage graph reconstruction to build two new classes of expressive graph representations. Secondly, we show how graph reconstruction boosts the expressive power of any GNN architecture while being a (provably) powerful inductive bias for invariances to vertex removals. Empirically,  we show how reconstruction can boost GNN's expressive power---while maintaining its invariance to permutations of the vertices---by solving seven graph property tasks not solvable by the original GNN. Further, we demonstrate how it boosts state-of-the-art GNN's performance across nine real-world benchmark datasets.","['Theory', 'Deep Learning', 'Graph Learning', 'Representation Learning']",[],"['Leonardo Cotta', 'Christopher Morris', 'Bruno Ribeiro']","['Vector Institute', 'Rheinisch Westfälische Technische Hochschule Aachen', 'Purdue University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26962,Fairness & Bias,Differentially Private Empirical Risk Minimization under the Fairness Lens,"Differential Privacy (DP) is an important privacy-enhancing technology for private machine learning systems. It allows to measure and bound the risk associated with an individual participation in a computation. However, it was recently observed that DP learning systems may exacerbate bias and unfairness for different groups of individuals. This paper builds on these important observations and sheds light on the causes of the disparate impacts arising in the problem of differentially private empirical risk minimization. It focuses on the accuracy disparity arising among groups of individuals in two well-studied DP learning methods: output perturbation and differentially private stochastic gradient descent. The paper analyzes which data and model properties are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately, and proposes guidelines to mitigate these effects. The proposed approach is evaluated on several datasets and settings.","['Fairness', 'Optimization', 'Machine Learning', 'Privacy']",[],"['Cuong Tran', 'My H Dinh', 'Ferdinando Fioretto']","['University of Virginia, Charlottesville', 'University of Virginia, Charlottesville', 'University of Virginia, Charlottesville']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27430,Fairness & Bias,Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback,"The ensemble method is a promising way to mitigate the overestimation issue in Q-learning, where multiple function approximators are used to estimate the action values. It is known that the estimation bias hinges heavily on the ensemble size (i.e., the number of  Q-function approximators used in the target), and that determining the 'right' ensemble size is highly nontrivial, because of the time-varying nature of the function approximation errors during the learning process. To tackle this challenge, we first derive an upper bound and a lower bound on the  estimation bias, based on which the ensemble size is  adapted to drive the bias to be nearly zero, thereby coping with the impact of the time-varying approximation errors accordingly. Motivated by the theoretic findings, we advocate that the ensemble method can be combined with Model Identification Adaptive Control (MIAC) for effective ensemble size adaptation. Specifically, we devise Adaptive Ensemble Q-learning (AdaEQ), a generalized ensemble method with two key steps: (a) approximation error characterization which serves as the feedback for flexibly controlling the ensemble size, and (b) ensemble size adaptation tailored towards minimizing the estimation bias.   Extensive experiments are carried out to show that AdaEQ can  improve the learning performance than the existing methods for the MuJoCo benchmark.",[],[],"['Hang Wang', 'Sen Lin', 'Junshan Zhang']","['University of California, Davis', 'University of Houston', 'University of California, Davis']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27877,Fairness & Bias,Counterfactual Explanations Can Be Manipulated,"Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions.  As they are deployed in critical applications (e.g. law enforcement, financial lending), it becomes important to ensure that we clearly understand the vulnerabilties of these methods and find ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the first framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More specifically, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust.  Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations find much lower cost recourse under a slight perturbation.  We describe how these models can unfairly provide low-cost recourse for specific subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.",['Robustness'],[],"['Dylan Z Slack', 'Sophie Hilgard', 'Himabindu Lakkaraju', 'Sameer Singh']","['University of California, Irvine', 'Harvard University', 'Harvard University', 'University of California, Irvine']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26999,Fairness & Bias,Time-series Generation by Contrastive Imitation,"Consider learning a generative model for time-series data. The sequential setting poses a unique challenge: Not only should the generator capture the *conditional* dynamics of (stepwise) transitions, but its open-loop rollouts should also preserve the *joint* distribution of (multi-step) trajectories. On one hand, autoregressive models trained by MLE allow learning and computing explicit transition distributions, but suffer from compounding error during rollouts. On the other hand, adversarial models based on GAN training alleviate such exposure bias, but transitions are implicit and hard to assess. In this work, we study a generative framework that seeks to combine the strengths of both: Motivated by a moment-matching objective to mitigate compounding error, we optimize a local (but forward-looking) *transition policy*, where the reinforcement signal is provided by a global (but stepwise-decomposable) *energy model* trained by contrastive estimation. At training, the two components are learned cooperatively, avoiding the instabilities typical of adversarial objectives. At inference, the learned policy serves as the generator for iterative sampling, and the learned energy serves as a trajectory-level measure for evaluating sample quality. By expressly training a policy to imitate sequential behavior of time-series features in a dataset, this approach embodies ""*generation by imitation*"". Theoretically, we illustrate the correctness of this formulation and the consistency of the algorithm. Empirically, we evaluate its ability to generate predictively useful samples from real-world datasets, verifying that it performs at the standard of existing benchmarks.",['Generative Model'],[],"['Daniel Jarrett', 'Ioana Bica', 'Mihaela van der Schaar']","['DeepMind', 'DeepMind', 'University of Cambridge']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26965,Fairness & Bias,Learning Optimal Predictive Checklists,"Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classifiers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufficiently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. We benchmark the performance of our method on seven clinical classification problems, and demonstrate its practical benefits by training a short-form checklist for PTSD screening. Our results show that our method can fit simple predictive checklists that perform well and that can easily be customized to obey a rich class of custom constraints.","['Fairness', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Haoran Zhang', 'Quaid Morris', 'Berk Ustun', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Memorial Sloan Kettering Cancer Centre', 'University of California, San Diego', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26666,Fairness & Bias,Certifying Robustness to Programmable Data Bias in Decision Trees,"Datasets can be biased due to societal inequities, human biases, under-representation of minorities, etc. Our goal is to certify that models produced by a learning algorithm are pointwise-robust to dataset biases. This is a challenging problem: it entails learning models for a large, or even infinite, number of datasets, ensuring that they all produce the same prediction. We focus on decision-tree learning due to the interpretable nature of the models. Our approach allows programmatically specifying \emph{bias models} across a variety of dimensions (e.g., label-flipping or missing data), composing types of bias, and targeting bias towards a specific group. To certify robustness, we use a novel symbolic technique to evaluate a decision-tree learner on a large, or infinite, number of datasets, certifying that each and every dataset produces the same prediction for a specific test point. We evaluate our approach on datasets that are commonly used in the fairness literature, and demonstrate our approach's viability on a range of bias models.","['Robustness', 'Fairness']",[],"['Anna P. Meyer', 'Aws Albarghouthi', ""Loris D'Antoni""]","['University of Wisconsin - Madison', 'Amazon', 'University of Wisconsin, Madison']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27192,Fairness & Bias,Introspective Distillation for Robust Question Answering,"Question answering (QA) models are well-known to exploit data bias, e.g., the language prior in visual QA and the position bias in reading comprehension. Recent debiasing methods achieve good out-of-distribution (OOD) generalizability with a considerable sacrifice of the in-distribution (ID) performance. Therefore, they are only applicable in domains where the test distribution is known in advance. In this paper, we present a novel debiasing method called Introspective Distillation (IntroD) to make the best of both worlds for QA. Our key technical contribution is to blend the inductive bias of OOD and ID by introspecting whether a training sample fits in the factual ID world or the counterfactual OOD one. Experiments on visual QA datasets VQA v2, VQA-CP, and reading comprehension dataset SQuAD demonstrate that our proposed IntroD maintains the competitive OOD performance compared to other debiasing methods, while sacrificing little or even achieving better ID performance compared to the non-debiasing ones.",[],[],"['Yulei Niu', 'Hanwang Zhang']","['Columbia University', 'Nanyang Technological University']","[None, None]"
https://nips.cc/virtual/2021/poster/26027,Fairness & Bias,Bandit Learning with Delayed Impact of Actions,"We consider a stochastic multi-armed bandit (MAB) problem with delayed impact of actions. In our setting, actions taken in the past impact the arm rewards in the subsequent future. This delayed impact of actions is prevalent in the real world. For example, the capability to pay back a loan for people in a certain social group might depend on historically how frequently that group has been approved loan applications. If banks keep rejecting loan applications to people in a disadvantaged group, it could create a feedback loop and further damage the chance of getting loans for people in that group. In this paper, we formulate this delayed and long-term impact of actions within the context of multi-armed bandits. We generalize the bandit setting to encode the dependency of this ``bias"" due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. We propose an algorithm that achieves a regret of $\tilde{O}(KT^{2/3})$ and show a matching regret lower bound of $\Omega(KT^{2/3})$, where $K$ is the number of arms and $T$ is the learning horizon. Our results complement the bandit literature by adding techniques to deal with actions with long-term impacts and have implications in designing fair algorithms.",['Bandits'],[],"['Wei Tang', 'Chien-Ju Ho', 'Yang Liu']","['Columbia University', 'Washington University in St. Louis', 'University of California, Santa Cruz']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27252,Privacy & Data Governance,FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective,"Federated learning (FL) is a popular distributed learning framework that trains a global model through iterative communications between a central server and edge devices. Recent works have demonstrated that FL is vulnerable to model poisoning attacks. Several server-based defense approaches (e.g. robust aggregation), have been proposed to mitigate such attacks. However, we empirically show that under extremely strong attacks, these defensive methods fail to guarantee the robustness of FL. More importantly, we observe that as long as the global model is polluted, the impact of attacks on the global model will remain in subsequent rounds even if there are no subsequent attacks. In this work, we propose a client-based defense, named White Blood Cell for Federated Learning (FL-WBC), which can mitigate model poisoning attacks that have already polluted the global model. The key idea of FL-WBC is to identify the parameter space where long-lasting attack effect on parameters resides and perturb that space during local training. Furthermore, we derive a certified robustness guarantee against model poisoning attacks and a convergence guarantee to FedAvg after applying our FL-WBC. We conduct experiments on FasionMNIST and CIFAR10 to evaluate the defense against state-of-the-art model poisoning attacks. The results demonstrate that our method can effectively mitigate model poisoning attack impact on the global model within 5 communication rounds with nearly no accuracy drop under both IID and Non-IID settings. Our defense is also complementary to existing server-based robust aggregation approaches and can further improve the robustness of FL under extremely strong attacks.","['Federated Learning', 'Robustness']",[],"['Jingwei Sun', 'Ang Li', 'Louis DiValentin', 'Yiran Chen', 'Hai Li']","['Duke University', 'University of Maryland, College Park', 'Accenture', 'Duke University', 'Duke University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27151,Privacy & Data Governance,TRS: Transferability Reduced Ensemble via Promoting Gradient Diversity and Model Smoothness,"Adversarial Transferability is an intriguing property - adversarial perturbation crafted against one model is also effective against another model, while these models are from different model families or training processes. To better protect ML systems against adversarial attacks, several questions are raised: what are the sufficient conditions for adversarial transferability, and how to bound it? Is there a way to reduce the adversarial transferability in order to improve the robustness of an ensemble ML model? To answer these questions, in this work we first theoretically analyze and outline sufficient conditions for adversarial transferability between models; then propose a practical algorithm to reduce the transferability between base models within an ensemble to improve its robustness. Our theoretical analysis shows that only promoting the orthogonality between gradients of base models is not enough to ensure low transferability; in the meantime, the model smoothness is an important factor to control the transferability. We also provide the lower and upper bounds of adversarial transferability under certain conditions. Inspired by our theoretical analysis, we propose an effective Transferability Reduced Smooth (TRS) ensemble training strategy to train a robust ensemble with low transferability by enforcing both gradient orthogonality and model smoothness between base models. We conduct extensive experiments on TRS and compare with 6 state-of-the-art ensemble baselines against 8 whitebox attacks on different datasets, demonstrating that the proposed TRS outperforms all baselines significantly.","['Adversarial Robustness and Security', 'Robustness']",[],"['Zhuolin Yang', 'Linyi Li', 'Xiaojun Xu', 'Shiliang Zuo', 'Pan Zhou', 'Benjamin I. P. Rubinstein', 'Ce Zhang', 'Bo Li']","['University of Illinois, Urbana Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Huazhong University of Science and Technology', 'The University of Melbourne', 'University of Chicago', 'University of Illinois, Urbana Champaign']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27235,Privacy & Data Governance,Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations,"When compared to the image classification models, black-box adversarial attacks against video classification models have been largely understudied. This could be possible because, with video, the temporal dimension poses significant additional challenges in gradient estimation. Query-efficient black-box attacks rely on effectively estimated gradients towards maximizing the probability of misclassifying the target video. In this work, we demonstrate that such effective gradients can be searched for by parameterizing the temporal structure of the search space with geometric transformations. Specifically, we design a novel iterative algorithm GEOmetric TRAnsformed Perturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP employs standard geometric transformation operations to reduce the search space for effective gradients into searching for a small group of parameters that define these operations. This group of parameters describes the geometric progression of gradients, resulting in a reduced and structured search space. Our algorithm inherently leads to successful perturbations with surprisingly few queries. For example, adversarial examples generated from GEO-TRAP have better attack success rates with ~73.55% fewer queries compared to the state-of-the-art method for video adversarial attacks on the widely used Jester dataset. Overall, our algorithm exposes vulnerabilities of diverse video classification models and achieves new state-of-the-art results under black-box settings on two large datasets.","['Adversarial Robustness and Security', 'Vision', 'Machine Learning']",[],"['Shasha Li', 'Abhishek Aich', 'Shitong Zhu', 'Salman Asif', 'Chengyu Song', 'Amit Roy-Chowdhury', 'Srikanth Krishnamurthy']","['Amazon', 'NEC Laboratories, America', 'Facebook', 'University of California, Riverside', 'University of California, Riverside', 'University of California, Riverside', ', University of California, Riverside']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28025,Privacy & Data Governance,Shift Invariance Can Reduce Adversarial Robustness,"Shift invariance is a critical property of CNNs that improves performance on classification.  However, we show that invariance to circular shifts can also lead to greater sensitivity to adversarial attacks.  We first characterize the margin between classes when a shift-invariant {\em linear} classifier is used. We show that the margin can only depend on the DC component of the signals.  Then, using results about infinitely wide networks, we show that in some simple cases, fully connected and shift-invariant neural networks produce linear decision boundaries.  Using this, we prove that shift invariance in neural networks produces adversarial examples for the simple case of two classes, each consisting of a single image with a black or white dot on a gray background.  This is more than a curiosity; we show empirically that with real datasets and realistic architectures, shift invariance reduces adversarial robustness.  Finally, we describe initial experiments using synthetic data to probe the source of this connection.","['Adversarial Robustness and Security', 'Deep Learning', 'Machine Learning', 'Robustness']",[],"['Vasu Singla', 'Songwei Ge', 'Ronen Basri', 'David Jacobs']","['University of Maryland, College Park', 'University of Maryland, College Park', 'Meta Platforms Inc.', 'University of Maryland']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28058,Privacy & Data Governance,Disrupting Deep Uncertainty Estimation Without Harming Accuracy,"Deep neural networks (DNNs) have proven to be powerful predictors and are widely used for various tasks. Credible uncertainty estimation of their predictions, however, is crucial for their deployment in many risk-sensitive applications. In this paper we present a novel and simple attack, which unlike adversarial attacks, does not cause incorrect predictions but instead cripples the network's capacity for uncertainty estimation. The result is that after the attack, the DNN is more confident of its incorrect predictions than about its correct ones without having its accuracy reduced. We present two versions of the attack. The first scenario focuses on a black-box regime (where the attacker has no knowledge of the target network) and the second scenario attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimations. We demonstrate successful attacks on three of the most popular uncertainty estimation methods: the vanilla softmax score, Deep Ensembles and MC-Dropout. Additionally, we show an attack on SelectiveNet, the selective classification architecture. We test the proposed attack on several contemporary architectures such as MobileNetV2 and EfficientNetB0, all trained to classify ImageNet.","['Adversarial Robustness and Security', 'Deep Learning', 'Machine Learning']",[],"['Ido Galil', 'Ran El-Yaniv']","['Computer Science Departmen, Technion- Institute of Technology', 'Technion']","['Israel', None]"
https://nips.cc/virtual/2021/poster/26485,Privacy & Data Governance,A Trainable Spectral-Spatial Sparse Coding Model for Hyperspectral Image Restoration,"Hyperspectral imaging offers new perspectives for diverse applications, ranging from the monitoring of the environment using airborne or satellite remote sensing, precision farming, food safety, planetary exploration, or astrophysics. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation,  and the lack of accurate ground-truth ""clean"" hyperspectral signals acquired on the spot makes restoration tasks challenging.  In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems where deep models tend to shine. In this paper, we advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data. We show on various denoising benchmarks that our method is computationally efficient and  significantly outperforms the state of the art.","['Deep Learning', 'Interpretability']",[],"['Theo Bodrito', 'Alexandre Zouaoui', 'Jocelyn Chanussot', 'Julien Mairal']","['INRIA', 'INRIA', 'INRIA', 'Inria']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27331,Privacy & Data Governance,Excess Capacity and Backdoor Poisoning,"A backdoor data poisoning attack is an adversarial attack wherein the attacker injects several watermarked, mislabeled training examples into a training set. The watermark does not impact the test-time performance of the model on typical data; however, the model reliably errs on watermarked examples. To gain a better foundational understanding of backdoor data poisoning attacks, we present a formal theoretical framework within which one can discuss backdoor data poisoning attacks for classification problems. We then use this to analyze important statistical and computational issues surrounding these attacks. On the statistical front, we identify a parameter we call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows us to argue about the robustness of several natural learning problems to backdoor attacks. Our results favoring the attacker involve presenting explicit constructions of backdoor attacks, and our robustness results show that some natural problem settings cannot yield successful backdoor attacks. From a computational standpoint, we show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. We then show that under similar assumptions, two closely related problems we call backdoor filtering and robust generalization are nearly equivalent. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors.","['Theory', 'Robustness', 'Adversarial Robustness and Security', 'Machine Learning']",[],"['Naren Sarayu Manoj', 'Avrim Blum']","['EPFL - EPF Lausanne', 'Toyota Technological Institute at Chicago']","[None, None]"
https://nips.cc/virtual/2021/poster/26136,Privacy & Data Governance,Clustering Effect of Adversarial Robust Models,"Adversarial robustness has received increasing attention along with the study of adversarial examples. So far, existing works show that robust models not only obtain robustness against various adversarial attacks but also boost the performance in some downstream tasks. However, the underlying mechanism of adversarial robustness is still not clear. In this paper, we interpret adversarial robustness from the perspective of linear components, and find that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, we propose a novel understanding of adversarial robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of our proposed clustering strategy. Our code is available at https://github.com/bymavis/Adv_Weight_NeurIPS2021.","['Clustering', 'Robustness', 'Adversarial Robustness and Security']",[],"['Yang Bai', 'Xin Yan', 'Yong Jiang', 'Shu-Tao Xia', 'Yisen Wang']","['Tencent Zhuque Lab', 'Tsinghua University', 'Tsinghua University', 'Shenzhen International Graduate School, Tsinghua University', 'Peking University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27499,Privacy & Data Governance,Accumulative Poisoning Attacks on Real-time Data,"Collecting training data from untrusted sources exposes machine learning services to poisoning adversaries, who maliciously manipulate training data to degrade the model accuracy. When trained on offline datasets, poisoning adversaries have to inject the poisoned data in advance before training, and the order of feeding these poisoned batches into the model is stochastic. In contrast, practical systems are more usually trained/fine-tuned on sequentially captured real-time data, in which case poisoning adversaries could dynamically poison each data batch according to the current model state. In this paper, we focus on the real-time settings and propose a new attacking strategy, which affiliates an accumulative phase with poisoning attacks to secretly (i.e., without affecting accuracy) magnify the destructive effect of a (poisoned) trigger batch. By mimicking online learning and federated learning on MNIST and CIFAR-10, we show that model accuracy significantly drops by a single update step on the trigger batch after the accumulative phase. Our work validates that a well-designed but straightforward attacking strategy can dramatically amplify the poisoning effects, with no need to explore complex techniques.","['Federated Learning', 'Online Learning', 'Machine Learning']",[],"['Tianyu Pang', 'Xiao Yang', 'Yinpeng Dong', 'Hang Su', 'Jun Zhu']","['Sea AI Lab', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tsinghua University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27458,Privacy & Data Governance,Visualizing the Emergence of Intermediate Visual Patterns in DNNs,"This paper proposes a method to visualize the discrimination power of intermediate-layer visual patterns encoded by a DNN. Specifically, we visualize (1) how the DNN gradually learns regional visual patterns in each intermediate layer during the training process, and (2) the effects of the DNN using non-discriminative patterns in low layers to construct disciminative patterns in middle/high layers through the forward propagation. Based on our visualization method, we can quantify knowledge points (i.e. the number of discriminative visual patterns) learned by the DNN to evaluate the representation capacity of the DNN. Furthermore, this method also provides new insights into signal-processing behaviors of existing deep-learning techniques, such as adversarial attacks and knowledge distillation.","['Adversarial Robustness and Security', 'Deep Learning']",[],"['Mingjie Li', 'Shaobo Wang', 'Quanshi Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27065,Privacy & Data Governance,Characterizing the risk of fairwashing,"Fairwashing refers to the risk that an unfair black-box model can be explained by a fairer model through post-hoc explanation manipulation. In this paper, we investigate the capability of fairwashing attacks by analyzing their fidelity-unfairness trade-offs. In particular, we show that fairwashed explanation models can generalize beyond the suing group (i.e., data points that are being explained), meaning that a fairwashed explainer can be used to rationalize subsequent unfair decisions of a black-box model. We also demonstrate that fairwashing attacks can transfer across black-box models, meaning that other black-box models can perform fairwashing without explicitly using their predictions. This generalization and transferability of fairwashing attacks imply that their detection will be difficult in practice. Finally, we propose an approach to quantify the risk of fairwashing, which is based on the computation of the range of the unfairness of high-fidelity explainers.",['Fairness'],[],"['Ulrich Aïvodji', 'Hiromi Arai', 'Sébastien Gambs', 'Satoshi Hara']","['École de technologie supérieure, Université du Québec', 'RIKEN', 'Université du Québec à Montréal', 'Osaka University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28776,Privacy & Data Governance,Provably Efficient Black-Box Action Poisoning Attacks Against Reinforcement Learning,"Due to the broad range of applications of reinforcement learning (RL), understanding the effects of adversarial attacks against RL model is essential for the safe applications of this model. Prior theoretical works on adversarial attacks against RL mainly focus on either reward poisoning attacks or environment poisoning attacks. In this paper, we introduce a new class of attacks named action poisoning attacks, where an adversary can change the action signal selected by the agent. Compared with existing attack models, the attacker’s ability in the proposed action poisoning attack model is more restricted, which brings some design challenges. We study the action poisoning attack in both white-box and black-box settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the black-box setting. We prove that LCB-H attack can force any efficient RL agent, whose dynamic regret scales sublinearly with the total number of steps taken, to choose actions according to a policy selected by the attacker very frequently, with only sublinear cost. In addition, we apply LCB-H attack against a very popular model-free RL algorithm: UCB-H. We show that, even in black-box setting, by spending only logarithm cost, the proposed LCB-H attack scheme can force the UCB-H agent to choose actions according to the policy selected by the attacker very frequently.","['Reinforcement Learning and Planning', 'Adversarial Robustness and Security']",[],"['Guanlin Liu', 'Lifeng Lai']","['University of California, Davis', 'University of California, Davis']","[None, None]"
https://nips.cc/virtual/2021/poster/27822,Privacy & Data Governance,Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending Against Adversarial Attacks,"Deep neural networks (DNNs) are well-known to be vulnerable to adversarial attacks, where malicious human-imperceptible perturbations are included in the input to the deep network to fool it into making a wrong classification. Recent studies have demonstrated that neural Ordinary Differential Equations (ODEs) are intrinsically more robust against adversarial attacks compared to vanilla DNNs. In this work, we propose a neural ODE with Lyapunov-stable equilibrium points for defending against adversarial attacks (SODEF). By ensuring that the equilibrium points of the ODE solution used as part of SODEF are Lyapunov-stable, the ODE solution for an input with a small perturbation converges to the same solution as the unperturbed input. We provide theoretical results that give insights into the stability of SODEF as well as the choice of regularizers to ensure its stability. Our analysis suggests that our proposed regularizers force the extracted feature points to be within a neighborhood of the Lyapunov-stable equilibrium points of the SODEF ODE. SODEF is compatible with many defense methods and can be applied to any neural network's final regressor layer to enhance its stability against adversarial attacks.","['Adversarial Robustness and Security', 'Deep Learning', 'Machine Learning']",[],"['QIYU KANG', 'Yang Song', 'Qinxu Ding', 'Wee Peng Tay']","['Nanyang Technological University', 'C3 AI', 'University of Social Sciences', 'Nanyang Technological University']","[None, None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/28871,Privacy & Data Governance,Learning Robust Hierarchical Patterns of Human Brain across Many fMRI Studies,"Multi-site fMRI studies face the challenge that the pooling introduces systematic non-biological site-specific variance due to hardware, software, and environment. In this paper, we propose to reduce site-specific variance in the estimation of hierarchical Sparsity Connectivity Patterns (hSCPs) in fMRI data via a simple yet effective matrix factorization while preserving biologically relevant variations. Our method leverages unsupervised adversarial learning to improve the reproducibility of the components. Experiments on simulated datasets display that the proposed method can estimate components with higher accuracy and reproducibility, while preserving age-related variation on a multi-center clinical data set.",['Domain Adaptation'],[],"['Dushyant Sahoo', 'Christos Davatzikos']","['School of Engineering and Applied Science, University of Pennsylvania', 'University of Pennsylvania']","[None, None]"
https://nips.cc/virtual/2021/poster/28868,Privacy & Data Governance,Adversarially Robust Change Point Detection,"Change point detection is becoming increasingly popular in many application areas. On one hand, most of the theoretically-justified methods are investigated in an ideal setting without model violations, or merely robust against identical heavy-tailed noise distribution across time and/or against isolate outliers; on the other hand, we are aware that there have been exponentially growing attacks from adversaries, who may pose systematic contamination on data to purposely create spurious change points or disguise true change points. In light of the timely need for a change point detection method that is robust against adversaries, we start with, arguably, the simplest univariate mean change point detection problem. The adversarial attacks are formulated through the Huber $\varepsilon$-contamination framework, which in particular allows the contamination distributions to be different at each time point. In this paper, we demonstrate a phase transition phenomenon in change point detection. This detection boundary is a function of the contamination proportion~$\varepsilon$ and is the first time shown in the literature. In addition, we derive the minimax-rate optimal localisation error rate, quantifying the cost of accuracy in terms of the contamination proportion. We propose a computationally feasible method, matching the minimax lower bound under certain conditions, saving for logarithmic factors. Extensive numerical experiments are conducted with comparisons to robust change point detection methods in the existing literature.",['Adversarial Robustness and Security'],[],"['Mengchu Li', 'Yi Yu']","['University of Warwick', 'University of Warwick']","[None, None]"
https://nips.cc/virtual/2021/poster/28827,Privacy & Data Governance,Neural Architecture Dilation for Adversarial Robustness,"With the tremendous advances in the architecture and scale of convolutional neural networks (CNNs) over the past few decades, they can easily reach or even exceed the performance of humans in certain tasks. However, a recently discovered shortcoming of CNNs is that they are vulnerable to adversarial attacks. Although the adversarial robustness of CNNs can be improved by adversarial training, there is a trade-off between standard accuracy and adversarial robustness. From the neural architecture perspective, this paper aims to improve the adversarial robustness of the backbone CNNs that have a satisfactory accuracy. Under a minimal computational overhead, the introduction of a dilation architecture is expected to be friendly with the standard performance of the backbone CNN while pursuing adversarial robustness. Theoretical analyses on the standard and adversarial error bounds naturally motivate the proposed neural architecture dilation algorithm. Experimental results on real-world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm to balance the accuracy and adversarial robustness.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Yanxi Li', 'Zhaohui Yang', 'Yunhe Wang', 'Chang Xu']","['University of Sydney, University of Sydney', 'Peking University', ""Huawei Noah's Ark Lab"", 'University of Sydney']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28697,Privacy & Data Governance,Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks,"Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Hanxun Huang', 'Yisen Wang', 'Sarah Monazam Erfani', 'Quanquan Gu', 'Xingjun Ma']","['University of Melbourne', 'Peking University', 'The University of Melbourne', 'University of California, Los Angeles', 'Fudan University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28685,Privacy & Data Governance,Morié Attack (MA): A New Potential Risk of Screen Photos,"Images, captured by a camera, play a critical role in training Deep Neural Networks (DNNs). Usually, we assume the images acquired by cameras are consistent with the ones perceived by human eyes. However, due to the different physical mechanisms between human-vision and computer-vision systems, the final perceived images could be very different in some cases, for example shooting on digital monitors. In this paper, we find a special phenomenon in digital image processing, the moiré effect, that could cause unnoticed security threats to DNNs. Based on it, we propose a Moiré Attack (MA) that generates the physical-world moiré pattern adding to the images by mimicking the shooting process of digital devices. Extensive experiments demonstrate that our proposed digital Moiré Attack (MA) is a perfect camouflage for attackers to tamper with DNNs with a high success rate ($100.0\%$ for untargeted and $97.0\%$ for targeted attack with the noise budget $\epsilon=4$), high transferability rate across different models, and high robustness under various defenses. Furthermore, MA owns great stealthiness because the moiré effect is unavoidable due to the camera's inner physical structure, which therefore hardly attracts the awareness of humans. Our code is available at https://github.com/Dantong88/Moire_Attack.","['Adversarial Robustness and Security', 'Deep Learning', 'Vision', 'Robustness']",[],"['Dantong Niu', 'Ruohao Guo', 'Yisen Wang']","['University of California, Berkeley', 'Peking University', 'Peking University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28680,Privacy & Data Governance,Medical Dead-ends and Learning to Identify High-Risk States and Treatments,"Machine learning has successfully framed many sequential decision making problems as either supervised prediction, or optimal decision-making policy identification via reinforcement learning. In data-constrained offline settings, both approaches may fail as they assume fully optimal behavior or rely on exploring alternatives that may not exist. We introduce an inherently different approach that identifies ""dead-ends"" of a state space. We focus on patient condition in the intensive care unit, where a ""medical dead-end"" indicates that a patient will expire, regardless of all potential future treatment sequences. We postulate ""treatment security"" as avoiding treatments with probability proportional to their chance of leading to dead-ends, present a formal proof, and frame discovery as an RL problem. We then train three independent deep neural models for automated state construction, dead-end discovery and confirmation. Our empirical results discover that dead-ends exist in real clinical data among septic patients, and further reveal gaps between secure treatments and those administered.","['Reinforcement Learning and Planning', 'Machine Learning']",[],"['Mehdi Fatemi', 'Taylor W. Killian', 'Jayakumar Subramanian', 'Marzyeh Ghassemi']","['Microsoft', 'Massachusetts Institute of Technology', 'Adobe Systems', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28605,Privacy & Data Governance,Multi-Objective SPIBB: Seldonian Offline Policy Improvement with Safety Constraints in Finite MDPs,"We study the problem of Safe Policy Improvement (SPI) under constraints in the offline Reinforcement Learning (RL) setting. We consider the scenario where: (i) we have a dataset collected under a known baseline policy, (ii) multiple reward signals are received from the environment inducing as many objectives to optimize. We present an SPI formulation for this RL setting that takes into account the preferences of the algorithm’s user for handling the trade-offs for different reward signals while ensuring that the new policy performs at least as well as the baseline policy along each individual objective. We build on traditional SPI algorithms and propose a novel method based on Safe Policy Iteration with Baseline Bootstrapping (SPIBB, Laroche et al., 2019) that provides high probability guarantees on the performance of the agent in the true environment. We show the effectiveness of our method on a synthetic grid-world safety task as well as in a real-world critical care context to learn a policy for the administration of IV fluids and vasopressors to treat sepsis.",['Reinforcement Learning and Planning'],[],"['Harsh Satija', 'Philip S. Thomas', 'Joelle Pineau', 'Romain Laroche']","['McGill University', 'College of Information and Computer Science, University of Massachusetts, Amherst', 'Facebook', 'Microsoft']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28561,Privacy & Data Governance,Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust Control Design: Implicit Regularization and Sample Complexity,"Direct policy search serves as one of the workhorses in modern reinforcement learning (RL), and its applications in continuous control tasks have recently attracted increasing attention. In this work, we investigate the convergence theory of policy gradient (PG) methods for learning the linear risk-sensitive and robust controller. In particular, we develop PG methods that can be implemented in a derivative-free fashion by sampling system trajectories, and establish both global convergence and sample complexity results in the solutions of two fundamental settings in risk-sensitive and robust control: the finite-horizon linear exponential quadratic Gaussian, and the finite-horizon linear-quadratic disturbance attenuation problems. As a by-product, our results also provide the first sample complexity for the global convergence of PG methods on solving zero-sum linear-quadratic dynamic games, a nonconvex-nonconcave minimax optimization problem that serves as a baseline setting in multi-agent reinforcement learning (MARL) with continuous spaces. One feature of our algorithms is that during the learning phase, a certain level of robustness/risk-sensitivity of the controller is preserved, which we termed as the implicit regularization property, and is an essential requirement in safety-critical control systems.","['Reinforcement Learning and Planning', 'Theory', 'Optimization']",[],"['Kaiqing Zhang', 'Xiangyuan Zhang', 'Bin Hu', 'Tamer Basar']","['University of Maryland, College Park', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28530,Privacy & Data Governance,Automated Discovery of Adaptive Attacks on Adversarial Defenses,"Reliable evaluation of adversarial defenses is a challenging task, currently limited to an expert who manually crafts attacks that exploit the defense’s inner workings, or to approaches based on ensemble of fixed attacks, none of which may be effective for the specific defense at hand. Our key observation is that adaptive attacks are composed from a set of reusable building blocks that can be formalized in a search space and used to automatically discover attacks for unknown defenses. We evaluated our approach on 24 adversarial defenses and show that it outperforms AutoAttack, the current state-of-the-art tool for reliable evaluation of adversarial defenses: our tool discovered significantly stronger attacks by producing 3.0%-50.8% additional adversarial examples for 10 models, while obtaining attacks with slightly stronger or similar strength for the remaining models.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Chengyuan Yao', 'Pavol Bielik', 'PETAR TSANKOV', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'LatticeFlow', 'LatticeFlow', 'Swiss Federal Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28523,Privacy & Data Governance,Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness,"We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck as a regularizer for learning an adversarially robust deep neural network classifier. In addition to the usual cross-entropy loss, we add regularization terms for every intermediate layer to ensure that the latent representations retain useful information for output prediction while reducing redundant information. We show that the HSIC bottleneck enhances robustness to adversarial attacks both theoretically and experimentally. In particular, we prove that the HSIC bottleneck regularizer reduces the sensitivity of the classifier to adversarial examples. Our experiments on multiple benchmark datasets and architectures demonstrate that incorporating an HSIC bottleneck regularizer attains competitive natural accuracy and improves adversarial robustness, both with and without adversarial examples during training. Our code and adversarially robust models are publicly available.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Zifeng Wang', 'Tong Jian', 'Aria Masoomi', 'Stratis Ioannidis', 'Jennifer Dy']","['Google', 'Analog Devices', 'Northeastern University', 'Northeastern University', 'Northeastern University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28481,Privacy & Data Governance,Are Transformers more robust than CNNs?,"Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks,  recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at: https://github.com/ytongbai/ViTs-vs-CNNs.","['Transformers', 'Deep Learning', 'Adversarial Robustness and Security', 'Robustness']",[],"['Yutong Bai', 'Jieru Mei', 'Alan Yuille', 'Cihang Xie']","['Facebook', 'Johns Hopkins University', 'Johns Hopkins University', 'University of California, Santa Cruz']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28470,Privacy & Data Governance,Reliable and Trustworthy Machine Learning for Health Using Dataset Shift Detection,"Unpredictable ML model behavior on unseen data, especially in the health domain, raises serious concerns about its safety as repercussions for mistakes can be fatal. In this paper, we explore the feasibility of using state-of-the-art out-of-distribution detectors for reliable and trustworthy diagnostic predictions. We select publicly available deep learning models relating to various health conditions (e.g., skin cancer, lung sound, and Parkinson's disease) using various input data types (e.g., image, audio, and motion data). We demonstrate that these models show unreasonable predictions on out-of-distribution datasets. We show that Mahalanobis distance- and Gram matrices-based out-of-distribution detection methods are able to detect out-of-distribution data with high accuracy for the health models that operate on different modalities. We then translate the out-of-distribution score into a human interpretable \textsc{confidence score} to investigate its effect on the users' interaction with health ML applications. Our user study shows that the \textsc{confidence score} helped the participants only trust the results with a high score to make a medical decision and disregard results with a low score. Through this work, we demonstrate that dataset shift is a critical piece of information for high-stake ML applications, such as medical diagnosis and healthcare, to provide reliable and trustworthy predictions to the users.","['Deep Learning', 'Machine Learning']",[],"['Chunjong Park', 'Anas Awadalla', 'Shwetak Patel']","['Research, Google', 'Department of Computer Science, University of Washington', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28463,Privacy & Data Governance,On the Algorithmic Stability of Adversarial Training,"The adversarial training is a popular tool to remedy the vulnerability of deep learning models against adversarial attacks, and there is rich theoretical literature on the training loss of adversarial training algorithms. In contrast, this paper studies the algorithmic stability of a generic adversarial training algorithm, which can further help to establish an upper bound for generalization error. By figuring out the stability upper bound and lower bound, we argue that the non-differentiability issue of adversarial training causes worse algorithmic stability than their natural counterparts. To tackle this problem, we consider a noise injection method. While the non-differentiability problem seriously affects the stability of adversarial training, injecting noise enables the training trajectory to avoid the occurrence of non-differentiability with dominating probability, hence enhancing the stability performance of adversarial training. Our analysis also studies the relation between the algorithm stability and numerical approximation error of adversarial attacks.","['Adversarial Robustness and Security', 'Deep Learning']",[],"['Yue Xing', 'Qifan Song', 'Guang Cheng']","['Michigan State University', 'Purdue University', 'University of California, Los Angeles']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28450,Privacy & Data Governance,Safe Pontryagin Differentiable Programming,"We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodology, which establishes a theoretical and algorithmic  framework to solve a broad class of safety-critical learning and control tasks---problems that require the guarantee of safety constraint satisfaction at any stage of the learning and control progress.  In the spirit of interior-point methods,   Safe PDP handles different types of system  constraints on states and inputs by incorporating them into the cost or loss through barrier functions. We prove three fundamentals  of the proposed  Safe PDP:  first, both the  solution and its gradient in the backward pass can be approximated by solving their  more efficient unconstrained counterparts;  second,   the approximation for both the  solution and its gradient can be controlled for arbitrary accuracy by a  barrier parameter;   and third,   importantly, all intermediate results throughout the approximation and optimization  strictly respect the  constraints,  thus guaranteeing safety throughout the entire learning and control process. We demonstrate the capabilities of   Safe PDP in solving various safety-critical tasks,  including safe policy optimization, safe motion planning, and learning MPCs from demonstrations, on different challenging systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.","['Reinforcement Learning and Planning', 'Optimization']",[],"['Wanxin Jin', 'Shaoshuai Mou', 'George J. Pappas']","['Arizona State University', 'Purdue University', 'School of Engineering and Applied Science, University of Pennsylvania']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28435,Privacy & Data Governance,Not All Low-Pass Filters are Robust in Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) are promising deep learning approaches in learning representations for graph-structured data. Despite the proliferation of such methods, it is well known that they are vulnerable to carefully crafted adversarial attacks on the graph structure. In this paper, we first conduct an adversarial vulnerability analysis based on matrix perturbation theory. We prove that the low- frequency components of the symmetric normalized Laplacian, which is usually used as the convolutional filter in GCNs, could be more robust against structural perturbations when their eigenvalues fall into a certain robust interval. Our results indicate that not all low-frequency components are robust to adversarial attacks and provide a deeper understanding of the relationship between graph spectrum and robustness of GCNs. Motivated by the theory, we present GCN-LFR, a general robust co-training paradigm for GCN-based models, that encourages transferring the robustness of low-frequency components with an auxiliary neural network. To this end, GCN-LFR could enhance the robustness of various kinds of GCN-based models against poisoning structural attacks in a plug-and-play manner. Extensive experiments across five benchmark datasets and five GCN-based models also confirm that GCN-LFR is resistant to the adversarial attacks without compromising on performance in the benign situation.","['Graph Learning', 'Robustness', 'Adversarial Robustness and Security', 'Theory', 'Deep Learning', 'Representation Learning']",[],"['Heng Chang', 'Yu Rong', 'Tingyang Xu', 'Yatao Bian', 'Shiji Zhou', 'Xin Wang', 'Junzhou Huang', 'Wenwu Zhu']","['Tsinghua University, Tsinghua University', 'Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'University of Texas, Arlington', 'Tsinghua University, Tsinghua University']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28421,Privacy & Data Governance,Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection,"Detecting out-of-distribution (OOD) samples is vital for developing machine learning based models for critical safety systems. Common approaches for OOD detection assume access to some OOD samples during training which may not be available in a real-life scenario. Instead, we utilize the {\em predictive normalized maximum likelihood} (pNML) learner, in which no assumptions are made on the tested input. We derive an explicit expression of the pNML and its generalization error, denoted as the regret, for a single layer neural network (NN). We show that this learner generalizes well when (i) the test vector resides in a subspace spanned by the eigenvectors associated with the large eigenvalues of the empirical correlation matrix of the training data, or (ii) the test sample is far from the decision boundary. Furthermore, we describe how to efficiently apply the derived pNML regret to any pretrained deep NN, by employing the explicit pNML for the last layer, followed by the softmax function. Applying the derived regret to deep NN requires neither additional tunable parameters nor extra data. We extensively evaluate our approach on 74 OOD detection benchmarks using DenseNet-100, ResNet-34, and WideResNet-40 models trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30 showing a significant improvement of up to 15.6% over recent leading methods.","['Theory', 'Deep Learning', 'Machine Learning']",[],"['Koby Bibas', 'Meir Feder', 'Tal Hassner']","['Tel Aviv University, Tel Aviv University', 'Tel Aviv University', 'Meta inc.']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28402,Privacy & Data Governance,Online Selective Classification with Limited Feedback,"Motivated by applications to resource-limited and safety-critical domains, we study selective classification in the online learning model, wherein a predictor may abstain from classifying an instance. For example, this may model an adaptive decision to invoke more resources on this instance. Two salient aspects of the setting we consider are that the data may be non-realisable, due to which abstention may be a valid long-term action, and that feedback is only received when the learner abstains, which models the fact that reliable labels are only available when the resource intensive processing is invoked. Within this framework, we explore strategies that make few mistakes, while not abstaining too many times more than the best-in-hindsight error-free classifier from a given class. That is, the one that makes no mistakes, while abstaining the fewest number of times. We construct simple versioning-based schemes for any $\mu \in (0,1],$ that make most $T^\mu$ mistakes while incurring $\tilde{O}(T^{1-\mu})$ excess abstention against adaptive adversaries. We further show that this dependence on $T$ is tight, and provide illustrative experiments on realistic datasets.","['Online Learning', 'Machine Learning']",[],"['Aditya Gangrade', 'Anil Kag', 'Ashok Cutkosky', 'Venkatesh Saligrama']","['Carnegie Mellon University', 'Snap Inc.', 'Boston University', 'Amazon']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28354,Privacy & Data Governance,A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks,"Adversarial examples for neural network image classifiers are known to be transferable: examples optimized to be misclassified by a source classifier are often misclassified as well by classifiers with different architectures. However, targeted adversarial examples—optimized to be classified as a chosen target class—tend to be less transferable between architectures. While prior research on constructing transferable targeted attacks has focused on improving the optimization procedure, in this work we examine the role of the source classifier. Here, we show that training the source classifier to be ""slightly robust""—that is, robust to small-magnitude adversarial examples—substantially improves the transferability of class-targeted and representation-targeted adversarial attacks, even between architectures as different as convolutional neural networks and transformers. The results we present provide insight into the nature of adversarial examples as well as the mechanisms underlying so-called ""robust"" classifiers.","['Transformers', 'Optimization', 'Robustness', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Jacob M. Springer', 'Melanie Mitchell', 'Garrett T. Kenyon']","['Carnegie Mellon University', 'Santa Fe Institute', 'Los Alamos National Laboratory']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28240,Privacy & Data Governance,EDGE: Explaining Deep Reinforcement Learning Policies,"With the rapid development of deep reinforcement learning (DRL) techniques, there is an increasing need to understand and interpret DRL policies. While recent research has developed explanation methods to interpret how an agent determines its moves, they cannot capture the importance of actions/states to a game's final result. In this work, we propose a novel self-explainable model that augments a Gaussian process with a customized kernel function and an interpretable predictor. Together with the proposed model, we also develop a parameter learning procedure that leverages inducing points and variational inference to improve learning efficiency. Using our proposed model, we can predict an agent's final rewards from its game episodes and extract time step importance within episodes as strategy-level explanations for that agent. Through experiments on Atari and MuJoCo games, we verify the explanation fidelity of our method and demonstrate how to employ interpretation to understand agent behavior, discover policy vulnerabilities, remediate policy errors, and even defend against adversarial attacks.","['Reinforcement Learning and Planning', 'Kernel Methods', 'Adversarial Robustness and Security', 'Generative Model', 'Interpretability']",[],"['Wenbo Guo', 'Xian Wu', 'Usmann Khan', 'Xinyu Xing']","['University of California, Santa Barbara', 'Pennsylvania State University', 'Institute of Technology', 'Pennsylvania State University']","[None, None, 'Georgia', None]"
https://nips.cc/virtual/2021/poster/28190,Privacy & Data Governance,Adversarial Robustness with Non-uniform Perturbations,"Robustness of machine learning models is critical for security related applications, where real-world adversaries are uniquely focused on evading neural network based detectors. Prior work mainly focus on crafting adversarial examples (AEs) with small uniform norm-bounded perturbations across features to maintain the requirement of imperceptibility. However, uniform perturbations do not result in realistic AEs in domains such as malware, finance, and social networks. For these types of applications, features typically have some semantically meaningful dependencies. The key idea of our proposed approach is to enable non-uniform perturbations that can adequately represent these feature dependencies during adversarial training. We propose using characteristics of the empirical data distribution, both on correlations between the features and the importance of the features themselves. Using experimental datasets for malware classification, credit risk prediction, and spam detection, we show that our approach is more robust to real-world attacks. Finally, we present robustness certification utilizing non-uniform perturbation bounds, and show that non-uniform bounds achieve better certification.","['Optimization', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Ecenaz Erdemir', 'Jeffrey Bickford', 'Luca Melis', 'Sergul Aydore']","['Imperial College London', 'Amazon', 'Meta', 'Amazon']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28144,Privacy & Data Governance,Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee,"The growing literature of Federated Learning (FL) has recently inspired Federated Reinforcement Learning (FRL) to encourage multiple agents to federatively build a better decision-making policy without sharing raw trajectories. Despite its promising applications, existing works on FRL fail to I) provide theoretical analysis on its convergence, and II) account for random system failures and adversarial attacks. Towards this end, we propose the first FRL framework the convergence of which is guaranteed and tolerant to less than half of the participating agents being random system failures or adversarial attackers. We prove that the sample efficiency of the proposed framework is guaranteed to improve with the number of agents and is able to account for such potential failures or attacks. All theoretical results are empirically verified on various RL benchmark tasks.","['Reinforcement Learning and Planning', 'Optimization', 'Adversarial Robustness and Security', 'Federated Learning']",[],"['Flint Xiaofeng Fan', 'Yining Ma', 'Zhongxiang Dai', 'Wei Jing', 'Cheston Tan', 'Bryan Kian Hsiang Low']","['National University of', 'Nanyang Technological University', 'Massachusetts Institute of Technology', 'NetEase, Inc.', 'A*STAR', 'National University of']","['Singapore', None, None, None, None, 'Singapore']"
https://nips.cc/virtual/2021/poster/28131,Privacy & Data Governance,Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints,"Evaluating adversarial robustness amounts to finding the minimum perturbation needed to have an input sample misclassified. The inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model. In this work, we overcome these limitations by proposing a fast minimum-norm (FMN) attack that works with different $\ell_p$-norm perturbation models ($p=0, 1, 2, \infty$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. It works by iteratively finding the sample misclassified with maximum confidence within an $\ell_p$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to minimize the distance of the current sample to the decision boundary. Extensive experiments show that FMN significantly outperforms existing $\ell_0$, $\ell_1$, and $\ell_\infty$-norm attacks in terms of perturbation size, convergence speed and computation time, while reporting comparable performances with state-of-the-art $\ell_2$-norm attacks. Our open-source code is available at: https://github.com/pralab/Fast-Minimum-Norm-FMN-Attack.","['Vision', 'Robustness', 'Machine Learning', 'Optimization', 'Adversarial Robustness and Security']",[],"['Maura Pintor', 'Fabio Roli', 'Wieland Brendel', 'Battista Biggio']","['University of Cagliari', 'University of Cagliari', 'ELLIS Institute Tübingen', 'University of Cagliari,']","[None, None, None, 'Italy']"
https://nips.cc/virtual/2021/poster/28087,Privacy & Data Governance,Adversarial Teacher-Student Representation Learning for Domain Generalization,"Domain generalization (DG) aims to transfer the learning task from a single or multiple source domains to unseen target domains. To extract and leverage the information which exhibits sufficient generalization ability, we propose a simple yet effective approach of Adversarial Teacher-Student Representation Learning, with the goal of deriving the domain generalizable representations via generating and exploring out-of-source data distributions. Our proposed framework advances Teacher-Student learning in an adversarial learning manner, which alternates between knowledge-distillation based representation learning and novel-domain data augmentation. The former progressively updates the teacher network for deriving domain-generalizable representations, while the latter synthesizes data out-of-source yet plausible distributions. Extensive image classification experiments on benchmark datasets in multiple and single source DG settings confirm that, our model exhibits sufficient generalization ability and performs favorably against state-of-the-art DG methods.","['Vision', 'Machine Learning', 'Representation Learning', 'Domain Adaptation']",[],"['Fu-En Yang', 'Yuan-Chia Cheng', 'Zu-Yun Shiau', 'Yu-Chiang Frank Wang']","['NVIDIA', 'National Taiwan University', 'National Taiwan University', 'NVIDIA']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28007,Privacy & Data Governance,Efficiently Learning One Hidden Layer ReLU Networks From Queries,"While the problem of PAC learning neural networks from samples has received considerable attention in recent years, in certain settings like model extraction attacks, it is reasonable to imagine having more than just the ability to observe random labeled examples. Motivated by this, we consider the following problem: given \emph{black-box query access} to a neural network $F$, recover $F$ up to some error. Formally, we show that if $F$ is an arbitrary one hidden layer neural network with ReLU activations, there is an algorithm with query complexity and runtime polynomial in all parameters which outputs a network $F’$ achieving low square loss relative to $F$ with respect to the Gaussian measure. While a number of works in the security literature have proposed and empirically demonstrated the effectiveness of certain algorithms for this problem, ours is to the best of our knowledge the first provable guarantee in this vein.","['Theory', 'Deep Learning']",[],"['Sitan Chen', 'Adam Klivans', 'Raghu Meka']","['University of California Berkeley', 'University of Texas, Austin', 'University of California, Los Angeles']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27569,Privacy & Data Governance,Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations,"Training-time safety violations have been a major concern when we deploy reinforcement learning algorithms in the real world. This paper explores the possibility of safe RL algorithms with zero training-time safety violations in the challenging setting where we are only given a safe but trivial-reward initial policy without any prior knowledge of the dynamics and additional offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe RL (CRABS), which iteratively learns barrier certificates, dynamics models, and policies. The barrier certificates are learned via adversarial training and ensure the policy's safety assuming calibrated learned dynamics. We also add a regularization term to encourage larger certified regions to enable better exploration. Empirical simulations show that zero safety violations are already challenging for a suite of simple environments with only 2-4 dimensional state space, especially if high-reward policies have to visit regions near the safety boundary.  Prior methods require hundreds of violations to achieve decent rewards on these tasks,  whereas our proposed algorithms incur zero violations.","['Reinforcement Learning and Planning', 'Adversarial Robustness and Security']",[],"['Yuping Luo', 'Tengyu Ma']","['Princeton University', 'Stanford University']","[None, None]"
https://nips.cc/virtual/2021/poster/27974,Privacy & Data Governance,Safe Policy Optimization with Local Generalized Linear Function Approximations,"Safe exploration is a key to applying reinforcement learning (RL) in safety-critical systems. Existing safe exploration methods guaranteed safety under the assumption of regularity, and it has been difficult to apply them to large-scale real problems. We propose a novel algorithm, SPO-LF, that optimizes an agent's policy while learning the relation between a locally available feature obtained by sensors and environmental reward/safety using generalized linear function approximations. We provide theoretical guarantees on its safety and optimality. We experimentally show that our algorithm is 1) more efficient in terms of sample complexity and computational cost and 2) more applicable to large-scale problems than previous safe RL methods with theoretical guarantees, and 3) comparably sample-efficient and safer compared with existing advanced deep RL methods with safety constraints.","['Reinforcement Learning and Planning', 'Theory', 'Optimization']",[],"['Akifumi Wachi', 'Yunyue Wei', 'Yanan Sui']","['LINE', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27929,Privacy & Data Governance,Adversarial Attack Generation Empowered by Min-Max Optimization,"The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness. Nevertheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the adversarial context. In this paper, we show how a general notion of min-max optimization over multiple domains can be leveraged to the design of different types of adversarial attacks. In particular, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as a min-max problem by introducing domain weights that are maximized over the probability simplex of the domain set. We showcase this unified framework in three attack generation problems -- attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations. Extensive experiments demonstrate that our approach leads to substantial attack improvement over the existing heuristic strategies as well as robustness improvement over state-of-the-art defense methods against multiple perturbation types. Furthermore, we find that the self-adjusted domain weights learned from min-max optimization can provide a holistic tool to explain the difficulty level of attack across domains.","['Adversarial Robustness and Security', 'Robustness', 'Optimization']",[],"['Jingkang Wang', 'Tianyun Zhang', 'Sijia Liu', 'Pin-Yu Chen', 'Jiacen Xu', 'Bo Li']","['Waabi', 'Cleveland State University', 'Michigan State University', 'International Business Machines', 'University of California, Irvine', 'University of Illinois, Urbana Champaign']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27909,Privacy & Data Governance,You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership,"Despite tremendous success in many application scenarios, the training and inference costs of using deep learning are also rapidly increasing over time. The lottery ticket hypothesis (LTH) emerges as a promising framework to leverage a special sparse subnetwork (i.e., $\textit{winning ticket}$) instead of a full model for both training and inference, that can lower both costs without sacrificing the performance. The main resource bottleneck of LTH is however the extraordinary cost to find the sparse mask of the winning ticket. That makes the found winning ticket become a valuable asset to the owners, highlighting the necessity of protecting its copyright. Our setting adds a new dimension to the recently soaring interest in protecting against the intellectual property (IP) infringement of deep models and verifying their ownerships, since they take owners' massive/unique resources to develop or train. While existing methods explored encrypted weights or predictions, we investigate a unique way to leverage sparse topological information to perform $\textit{lottery verification}$, by developing several graph-based signatures that can be embedded as credentials. By further combining trigger set-based methods, our proposal can work in both white-box and black-box verification scenarios. Through extensive experiments, we demonstrate the effectiveness of lottery verification in diverse models (ResNet-20, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100. Specifically, our verification is shown to be robust to removal attacks such as model fine-tuning and pruning, as well as several ambiguity attacks. Our codes are available at https://github.com/VITA-Group/NO-stealing-LTH.","['Deep Learning', 'Graph Learning']",[],"['Xuxi Chen', 'Tianlong Chen', 'Zhenyu Zhang', 'Zhangyang Wang']","['University of Texas at Austin', 'Massachusetts Institute of Technology', 'University of Texas at Austin', 'University of Texas at Austin']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27870,Privacy & Data Governance,Relaxing Local Robustness,"Certifiable local robustness, which rigorously precludes small-norm adversarial examples, has received significant attention as a means of addressing security concerns in deep learning. However, for some classification problems, local robustness is not a natural objective, even in the presence of adversaries; for example, if an image contains two classes of subjects, the correct label for the image may be considered arbitrary between the two, and thus enforcing strict separation between them is unnecessary. In this work, we introduce two relaxed safety properties for classifiers that address this observation: (1) relaxed top-k robustness, which serves as the analogue of top-k accuracy; and (2) affinity robustness, which specifies which sets of labels must be separated by a robustness margin, and which can be $\epsilon$-close in $\ell_p$ space. We show how to construct models that can be efficiently certified against each relaxed robustness property, and trained with very little overhead relative to standard gradient descent. Finally, we demonstrate experimentally that these relaxed variants of robustness are well-suited to several significant classification problems, leading to lower rejection rates and higher certified accuracies than can be obtained when certifying ""standard"" local robustness.","['Optimization', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Klas Leino', 'Matt Fredrikson']","['School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University']","[None, None]"
https://nips.cc/virtual/2021/poster/27832,Privacy & Data Governance,Sampling  with Trusthworthy Constraints:  A Variational Gradient Framework,"Sampling-based inference and learning techniques, especially Bayesian inference, provide an essential approach to handling uncertainty in machine learning (ML). As these techniques are increasingly used in daily life, it becomes essential to safeguard the ML systems with various trustworthy-related constraints, such as fairness, safety, interpretability. Mathematically, enforcing these constraints in probabilistic inference can be cast into sampling from intractable distributions subject to general nonlinear constraints, for which practical efficient algorithms are still largely missing. In this work, we propose a family of constrained sampling algorithms which generalize Langevin Dynamics (LD) and Stein Variational Gradient Descent (SVGD) to incorporate a moment constraint specified by a general nonlinear function. By exploiting the gradient flow structure of LD and SVGD, we derive two types of algorithms for handling constraints, including a primal-dual gradient approach and the constraint controlled gradient descent approach. We investigate the continuous-time mean-field limit of these algorithms and show that they have O(1/t) convergence under mild conditions. Moreover, the LD variant converges linearly assuming that a log Sobolev like inequality holds. Various numerical experiments are conducted to demonstrate the efficiency of our algorithms in trustworthy settings.","['Fairness', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Xingchao Liu', 'Xin Tong', 'qiang liu']","['University of Texas, Austin', 'National University of', 'Dartmouth College']","[None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/27835,Privacy & Data Governance,Anti-Backdoor Learning: Training Clean Models on Poisoned Data,"Backdoor attack has emerged as a major security threat to deep neural networks (DNNs). While existing defense methods have demonstrated promising results on detecting or erasing backdoors, it is still not clear whether robust training methods can be devised to prevent the backdoor triggers being injected into the trained model in the first place. In this paper, we introduce the concept of \emph{anti-backdoor learning}, aiming to train \emph{clean} models given backdoor-poisoned data. We frame the overall learning process as a dual-task of learning the \emph{clean} and the \emph{backdoor} portions of data. From this view, we identify two inherent characteristics of backdoor attacks as their weaknesses: 1) the models learn backdoored data much faster than learning with clean data, and the stronger the attack the faster the model converges on backdoored data; 2) the backdoor task is tied to a specific class (the backdoor target class). Based on these two weaknesses, we propose a general learning scheme, Anti-Backdoor Learning (ABL), to automatically prevent backdoor attacks during training. ABL introduces a two-stage \emph{gradient ascent} mechanism for standard training to 1) help isolate backdoor examples at an early training stage, and 2) break the correlation between backdoor examples and the target class at a later training stage. Through extensive experiments on multiple benchmark datasets against 10 state-of-the-art attacks, we empirically show that ABL-trained models on backdoor-poisoned data achieve the same performance as they were trained on purely clean data. Code is available at \url{https://github.com/bboylyg/ABL}.",['Deep Learning'],[],"['Yige Li', 'Xixiang Lyu', 'Nodens Koren', 'Lingjuan Lyu', 'Bo Li', 'Xingjun Ma']","['Xidian University', ""Xi'an University of Electronic Science and Technology"", 'ETHZ - ETH Zurich', 'Sony Research', 'University of Illinois, Urbana Champaign', 'Fudan University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27819,Privacy & Data Governance,Sageflow: Robust Federated Learning against Both Stragglers and Adversaries,"While federated learning (FL) allows efficient model training with local data at edge devices, among major issues still to be resolved are: slow devices known as stragglers and malicious attacks launched by adversaries.   While the presence of both of these issues raises serious concerns in practical FL systems, no known schemes or combinations of schemes effectively address them at the same time. We propose Sageflow, staleness-aware grouping with entropy-based filtering and loss-weighted averaging, to handle both stragglers and adversaries simultaneously. Model grouping and weighting according to staleness (arrival delay) provides robustness against stragglers, while entropy-based filtering and loss-weighted averaging, working in a highly complementary fashion at each grouping stage,  counter a wide range of adversary attacks. A theoretical bound is established to provide key insights into the convergence behavior of Sageflow. Extensive experimental results show that Sageflow outperforms various existing methods aiming to handle stragglers/adversaries.","['Federated Learning', 'Robustness']",[],"['Jungwuk Park', 'Dong-Jun Han', 'Minseok Choi', 'Jaekyun Moon']","['Korea Advanced Institute of Science and Technology', 'Purdue University', 'Jeju National University', 'KAIST']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27787,Privacy & Data Governance,Detecting Anomalous Event Sequences with Temporal Point Processes,"Automatically detecting anomalies in event data can provide substantial value in domains such as healthcare, DevOps, and information security. In this paper, we frame the problem of detecting anomalous continuous-time event sequences as out-of-distribution (OOD) detection for temporal point processes (TPPs). First, we show how this problem can be approached using goodness-of-fit (GoF) tests. We then demonstrate the limitations of popular GoF statistics for TPPs and propose a new test that addresses these shortcomings. The proposed method can be combined with various TPP models, such as neural TPPs, and is easy to implement. In our experiments, we show that the proposed statistic excels at both traditional GoF testing, as well as at detecting anomalies in simulated and real-world data.",['Generative Model'],[],"['Oleksandr Shchur', 'Ali Caner Turkmen', 'Tim Januschowski', 'Jan Gasthaus', 'Stephan Günnemann']","['Amazon', 'Amazon', 'Amazon', 'Meta', 'Technical University Munich']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27754,Privacy & Data Governance,Random Noise Defense Against Query-Based Black-Box Attacks,"The query-based black-box attacks have raised serious threats to machine learning models in many real applications. In this work, we study a lightweight defense method, dubbed Random Noise Defense (RND), which adds proper Gaussian noise to each query. We conduct the theoretical analysis about the effectiveness of RND against query-based black-box attacks and the corresponding adaptive attacks. Our theoretical results reveal that the defense performance of RND is determined by the magnitude ratio between the noise induced by RND and the noise added by the attackers for gradient estimation or local search.  The large magnitude ratio leads to the stronger defense performance of RND, and it's also critical for mitigating adaptive attacks. Based on our analysis, we further propose to combine RND with a plausible Gaussian augmentation Fine-tuning (RND-GF). It enables RND to add larger noise to each query while maintaining the clean accuracy to obtain a better trade-off between clean accuracy and defense performance. Additionally, RND can be flexibly combined with the existing defense methods to further boost the adversarial robustness, such as adversarial training (AT). Extensive experiments on CIFAR-10 and ImageNet verify our theoretical findings and the effectiveness of RND and RND-GF.","['Adversarial Robustness and Security', 'Robustness', 'Machine Learning']",[],"['Zeyu Qin', 'Yanbo Fan', 'Hongyuan Zha', 'Baoyuan Wu']","['The  University of Science and Technology', 'Tencent AI Lab', 'The Chinese University of , Shenzhen', 'The Chinese University of , Shenzhen']","['Hong Kong', None, 'Hong Kong', 'Hong Kong']"
https://nips.cc/virtual/2021/poster/27717,Privacy & Data Governance,Collaborative Uncertainty in Multi-Agent Trajectory Forecasting,"Uncertainty modeling is critical in trajectory-forecasting systems for both interpretation and safety reasons. To better predict the future trajectories of multiple agents, recent works have introduced interaction modules to capture interactions among agents. This approach leads to correlations among the predicted trajectories. However, the uncertainty brought by such correlations is neglected. To fill this gap, we propose a novel concept, collaborative uncertainty (CU), which models the uncertainty resulting from the interaction module. We build a general CU-based framework to make a prediction model learn the future trajectory and the corresponding uncertainty. The CU-based framework is integrated as a plugin module to current state-of-the-art (SOTA) systems and deployed in two special cases based on multivariate Gaussian and Laplace distributions. In each case, we conduct extensive experiments on two synthetic datasets and two public, large-scale benchmarks of trajectory forecasting. The results are promising: 1) The results of synthetic datasets show that CU-based framework allows the model to nicely rebuild the ground-truth distribution. 2) The results of trajectory forecasting benchmarks demonstrate that the CU-based framework steadily helps SOTA systems improve their performances. Specially, the proposed CU-based framework helps VectorNet improve by 57 cm regarding Final Displacement Error on nuScenes dataset. 3) The visualization results of CU illustrate that the value of CU is highly related to the amount of the interactive information among agents.",['Deep Learning'],[],"['Bohan Tang', 'Yiqi Zhong', 'Gang Wang', 'Siheng Chen', 'Ya Zhang']","['University of Oxford', 'University of Southern California', 'Beijing Institute of Technology', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27698,Privacy & Data Governance,Counterexample Guided RL Policy Refinement Using Bayesian Optimization,"Constructing Reinforcement Learning (RL) policies that adhere to safety requirements is an emerging field of study. RL agents learn via trial and error with an objective to optimize a reward signal. Often policies that are designed to accumulate rewards do not satisfy safety specifications. We present a methodology for counterexample guided refinement of a trained RL policy against a given safety specification. Our approach has two main components. The first component is an approach to discover failure trajectories using Bayesian optimization over multiple parameters of uncertainty from a policy learnt in a model-free setting. The second component selectively modifies the failure points of the policy using gradient-based updates. The approach has been tested on several RL environments, and we demonstrate that the policy can be made to respect the safety specifications through such targeted changes.","['Reinforcement Learning and Planning', 'Optimization']",[],"['Briti Gangopadhyay', 'Pallab Dasgupta']","['Sony Group Coorporation', 'n Institute of Technology Kharagpur']","[None, 'India']"
https://nips.cc/virtual/2021/poster/27590,Privacy & Data Governance,Safe Reinforcement Learning by Imagining the Near Future,"Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states. We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.",['Reinforcement Learning and Planning'],[],"['Garrett Thomas', 'Yuping Luo', 'Tengyu Ma']","['Stanford University', 'Princeton University', 'Stanford University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27612,Privacy & Data Governance,Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics,"Deep neural networks excel at image classification, but their performance is far less robust to input perturbations than human perception. In this work we explore whether this shortcoming may be partly addressed by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We take inspiration from a popular framework in neuroscience: ""predictive coding"". At each layer of the hierarchical model, generative feedback ""predicts"" (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network’s representations across timesteps, and to optimize the network's feedback weights over the natural image dataset--a form of unsupervised training. We show that implementing this strategy into two popular networks, VGG16 and EfficientNetB0, improves their robustness against various corruptions and adversarial attacks. We hypothesize that other feedforward networks could similarly benefit from the proposed framework. To promote research in this direction, we provide an open-sourced PyTorch-based package called \textit{Predify}, which can be used to implement and investigate the impacts of the predictive coding dynamics in any convolutional neural network.","['Vision', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning', 'Neuroscience']",[],"['Bhavin Choksi', 'Milad Mozafari', ""Callum Biggs O'May"", 'B. ADOR', 'Andrea Alamia', 'Rufin VanRullen']","['CNRS', 'CNRS', 'Centre de Recherche Cerveau et Cognition', 'Centre de Recherche Cerveau et Cognition', 'CNRS', 'CNRS']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27564,Privacy & Data Governance,Towards a Unified Game-Theoretic View of Adversarial Perturbations and Robustness,"This paper provides a unified view to explain different adversarial attacks and defense methods, i.e. the view of multi-order interactions between input variables of DNNs. Based on the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide a potential method to unify adversarial perturbations and robustness, which can explain the existing robustness-boosting methods in a principle way. Besides, our findings also make a revision of previous inaccurate understanding of the shape bias of adversarially learned features. Our code is available online at https://github.com/Jie-Ren/A-Unified-Game-Theoretic-Interpretation-of-Adversarial-Robustness.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Jie Ren', 'Die Zhang', 'Yisen Wang', 'Lu Chen', 'Zhanpeng Zhou', 'Yiting Chen', 'Xu Cheng', 'Xin Wang', 'Meng Zhou', 'Jie Shi', 'Quanshi Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Peking University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Nanjing University of Science and Technology', 'Shanghai Jiao Tong University', 'CMU, Carnegie Mellon University', 'Huawei International.', 'Shanghai Jiao Tong University']","[None, None, None, None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27544,Privacy & Data Governance,Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks,"Adversarial attacks based on randomized search schemes have obtained state-of-the-art results in black-box robustness evaluation recently. However, as we demonstrate in this work, their efficiency in different query budget regimes depends on manual design and heuristic tuning of the underlying proposal distributions. We study how this issue can be addressed by adapting the proposal distribution online based on the information obtained during the attack. We consider Square Attack, which is a state-of-the-art score-based black-box attack, and demonstrate how its performance can be improved by a learned controller that adjusts the parameters of the proposal distribution online during the attack. We train the controller using gradient-based end-to-end training on a CIFAR10 model with white box access. We demonstrate that plugging the learned controller into the attack consistently improves its black-box robustness estimate in different query regimes by up to 20% for a wide range of different models with black-box access. We further show that the learned adaptation principle transfers well to the other data distributions such as CIFAR100 or ImageNet and to the targeted attack setting.","['Adversarial Robustness and Security', 'Meta Learning', 'Robustness']",[],"['Maksym Yatsura', 'Jan Hendrik Metzen', 'Matthias Hein']","['Robert Bosch GmbH, Bosch', 'Bosch Center Artificial Intelligence', 'University of Tübingen']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27483,Privacy & Data Governance,Alignment Attention by Matching Key and Query Distributions,"The neural attention mechanism has been incorporated into deep neural networks to achieve state-of-the-art performance in various domains. Most such models use multi-head self-attention which is appealing for the ability to attend to information from different perspectives. This paper introduces alignment attention that explicitly encourages self-attention to match the distributions of the key and query within each head. The resulting alignment attention networks can be optimized as an unsupervised regularization in the existing attention framework. It is simple to convert any models with self-attention, including pre-trained ones, to the proposed alignment attention. On a variety of language understanding tasks, we show the effectiveness of our method in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our approach on graph attention and visual question answering, showing the great potential of incorporating our alignment method into various attention-related tasks.","['Graph Learning', 'Robustness', 'Vision', 'Language', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Shujian Zhang', 'XINJIE FAN', 'Huangjie Zheng', 'Korawat Tanwisuth', 'Mingyuan Zhou']","['University of Texas, Austin', 'University of Texas, Austin', 'University of Texas, Austin', 'University of Texas, Austin', 'The University of Texas at Austin']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27413,Privacy & Data Governance,Adversarially robust learning for security-constrained optimal power flow,"In recent years, the ML community has seen surges of interest in both adversarially robust learning and implicit layers, but connections between these two areas have seldom been explored. In this work, we combine innovations from these areas to tackle the problem of N-k security-constrained optimal power flow (SCOPF). N-k SCOPF is a core problem for the operation of electrical grids, and aims to schedule power generation in a manner that is robust to potentially $k$ simultaneous equipment outages. Inspired by methods in adversarially robust training, we frame N-k SCOPF as a minimax optimization problem -- viewing power generation settings as adjustable parameters and equipment outages as (adversarial) attacks -- and solve this problem via gradient-based techniques. The loss function of this minimax problem involves resolving implicit equations representing grid physics and operational decisions, which we differentiate through via the implicit function theorem. We demonstrate the efficacy of our framework in solving N-3 SCOPF, which has traditionally been considered as prohibitively expensive to solve given that the problem size depends combinatorially on the number of potential outages.","['Adversarial Robustness and Security', 'Robustness', 'Optimization']",[],"['Priya L. Donti', 'Aayushya Agarwal', 'Neeraj Vijay Bedmutha', 'Larry Pileggi', 'J Zico Kolter']","['Massachusetts Institute of Technology', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27374,Privacy & Data Governance,Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples,"Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. However, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues. A popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning. However, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. We find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries. To this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples. For the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model. The experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. Code is available at https://github.com/iamkanghyunchoi/qimera.","['Deep Learning', 'Privacy']",[],"['Kanghyun Choi', 'Deokki Hong', 'Noseong Park', 'Youngsok Kim', 'Jinho Lee']","['Seoul National University', 'Sapeon Korea', 'Yonsei University', 'Yonsei University', 'Seoul National University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27272,Privacy & Data Governance,Adversarial Robustness with Semi-Infinite Constrained Learning,"Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains.  While adversarial training can mitigate this issue in practice, state-of-the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape. Thus, there is a gap between the theory and practice of robust learning, particularly with respect to when and why adversarial training works.  In this paper, we take a constrained learning approach to address these questions and to provide a theoretical foundation for robust learning. In particular, we leverage semi-infinite optimization and non-convex duality theory to show that adversarial training is equivalent to a statistical problem over perturbation distributions. Notably, we show that a myriad of previous robust training techniques can be recovered for particular, sub-optimal choices of these distributions. Using these insights, we then propose a hybrid Langevin Markov Chain Monte Carlo approach for which several common algorithms (e.g., PGD) are special cases. Finally, we show that our approach can mitigate the trade-off between nominal and robust performance, yielding state-of-the-art results on MNIST and CIFAR-10.  Our code is available at: https://github.com/arobey1/advbench.","['Optimization', 'Robustness', 'Theory', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Alexander Robey', 'Luiz F. O. Chamon', 'George J. Pappas', 'Hamed Hassani', 'Alejandro Ribeiro']","['School of Engineering and Applied Science, University of Pennsylvania', 'Universität Stuttgart', 'School of Engineering and Applied Science, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27251,Privacy & Data Governance,Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time,"In several real world applications, machine learning models are deployed to make predictions on data whose distribution changes gradually along time, leading to a drift between the train and test distributions. Such models are often re-trained on new data periodically, and they hence need to generalize to data not too far into the future. In this context, there is much prior work on enhancing temporal generalization, e.g. continuous transportation of past data, kernel smoothed time-sensitive parameters and more recently, adversarial learning of time-invariant features. However, these methods share several limitations, e.g, poor scalability, training instability, and dependence on unlabeled data from the future. Responding to the above limitations, we propose a simple method that starts with a model with time-sensitive parameters but regularizes its temporal complexity using a Gradient Interpolation  (GI) loss. GI allows the decision boundary to change along time and can still prevent overfitting to the limited training time snapshots  by allowing task-specific control over changes along time. We compare our method to existing baselines on multiple real-world datasets, which show that GI outperforms more complicated generative and adversarial approaches on the one hand, and simpler gradient regularization methods on the other.","['Machine Learning', 'Domain Adaptation']",[],"['Anshul Nasery', 'Soumyadeep Thakur', 'Vihari Piratla', 'Abir De', 'Sunita Sarawagi']","['University of Washington', 'n Institute of Technology, Bombay', 'University of Cambridge', 'n Institute of Technology Bombay,', 'IIT Bombay']","[None, 'India', None, 'India', None]"
https://nips.cc/virtual/2021/poster/27236,Privacy & Data Governance,Evaluating Gradient Inversion Attacks and Defenses in Federated Learning,"Gradient inversion attack (or input recovery from gradient) is an emerging threat to the security and privacy preservation of Federated learning, whereby malicious eavesdroppers or participants in the protocol can recover (partially) the clients' private data. This paper evaluates existing attacks and defenses. We find that some attacks make strong assumptions about the setup. Relaxing such assumptions can substantially weaken these attacks. We then evaluate the benefits of three proposed defense mechanisms against gradient inversion attacks. We show the trade-offs of privacy leakage and data utility of these defense methods, and find that combining them in an appropriate manner makes the attack less effective, even under the original strong assumptions. We also estimate the computation cost of end-to-end recovery of a single image under each evaluated defense. Our findings suggest that the state-of-the-art attacks can currently be defended against with minor data utility loss, as summarized in a list of potential strategies.","['Federated Learning', 'Privacy']",[],"['Yangsibo Huang', 'Samyak Gupta', 'Zhao Song', 'Kai Li', 'Sanjeev Arora']","['Princeton University', 'Princeton University', 'Adobe Research', 'Princeton University', 'Princeton University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27196,Privacy & Data Governance,Robust Deep Reinforcement Learning through Adversarial Loss,"Recent studies have shown that deep reinforcement learning agents are vulnerable to small adversarial perturbations on the agent's inputs, which raises concerns about deploying such agents in the real world. To address this issue, we propose RADIAL-RL, a principled framework to train reinforcement learning agents with improved robustness against $l_p$-norm bounded adversarial attacks. Our framework is compatible with popular deep reinforcement learning algorithms and we demonstrate its performance with deep Q-learning, A3C and PPO. We experiment on three deep RL benchmarks (Atari, MuJoCo and ProcGen) to show the effectiveness of our robust training algorithm. Our RADIAL-RL agents consistently outperform prior methods when tested against attacks of varying strength and are more computationally efficient to train. In addition, we propose a new evaluation method called Greedy-Worst-Case Reward (GWC) to measure attack agnostic robustness of deep RL agents. We show that GWC can be evaluated efficiently and is a good estimate of the reward under the worst possible sequence of adversarial attacks. All code used for our experiments is available at https://github.com/tuomaso/radial_rl_v2.","['Reinforcement Learning and Planning', 'Robustness', 'Adversarial Robustness and Security']",[],"['Tuomas Oikarinen', 'Wang Zhang', 'Alexandre Megretski', 'Luca Daniel', 'Tsui-Wei Weng']","['University of California, San Diego', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of California, San Diego']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27165,Privacy & Data Governance,Manipulating SGD with Data Ordering Attacks,"Machine learning is vulnerable to a wide variety of attacks. It is now well understood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we find that the attacker can either prevent the model from learning, or poison it to learn behaviours specified by the attacker. Furthermore, we find that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not specific to the model or dataset, but rather target the stochastic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to find that the adversary can disrupt model training and even introduce backdoors.","['Vision', 'Machine Learning']",[],"['I Shumailov', 'Zakhar Shumaylov', 'Dmitry Kazhdan', 'Yiren Zhao', 'Nicolas Papernot', 'Murat A Erdogdu', 'Ross Anderson']","['Google DeepMind', 'University of Cambridge', 'University of Cambridge', 'Imperial College London', 'University of Toronto', 'University of Toronto', 'University of Edinburgh, University of Edinburgh']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27010,Privacy & Data Governance,SSAL: Synergizing between Self-Training and Adversarial Learning for Domain Adaptive Object Detection,"We study adapting trained object detectors to unseen domains manifesting significant variations of object appearance, viewpoints and backgrounds. Most current methods align domains by either using image or instance-level feature alignment in an adversarial fashion. This often suffers due to the presence of unwanted background and as such lacks class-specific alignment. A common remedy to promote class-level alignment is to use high confidence predictions on the unlabelled domain as pseudo labels. These high confidence predictions are often fallacious since the model is poorly calibrated under domain shift. In this paper, we propose to leverage model’s predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. Specifically, we measure predictive uncertainty on class assignments and the bounding box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-supervision, whereas the ones with higher uncertainty are used to generate tiles for an adversarial feature alignment stage. This synergy between tiling around the uncertain object regions and generating pseudo-labels from highly certain object regions allows us to capture both the image and instance level context during the model adaptation stage. We perform extensive experiments covering various domain shift scenarios. Our approach improves upon existing state-of-the-art methods with visible margins.","['Deep Learning', 'Vision', 'Domain Adaptation']",[],"['Muhammad Akhtar Munir', 'Muhammad Haris Khan', 'M. Saquib Sarfraz', 'Mohsen Ali']","['Mohamed bin Zayed University of Artificial Intelligence', 'MBZUAI', 'Karlsruher Institut für Technologie', 'Information Technology University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26917,Privacy & Data Governance,Antipodes of Label Differential Privacy: PATE and ALIBI,"We consider the privacy-preserving machine learning (ML) setting where the trained model must satisfy differential privacy (DP) with respect to the labels of the training examples. We propose two novel approaches based on, respectively, the Laplace mechanism and the PATE framework, and demonstrate their effectiveness on standard benchmarks. While recent work by Ghazi et al. proposed Label DP schemes based on a randomized response mechanism, we argue that additive Laplace noise coupled with Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover, we show how to achieve very strong privacy levels in some regimes, with our adaptation of the PATE framework that builds on recent advances in semi-supervised learning. We complement theoretical analysis of our algorithms' privacy guarantees with empirical evaluation of their memorization properties. Our evaluation suggests that comparing different algorithms according to their provable DP guarantees can be misleading and favor a less private algorithm with a tighter analysis. Code for implementation of algorithms and memorization attacks is available from https://github.com/facebookresearch/label_dp_antipodes.","['Semi-Supervised Learning', 'Machine Learning', 'Privacy']",[],"['Mani Malek Esmaeili', 'Ilya Mironov', 'Karthik Prasad', 'Igor Shilov', 'Florian Tramer']","['Facebook', 'Facebook', 'Facebook AI', 'Imperial College London', 'ETHZ - ETH Zurich']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27598,Privacy & Data Governance,Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training,"Delusive attacks aim to substantially deteriorate the test accuracy of the learning model by slightly perturbing the features of correctly labeled training examples. By formalizing this malicious attack as finding the worst-case training data within a specific $\infty$-Wasserstein ball, we show that minimizing adversarial risk on the perturbed data is equivalent to optimizing an upper bound of natural risk on the original data. This implies that adversarial training can serve as a principled defense against delusive attacks. Thus, the test accuracy decreased by delusive attacks can be largely recovered by adversarial training. To further understand the internal mechanism of the defense, we disclose that adversarial training can resist the delusive perturbations by preventing the learner from overly relying on non-robust features in a natural setting. Finally, we complement our theoretical findings with a set of experiments on popular benchmark datasets, which show that the defense withstands six different practical attacks. Both theoretical and empirical results vote for adversarial training when confronted with delusive adversaries.",['Adversarial Robustness and Security'],[],"['Lue Tao', 'Lei Feng', 'Jinfeng Yi', 'Sheng-Jun Huang', 'Songcan Chen']","['Nanjing University', 'Nanyang Technological University', 'JD AI Research', 'Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26781,Privacy & Data Governance,Learning to Simulate Self-driven Particles System with Coordinated Policy Optimization,"Self-Driven Particles (SDP) describe a category of multi-agent systems common in everyday life, such as flocking birds and traffic flows. In a SDP system, each agent pursues its own goal and constantly changes its cooperative or competitive behaviors with its nearby agents. Manually designing the controllers for such SDP system is time-consuming, while the resulting emergent behaviors are often not realistic nor generalizable. Thus the realistic simulation of SDP systems remains challenging. Reinforcement learning provides an appealing alternative for automating the development of the controller for SDP. However, previous multi-agent reinforcement learning (MARL) methods define the agents to be teammates or enemies before hand, which fail to capture the essence of SDP where the role of each agent varies to be cooperative or competitive even within one episode. To simulate SDP with MARL, a key challenge is to coordinate agents' behaviors while still maximizing individual objectives. Taking traffic simulation as the testing bed, in this work we develop a novel MARL method called Coordinated Policy Optimization (CoPO), which incorporates social psychology principle to learn neural controller for SDP. Experiments show that the proposed method can achieve superior performance compared to MARL baselines in various metrics. Noticeably the trained vehicles exhibit complex and diverse social behaviors that improve performance and safety of the population as a whole. Demo video and source code are available at: https://decisionforce.github.io/CoPO/","['Reinforcement Learning and Planning', 'Optimization']",[],"['Zhenghao Peng', 'Quanyi Li', 'Ka Ming Hui', 'Chunxiao Liu', 'Bolei Zhou']","['University of California, Los Angeles', 'University of Edinburgh', 'The Chinese University of', 'Tsinghua University', 'University of California, Los Angeles']","[None, None, 'Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/26761,Privacy & Data Governance,Backdoor Attack with Imperceptible Input and Latent Modification,"Recent studies have shown that deep neural networks (DNN) are vulnerable to various adversarial attacks. In particular, an adversary can inject a stealthy backdoor into a model such that the compromised model will behave normally without the presence of the trigger. Techniques for generating backdoor images that are visually imperceptible from clean images have also been developed recently, which further enhance the stealthiness of the backdoor attacks from the input space. Along with the development of attacks, defense against backdoor attacks is also evolving. Many existing countermeasures found that backdoor tends to leave tangible footprints in the latent or feature space, which can be utilized to mitigate backdoor attacks. In this paper, we extend the concept of imperceptible backdoor from the input space to the latent representation, which significantly improves the effectiveness against the existing defense mechanisms, especially those relying on the distinguishability between clean inputs and backdoor inputs in latent space. In the proposed framework, the trigger function will learn to manipulate the input by injecting imperceptible input noise while matching the latent representations of the clean and manipulated inputs via a Wasserstein-based regularization of the corresponding empirical distributions. We formulate such an objective as a non-convex and constrained optimization problem and solve the problem with an efficient stochastic alternating optimization procedure. We name the proposed backdoor attack as Wasserstein Backdoor (WB), which achieves a high attack success rate while being stealthy from both the input and latent spaces, as tested in several benchmark datasets, including MNIST, CIFAR10, GTSRB, and TinyImagenet.","['Adversarial Robustness and Security', 'Deep Learning', 'Optimization', 'Generative Model']",[],"['Khoa Doan', 'Yingjie Lao', 'Ping Li']","['VinUniversity', 'Tufts University', 'Rutgers University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26751,Privacy & Data Governance,Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing,"Unsupervised domain adaptation which aims to adapt models trained on a labeled source domain to a completely unlabeled target domain has attracted much attention in recent years. While many domain adaptation techniques have been proposed for images, the problem of unsupervised domain adaptation in videos remains largely underexplored. In this paper, we introduce Contrast and Mix (CoMix), a new contrastive learning framework that aims to learn discriminative invariant feature representations for unsupervised video domain adaptation. First, unlike existing methods that rely on adversarial learning for feature alignment, we utilize temporal contrastive learning to bridge the domain gap by maximizing the similarity between encoded representations of an unlabeled video at two different speeds as well as minimizing the similarity between different videos played at different speeds. Second, we propose a novel extension to the temporal contrastive loss by using background mixing that allows additional positives per anchor, thus adapting contrastive learning to leverage action semantics shared across both domains. Moreover, we also integrate a supervised contrastive learning objective using target pseudo-labels to enhance discriminability of the latent space for video domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed approach over state-of-the-art methods. Project page: https://cvir.github.io/projects/comix.","['Contrastive Learning', 'Domain Adaptation']",[],"['Rutav Shah', 'Rameswar Panda', 'Kate Saenko', 'Abir Das']","['University of Texas at Austin', 'MIT-IBM Watson AI Lab', 'Boston University', 'n Institute of Technology Kharagpur']","[None, None, None, 'India']"
https://nips.cc/virtual/2021/poster/26667,Privacy & Data Governance,Class-Disentanglement and Applications in Adversarial Detection and Defense,"What is the minimum necessary information required by a neural net $D(\cdot)$ from an image $x$ to accurately predict its class? Extracting such information in the input space from $x$ can allocate the areas $D(\cdot)$ mainly attending to and shed novel insights to the detection and defense of adversarial attacks. In this paper, we propose ''class-disentanglement'' that trains a variational autoencoder $G(\cdot)$ to extract this class-dependent information as $x - G(x)$ via a trade-off between reconstructing $x$ by $G(x)$ and classifying $x$ by $D(x-G(x))$, where the former competes with the latter in decomposing $x$ so the latter retains only necessary information for classification in $x-G(x)$. We apply it to both clean images and their adversarial images and discover that the perturbations generated by adversarial attacks mainly lie in the class-dependent part $x-G(x)$. The decomposition results also provide novel interpretations to classification and attack models. Inspired by these observations, we propose to conduct adversarial detection and adversarial defense respectively on $x - G(x)$ and $G(x)$, which consistently outperform the results on the original $x$. In experiments, this simple approach substantially improves the detection and defense against different types of adversarial attacks.","['Adversarial Robustness and Security', 'Machine Learning']",[],"['Kaiwen Yang', 'Tianyi Zhou', 'Yonggang Zhang', 'Xinmei Tian', 'Dacheng Tao']","['University of Science and Technology of', 'University of Maryland, College Park', 'Baptist University', 'University of Science and Technology of', 'University of Sydney']","['China', None, 'Hong Kong', 'China', None]"
https://nips.cc/virtual/2021/poster/26631,Privacy & Data Governance,Robust and Fully-Dynamic Coreset for Continuous-and-Bounded Learning (With Outliers) Problems,"In many machine learning tasks, a common approach for dealing with large-scale data is to build a small summary, {\em e.g.,} coreset,  that can efficiently represent the original input. However, real-world datasets usually contain outliers and most existing coreset construction methods are not resilient against outliers (in particular, an outlier can be located arbitrarily in the space by an adversarial attacker). In this paper, we propose a novel robust coreset method for the {\em continuous-and-bounded learning} problems (with outliers) which includes a broad range of popular optimization objectives in machine learning, {\em e.g.,} logistic regression and $ k $-means clustering. Moreover, our robust coreset  can be efficiently maintained in fully-dynamic environment. To the best of our knowledge, this is the first robust and fully-dynamic coreset construction method for these optimization problems. Another highlight is that our coreset size can depend on the doubling dimension of the parameter space, rather than the VC dimension of the objective function which could be very large or even challenging to compute. Finally, we conduct the experiments on real-world datasets to evaluate the effectiveness of our proposed robust coreset method.","['Clustering', 'Optimization', 'Adversarial Robustness and Security', 'Machine Learning']",[],"['Zixiu Wang', 'Yiwen Guo', 'Hu Ding']","['Meituan', 'ByteDance', 'University of Science and Technology of']","[None, None, 'China']"
https://nips.cc/virtual/2021/poster/26638,Privacy & Data Governance,A Separation Result Between Data-oblivious and Data-aware Poisoning Attacks,"Poisoning attacks have emerged as a significant security threat to machine learning algorithms. It has been demonstrated that adversaries who make small changes to the training set, such as adding specially crafted data points, can hurt the performance of the output model. Most of these attacks require the full knowledge of training data. This leaves open the possibility of achieving the same attack results using poisoning attacks that do not have the full knowledge of the clean training set. In this work, we initiate a theoretical study of the problem above. Specifically, for the case of feature selection with LASSO, we show that \emph{full information} adversaries (that craft poisoning examples based on the rest of the training data) are provably much more devastating compared to the optimal attacker that is \emph{oblivious} to the training set yet has access to the distribution of the data.  Our separation result shows that the two settings of data-aware and data-oblivious are fundamentally different and we cannot hope to achieve the same attack or defense results in these scenarios.",['Machine Learning'],[],"['Samuel Deng', 'Sanjam Garg', 'Somesh Jha', 'Saeed Mahloujifar', 'Mohammad Mahmoody', 'Abhradeep Guha Thakurta']","['Columbia University', 'University of California Berkeley', 'Department of Computer Science, University of Wisconsin, Madison', 'Meta', 'University of Virginia', 'Google']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26582,Privacy & Data Governance,Learning Stable Deep Dynamics Models for Partially Observed or Delayed Dynamical Systems,"Learning how complex dynamical systems evolve over time is a key challenge in system identification. For safety critical systems, it is often crucial that the learned model is guaranteed to converge to some equilibrium point. To this end, neural ODEs regularized with neural Lyapunov functions are a promising approach when states are fully observed. For practical applications however, {\em partial observations} are the norm. As we will demonstrate, initialization of unobserved augmented states can become a key problem for neural ODEs. To alleviate this issue, we propose to augment the system's state with its history. Inspired by state augmentation in discrete-time systems, we thus obtain {\em neural delay differential equations}. Based on classical time delay stability analysis, we then show how to ensure stability of the learned models, and theoretically analyze our approach. Our experiments demonstrate its applicability to stable system identification of partially observed systems and learning a stabilizing feedback policy in delayed feedback control.","['Deep Learning', 'Machine Learning']",[],"['Andreas Schlaginhaufen', 'Philippe Wenk', 'Andreas Krause', 'Florian Dörfler']","['EPFL - EPF Lausanne', '', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26535,Privacy & Data Governance,Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes,"Quantization is a popular technique that transforms the parameter representation of a neural network from floating-point numbers into lower-precision ones (e.g., 8-bit integers). It reduces the memory footprint and the computational cost at inference, facilitating the deployment of resource-hungry models. However, the parameter perturbations caused by this transformation result in behavioral disparities between the model before and after quantization. For example, a quantized model can misclassify some test-time samples that are otherwise classified correctly. It is not known whether such differences lead to a new security vulnerability. We hypothesize that an adversary may control this disparity to introduce specific behaviors that activate upon quantization. To study this hypothesis, we weaponize quantization-aware training and propose a new training framework to implement adversarial quantization outcomes. Following this framework, we present three attacks we carry out with quantization: (i) an indiscriminate attack for significant accuracy loss; (ii) a targeted attack against specific samples; and (iii) a backdoor attack for controlling the model with an input trigger. We further show that a single compromised model defeats multiple quantization schemes, including robust quantization techniques. Moreover, in a federated learning scenario, we demonstrate that a set of malicious participants who conspire can inject our quantization-activated backdoor. Lastly, we discuss potential counter-measures and show that only re-training consistently removes the attack artifacts. Our code is available at https://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation","['Federated Learning', 'Deep Learning', 'Adversarial Robustness and Security']",[],"['Sanghyun Hong', 'Michael-Andrei Panaitescu-Liess', 'Yigitcan Kaya', 'Tudor Dumitras']","['Oregon State University', 'University of Maryland, College Park', 'University of California, Santa Barbara', 'University of Maryland, College Park']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26512,Privacy & Data Governance,Subgame solving without common knowledge,"In imperfect-information games, subgame solving is significantly more challenging than in perfect-information games, but in the last few years, such techniques have been developed. They were the key ingredient to the milestone of superhuman play in no-limit Texas hold'em poker. Current subgame-solving techniques analyze the entire common-knowledge closure of the player's current information set, that is, the smallest set of nodes within which it is common knowledge that the current node lies. While this is acceptable in games like poker where the common-knowledge closure is relatively small, many practical games have more complex information structure, which renders the common-knowledge closure impractically large to enumerate or even reasonably approximate. We introduce an approach that overcomes this obstacle, by instead working with only low-order knowledge. Our approach allows an agent, upon arriving at an infoset, to basically prune any node that is no longer reachable, thereby massively reducing the game tree size relative to the common-knowledge subgame. We prove that, as is, our approach can increase exploitability compared to the blueprint strategy. However, we develop three avenues by which safety can be guaranteed. First, safety is guaranteed if the results of subgame solves are incorporated back into the blueprint. Second, we provide a method where safety is achieved by limiting the infosets at which subgame solving is performed. Third, we prove that our approach, when applied at every infoset reached during play, achieves a weaker notion of equilibrium, which we coin affine equilibrium, and which may be of independent interest. We show that affine equilibria cannot be exploited by any Nash strategy of the opponent, so an opponent who wishes to exploit must open herself to counter-exploitation. Even without the safety-guaranteeing additions, experiments on medium-sized games show that our approach always reduced exploitability in practical games even when applied at every infoset, and a depth-limited version of it led to---to our knowledge---the first strong AI for the challenge problem dark chess.",[],[],"['Brian Hu Zhang', 'Tuomas Sandholm']","['Carnegie Mellon University', 'Carnegie Mellon University']","[None, None]"
https://nips.cc/virtual/2021/poster/26508,Privacy & Data Governance,Adversarial Examples Make Strong Poisons,"The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data.  In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. In fact, adversarial examples with labels re-assigned by the crafting network remain effective for training, suggesting that adversarial examples contain useful semantic content, just with the ""wrong"" labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.","['Adversarial Robustness and Security', 'Robustness', 'Machine Learning']",[],"['Liam H Fowl', 'Micah Goldblum', 'Ping-yeh Chiang', 'Jonas Geiping', 'Tom Goldstein']","['Google', 'New York University', 'University of Maryland, College Park', 'ELLIS Institute Tübingen', 'University of Maryland, College Park']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26489,Privacy & Data Governance,Training Certifiably Robust Neural Networks with Efficient Local Lipschitz Bounds,"Certified robustness is a desirable property for deep neural networks in safety-critical applications, and popular training algorithms can certify robustness of a neural network by computing a global bound on its Lipschitz constant. However, such a bound is often loose: it tends to over-regularize the neural network and degrade its natural accuracy. A tighter Lipschitz bound may provide a better tradeoff between natural and certified accuracy, but is generally hard to compute exactly due to non-convexity of the network. In this work, we propose an efficient and trainable \emph{local} Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Specifically, when computing the induced norm of a weight matrix, we eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global Lipschitz constant of the neural network. Our method can be used as a plug-in module to tighten the Lipschitz bound in many certifiable training algorithms. Furthermore, we propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local Lipschitz bound. Experimentally, we show that our method consistently outperforms state-of-the-art methods in both clean and certified accuracy on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Yujia Huang', 'Huan Zhang', 'Yuanyuan Shi', 'J Zico Kolter', 'Anima Anandkumar']","['California Institute of Technology', 'University of Illinois at Urbana-Champaign', 'University of California, San Diego', 'Carnegie Mellon University', 'California Institute of Technology']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26465,Privacy & Data Governance,Novel Upper Bounds for the Constrained Most Probable Explanation Task,"We propose several schemes for upper bounding the optimal value of the constrained most probable explanation (CMPE) problem. Given a set of discrete random variables, two probabilistic graphical models defined over them and a real number $q$, this problem involves finding an assignment of values to all the variables such that the probability of the assignment is maximized according to the first model and is bounded by $q$ w.r.t. the second model. In prior work, it was shown that CMPE is a unifying problem with several applications and special cases including the nearest assignment problem, the decision preserving most probable explanation task and robust estimation. It was also shown that CMPE is NP-hard even on tractable models such as bounded treewidth networks and is hard for integer linear programming methods because it includes a dense global constraint. The main idea in our approach is to simplify the problem via Lagrange relaxation and decomposition to yield either a knapsack problem or the unconstrained most probable explanation (MPE) problem, and then solving the two problems, respectively using specialized knapsack algorithms and mini-buckets based upper bounding schemes. We evaluate our proposed scheme along several dimensions including quality of the bounds and computation time required on various benchmark graphical models and how it can be used to find heuristic, near-optimal feasible solutions in an example application pertaining to robust estimation and adversarial attacks on classifiers.","['Interpretability', 'Graph Learning', 'Optimization', 'Adversarial Robustness and Security']",[],"['Tahrima Rahman', 'Sara Rouhani', 'Vibhav Giridhar Gogate']","['University of Texas, Dallas', 'University of Texas, Dallas', 'University of Texas at Dallas']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26458,Privacy & Data Governance,Robustness of Graph Neural Networks at Scale,"Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.","['Optimization', 'Robustness', 'Graph Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Simon Geisler', 'Tobias Schmidt', 'Hakan Şirin', 'Daniel Zügner', 'Aleksandar Bojchevski', 'Stephan Günnemann']","['Technical University Munich', 'Technical University Munich', 'Technical University Munich', 'Microsoft', 'Universität Köln', 'Technical University Munich']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26391,Privacy & Data Governance,Drawing Robust Scratch Tickets: Subnetworks with Inborn Robustness Are Found within Randomly Initialized Networks,"Deep Neural Networks (DNNs) are known to be vulnerable to adversarial attacks, i.e., an imperceptible perturbation to the input can mislead DNNs trained on clean images into making erroneous predictions. To tackle this, adversarial training is currently the most effective defense method, by augmenting the training set with adversarial samples generated on the fly. \textbf{Interestingly, we discover for the first time that there exist subnetworks with inborn robustness, matching or surpassing the robust accuracy of the adversarially trained networks with comparable model sizes, within randomly initialized networks without any model training}, indicating that adversarial training on model weights is not indispensable towards adversarial robustness. We name such subnetworks Robust Scratch Tickets (RSTs), which are also by nature efficient. Distinct from the popular lottery ticket hypothesis, neither the original dense networks nor the identified RSTs need to be trained. To validate and understand this fascinating finding, we further conduct extensive experiments to study the existence and properties of RSTs under different models, datasets, sparsity patterns, and attacks, drawing insights regarding the relationship between DNNs’ robustness and their initialization/overparameterization. Furthermore, we identify the poor adversarial transferability between RSTs of different sparsity ratios drawn from the same randomly initialized dense network, and propose a Random RST Switch (R2S) technique, which randomly switches between different RSTs, as a novel defense method built on top of RSTs. We believe our findings about RSTs have opened up a new perspective to study model robustness and extend the lottery ticket hypothesis.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Yonggan Fu', 'Qixuan Yu', 'Yang Zhang', 'Shang Wu', 'Xu Ouyang', 'David Daniel Cox', 'Yingyan Lin']","['Institute of Technology', 'Rice University', 'International Business Machines', 'Northwestern University, Northwestern University', 'University of Virginia, Charlottesville', 'International Business Machines', 'Institute of Technology']","['Georgia', None, None, None, None, None, 'Georgia']"
https://nips.cc/virtual/2021/poster/26325,Privacy & Data Governance,Optimal Best-Arm Identification Methods for Tail-Risk Measures,"Conditional value-at-risk (CVaR) and value-at-risk (VaR) are popular tail-risk measures in finance and insurance industries as well as in highly reliable, safety-critical uncertain environments where often the underlying probability distributions are heavy-tailed. We use the multi-armed bandit best-arm identification framework and consider the problem of identifying the arm from amongst finitely many that has the smallest CVaR, VaR, or weighted sum of CVaR and mean. The latter captures the risk-return trade-off common in finance. Our main contribution is an optimal $\delta$-correct algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as $ \delta$ approaches $0$). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. En-route, we develop new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures.","['Reinforcement Learning and Planning', 'Optimization', 'Bandits']",[],"['Shubhada Agrawal', 'Wouter M Koolen', 'Sandeep Kumar Juneja']","['Institute of Technology', 'Centrum voor Wiskunde en Informatica', 'Tata Institute of Fundamental Research']","['Georgia', None, None]"
https://nips.cc/virtual/2021/poster/26329,Privacy & Data Governance,CAFE: Catastrophic Data Leakage in Vertical Federated Learning,"Recent studies show that private training data can be leaked through the gradients sharing mechanism deployed in distributed machine learning systems, such as federated learning (FL). Increasing batch size to complicate data recovery is often viewed as a promising defense strategy against data leakage. In this paper, we revisit this defense premise and propose an advanced data leakage attack with theoretical justification to efficiently recover batch data from the shared aggregated gradients. We name our proposed method as catastrophic data leakage in vertical federated learning (CAFE). Comparing to existing data leakage attacks, our extensive experimental results on vertical FL settings demonstrate the effectiveness of CAFE to perform large-batch data leakage attack with improved data recovery quality. We also propose a practical countermeasure to mitigate CAFE. Our results suggest that private data participated in standard FL, especially the vertical case, have a high risk of being leaked from the training gradients. Our analysis implies unprecedented and practical data leakage risks in those learning settings. The code of our work is available at https://github.com/DeRafael/CAFE.","['Federated Learning', 'Machine Learning', 'Privacy']",[],"['Xiao Jin', 'Pin-Yu Chen', 'Chia-Yi Hsu', 'Chia-Mu Yu', 'Tianyi Chen']","['Rensselaer Polytechnic Institute', 'International Business Machines', 'National Chiao Tung University', 'National Yang Ming Chiao Tung University', 'Rensselaer Polytechnic Institute']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26328,Privacy & Data Governance,Topological Detection of Trojaned Neural Networks,"Deep neural networks are known to have security issues. One particular threat is the Trojan attack. It occurs when the attackers stealthily manipulate the model's behavior through Trojaned training samples, which can later be exploited. Guided by basic neuroscientific principles, we discover subtle -- yet critical -- structural deviation characterizing Trojaned models. In our analysis we use topological tools. They allow us to model high-order dependencies in the networks, robustly compare different networks, and localize structural abnormalities. One interesting observation is that Trojaned models develop short-cuts from shallow to deep layers. Inspired by these observations, we devise a strategy for robust detection of Trojaned models. Compared to standard baselines it displays better performance on multiple benchmarks.",['Deep Learning'],[],"['Songzhu Zheng', 'Hubert Wagner', 'Chao Chen']","['Morgan Stanley', 'Institute of Science and Technology', 'State University of New York at Stony Brook']","[None, 'Austria', None]"
https://nips.cc/virtual/2021/poster/26314,Privacy & Data Governance,Adversarial Robustness without Adversarial Training: A Teacher-Guided Curriculum Learning Approach,"Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves significant performance gains with a little extra effort (10-20%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy. We have performed extensive experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.","['Adversarial Robustness and Security', 'Robustness']",[],"['Anindya Sarkar', 'Anirban Sarkar', 'Sowrya Gali', 'Vineeth N. Balasubramanian']","['Washington University, Saint Louis', 'Cold Spring Harbor Laboratory', 'n Institute of Technology Hyderabad', 'n Institute of Technology Hyderabad']","[None, None, 'India', 'India']"
https://nips.cc/virtual/2021/poster/26297,Privacy & Data Governance,On the Convergence of Prior-Guided Zeroth-Order Optimization Algorithms,"Zeroth-order (ZO) optimization is widely used to handle challenging tasks, such as query-based black-box adversarial attacks and reinforcement learning. Various attempts have been made to integrate prior information into the gradient estimation procedure based on finite differences, with promising empirical results. However, their convergence properties are not well understood. This paper makes an attempt to fill up this gap by analyzing the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. We provide a convergence guarantee for the prior-guided random gradient-free (PRGF) algorithms. Moreover, to further accelerate over greedy descent methods, we present a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. Finally, our theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks.","['Reinforcement Learning and Planning', 'Optimization', 'Adversarial Robustness and Security']",[],"['Shuyu Cheng', 'Guoqiang Wu', 'Jun Zhu']","['Tsinghua University, Tsinghua University', 'Shandong University', 'Tsinghua University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26270,Privacy & Data Governance,Towards Efficient and Effective Adversarial Training,"The vulnerability of Deep Neural Networks to adversarial attacks has spurred immense interest towards improving their robustness. However, present state-of-the-art adversarial defenses involve the use of 10-step adversaries during training, which renders them computationally infeasible for application to large-scale datasets. While the recent single-step defenses show promising direction, their robustness is not on par with multi-step training methods. In this work, we bridge this performance gap by introducing a novel Nuclear-Norm regularizer on network predictions to enforce function smoothing in the vicinity of data samples.  While prior works consider each data sample independently, the proposed regularizer uses the joint statistics of adversarial samples across a training minibatch to enhance optimization during both attack generation and training, obtaining state-of-the-art results amongst efficient defenses. We achieve further gains by incorporating exponential averaging of network weights over training iterations. We finally introduce a Hybrid training approach that combines the effectiveness of a two-step variant of the proposed defense with the efficiency of a single-step defense. We demonstrate superior results when compared to multi-step defenses such as TRADES and PGD-AT as well, at a significantly lower computational cost.","['Adversarial Robustness and Security', 'Deep Learning', 'Optimization', 'Robustness']",[],"['Gaurang Sriramanan', 'Sravanti Addepalli', 'Arya Baburaj', 'Venkatesh Babu Radhakrishnan']","['University of Maryland, College Park', 'Google', 'n Institute of Science, Bangalore', 'n Institute of Science']","[None, None, 'India', 'India']"
https://nips.cc/virtual/2021/poster/26272,Privacy & Data Governance,Revisiting 3D Object Detection From an Egocentric Perspective,"3D object detection is a key module for safety-critical robotics applications such as autonomous driving. For these applications, we care most about how the detections affect the ego-agent’s behavior and safety (the egocentric perspective). Intuitively, we seek more accurate descriptions of object geometry when it’s more likely to interfere with the ego-agent’s motion trajectory. However, current detection metrics, based on box Intersection-over-Union (IoU), are object-centric and aren’t designed to capture the spatio-temporal relationship between objects and the ego-agent. To address this issue, we propose a new egocentric measure to evaluate 3D object detection,  namely Support Distance Error (SDE). Our analysis based on SDE reveals that the egocentric detection quality is bounded by the coarse geometry of the bounding boxes. Given the insight that SDE would benefit from more accurate geometry descriptions, we propose to represent objects as amodal contours, specifically amodal star-shaped polygons, and devise a simple model, StarPoly, to predict such contours. Our experiments on the large-scale Waymo Open Dataset show that SDE better reflects the impact of detection quality on the ego-agent’s safety compared to IoU; and the estimated contours from StarPoly consistently improve the egocentric detection quality over recent 3D object detectors.",['Vision'],[],"['Boyang Deng', 'Charles R. Qi', 'Mahyar Najibi', 'Thomas Funkhouser', 'Yin Zhou', 'Dragomir Anguelov']","['Stanford University', 'Waymo', 'Apple', 'Princeton University', 'Waymo', 'Waymo']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26087,Privacy & Data Governance,On Success and Simplicity: A Second Look at Transferable Targeted Attacks,"Achieving transferability of targeted attacks is reputed to be remarkably difficult. The current state of the art has resorted to resource-intensive solutions that necessitate training model(s) for each target class with additional data. In our investigation, we find, however, that simple transferable attacks which require neither model training nor additional data can achieve surprisingly strong targeted transferability. This insight has been overlooked until now, mainly because the widespread practice of attacking with only few iterations has largely limited the attack convergence to optimal targeted transferability. In particular, we, for the first time, identify that a very simple logit loss can largely surpass the commonly adopted cross-entropy loss, and yield even better results than the resource-intensive state of the art. Our analysis spans a variety of transfer scenarios, especially including three new, realistic scenarios: an ensemble transfer scenario with little model similarity, a worse-case scenario with low-ranked target classes, and also a real-world attack on the Google Cloud Vision API. Results in these new transfer scenarios demonstrate that the commonly adopted, easy scenarios cannot fully reveal the actual strength of different attacks and may cause misleading comparative results. We also show the usefulness of the simple logit loss for generating targeted universal adversarial perturbations in a data-free manner. Overall, the aim of our analysis is to inspire a more meaningful evaluation on targeted transferability. Code is available at https://github.com/ZhengyuZhao/Targeted-Tansfer.",['Adversarial Robustness and Security'],[],"['Zhengyu Zhao', 'Zhuoran Liu', 'Martha Larson']","[""Xi'an Jiaotong University"", 'Radboud University', 'Radboud University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28350,Privacy & Data Governance,Variational Model Inversion Attacks,"Given the ubiquity of deep neural networks, it is important that these models do not reveal information about sensitive data that they have been trained on. In model inversion attacks, a malicious user attempts to recover the private dataset used to train a supervised neural network. A successful model inversion attack should generate realistic and diverse samples that accurately describe each of the classes in the private dataset. In this work, we provide a probabilistic interpretation of model inversion attacks, and formulate a variational objective that accounts for both diversity and accuracy. In order to optimize this variational objective, we choose a variational family defined in the code space of a deep generative model, trained on a public auxiliary dataset that shares some structural similarity with the target dataset.  Empirically, our method substantially improves performance in terms of target attack accuracy, sample realism, and diversity on datasets of faces and chest X-ray images.","['Deep Learning', 'Generative Model']",[],"['Kuan-Chieh Wang', 'YAN FU', 'Ke Li', 'Ashish J Khisti', 'Richard Zemel', 'Alireza Makhzani']","['Stanford University', 'Toronto University', 'Simon Fraser University', 'Toronto University', 'Department of Computer Science, Columbia University', 'Vector Institute']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26067,Privacy & Data Governance,A PAC-Bayes Analysis of Adversarial Robustness,"We propose the first general PAC-Bayesian generalization bounds for adversarial robustness, that estimate, at test time, how much a model will be invariant to imperceptible perturbations in the input. Instead of deriving a worst-case analysis of the risk of a hypothesis over all the possible perturbations, we leverage the PAC-Bayesian framework to bound the averaged risk on the perturbations for majority votes (over the whole class of hypotheses). Our theoretically founded analysis has the advantage to provide general bounds (i) that are valid for any kind of attacks (i.e., the adversarial attacks), (ii) that are tight thanks to the PAC-Bayesian framework, (iii) that can be directly minimized during the learning phase to obtain a robust model on different attacks at test time.","['Adversarial Robustness and Security', 'Robustness']",[],"['Paul Viallard', 'Guillaume Eric VIDOT', 'Amaury Habrard', 'Emilie Morvant']","['INRIA Paris', 'IRIT', 'Université Saint-Etienne, Laboratoire Hubert Curien', 'University Jean Monnet']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/25987,Privacy & Data Governance,Adversarial Attacks on Graph Classifiers via Bayesian Optimisation,"Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models.","['Graph Learning', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Xingchen Wan', 'Henry Kenlay', 'Binxin Ru', 'Arno Blaas', 'Michael Osborne', 'Xiaowen Dong']","['Google', 'Exscientia', 'University of Oxford', 'Apple', 'University of Oxford', 'University of Oxford']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25959,Privacy & Data Governance,Adversarial Feature Desensitization,"Neural networks are known to be vulnerable to adversarial attacks -- slight but carefully constructed perturbations of the inputs which can drastically impair the network's performance. Many defense methods have been proposed for improving robustness of  deep networks by training them on adversarially perturbed inputs. However, these models often remain vulnerable to new types of attacks not seen during training, and even to slightly stronger versions of previously seen  attacks. In this work, we propose a novel approach to  adversarial robustness, which builds upon the insights from the domain adaptation field. Our method, called Adversarial Feature Desensitization (AFD), aims at learning  features that are invariant towards adversarial perturbations of the inputs. This is achieved through a game where we learn features that are both predictive and robust (insensitive to adversarial attacks), i.e. cannot be used to discriminate between natural and adversarial data. Empirical results on several benchmarks  demonstrate the effectiveness of the proposed approach against a wide range of attack types and attack strengths. Our code is available at https://github.com/BashivanLab/afd.","['Adversarial Robustness and Security', 'Deep Learning', 'Domain Adaptation', 'Robustness']",[],"['Pouya Bashivan', 'Reza Bayat', 'Adam Ibrahim', 'Kartik Ahuja', 'Mojtaba Faramarzi', 'Touraj Laleh', 'Blake Aaron Richards', 'Irina Rish']","['McGill University', 'Université de Montréal', 'University of Montreal', 'FAIR (Meta)', 'Mila', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'McGill University', 'University of Montreal']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27644,Privacy & Data Governance,Learning Transferable Adversarial Perturbations,"While effective, deep neural networks (DNNs) are vulnerable to adversarial attacks. In particular, recent work has shown that such attacks could be generated by another deep network, leading to significant speedups over optimization-based perturbations. However, the ability of such generative methods to generalize to different test-time situations has not been systematically studied. In this paper, we, therefore, investigate the transferability of generated perturbations when the conditions at inference time differ from the training ones in terms of the target architecture, target data, and target task. Specifically, we identify the mid-level features extracted by the intermediate layers of DNNs as common ground across different architectures, datasets, and tasks. This lets us introduce a loss function based on such mid-level features to learn an effective, transferable perturbation generator. Our experiments demonstrate that our approach outperforms the state-of-the-art universal and transferable attack strategies.","['Adversarial Robustness and Security', 'Deep Learning', 'Optimization']",[],"['Krishna kanth Nakka', 'Mathieu Salzmann']","['Huawei Technologies Ltd.', 'Swiss Federal Institute of Technology Lausanne']","[None, None]"
https://nips.cc/virtual/2021/poster/27128,Privacy & Data Governance,ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers,"Adversarial patch attacks that craft the pixels in a confined region of the input images show their powerful attack effectiveness in physical environments even with noises or deformations. Existing certified defenses towards adversarial patch attacks work well on small images like MNIST and CIFAR-10 datasets, but achieve very poor certified accuracy on higher-resolution images like ImageNet. It is urgent to design both robust and effective defenses against such a practical and harmful attack in industry-level larger images. In this work, we propose the certified defense methodology that achieves high provable robustness for high-resolution images and largely improves the practicality for real adoption of the certified defense. The basic insight of our work is that the adversarial patch intends to leverage localized superficial important neurons (SIN) to manipulate the prediction results. Hence, we leverage the SIN-based DNN compression techniques to significantly improve the certified accuracy, by reducing the adversarial region searching overhead and filtering the prediction noises. Our experimental results show that the certified accuracy is increased from 36.3%  (the state-of-the-art certified detection)  to 60.4%on the ImageNet dataset, largely pushing the certified defenses for practical use.",['Robustness'],[],"['Husheng Han', 'Kaidi Xu', 'Xing Hu', 'Xiaobing Chen', 'Ling Liang', 'Zidong Du', 'Qi Guo', 'Yanzhi Wang', 'Yunji Chen']","['Institude of Computer Technology, Chinese Academy of Sciences', 'Drexel University', ', Chinese Academy of Sciences', 'ict, cas', 'University of California, Santa Barbara', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Northeastern University', 'Institute of Computing Technology, Chinese Academy of Sciences']","[None, None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26092,Privacy & Data Governance,How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?,"The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.","['Adversarial Robustness and Security', 'Robustness', 'Language']",[],"['Xinshuai Dong', 'Anh Tuan Luu', 'Min Lin', 'Shuicheng Yan', 'Hanwang Zhang']","['Carnegie Mellon University', 'Nanyang Technological University', 'Sea AI Lab', 'National University of', 'Nanyang Technological University']","[None, None, None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/27157,Privacy & Data Governance,Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis,"Tabular data synthesis has received wide attention in the literature. This is because available data is often limited, incomplete, or cannot be obtained easily, and data privacy is becoming increasingly important. In this work, we present a generalized GAN framework for tabular synthesis, which combines the adversarial training of GANs and the negative log-density regularization of invertible neural networks. The proposed framework can be used for two distinctive objectives. First,  we can further improve the synthesis quality, by decreasing the negative log-density of real records in the process of adversarial training. On the other hand, by increasing the negative log-density of real records, realistic fake records can be synthesized in a way that they are not too much close to real records and reduce the chance of potential information leakage. We conduct experiments with real-world datasets for classification, regression, and privacy attacks. In general, the proposed method demonstrates the best synthesis quality (in terms of task-oriented evaluation metrics, e.g., F1) when decreasing the negative log-density during the adversarial training. If increasing the negative log-density, our experimental results show that the distance between real and fake records increases, enhancing robustness against privacy attacks.","['Privacy', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning', 'Generative Model']",[],"['JAEHOON LEE', 'Jihyeon Hyeong', 'Jinsung Jeon', 'Noseong Park', 'Jihoon Cho']","['LG AI RESEARCH', 'Yonsei University', 'Yonsei University', 'Yonsei University', 'Samsung SDS']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28047,Privacy & Data Governance,Adversarial Robustness of Streaming Algorithms through Importance Sampling,"Robustness against adversarial attacks has recently been at the forefront of algorithmic design for machine learning tasks. In the adversarial streaming model, an adversary gives an algorithm a sequence of adaptively chosen updates $u_1,\ldots,u_n$ as a data stream. The goal of the algorithm is to compute or approximate some predetermined function for every prefix of the adversarial stream, but the adversary may generate future updates based on previous outputs of the algorithm. In particular, the adversary may gradually learn the random bits internally used by an algorithm to manipulate dependencies in the input. This is especially problematic as many important problems in the streaming model require randomized algorithms, as they are known to not admit any deterministic algorithms that use sublinear space. In this paper, we introduce adversarially robust streaming algorithms for central machine learning and algorithmic tasks, such as regression and clustering, as well as their more general counterparts, subspace embedding, low-rank approximation, and coreset construction. For regression and other numerical linear algebra related tasks, we consider the row arrival streaming model. Our results are based on a simple, but powerful, observation that many importance sampling-based algorithms give rise to adversarial robustness which is in contrast to sketching based algorithms, which are very prevalent in the streaming literature but suffer from adversarial attacks. In addition, we show that the well-known merge and reduce paradigm in streaming is adversarially robust. Since the merge and reduce paradigm allows coreset constructions in the streaming setting, we thus obtain robust algorithms for $k$-means, $k$-median, $k$-center, Bregman clustering, projective clustering, principal component analysis (PCA) and non-negative matrix factorization. To the best of our knowledge, these are the first adversarially robust results for these problems yet require no new algorithmic implementations. Finally, we empirically confirm the robustness of our algorithms on various adversarial attacks and demonstrate that by contrast, some common existing algorithms are not robust.","['Clustering', 'Robustness', 'Adversarial Robustness and Security', 'Machine Learning']",[],"['Vladimir Braverman', 'Mariano Schain', 'Sandeep Silwal', 'Samson Zhou']","['Rice University', 'Google', 'Massachusetts Institute of Technology', 'Texas A&M University - College Station']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27350,Privacy & Data Governance,Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks,"One major problem in black-box adversarial attacks is the high query complexity in the hard-label attack setting, where only the top-1 predicted label is available. In this paper, we propose a novel geometric-based approach called Tangent Attack (TA), which identifies an optimal tangent point of a virtual hemisphere located on the decision boundary to reduce the distortion of the attack. Assuming the decision boundary is locally flat, we theoretically prove that the minimum $\ell_2$ distortion can be obtained by reaching the decision boundary along the tangent line passing through such tangent point in each iteration. To improve the robustness of our method, we further propose a generalized method which replaces the hemisphere with a semi-ellipsoid to adapt to curved decision boundaries. Our approach is free of pre-training. Extensive experiments conducted on the ImageNet and CIFAR-10 datasets demonstrate that our approach can consume only a small number of queries to achieve the low-magnitude distortion. The implementation source code is released online.","['Adversarial Robustness and Security', 'Robustness']",[],"['Chen Ma', 'Xiangyu Guo', 'Li Chen', 'Jun-Hai Yong', 'Yisen Wang']","['Zhejiang University of Technology', 'Meta Platforms, Inc', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Peking University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26894,Privacy & Data Governance,Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs,"We address the issue of safety in reinforcement learning. We pose the problem in an episodic framework of a constrained Markov decision process. Existing results have shown that it is possible to achieve a reward regret of $\tilde{\mathcal{O}}(\sqrt{K})$ while allowing an $\tilde{\mathcal{O}}(\sqrt{K})$ constraint violation in $K$ episodes. A critical question that arises is whether it is possible to keep the constraint violation even smaller. We show that when a strictly safe policy is known, then one can confine the system to zero constraint violation with arbitrarily high probability while keeping the reward regret of order $\tilde{\mathcal{O}}(\sqrt{K})$. The algorithm which does so employs the principle of optimistic pessimism in the face of uncertainty to achieve safe exploration. When no strictly safe policy is known, though one is known to exist, then it is possible to restrict the system to bounded constraint violation with arbitrarily high probability. This is shown to be realized by a primal-dual algorithm with an optimistic primal estimate and a pessimistic dual update.",['Reinforcement Learning and Planning'],[],"['Tao Liu', 'Ruida Zhou', 'Dileep Kalathil', 'Panganamala Kumar']","['Texas A&M', 'University of California, Los Angeles', 'Texas A&M University', 'Texas A&M']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27461,Privacy & Data Governance,Bandit Quickest Changepoint Detection,"Many industrial and security applications employ a suite of sensors for detecting abrupt changes in temporal behavior patterns. These abrupt changes typically manifest locally, rendering only a small subset of sensors informative. Continuous monitoring of every sensor can be expensive due to resource constraints, and serves as a motivation for the bandit quickest changepoint detection problem, where sensing actions (or sensors) are sequentially chosen, and only measurements corresponding to chosen actions are observed. We derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions. We then propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. We derive expected delay bounds for the proposed scheme and show that these bounds match our information-theoretic lower bounds at low false alarm rates, establishing optimality of the proposed method. We then perform a number of experiments on synthetic and real datasets demonstrating the effectiveness of our proposed method.","['Reinforcement Learning and Planning', 'Bandits']",[],"['Aditya Gopalan', 'Braghadeesh Lakshminarayanan', 'Venkatesh Saligrama']","['n Institute of Science', 'KTH Royal Institute of Technology, Stockholm,', 'Amazon']","['India', 'Sweden', None]"
https://nips.cc/virtual/2021/poster/28216,Privacy & Data Governance,Infinite Time Horizon Safety of Bayesian Neural Networks,"Bayesian neural networks (BNNs) place distributions over the weights of a neural network to model uncertainty in the data and the network's prediction. We consider the problem of verifying safety when running a Bayesian neural network policy in a feedback loop with infinite time horizon systems. Compared to the existing sampling-based approaches, which are inapplicable to the infinite time horizon setting, we train a separate deterministic neural network that serves as an infinite time horizon safety certificate. In particular, we show that the certificate network guarantees the safety of the system over a subset of the BNN weight posterior's support. Our method first computes a safe weight set and then alters the BNN's weight posterior to reject samples outside this set. Moreover, we show how to extend our approach to a safe-exploration reinforcement learning setting, in order to avoid unsafe trajectories during the training of the policy. We evaluate our approach on a series of reinforcement learning benchmarks, including non-Lyapunovian safety specifications.","['Reinforcement Learning and Planning', 'Deep Learning']",[],"['Mathias Lechner', 'Đorđe Žikelić', 'Krishnendu Chatterjee', 'Thomas A Henzinger']","['Massachusetts Institute of Technology', 'Management University', 'Institute of Science and Technology', 'Institute of Science and Technology']","[None, 'Singapore', 'Austria', 'Austria']"
https://nips.cc/virtual/2021/poster/26692,Privacy & Data Governance,Deep Synoptic Monte-Carlo Planning in Reconnaissance Blind Chess,"This paper introduces deep synoptic Monte Carlo planning (DSMCP) for large imperfect information games. The algorithm constructs a belief state with an unweighted particle filter and plans via playouts that start at samples drawn from the belief state. The algorithm accounts for uncertainty by performing inference on ""synopses,"" a novel stochastic abstraction of information states. DSMCP is the basis of the program Penumbra, which won the official 2020 reconnaissance blind chess competition versus 33 other programs. This paper also evaluates algorithm variants that incorporate caution, paranoia, and a novel bandit algorithm. Furthermore, it audits the synopsis features used in Penumbra with per-bit saliency statistics.","['Reinforcement Learning and Planning', 'Deep Learning', 'Bandits']",[],['Gregory Clark'],['Google'],[None]
https://nips.cc/virtual/2021/poster/26926,Privacy & Data Governance,Adversarially Robust 3D Point Cloud Recognition Using Self-Supervisions,"3D point cloud data is increasingly used in safety-critical applications such as autonomous driving. Thus, the robustness of 3D deep learning models against adversarial attacks becomes a major consideration. In this paper, we systematically study the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, we study MLP-based (PointNet), convolution-based (DGCNN), and transformer-based (PCT) 3D architectures. Through extensive experimentation, we demonstrate that appropriate applications of self-supervision can significantly enhance the robustness in 3D point cloud recognition, achieving considerable improvements compared to the standard adversarial training baseline. Our analysis reveals that local feature learning is desirable for adversarial robustness in point clouds since it limits the adversarial propagation between the point-level input perturbations and the model's final output. This insight also explains the success of DGCNN and the jigsaw proxy task in achieving stronger 3D adversarial robustness.","['Transformers', 'Robustness', 'Self-Supervised Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Jiachen Sun', 'Yulong Cao', 'Christopher Choy', 'Zhiding Yu', 'Anima Anandkumar', 'Zhuoqing Mao', 'Chaowei Xiao']","['University of Michigan', 'NVIDIA', 'NVIDIA', 'NVIDIA', 'California Institute of Technology', 'University of Michigan', 'University of Wisconsin - Madison']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28031,Privacy & Data Governance,Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning,"Unsupervised domain adaptation (UDA) enables cross-domain learning without target domain labels by transferring knowledge from a labeled source domain whose distribution differs from that of the target. However, UDA is not always successful and several accounts of `negative transfer' have been reported in the literature. In this work, we prove a simple lower bound on the target domain error that complements the existing upper bound. Our bound shows the insufficiency of minimizing source domain error and marginal distribution mismatch for a guaranteed reduction in the target domain error, due to the possible increase of induced labeling function mismatch. This insufficiency is further illustrated through simple distributions for which the same UDA approach succeeds, fails, and may succeed or fail with an equal chance. Motivated from this, we propose novel data poisoning attacks to fool UDA methods into learning representations that produce large target domain errors. We evaluate the effect of these attacks on popular UDA methods using benchmark datasets where they have been previously shown to be successful. Our results show that poisoning can significantly decrease the target domain accuracy, dropping it to almost 0% in some cases, with the addition of only 10% poisoned data in the source domain. The failure of these UDA methods demonstrates their limitations at guaranteeing cross-domain generalization consistent with our lower bound. Thus, evaluating UDA methods in adversarial settings such as data poisoning provides a better sense of their robustness to data distributions unfavorable for UDA.","['Robustness', 'Domain Adaptation']",[],"['Akshay Mehra', 'Bhavya Kailkhura', 'Pin-Yu Chen', 'Jihun Hamm']","['Tulane University', 'Lawrence Livermore National Laboratory', 'International Business Machines', 'Ohio State University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27159,Privacy & Data Governance,Formalizing Generalization and Adversarial Robustness of Neural Networks to Weight Perturbations,"Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forward neural networks in terms of the robustness in pairwise class margin and its generalization behavior under weight perturbation. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.","['Robustness', 'Machine Learning', 'Theory', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Yu-Lin Tsai', 'Chia-Yi Hsu', 'Chia-Mu Yu', 'Pin-Yu Chen']","['National Yang Ming Chiao Tung University', 'National Chiao Tung University', 'National Yang Ming Chiao Tung University', 'International Business Machines']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26715,Privacy & Data Governance,Observation-Free Attacks on Stochastic Bandits,"We study data corruption attacks on stochastic multi arm bandit algorithms. Existing attack methodologies assume that the attacker can observe the multi arm bandit algorithm's realized behavior which is in contrast to the adversaries modeled in the robust multi arm bandit algorithms literature. To the best of our knowledge, we develop the first data corruption attack on stochastic multi arm bandit algorithms which works without observing the algorithm's realized behavior. Through this attack, we also discover a sufficient condition for a stochastic multi arm bandit algorithm to be susceptible to adversarial data corruptions. We show that any bandit algorithm that makes decisions just using the empirical mean reward, and the number of times that arm has been pulled in the past can suffer from linear regret under data corruption attacks. We further show that various popular stochastic multi arm bandit algorithms such UCB, $\epsilon$-greedy and Thompson Sampling satisfy this sufficient condition and are thus prone to data corruption attacks. We further analyze the behavior of our attack for these algorithms and show that using only $o(T)$ corruptions, our attack can force these algorithms to select a potentially non-optimal target arm preferred by the attacker for all but $o(T)$ rounds.",['Bandits'],[],"['Yinglun Xu', 'Bhuvesh Kumar', 'Jacob Abernethy']","['University of Illinois, Urbana Champaign', 'TikTok Inc', 'Research, Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26965,Privacy & Data Governance,Learning Optimal Predictive Checklists,"Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classifiers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufficiently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. We benchmark the performance of our method on seven clinical classification problems, and demonstrate its practical benefits by training a short-form checklist for PTSD screening. Our results show that our method can fit simple predictive checklists that perform well and that can easily be customized to obey a rich class of custom constraints.","['Fairness', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Haoran Zhang', 'Quaid Morris', 'Berk Ustun', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Memorial Sloan Kettering Cancer Centre', 'University of California, San Diego', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28592,Security,Instance-optimal Mean Estimation Under Differential Privacy,"Mean estimation under differential privacy is a fundamental problem, but worst-case optimal mechanisms do not offer meaningful utility guarantees in practice when the global sensitivity is very large.  Instead, various heuristics have been proposed to reduce the error on real-world data that do not resemble the worst-case instance.  This paper takes a principled approach, yielding a mechanism that is instance-optimal in a strong sense.  In addition to its theoretical optimality, the mechanism is also simple and practical, and adapts to a variety of data characteristics without the need of parameter tuning.  It easily extends to the local and shuffle model as well.",['Privacy'],[],"['Ziyue Huang', 'Yuting Liang', 'Ke Yi']","['Tencent', 'University of Science and Technology', 'The  University of Science and Technology']","[None, 'Hong Kong', 'Hong Kong']"
https://nips.cc/virtual/2021/poster/27635,Security,Parameter-free HE-friendly Logistic Regression,"Privacy in machine learning has been widely recognized as an essential ethical and legal issue, because the data used for machine learning may contain sensitive information. Homomorphic encryption has recently attracted attention as a key solution to preserve privacy in machine learning applications. However, current approaches on the training of encrypted machine learning have relied heavily on hyperparameter selection, which should be avoided owing to the extreme difficulty of conducting validation on encrypted data. In this study, we propose an effective privacy-preserving logistic regression method that is free from the approximation of the sigmoid function and hyperparameter selection. In our framework, a logistic regression model can be transformed into the corresponding ridge regression for the logit function. We provide a theoretical background for our framework by suggesting a new generalization error bound on the encrypted data. Experiments on various real-world data show that our framework achieves better classification results while reducing latency by $\sim68\%$, compared to the previous models.","['Machine Learning', 'Privacy']",[],"['Junyoung Byun', 'Woojin Lee', 'Jaewook Lee']","['Seoul National University', 'Seoul National University', 'Seoul National University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26488,Security,Privately Publishable Per-instance Privacy,"We consider how to privately share the personalized privacy losses incurred by objective perturbation, using per-instance differential privacy (pDP). Standard differential privacy (DP) gives us a worst-case bound that might be orders of magnitude larger than the privacy loss to a particular individual relative to a fixed dataset. The pDP framework provides a more fine-grained analysis of the privacy guarantee to a target individual, but the per-instance privacy loss itself might be a function of sensitive data. In this paper, we analyze the per-instance privacy loss of releasing a private empirical risk minimizer learned via objective perturbation, and propose a group of methods to privately and accurately publish the pDP losses at little to no additional privacy cost.",['Privacy'],[],"['Rachel Emily Redberg', 'Yu-Xiang Wang']","['UC Santa Barbara', 'UC Santa Barbara']","[None, None]"
https://nips.cc/virtual/2021/poster/27315,Security,Analyzing the Confidentiality of Undistillable Teachers in Knowledge Distillation,"Knowledge distillation (KD) has recently been identified as a method that can unintentionally leak private information regarding the details of a teacher model to an unauthorized student. Recent research in developing undistillable nasty teachers that can protect model confidentiality has gained significant attention. However, the level of protection these nasty models offer has been largely untested. In this paper, we show that transferring knowledge to a shallow sub-section of a student can largely reduce a teacher’s influence. By exploring the depth of the shallow subsection, we then present a distillation technique that enables a skeptical student model to learn even from a nasty teacher. To evaluate the efficacy of our skeptical students, we conducted experiments with several models with KD on both training data-available and data-free scenarios for various datasets. While distilling from nasty teachers, compared to the normal student models, skeptical students consistently provide superior classification performance of up to ∼59.5%. Moreover, similar to normal students, skeptical students maintain high classification accuracy when distilled from a normal teacher, showing their efficacy irrespective of the teacher being nasty or not. We believe the ability of skeptical students to largely diminish the KD-immunity of potentially nasty teachers will motivate the research community to create more robust mechanisms for model confidentiality. We have open-sourced the code at https://github.com/ksouvik52/Skeptical2021","['Machine Learning', 'Privacy']",[],"['Souvik Kundu', 'Qirui Sun', 'Yao Fu', 'Massoud Pedram', 'Peter Anthony Beerel']","['Intel', 'City University of', 'University of Southern California', 'University of Southern California', 'University of Southern California']","[None, 'Hong Kong', None, None, None]"
https://nips.cc/virtual/2021/poster/28786,Security,Exact Privacy Guarantees for Markov Chain Implementations of the Exponential Mechanism with Artificial Atoms,"Implementations of the exponential mechanism in differential privacy often require sampling from intractable distributions. When approximate procedures like Markov chain Monte Carlo (MCMC) are used, the end result incurs costs to both privacy and accuracy. Existing work has examined these effects asymptotically, but implementable finite sample results are needed in practice so that users can specify privacy budgets in advance and implement samplers with exact privacy guarantees. In this paper, we use tools from ergodic theory and perfect simulation to design exact finite runtime sampling algorithms for the exponential mechanism by introducing an intermediate modified target distribution using artificial atoms. We propose an additional modification of this sampling algorithm that maintains its $\epsilon$-DP guarantee and has improved runtime at the cost of some utility. We then compare these methods in scenarios where we can explicitly calculate a $\delta$ cost (as in $(\epsilon, \delta)$-DP) incurred when using standard MCMC techniques. Much as there is a well known trade-off between privacy and utility, we demonstrate that there is also a trade-off between privacy guarantees and runtime.","['Theory', 'Generative Model', 'Privacy']",[],"['Jeremy Seeman', 'Matthew Reimherr']","['Pennsylvania State University', 'Pennsylvania State University']","[None, None]"
https://nips.cc/virtual/2021/poster/28755,Security,Privately Learning Mixtures of Axis-Aligned Gaussians,"We consider the problem of learning multivariate Gaussians under the constraint of approximate differential privacy. We prove that $\widetilde{O}(k^2 d \log^{3/2}(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to learn a mixture of $k$ axis-aligned Gaussians in $\mathbb{R}^d$ to within total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-differential privacy. This is the first result for privately learning mixtures of unbounded axis-aligned (or even unbounded univariate) Gaussians. If the covariance matrices of each of the Gaussians is the identity matrix, we show that $\widetilde{O}(kd/\alpha^2 + kd \log(1/\delta) / \alpha \varepsilon)$ samples are sufficient. To prove our results, we design a new technique for privately learning mixture distributions.  A class of distributions $\mathcal{F}$ is said to be list-decodable if there is an algorithm that, given ""heavily corrupted"" samples from $f \in \mathcal{F}$, outputs a list of distributions one of which approximates $f$. We show that if $\mathcal{F}$ is privately list-decodable then we can learn mixtures of distributions in $\mathcal{F}$. Finally, we show axis-aligned Gaussian distributions are privately list-decodable, thereby proving mixtures of such distributions are privately learnable.",['Privacy'],[],"['Ishaq Aden-Ali', 'Hassan Ashtiani', 'Christopher Liaw']","['University of California, Berkeley', 'McMaster University', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28666,Security,Exploiting Data Sparsity in Secure Cross-Platform Social Recommendation,"Social recommendation has shown promising improvements over traditional systems since it leverages social correlation data as an additional input. Most existing work assumes that all data are available to the recommendation platform. However, in practice, user-item interaction data (e.g.,rating) and user-user social data are usually generated by different platforms, and both of which contain sensitive information.  Therefore, ""How to perform secure and efficient social recommendation across different platforms, where the data are highly-sparse in nature"" remains an important challenge. In this work, we bring secure computation techniques into social recommendation, and propose S3Rec, a sparsity-aware secure cross-platform social recommendation framework. As a result, our model can not only improve the recommendation performance of the rating platform by incorporating the sparse social data on the social platform, but also protect data privacy of both platforms. Moreover, to further improve model training efficiency, we propose two secure sparse matrix multiplication protocols based on homomorphic encryption and private information retrieval. Our experiments on two benchmark datasets demonstrate the effectiveness of S3Rec.","['Machine Learning', 'Privacy']",[],"['Jamie Cui', 'Chaochao Chen', 'Lingjuan Lyu', 'Carl Yang', 'Wang Li']","['Ant Group', 'Zhejiang University', 'Sony Research', 'Emory University', 'Shanghai Jiao Tong University, Tsinghua University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28241,Security,Differentially Private n-gram Extraction,"We revisit the problem of $n$-gram extraction in the differential privacy setting. In this problem, given a corpus of private text data, the goal is to release as many $n$-grams as possible while preserving user level privacy. Extracting $n$-grams is a fundamental subroutine in many NLP applications such as sentence completion, auto response generation for emails, etc. The problem also arises in other applications such as sequence mining, trajectory analysis, etc., and is a generalization of recently studied differentially private set union (DPSU) by Gopi et al. (2020). In this paper, we develop a new differentially private algorithm for this problem which, in our experiments, significantly outperforms the state-of-the-art. Our improvements stem from combining recent advances in DPSU, privacy accounting, and new heuristics for pruning in the tree-based approach initiated by Chen et al. (2012).",['Privacy'],[],"['Kunho Kim', 'Sivakanth Gopi', 'Janardhan Kulkarni', 'Sergey Yekhanin']","['Microsoft', 'Microsoft Research', 'Microsoft', 'Microsoft']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28151,Security,Photonic Differential Privacy with Direct Feedback Alignment,"Optical Processing Units (OPUs) -- low-power photonic chips dedicated to large scale random projections -- have been used in previous work to train deep neural networks using Direct Feedback Alignment (DFA), an effective alternative to backpropagation. Here, we demonstrate how to leverage the intrinsic noise of optical random projections to build a differentially private DFA mechanism, making OPUs a solution of choice to provide a \emph{private-by-design} training. We provide a theoretical analysis of our adaptive privacy mechanism, carefully measuring how the noise of optical random projections propagates in the process and gives rise to provable Differential Privacy. Finally, we conduct experiments demonstrating the ability of our learning procedure to achieve solid end-task performance.","['Deep Learning', 'Privacy']",[],"['Ruben Ohana', 'Hamlet Jesse Medina Ruiz', 'Julien Launay', 'Alessandro Cappelli', 'Iacopo Poli', 'Liva Ralaivola']","['Flatiron Institute', 'Criteo', 'HuggingFace', 'huggingface', 'LightOn', 'Criteo']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27916,Security,Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods,"We study private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset, subject to differential privacy, that approximately preserves the answers to a large collection of statistical queries. We first present an algorithmic framework that unifies a long line of iterative algorithms in the literature. Under this framework, we propose two new methods. The first method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. Our second method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM and PEP by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. We demonstrate that PEP and GEM empirically outperform existing algorithms. Furthermore, we show that GEM nicely incorporates prior information from public data while overcoming limitations of PMW^Pub, the existing state-of-the-art method that also leverages public data.","['Optimization', 'Privacy', 'Machine Learning', 'Deep Learning', 'Generative Model']",[],"['Terrance Liu', 'Giuseppe Vietri', 'Steven Wu']","['Carnegie Mellon University', 'University of Minnesota, Minneapolis', 'Carnegie Mellon University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27850,Security,Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data,"Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efficiency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adaptation without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA. HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target samples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Specifically, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups.","['Domain Adaptation', 'Privacy', 'Transfer Learning', 'Machine Learning', 'Contrastive Learning']",[],"['Jiaxing Huang', 'Aoran Xiao', 'Shijian Lu']","['Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27840,Security,G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators,"Recent advances in machine learning have largely benefited from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differentially private data generator that preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guarantees. Compared to existing approaches, G-PATE significantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that flows from teacher discriminators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator.  Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the first work being able to generate high-dimensional image data with high data utility under limited privacy budgets ($\varepsilon \le 1$). Our code is available at https://github.com/AI-secure/G-PATE.","['Generative Model', 'Machine Learning', 'Privacy']",[],"['Yunhui Long', 'Boxin Wang', 'Zhuolin Yang', 'Bhavya Kailkhura', 'Aston Zhang', 'Carl A. Gunter', 'Bo Li']","['University of Illinois, Urbana Champaign', 'NVIDIA', 'University of Illinois, Urbana Champaign', 'Lawrence Livermore National Laboratory', 'AWS', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27837,Security,On the Sample Complexity of Privately Learning Axis-Aligned Rectangles,"We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a finite grid $X^d\subseteq\mathbb{R}^d$ with differential privacy. Existing results show that the sample complexity of this problem is at most $\min\left\{ d{\cdot}\log|X| \;,\; d^{1.5}{\cdot}\left(\log^*|X| \right)^{1.5}\right\}$. That is, existing constructions either require sample complexity that grows linearly with $\log|X|$, or else it grows super linearly with the dimension $d$.  We present a novel algorithm that reduces the sample complexity to only $\tilde{O}\left\{d{\cdot}\left(\log^*|X|\right)^{1.5}\right\}$,  attaining a dimensionality optimal dependency without requiring the sample complexity to grow with $\log|X|$. The technique used in order to attain this improvement involves the deletion of ""exposed"" data-points on the go, in a fashion designed to avoid the cost of the adaptive composition theorems. The core of this technique may be of individual interest, introducing a new method for constructing statistically-efficient private algorithms.","['Theory', 'Privacy']",[],"['Menachem Sadigurschi', 'Uri Stemmer']","['Ben Gurion University of the Negev, Technion', 'Tel Aviv University']","[None, None]"
https://nips.cc/virtual/2021/poster/27820,Security,Differentially Private Model Personalization,"We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution $P_i$. Assuming some shared structure among the problems $P_i$, can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint, user-level differential privacy---that is, we control what is leaked about each user's entire data set. We provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user-level privacy guarantees for our general approach. When the problems $P_i$ are linear regression problems with each user's regression vector lying in a common, unknown low-dimensional subspace, we show that our efficient algorithms satisfy nearly optimal estimation error guarantees. We also establish a general, information-theoretic upper bound via an exponential mechanism-based algorithm.","['Theory', 'Privacy']",[],"['Prateek Jain', 'J Keith Rush', 'Adam Smith', 'Shuang Song', 'Abhradeep Guha Thakurta']","['Google', 'Google', 'Boston University', 'Google', 'Google']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27733,Security,PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization,"The burst of applications empowered by massive data have aroused unprecedented privacy concerns in AI society. Currently, data confidentiality protection has been one core issue during deep model training. Federated Learning (FL), which enables privacy-preserving training across multiple silos, gained rising popularity for its parameter-only communication. However, previous works have shown that FL revealed a significant performance drop if the data distributions are heterogeneous among different clients, especially when the clients have cross-domain characteristic, such as traffic, aerial and in-door. To address this challenging problem, we propose a novel idea, PartialFed, which loads a subset of the global model’s parameters rather than loading the entire model used in most previous works. We first validate our algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix. Then we develop PartialFed-Adaptive, which automatically selects personalized loading strategy for each client. The superiority of our algorithm is proved by demonstrating the new state-of-the-art results on cross-domain federated classification and detection. In particular, solely by initializing a small fraction of layers locally, we improve the performance of FedAvg on Office-Home and UODB by 4.88% and 2.65%, respectively. Further studies show that the adaptive strategy performs significantly better on domains with large deviation, e.g. improves AP50 by 4.03% and 4.89% on aerial and medical image detection compared to FedAvg.","['Federated Learning', 'Machine Learning', 'Privacy']",[],"['Benyuan Sun', 'Hongxing Huo']","['Huawei Technologies Ltd.', 'Huawei Technologies Ltd.']","[None, None]"
https://nips.cc/virtual/2021/poster/27669,Security,Federated Split Task-Agnostic  Vision Transformer for COVID-19 CXR Diagnosis,"Federated learning, which shares the weights of the neural network across clients, is gaining attention in the healthcare sector as it enables training on a large corpus of decentralized data while maintaining data privacy. For example, this enables neural network training for COVID-19 diagnosis on chest X-ray (CXR) images without collecting patient CXR data across multiple hospitals. Unfortunately, the exchange of the weights quickly consumes the network bandwidth if highly expressive network architecture is employed. So-called split learning partially solves this problem by dividing a neural network into a client and a server part, so that the client part of the network takes up less extensive computation resources and bandwidth. However, it is not clear how to find the optimal split without sacrificing the overall network performance. To amalgamate these methods and thereby maximize their distinct strengths, here we show that the Vision Transformer, a recently developed deep learning architecture with straightforward decomposable configuration, is ideally suitable for split learning without sacrificing performance. Even under the non-independent and identically distributed data distribution which emulates a real collaboration between hospitals using CXR datasets from multiple sources, the proposed framework was able to attain performance comparable to data-centralized training. In addition, the proposed framework along with heterogeneous multi-task clients also improves individual task performances including the diagnosis of COVID-19, eliminating the need for sharing large weights with innumerable parameters. Our results affirm the suitability of Transformer for collaborative learning in medical imaging and pave the way forward for future real-world implementations.","['Transformers', 'Deep Learning', 'Privacy', 'Federated Learning']",[],"['Sangjoon Park', 'Gwanghyun Kim', 'Jeongsol Kim', 'Boah Kim', 'Jong Chul Ye']","['Department of Radiation Oncology, Severance Hospital', 'Seoul National University', 'Korea Advanced Institute of Science and Technology', 'National Institutes of Health', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27285,Security,RelaySum for Decentralized Deep Learning on Heterogeneous Data,"In decentralized machine learning, workers compute model updates on their local data. Because the workers only communicate with few neighbors without central coordination, these updates propagate progressively over the network. This paradigm enables distributed training on networks without all-to-all connectivity, helping to protect data privacy as well as to reduce the communication cost of distributed training in data centers. A key challenge, primarily in decentralized deep learning, remains the handling of differences between the workers' local data distributions. To tackle this challenge, we introduce the RelaySum mechanism for information propagation in decentralized learning. RelaySum uses spanning trees to distribute information exactly uniformly across all workers with finite delays depending on the distance between nodes. In contrast, the typical gossip averaging mechanism only distributes data uniformly asymptotically while using the same communication volume per step as RelaySum. We prove that RelaySGD, based on this mechanism, is independent of data heterogeneity and scales to many workers, enabling highly accurate decentralized deep learning on heterogeneous data.","['Deep Learning', 'Optimization', 'Machine Learning', 'Privacy']",[],"['Thijs Vogels', 'Lie He', 'Anastasia Koloskova', 'Sai Praneeth Karimireddy', 'Tao Lin', 'Sebastian U Stich', 'Martin Jaggi']","['Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'University of California, Berkeley', 'Westlake University', 'CISPA Helmholtz Center for Information Security', 'EPFL']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27268,Security,Don’t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence,"Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efficient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for controlling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images.","['Generative Model', 'Machine Learning', 'Optimal Transport', 'Privacy']",[],"['Tianshi Cao', 'Alex Bie', 'Arash Vahdat', 'Sanja Fidler', 'Karsten Kreis']","['University of Toronto', 'University of Waterloo', 'NVIDIA', 'University of Toronto', 'NVIDIA']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27238,Security,Gradient Inversion with Generative Image Prior,"Federated Learning (FL) is a distributed learning framework, in which the local data never leaves clients’ devices to preserve privacy, and the server trains models on the data via accessing only the gradients of those local data. Without further privacy mechanisms such as differential privacy, this leaves the system vulnerable against an attacker who inverts those gradients to reveal clients’ sensitive data. However, a gradient is often insufficient to reconstruct the user data without any prior knowledge. By exploiting a generative model pretrained on the data distribution, we demonstrate that data privacy can be easily breached. Further, when such prior knowledge is unavailable, we investigate the possibility of learning the prior from a sequence of gradients seen in the process of FL training. We experimentally show that the prior in a form of generative model is learnable from iterative interactions in FL. Our findings demonstrate that additional mechanisms are necessary to prevent privacy leakage in FL.","['Federated Learning', 'Generative Model', 'Privacy']",[],"['Jinwoo Jeon', 'Jaechang Kim', 'Kangwook Lee', 'Sewoong Oh', 'Jungseul Ok']","['POSTECH', 'POSTECH', 'University of Wisconsin, Madison', 'University of Washington', 'POSTECH']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27211,Security,Local Differential Privacy for Regret Minimization in Reinforcement Learning,"Reinforcement learning algorithms are widely used in domains where it is desirable to provide a personalized service. In these domains it is common that user data contains sensitive information that needs to be protected from third parties. Motivated by this, we study privacy in the context of finite-horizon Markov Decision Processes (MDPs) by requiring information to be obfuscated on the user side. We formulate this notion of privacy for RL by leveraging the local differential privacy (LDP) framework. We establish a lower bound for regret minimization in finite-horizon MDPs with LDP guarantees which shows that guaranteeing privacy has a multiplicative effect on the regret. This result shows that while LDP is an appealing notion of privacy, it makes the learning problem significantly more complex. Finally, we present an optimistic algorithm that simultaneously satisfies $\varepsilon$-LDP requirements, and achieves $\sqrt{K}/\varepsilon$ regret in any finite-horizon MDP after $K$ episodes,  matching the lower bound dependency on the number of episodes $K$.","['Reinforcement Learning and Planning', 'Theory', 'Privacy']",[],"['Evrard Garcelon', 'Vianney Perchet', 'Ciara Pike-Burke', 'Matteo Pirotta']","['Facebook', 'Ensae ParisTech', 'Imperial College London', 'Meta']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26983,Security,Covariance-Aware Private Mean Estimation Without Private Covariance Estimation,"We present two sample-efficient differentially private mean estimators for $d$-dimensional (sub)Gaussian distributions with unknown covariance. Informally, given $n \gtrsim d/\alpha^2$ samples from such a distribution with mean $\mu$ and covariance $\Sigma$, our estimators output $\tilde\mu$ such that $\| \tilde\mu - \mu \|_{\Sigma} \leq \alpha$, where $\| \cdot \|_{\Sigma}$ is the \emph{Mahalanobis distance}. All previous estimators with the same guarantee either require strong a priori bounds on the covariance matrix or require $\Omega(d^{3/2})$ samples. Each of our estimators is based on a simple, general approach to designing differentially private mechanisms, but with novel technical steps to make the estimator private and sample-efficient. Our first estimator samples a point with approximately maximum Tukey depth using the exponential mechanism, but restricted to the set of points of large Tukey depth. Proving that this mechanism is private requires a novel analysis. Our second estimator perturbs the empirical mean of the data set with noise calibrated to the empirical covariance. Only the mean is released, however; the covariance is only used internally. Its sample complexity guarantees hold more generally for subgaussian distributions, albeit with a slightly worse dependence on the privacy parameter. For both estimators, careful preprocessing of the data is required to satisfy differential privacy.","['Theory', 'Privacy']",[],"['Gavin R Brown', 'Marco Gaboardi', 'Adam Smith', 'Jonathan Ullman', 'Lydia Zakynthinou']","['Boston University', 'Boston University', 'Boston University', 'Northeastern University', 'Northeastern University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26985,Security,Generalized Linear Bandits with Local Differential Privacy,"Contextual bandit algorithms are useful in personalized online decision-making. However, many applications such as personalized medicine and online advertising require the utilization of individual-specific information for effective learning, while user's data should remain private from the server due to privacy concerns. This motivates the introduction of local differential privacy (LDP), a stringent notion in privacy, to contextual bandits. In this paper, we design LDP algorithms for stochastic generalized linear bandits to achieve the same regret bound as in non-privacy settings. Our main idea is to develop a stochastic gradient-based estimator and update mechanism to ensure LDP. We then exploit the flexibility of stochastic gradient descent (SGD), whose theoretical guarantee for bandit problems is rarely explored, in dealing with generalized linear bandits. We also develop an estimator and update mechanism based on Ordinary Least Square (OLS) for linear bandits. Finally, we conduct experiments with both simulation and real-world datasets to demonstrate the consistently superb performance of our algorithms under LDP constraints with reasonably small parameters $(\varepsilon, \delta)$ to ensure strong privacy protection.","['Optimization', 'Bandits', 'Privacy']",[],"['Zhipeng Liang', 'Yang Wang', 'Jiheng Zhang']","['University of Science and Technology', 'The  University of Science and Technology', 'The  University of Science and Technology']","['Hong Kong', 'Hong Kong', 'Hong Kong']"
https://nips.cc/virtual/2021/poster/26917,Security,Antipodes of Label Differential Privacy: PATE and ALIBI,"We consider the privacy-preserving machine learning (ML) setting where the trained model must satisfy differential privacy (DP) with respect to the labels of the training examples. We propose two novel approaches based on, respectively, the Laplace mechanism and the PATE framework, and demonstrate their effectiveness on standard benchmarks. While recent work by Ghazi et al. proposed Label DP schemes based on a randomized response mechanism, we argue that additive Laplace noise coupled with Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover, we show how to achieve very strong privacy levels in some regimes, with our adaptation of the PATE framework that builds on recent advances in semi-supervised learning. We complement theoretical analysis of our algorithms' privacy guarantees with empirical evaluation of their memorization properties. Our evaluation suggests that comparing different algorithms according to their provable DP guarantees can be misleading and favor a less private algorithm with a tighter analysis. Code for implementation of algorithms and memorization attacks is available from https://github.com/facebookresearch/label_dp_antipodes.","['Semi-Supervised Learning', 'Machine Learning', 'Privacy']",[],"['Mani Malek Esmaeili', 'Ilya Mironov', 'Karthik Prasad', 'Igor Shilov', 'Florian Tramer']","['Facebook', 'Facebook', 'Facebook AI', 'Imperial College London', 'ETHZ - ETH Zurich']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26906,Security,Deep Learning with Label Differential Privacy,"The Randomized Response (RR) algorithm is a classical technique to improve robustness in survey aggregation, and has been widely adopted in applications with differential privacy guarantees. We propose a novel algorithm, Randomized Response with Prior (RRWithPrior), which can provide more accurate results while maintaining the same level of privacy guaranteed by RR. We then apply RRWithPrior to learn neural networks with label differential privacy (LabelDP), and show that when only the label needs to be protected, the model performance can be significantly improved over the previous state-of-the-art private baselines. Moreover, we study different ways to obtain priors, which when used with RRWithPrior can additionally improve the model performance, further reducing the accuracy gap between private and non-private models. We complement the empirical results with theoretical analysis showing that LabelDP is provably easier than protecting both the inputs and labels.","['Deep Learning', 'Self-Supervised Learning', 'Robustness', 'Privacy']",[],"['Badih Ghazi', 'Noah Golowich', 'Ravi Kumar', 'Pasin Manurangsi', 'Chiyuan Zhang']","['Google', 'Massachusetts Institute of Technology', 'Google', 'Google', 'Google']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26681,Security,Differentially Private Learning with Adaptive Clipping,"Existing approaches for training neural networks with user-level differential privacy (e.g., DP Federated Averaging) in federated learning (FL) settings involve bounding the contribution of each user's model update by {\em clipping} it to some constant value. However there is no good {\em a priori} setting of the clipping norm across tasks and learning settings: the update norm distribution depends on the model architecture and loss, the amount of data on each device, the client learning rate, and possibly various other parameters. We propose a method wherein instead of a fixed clipping norm, one clips to a value at a specified quantile of the update norm distribution, where the value at the quantile is itself estimated online, with differential privacy. The method tracks the quantile closely, uses a negligible amount of privacy budget, is compatible with other federated learning technologies such as compression and secure aggregation, and has a straightforward joint DP analysis with DP-FedAvg. Experiments demonstrate that adaptive clipping to the median update norm works well across a range of federated learning tasks, eliminating the need to tune any clipping hyperparameter.","['Federated Learning', 'Deep Learning', 'Privacy']",[],"['Galen Andrew', 'Om Thakkar', 'Hugh Brendan McMahan', 'Swaroop Ramaswamy']","['Google', 'Google', 'Google', 'Google']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26652,Security,Renyi Differential Privacy of The Subsampled Shuffle Model In Distributed Learning,"We study privacy in a distributed learning framework, where clients collaboratively build a learning model iteratively through interactions with a server from whom we need privacy. Motivated by stochastic optimization and the federated learning (FL) paradigm, we focus on the case where a small fraction of data samples are randomly sub-sampled in each round to participate in the learning process, which also enables privacy amplification.  To obtain even stronger local privacy guarantees, we study this in the shuffle privacy model, where each client randomizes its response using a local differentially private (LDP) mechanism and the server only receives a random permutation (shuffle) of the clients' responses without their association to each client. The principal result of this paper is a privacy-optimization performance trade-off for discrete randomization mechanisms in this sub-sampled shuffle privacy model. This is enabled through a new theoretical technique to analyze the Renyi Differential Privacy (RDP) of the sub-sampled shuffle model.  We numerically demonstrate that, for important regimes, with composition our bound yields significant improvement in privacy guarantee over the state-of-the-art approximate Differential Privacy (DP) guarantee (with strong composition) for sub-sampled shuffled models. We also demonstrate numerically significant improvement in privacy-learning performance operating point using real data sets. Despite these advances, an open question is to bridge the gap between lower and upper privacy bounds in our RDP analysis.","['Federated Learning', 'Optimization', 'Privacy']",[],"['Antonious M. Girgis', 'Deepesh Data', 'Suhas Diggavi']","['University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26644,Security,Remember What You Want to Forget: Algorithms for Machine Unlearning,"We study the problem of unlearning datapoints from a learnt model. The learner first receives a dataset $S$ drawn i.i.d. from an unknown distribution, and outputs a model $\widehat{w}$ that performs well on  unseen samples from the same distribution. However, at some point in the future, any training datapoint $z \in S$ can request to be unlearned, thus prompting the learner to modify its output model while still ensuring the same accuracy guarantees.  We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity. For the setting of convex losses, we provide an unlearning algorithm that can unlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In comparison, in general, differentially private learning (which implies unlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This demonstrates a novel separation between differential privacy and machine unlearning.","['Theory', 'Privacy']",[],"['Ayush Sekhari', 'Jayadev Acharya', 'Gautam Kamath', 'Ananda Theertha Suresh']","['Massachusetts Institute of Technology', 'Cornell University', 'University of Waterloo', 'Google']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26628,Security,Numerical Composition of Differential Privacy,"We give a fast algorithm to compose privacy guarantees of differentially private (DP) algorithms to arbitrary accuracy. Our method is based on the notion of privacy loss random variables to quantify the privacy loss of DP algorithms. The running time and memory needed for our algorithm to approximate the privacy curve of a DP algorithm composed with itself $k$ times is $\tilde{O}(\sqrt{k})$. This improves over the best prior method by Koskela et al. (2020) which requires $\tilde{\Omega}(k^{1.5})$ running time. We demonstrate the utility of our algorithm by accurately computing the privacy loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm speeds up the privacy computations by a few orders of magnitude compared to prior work, while maintaining similar accuracy.",['Privacy'],[],"['Sivakanth Gopi', 'Lukas Wutschitz']","['Microsoft Research', 'Microsoft']","[None, None]"
https://nips.cc/virtual/2021/poster/28283,Security,Exploiting the Intrinsic Neighborhood Structure for Source-free Domain Adaptation,"Domain adaptation (DA) aims to alleviate the domain shift between source domain and target domain. Most DA methods require access to the source data, but often that is not possible (e.g. due to data privacy or intellectual property). In this paper, we address the challenging source-free domain adaptation (SFDA) problem, where the source pretrained model is adapted to the target domain in the absence of source data. Our method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. We capture this intrinsic structure by defining local affinity of the target data, and encourage label consistency among data with high local affinity. We observe that higher affinity should be assigned to reciprocal neighbors, and propose a self regularization loss to decrease the negative impact of noisy neighbors. Furthermore, to aggregate information with more context, we consider expanded neighborhoods with small affinity values. In the experimental results we verify that the inherent structure of the target features is an important source of information for domain adaptation. We demonstrate that this local structure can be efficiently captured by considering the local neighbors, the reciprocal neighbors, and the expanded neighborhood. Finally, we achieve state-of-the-art performance on several 2D image and 3D point cloud recognition datasets. Code is available in https://github.com/Albert0147/SFDA_neighbors.","['Domain Adaptation', 'Privacy']",[],"['Shiqi Yang', 'Yaxing Wang', 'Joost van de weijer', 'Luis Herranz', 'SHANGLING JUI']","['Sony', 'Computer Vision Center, Universitat Autònoma de Barcelona', 'Universitat Autónoma de Barcelona', 'Computer Vision Center, Universitat Autònoma de Barcelona', 'Huawei Technologies Ltd.']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26381,Security,Adaptive Machine Unlearning,"Data deletion algorithms aim to remove the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. However, for sequences of deletions, most prior work in the non-convex setting gives valid guarantees only for sequences that are chosen independently of the models that are published. If people choose to delete their data as a function of the published models (because they don’t like what the models reveal about them, for example), then the update sequence is adaptive. In this paper, we give a general reduction from deletion guarantees against adaptive sequences to deletion guarantees against non-adaptive sequences, using differential privacy and its connection to max information. Combined with ideas from prior work which give guarantees for non-adaptive deletion sequences, this leads to extremely flexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletion guarantees for adaptive deletion sequences. We show in theory how prior work for non-convex models fails against adaptive deletion sequences, and use this intuition to design a practical attack against the SISA algorithm of Bourtoule et al. [2021] on CIFAR-10, MNIST, Fashion-MNIST.","['Theory', 'Privacy']",[],"['Varun Gupta', 'Christopher Jung', 'Seth Neel', 'Aaron Roth', 'Saeed Sharifi -Malvajerdi', 'Christopher Waites']","['School of Engineering and Applied Science, University of Pennsylvania', 'Stanford University', 'Harvard University', 'Amazon', 'University of Pennsylvania', 'Stanford University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28853,Security,Locally private online change point detection,"We study online change point detection problems under the constraint of local differential privacy (LDP) where, in particular, the statistician does not have access to the raw data.  As a concrete problem, we study a multivariate nonparametric regression problem.  At each time point $t$, the raw data are assumed to be of the form $(X_t, Y_t)$, where $X_t$ is a $d$-dimensional feature vector and $Y_t$ is a response variable. Our primary aim is to detect changes in the regression function $m_t(x)=\mathbb{E}(Y_t |X_t=x)$ as soon as the change occurs.  We provide algorithms which respect the LDP constraint, which control the false alarm probability, and which detect changes with a minimal (minimax rate-optimal) delay.  To quantify the cost of privacy, we also present the optimal rate in the benchmark, non-private setting.  These non-private results are also new to the literature and thus are interesting \emph{per se}.  In addition, we study the univariate mean online change point detection problem, under privacy constraints.  This serves as the blueprint of studying more complicated private change point detection problems.",['Privacy'],[],"['Thomas Berrett', 'Yi Yu']","['The university of Warwick', 'University of Warwick']","[None, None]"
https://nips.cc/virtual/2021/poster/26102,Security,Differentially Private Federated Bayesian Optimization with Distributed Exploration,"Bayesian optimization (BO) has recently been extended to the federated learning (FL) setting by the federated Thompson sampling (FTS) algorithm, which has promising applications such as federated hyperparameter tuning. However, FTS is not equipped with a rigorous privacy guarantee which is an important consideration in FL. Recent works have incorporated differential privacy (DP) into the training of deep neural networks through a general framework for adding DP to iterative algorithms. Following this general DP framework, our work here integrates DP into FTS to preserve user-level privacy. We also leverage the ability of this general DP framework to handle different parameter vectors, as well as the technique of local modeling for BO, to further improve the utility of our algorithm through distributed exploration (DE). The resulting differentially private FTS with DE (DP-FTS-DE) algorithm is endowed with theoretical guarantees for both the privacy and utility and is amenable to interesting theoretical insights about the privacy-utility trade-off. We also use real-world experiments to show that DP-FTS-DE achieves high utility (competitive performance) with a strong privacy guarantee (small privacy loss) and induces a trade-off between privacy and utility.","['Reinforcement Learning and Planning', 'Optimization', 'Privacy', 'Federated Learning', 'Deep Learning']",[],"['Zhongxiang Dai', 'Bryan Kian Hsiang Low', 'Patrick Jaillet']","['Massachusetts Institute of Technology', 'National University of', 'Massachusetts Institute of Technology']","[None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/26031,Security,An Uncertainty Principle is a Price of Privacy-Preserving Microdata,"Privacy-protected microdata are often the desired output of a differentially private algorithm since  microdata is familiar and convenient for downstream users. However, there is a statistical price for this kind of convenience. We show that an uncertainty principle governs the trade-off between accuracy for a population of interest (``sum query'') vs. accuracy for its component sub-populations (``point queries''). Compared to differentially private query answering systems that are not required to produce microdata, accuracy can degrade by a logarithmic factor. For example, in the case of pure differential privacy, without the microdata requirement, one can provide noisy answers to the sum query and all point queries while guaranteeing that each answer has squared error $O(1/\epsilon^2)$. With the microdata requirement, one must choose between allowing an additional $\log^2(d)$ factor ($d$ is the number of point queries) for some point queries or allowing an extra $O(d^2)$ factor for the sum query. We present lower bounds for pure, approximate, and concentrated differential privacy. We propose mitigation strategies and create a collection of benchmark datasets that can be used for public study of this problem.",['Privacy'],[],"['John M. Abowd', 'Robert Ashmead', 'Ryan Cumings-Menon', 'Simson L. Garfinkel', 'Daniel Kifer', 'Philip Leclerc', 'William Sexton', 'Ashley E Simpson', 'Christine Task', 'Pavel Zhuravlev']","['U.S. Census Bureau', 'U.S. Census Bureau', 'Census Bureau', 'Massachusetts Institute of Technology', 'Pennsylvania State University', 'U.S. Census Bureau', 'Tumult Labs', 'Knexus Research Corp.', 'Knexus Research Corporation', 'Census Bureau']","[None, None, 'US', None, None, None, None, None, None, 'US']"
https://nips.cc/virtual/2021/poster/26020,Security,User-Level Differentially Private Learning via Correlated Sampling,"Most works in learning with differential privacy (DP) have focused on the setting where each user has a single sample. In this work, we consider the setting where each user holds $m$ samples and the privacy protection is enforced at the level of each user's data.  We show that, in this setting, we may learn with a much fewer number of users. Specifically, we show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an $(\epsilon, \delta)$-DP algorithm using only $O(\log(1/\delta)/\epsilon)$ users. For $\epsilon$-DP algorithms, we show that we can learn using only $O_{\epsilon}(d)$ users even in the local model, where $d$ is the probabilistic representation dimension. In both cases, we show a nearly-matching lower bound on the number of users required. A crucial component of our results is a generalization of global stability [Bun, Livni, Moran, FOCS 2020]  that allows the use of public randomness. Under this relaxed notion, we employ a correlated sampling strategy to show that the global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the number of samples.",['Privacy'],[],"['Badih Ghazi', 'Ravi Kumar', 'Pasin Manurangsi']","['Google', 'Google', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/25962,Security,Locally differentially private estimation of functionals of discrete distributions,"We study the  problem of estimating non-linear functionals of discrete distributions in the context of local differential privacy. The initial data $x_1,\ldots,x_n \in[K]$ are supposed i.i.d. and distributed according to an unknown discrete distribution $p = (p_1,\ldots,p_K)$. Only $\alpha$-locally differentially private (LDP) samples $z_1,...,z_n$ are publicly available, where the term 'local' means that each $z_i$ is produced using one individual attribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e. they are allowed to use already published confidential data) or non-interactive. We describe the behavior of the quadratic risk for estimating the power sum functional $F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$, $\gamma >0$ as a function of $K, \, n$ and $\alpha$. In the non-interactive case, we study twol plug-in type estimators of $F_{\gamma}$, for all $\gamma >0$, that are similar to the MLE analyzed by Jiao et al. (2017) in the multinomial model. However, due to the privacy constraint the rates we attain are slower and similar to those obtained in the Gaussian model by Collier et al. (2020). In the sequentially interactive case, we introduce for all $\gamma >1$ a two-step procedure which attains the parametric rate $(n \alpha^2)^{-1/2}$ when $\gamma \geq 2$.  We give lower bounds results over all $\alpha-$LDP mechanisms and over all estimators using the private samples.",['Privacy'],[],"['Cristina Butucea', 'Yann Issartel']","['CREST, ENSAE, IP Paris', 'Ensae ParisTech']","[None, None]"
https://nips.cc/virtual/2021/poster/27581,Security,The Skellam Mechanism for Differentially Private Federated Learning,"We introduce the multi-dimensional Skellam mechanism, a discrete differential privacy mechanism based on the difference of two independent Poisson random variables. To quantify its privacy guarantees, we analyze the privacy loss distribution via a numerical evaluation and provide a sharp bound on the Rényi divergence between two shifted Skellam distributions. While useful in both centralized and distributed privacy applications, we investigate how it can be applied in the context of federated learning with secure aggregation under communication constraints. Our theoretical findings and extensive experimental evaluations demonstrate that the Skellam mechanism provides the same privacy-accuracy trade-offs as the continuous Gaussian mechanism, even when the precision is low. More importantly, Skellam is closed under summation and sampling from it only requires sampling from a Poisson distribution -- an efficient routine that ships with all machine learning and data analysis software packages. These features, along with its discrete nature and competitive privacy-accuracy trade-offs, make it an attractive practical alternative to the newly introduced discrete Gaussian mechanism.","['Federated Learning', 'Machine Learning', 'Privacy']",[],"['Naman Agarwal', 'Peter Kairouz', 'Ziyu Liu']","['Google', 'Google', 'Stanford University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27157,Security,Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis,"Tabular data synthesis has received wide attention in the literature. This is because available data is often limited, incomplete, or cannot be obtained easily, and data privacy is becoming increasingly important. In this work, we present a generalized GAN framework for tabular synthesis, which combines the adversarial training of GANs and the negative log-density regularization of invertible neural networks. The proposed framework can be used for two distinctive objectives. First,  we can further improve the synthesis quality, by decreasing the negative log-density of real records in the process of adversarial training. On the other hand, by increasing the negative log-density of real records, realistic fake records can be synthesized in a way that they are not too much close to real records and reduce the chance of potential information leakage. We conduct experiments with real-world datasets for classification, regression, and privacy attacks. In general, the proposed method demonstrates the best synthesis quality (in terms of task-oriented evaluation metrics, e.g., F1) when decreasing the negative log-density during the adversarial training. If increasing the negative log-density, our experimental results show that the distance between real and fake records increases, enhancing robustness against privacy attacks.","['Privacy', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning', 'Generative Model']",[],"['JAEHOON LEE', 'Jihyeon Hyeong', 'Jinsung Jeon', 'Noseong Park', 'Jihoon Cho']","['LG AI RESEARCH', 'Yonsei University', 'Yonsei University', 'Yonsei University', 'Samsung SDS']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26338,Security,Differentially Private Sampling from Distributions,"We initiate an investigation of  private sampling from distributions. Given a dataset with $n$ independent observations from an unknown distribution $P$, a sampling algorithm must output a single observation from a distribution that is close in total variation distance to $P$ while satisfying differential privacy. Sampling abstracts the goal of generating small amounts of realistic-looking data. We provide tight upper and lower bounds for the dataset size needed for this task for three natural families of distributions: arbitrary distributions on $\{1,\ldots ,k\}$, arbitrary product distributions on $\{0,1\}^d$, and product distributions on on $\{0,1\}^d$ with bias in each coordinate bounded away from 0 and 1. We demonstrate that, in some parameter regimes, private sampling requires asymptotically fewer observations than learning a description of $P$ nonprivately; in other regimes, however, private sampling proves to be as difficult as private learning. Notably, for some classes of distributions, the overhead in the number of observations needed for private learning compared to non-private learning is completely captured by the number of observations needed for private sampling.",['Privacy'],[],"['Sofya Raskhodnikova', 'Satchit Sivakumar', 'Adam Smith', 'Marika Swanberg']","['Boston University, Boston University', 'Boston University', 'Boston University', 'Boston University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26585,Security,Learning with User-Level Privacy,"We propose and analyze algorithms to solve a range of learning tasks under user-level differential privacy constraints. Rather than guaranteeing only the privacy of individual samples, user-level DP protects a user's entire contribution ($m \ge 1$ samples), providing more stringent but more realistic protection against information leaks.  We show that for high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy, the privacy cost decreases as $O(1/\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy cost decreases at a faster $O(1/n)$ rate.  We complement these results with lower bounds showing the minimax optimality of our algorithms for mean estimation and stochastic convex optimization. Our algorithms rely on novel techniques for private mean estimation in arbitrary dimension with error scaling as the concentration radius $\tau$ of the distribution rather than the entire range.","['Optimization', 'Privacy']",[],"['Daniel Asher Nathan Levy', 'Ziteng Sun', 'Kareem Amin', 'Satyen Kale', 'Alex Kulesza', 'Mehryar Mohri', 'Ananda Theertha Suresh']","['Stanford University', 'Google', 'Google', 'Google', 'Google', 'New York University', 'Google']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28045,Security,Differential Privacy Over Riemannian Manifolds,"In this work we consider the problem of releasing a differentially private statistical summary that resides on a Riemannian manifold.  We present an extension of the Laplace or K-norm mechanism that utilizes intrinsic distances and volumes on the manifold.  We also consider in detail the specific case where the summary is the Fr\'echet mean of data residing on a manifold.  We demonstrate that our mechanism is rate optimal and depends only on the dimension of the manifold, not on the dimension of any ambient space, while also showing how ignoring the manifold structure can decrease the utility of the sanitized summary.  We illustrate our framework in two examples of particular interest in statistics: the space of symmetric positive definite matrices, which is used for covariance matrices, and the sphere, which can be used as a space for modeling discrete distributions.",['Privacy'],[],"['Matthew Reimherr', 'Karthik Bharath', 'Carlos J Soto']","['Pennsylvania State University', 'University of Nottingham', 'University of Massachusetts at Amherst']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26962,Security,Differentially Private Empirical Risk Minimization under the Fairness Lens,"Differential Privacy (DP) is an important privacy-enhancing technology for private machine learning systems. It allows to measure and bound the risk associated with an individual participation in a computation. However, it was recently observed that DP learning systems may exacerbate bias and unfairness for different groups of individuals. This paper builds on these important observations and sheds light on the causes of the disparate impacts arising in the problem of differentially private empirical risk minimization. It focuses on the accuracy disparity arising among groups of individuals in two well-studied DP learning methods: output perturbation and differentially private stochastic gradient descent. The paper analyzes which data and model properties are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately, and proposes guidelines to mitigate these effects. The proposed approach is evaluated on several datasets and settings.","['Fairness', 'Optimization', 'Machine Learning', 'Privacy']",[],"['Cuong Tran', 'My H Dinh', 'Ferdinando Fioretto']","['University of Virginia, Charlottesville', 'University of Virginia, Charlottesville', 'University of Virginia, Charlottesville']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28032,Security,Individual Privacy Accounting via a Rényi Filter,"We consider a sequential setting in which a single dataset of individuals is used to perform adaptively-chosen analyses, while ensuring that the differential privacy loss of each participant does not exceed a pre-specified privacy budget. The standard approach to this problem relies on bounding a worst-case estimate of the privacy loss over all individuals and all possible values of their data, for every single analysis. Yet, in many scenarios this approach is overly conservative, especially for ""typical"" data points which incur little privacy loss by participation in most of the analyses. In this work, we give a method for tighter privacy loss accounting based on the value of a personalized privacy loss estimate for each individual in each analysis. To implement the accounting method we design a filter for Rényi differential privacy. A filter is a tool that ensures that the privacy parameter of a composed sequence of algorithms with adaptively-chosen privacy parameters does not exceed a pre-specified budget. Our filter is simpler and tighter than the known filter for $(\epsilon,\delta)$-differential privacy by Rogers et al. (2016). We apply our results to the analysis of noisy gradient descent and show that personalized accounting can be practical, easy to implement, and can only make the privacy-utility tradeoff tighter.","['Optimization', 'Privacy']",[],"['Vitaly Feldman', 'Tijana Zrnic']","['Apple AI Research', 'Stanford University']","[None, None]"
https://nips.cc/virtual/2021/poster/28210,Security,Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent,"What is the information leakage of an iterative randomized learning algorithm about its training data, when the internal state of the algorithm is \emph{private}? How much is the contribution of each specific training epoch to the information leakage through the released model? We study this problem for noisy gradient descent algorithms, and model the \emph{dynamics} of R\'enyi differential privacy loss throughout the training process.  Our analysis traces a provably \emph{tight} bound on the R\'enyi divergence between the pair of probability distributions over parameters of models trained on neighboring datasets.  We prove that the privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy loss by upper-bounding its total value over all intermediate gradient computations). For Lipschitz, smooth, and strongly convex loss functions, we prove optimal utility with a small gradient complexity for noisy gradient descent algorithms.","['Optimization', 'Privacy']",[],"['Rishav Chourasia', 'Jiayuan Ye', 'Reza Shokri']","['National University of', 'National University of', 'National University of']","['Singapore', 'Singapore', 'Singapore']"