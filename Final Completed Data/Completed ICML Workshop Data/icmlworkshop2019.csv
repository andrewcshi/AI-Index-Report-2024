link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://icml.cc/virtual/2019/workshop/3510,Fairness & Bias,Generative Modeling and Model-Based Reasoning for Robotics and AI,"In order to meet the diverse challenges in solving many real-world problems, an intelligent agent has to be able to dynamically construct a model of its environment. Objects facilitate the modular reuse of prior knowledge and the combinatorial construction of such models. In this work, we argue that dynamically bound features (objects) do not simply emerge in connectionist models of the world. We identify several requirements that need to be fulfilled in overcoming this limitation and highlight corresponding inductive biases.",[],[],"['Aravind Rajeswaran', 'Emanuel Todorov', 'Igor Mordatch', 'William Agnew', 'Amy Zhang', 'Joelle Pineau', 'Michael Chang', 'Dumitru Erhan', 'Sergey Levine', 'Kimberly Stachenfeld', 'Marvin Zhang']",[],[]
https://icml.cc/virtual/2019/workshop/3522,Fairness & Bias,Identifying and Understanding Deep Learning Phenomena,"The bias-variance tradeoff tells us that as model complexity increases, bias falls and variances increases, leading to a U-shaped test error curve. However, recent empirical results with over-parameterized neural networks are marked by a striking absence of the classic U-shaped test error curve: test error keeps decreasing in wider networks. This suggests that there might not be a bias-variance tradeoff in neural networks with respect to network width, unlike was originally claimed by, e.g., Geman et al. (1992). Motivated by the shaky evidence used to support this claim in neural networks, we measure bias and variance in the modern setting. We find that both bias and variance can decrease as the number of parameters grows. To better understand this, we introduce a new decomposition of the variance to disentangle the effects of optimization and data sampling. We also provide theoretical analysis in a simplified setting that is consistent with our empirical findings.",[],[],"['Hanie Sedghi', 'Samy Bengio', 'Kenji Hata', 'Aleksander Madry', 'Ari Morcos', 'Behnam Neyshabur', 'Maithra Raghu', 'Ali Rahimi', 'Ludwig Schmidt', 'Ying Xiao']",[],[]
https://icml.cc/virtual/2019/workshop/3526,Security,Workshop on the Security and Privacy of Machine Learning,"There is growing recognition that machine learning (ML) exposes new security and privacy vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited but expanding. In this talk, we explore the threat model space of ML algorithms through the lens of Saltzer and Schroeder's principles for the design of secure computer systems. This characterization of the threat space prompts an investigation of current and future research directions. We structure our discussion around three of these directions, which we believe are likely to lead to significant progress. The first encompasses a spectrum of approaches to verification and admission control, which is a prerequisite to enable fail-safe defaults in machine learning systems. The second seeks to design mechanisms for assembling reliable records of compromise that would help understand the degree to which vulnerabilities are exploited by adversaries, as well as favor psychological acceptability of machine learning applications. The third pursues formal frameworks for security and privacy in machine learning, which we argue should strive to align machine learning goals such as generalization with security and privacy desiderata like robustness or privacy. Key insights resulting from these three directions pursued both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by systematizing best practices in our community.",[],[],"['Nicolas Papernot', 'Florian Tramer', 'Bo Li', 'Dan Boneh', 'David Evans', 'Somesh Jha', 'Percy Liang', 'Patrick McDaniel', 'Jacob Steinhardt', 'Dawn Song']",[],[]