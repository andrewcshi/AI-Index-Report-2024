link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://icml.cc/virtual/2021/workshop/8355,Transparency & Explainability,"ICML Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI","This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. Deep neural networks (DNNs) have undoubtedly brought great success to a wide range of applications in computer vision, computational linguistics, and AI. However, foundational principles underlying the DNNs' success and their resilience to adversarial attacks are still largely missing. Interpreting and theorizing the internal mechanisms of DNNs becomes a compelling yet controversial topic. This workshop pays a special interest in theoretic foundations, limitations, and new application trends in the scope of XAI. These issues reflect new bottlenecks in the future development of XAI.",[],[],"['Quanshi Zhang', 'Tian Han', 'Lixin Fan', 'Zhanxing Zhu', 'Hang Su', 'Ying Nian Wu']",[],[]
https://icml.cc/virtual/2021/workshop/8363,Transparency & Explainability,ICML Workshop on Algorithmic Recourse,"Algorithmic recourse explanations inform stakeholders on how to act to revert unfavorable predictions. However, in general ML models do not predict well in interventional distributions. Thus, an action that changes the prediction in the desired way may not lead to an improvement of the underlying target. Such recourse is neither meaningful nor robust to model refits. Extending the work of Karimi et al. (2021), we propose meaningful algorithmic recourse (MAR) that only recommends actions that improve both prediction and target. We justify this selection constraint by highlighting the differences between model audit and meaningful, actionable recourse explanations. Additionally, we introduce a relaxation of MAR called effective algorithmic recourse (EAR), which, under certain assumptions, yields meaningful recourse by only allowing interventions on causes of the target.",[],[],"['Stratis Tsirtsis', 'Amir-Hossein Karimi', 'Ana Lucic', 'Manuel Gomez-Rodriguez', 'Isabel Valera', 'Hima Lakkaraju']",[],[]
https://icml.cc/virtual/2021/workshop/8356,Fairness & Bias,"Machine Learning for Data: Automated Creation, Privacy, Bias","How to generate conditional synthetic data for a domain without utilizing information about its labels/attributes? Our work presents a solution to the above question. We propose a transfer learning-based framework utilizing normalizing flows, coupled with both maximum-likelihood and adversarial training. We model a source domain (labels available) and a target domain (labels unavailable) with individual normalizing flows, and perform domain alignment to a common latent space using adversarial discriminators. Due to the invertible property of flow models, the mapping has exact cycle consistency. We also learn the joint distribution of the data samples and attributes in the source domain by employing an encoder to map attributes to the latent space via adversarial training. During the synthesis phase, given any combination of attributes, our method can generate synthetic samples conditioned on them in the target domain. Empirical studies confirm the effectiveness of our method on benchmarked datasets. We envision our method to be particularly useful for synthetic data generation in label-scarce systems by generating non-trivial augmentations via attribute transformations. These synthetic samples will introduce more entropy into the label-scarce domain than their geometric and photometric transformation counterparts, helpful for robust downstream tasks.",[],[],"['Zhiting Hu', 'Li Erran Li', 'Willie Neiswanger', 'Benedikt Boecking', 'Yi Xu', 'Belinda Zeng']",[],[]
https://icml.cc/virtual/2021/workshop/8359,Fairness & Bias,International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2021 (FL-ICML'21),"Federated learning (FL) is an emerging practical framework for effective and scalable machine learning among multiple participants, such as end users, organizations and companies. However, most existing FL or distributed learning frameworks have not well addressed two important issues together: collaborative fairness and adversarial robustness (e.g. free-riders and malicious participants). In conventional FL, all participants receive the global model (equal rewards), which might be unfair to the high-contributing participants. Furthermore, due to the lack of a safeguard mechanism, free-riders or malicious adversaries could game the system to access the global model for free or to sabotage it. In this paper, we propose a novel Robust and Fair Federated Learning (RFFL) framework to achieve collaborative fairness and adversarial robustness simultaneously via a reputation mechanism. RFFL maintains a reputation for each participant by examining their contributions via their uploaded gradients (using vector similarity) and thus identifies non-contributing or malicious participants to be removed. Our approach differentiates itself by not requiring any auxiliary/validation dataset. Extensive experiments on benchmark datasets show that RFFL can achieve high fairness and is very robust to different types of adversaries while achieving competitive predictive accuracy.",[],[],"['Nathalie Baracaldo', 'Olivia Choudhury', 'Gauri Joshi', 'Peter Richtarik', 'Praneeth Vepakomma', 'Shiqiang Wang', 'Han Yu']",[],[]
https://icml.cc/virtual/2021/workshop/8376,Privacy & Data Governance,Theory and Practice of Differential Privacy,"Bayesian inference has great promise for the privacy-preserving analysis of sensitive data, as posterior sampling automatically preserves differential privacy, an algorithmic notion of data privacy, under certain conditions (Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample (OPS) approach elegantly provides privacy ""for free,"" it is data inefficient in the sense of asymptotic relative efficiency (ARE). We show that a simple alternative based on the Laplace mechanism, the workhorse of differential privacy, is as asymptotically efficient as non-private posterior inference, under general assumptions. This technique also has practical advantages including efficient use of the privacy budget for MCMC. We demonstrate the practicality of our approach on a time-series analysis of sensitive military records from the Afghanistan and Iraq wars disclosed by the Wikileaks organization.",[],[],"['Rachel Cummings', 'Gautam Kamath']",[],[]
https://icml.cc/virtual/2021/workshop/8359,Privacy & Data Governance,International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2021 (FL-ICML'21),"Federated learning (FL) is an emerging practical framework for effective and scalable machine learning among multiple participants, such as end users, organizations and companies. However, most existing FL or distributed learning frameworks have not well addressed two important issues together: collaborative fairness and adversarial robustness (e.g. free-riders and malicious participants). In conventional FL, all participants receive the global model (equal rewards), which might be unfair to the high-contributing participants. Furthermore, due to the lack of a safeguard mechanism, free-riders or malicious adversaries could game the system to access the global model for free or to sabotage it. In this paper, we propose a novel Robust and Fair Federated Learning (RFFL) framework to achieve collaborative fairness and adversarial robustness simultaneously via a reputation mechanism. RFFL maintains a reputation for each participant by examining their contributions via their uploaded gradients (using vector similarity) and thus identifies non-contributing or malicious participants to be removed. Our approach differentiates itself by not requiring any auxiliary/validation dataset. Extensive experiments on benchmark datasets show that RFFL can achieve high fairness and is very robust to different types of adversaries while achieving competitive predictive accuracy.",[],[],"['Nathalie Baracaldo', 'Olivia Choudhury', 'Gauri Joshi', 'Peter Richtarik', 'Praneeth Vepakomma', 'Shiqiang Wang', 'Han Yu']",[],[]
https://icml.cc/virtual/2021/workshop/8355,Security,"ICML Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI","This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. Deep neural networks (DNNs) have undoubtedly brought great success to a wide range of applications in computer vision, computational linguistics, and AI. However, foundational principles underlying the DNNs' success and their resilience to adversarial attacks are still largely missing. Interpreting and theorizing the internal mechanisms of DNNs becomes a compelling yet controversial topic. This workshop pays a special interest in theoretic foundations, limitations, and new application trends in the scope of XAI. These issues reflect new bottlenecks in the future development of XAI.",[],[],"['Quanshi Zhang', 'Tian Han', 'Lixin Fan', 'Zhanxing Zhu', 'Hang Su', 'Ying Nian Wu']",[],[]
https://icml.cc/virtual/2021/workshop/8370,Security,A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning,"We present DeClaW, a system for detecting, classifying, and warning of adversarial inputs presented to a classification neural network. In contrast to current state-of-the-art methods that, given an input, detect whether an input is clean or adversarial, we aim to also identify the types of adversarial attack (e.g., PGD, Carlini-Wagner or clean). To achieve this, we extract statistical profiles, which we term as anomaly feature vectors, from a set of latent features. Preliminary findings suggest that AFVs can help distinguish among several types of adversarial attacks (e.g., PGD versus Carlini-Wagner) with close to 93% accuracy on the CIFAR-10 dataset. The results open the door to using AFV-based methods for exploring not only adversarial attack detection but also classification of the attack type and then design of attack-specific mitigation strategies.",[],[],"['Hang Su', 'Yinpeng Dong', 'Tianyu Pang', 'Eric Wong', 'Zico Kolter', 'Shuo Feng', 'Bo Li', 'Henry Liu', 'Dan Hendrycks', 'Francesco Croce', 'Leslie Rice', 'Tian Tian']",[],[]