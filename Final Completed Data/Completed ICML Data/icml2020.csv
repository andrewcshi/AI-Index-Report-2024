link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://dl.acm.org/doi/10.5555/3524938.3525082,Transparency & Explainability,Graph Optimal Transport for Cross-Domain Alignment,"Cross-domain alignment between two sets of entities (e.g., objects in an image, words in a sentence) is fundamental to both computer vision and natural language processing. Existing methods mainly focus on designing advanced attention mechanisms to simulate soft alignment, where no training signals are provided to explicitly encourage alignment. Plus, the learned attention matrices are often dense and difficult to interpret. We propose Graph Optimal Transport (GOT), a principled framework that builds upon recent advances in Optimal Transport (OT). In GOT, cross-domain alignment is formulated as a graph matching problem, by representing entities as a dynamically-constructed graph. Two types of OT distances are considered: (i) Wasserstein distance (WD) for node (entity) matching; and (ii) Gromov-Wasserstein distance (GWD) for edge (structure) matching. Both WD and GWD can be incorporated into existing neural network models, effectively acting as a drop-in regularizer.The inferred transport plan also yields sparse and self-normalized alignment, enhancing the interpretability of the learned model. Experiments show consistent outperformance of GOT over baselines across a wide range of tasks, including image-text retrieval, visual question answering, image captioning, machine translation, and text summarization.",[],[],"['Liqun Chen', 'Zhe Gan', 'Yu Cheng', 'Linjie Li', 'Lawrence Carin', 'Jingjing Liu']","['Duke University', 'Microsoft Dynamics 365 AI Research', 'Microsoft Dynamics 365 AI Research', 'Microsoft Dynamics 365 AI Research', 'Duke University', 'Microsoft Dynamics 365 AI Research']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525073,Transparency & Explainability,Invariant Rationalization,"Selective rationalization improves neural network interpretability by identifying a small subset of input features — the rationale — that best explains or supports the prediction. A typical rationalization criterion, i.e. maximum mutual information (MMI), finds the rationale that maximizes the prediction performance based only on the rationale. However, MMI can be problematic because it picks up spurious correlations between the input features and the output.  Instead, we introduce a game-theoretic invariant rationalization criterion where the rationales are constrained to enable the same predictor to be optimal across different environments. We show both theoretically and empirically that the proposed rationales can rule out spurious correlations and generalize better to different test scenarios. The resulting explanations also align better with human judgments. Our implementations are publicly available at https://github.com/code-terminator/invariant_rationalization.",[],[],"['Shiyu Chang', 'Yang Zhang', 'Mo Yu', 'Tommi S. Jaakkola']","['MIT-IBM Watson AI Lab', 'MIT-IBM Watson AI Lab', 'IBM Research', 'CSAIL MIT']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525338,Transparency & Explainability,Parameterized Rate-Distortion Stochastic Encoder,"We propose a novel gradient-based tractable approach for the Blahut-Arimoto (BA) algorithm to compute the rate-distortion function where the BA algorithm is fully parameterized. This results in a rich and flexible framework to learn a new class of stochastic encoders, termed PArameterized RAte-DIstortion Stochastic Encoder (PARADISE). The framework can be applied to a wide range of settings from semi-supervised, multi-task to supervised and robust learning. We show that the training objective of PARADISE can be seen as a form of regularization that helps improve generalization. With an emphasis on robust learning we further develop a novel posterior matching objective to encourage smoothness on the loss function and show that PARADISE can significantly improve interpretability as well as robustness to adversarial attacks on the CIFAR-10 and ImageNet datasets. In particular, on the CIFAR-10 dataset, our model reduces standard and adversarial error rates in comparison to the state-of-the-art by 50% and 41%, respectively without the expensive computational cost of adversarial training.",[],[],"['Quan Hoang', 'Trung Le', 'Dinh Phung']","['Department of DSAI, Faculty of Information Technology, Monash University', 'Department of DSAI, Faculty of Information Technology, Monash University', 'Department of DSAI, Faculty of Information Technology, Monash University']","['Australia', 'Australia', 'Australia']"
https://dl.acm.org/doi/10.5555/3524938.3525206,Transparency & Explainability,Decision Trees for Decision-Making under the Predict-then-Optimize Framework,"We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize (SPO) loss, and we propose a tractable methodology called SPO Trees (SPOTs) for training decision trees under this loss. SPOTs benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that SPOTs simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., CART) trained to minimize prediction error.",[],[],"['Adam N. Elmachtoub', 'Jason Cheuk Nam Liang', 'Ryan McNellis']","['Department of Industrial Engineering and Operations Research and Data Science Institute, Columbia University, NY', 'Operations Research Center, Massachusetts Institute of Technology, MA', 'Department of Industrial Engineering and Operations Research and Data Science Institute, Columbia University, NY and Amazon, NY']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525084,Transparency & Explainability,Mapping natural-language problems to formal-language solutions using structured neural representations,"Generating formal-language programs represented by relational tuples, such as Lisp programs or mathematical operations, to solve problems stated in natural language is a challenging task because it requires explicitly capturing discrete symbolic structural information implicit in the input. However, most general neural sequence models do not explicitly capture such structural information, limiting their performance on these tasks. In this paper, we propose a new encoder-decoder model based on a structured neural representation, Tensor Product Representations (TPRs), for mapping Natural-language problems to Formal-language solutions, called TPN2F. The encoder of TP-N2F employs TPR ‘binding’ to encode natural-language symbolic structure in vector space and the decoder uses TPR ‘unbinding’ to generate, in symbolic space, a sequential program represented by relational tuples, each consisting of a relation (or operation) and a number of arguments. TP-N2F considerably outperforms LSTM-based seq2seq models on two benchmarks and creates new state-of-the-art results. Ablation studies show that improvements can be attributed to the use of structured TPRs explicitly in both the encoder and decoder. Analysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F.",[],[],"['Kezhen Chen', 'Qiuyuan Huang', 'Hamid Palangi', 'Paul Smolensky', 'Kenneth D. Forbus', 'Jianfeng Gao']","['Microsoft Research, Redmond and Department of Computer Science, Northwestern University, Evanston', 'Microsoft Research, Redmond', 'Microsoft Research, Redmond', 'Microsoft Research, Redmond and Department of Cognitive Science, Johns Hopkins University, Baltimore', 'Department of Computer Science, Northwestern University, Evanston', 'Microsoft Research, Redmond']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525670,Transparency & Explainability,Transparency Promotion with Model-Agnostic Linear Competitors,"We propose a novel type of hybrid model for multi-class classification, which utilizes competing linear models to collaborate with an existing black-box model, promoting transparency in the decision-making process. Our proposed hybrid model, Model-Agnostic Linear Competitors (MALC), brings together the interpretable power of linear models and the good predictive performance of the state-of-the-art black-box models. We formulate the training of a MALC model as a convex optimization problem, optimizing the predictive accuracy and transparency (defined as the percentage of data captured by the linear models) in the objective function. Experiments show that MALC offers more model flexibility for users to balance transparency and accuracy, in contrast to the currently available choice of either a pure black-box model or a pure interpretable model. The human evaluation also shows that more users are likely to choose MALC for this model flexibility compared with interpretable models and black-box models.",[],[],"['Hassan Rafique', 'Tong Wang', 'Qihang Lin', 'Arshia Sighani']","['Program in Applied Mathematical and Computational Sciences, The University of Iowa, Iowa City, Iowa', 'Department of Business Analytics, The University of Iowa, Iowa City, Iowa', 'Department of Business Analytics, The University of Iowa, Iowa City, Iowa', 'BASIS Independent Silicon Valley, San Jose, California']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525519,Transparency & Explainability,Hallucinative Topological Memory for Zero-Shot Visual Planning,"In visual planning (VP), an agent learns to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. Most previous works on VP approached the problem by planning in a learned latent space, resulting in low-quality visual plans, and difficult training algorithms. Here, instead, we propose a simple VP method that plans directly in image space and displays competitive performance. We build on the semi-parametric topological memory (SPTM) method: image samples are treated as nodes in a graph, the graph connectivity is learned from image sequence data, and planning can be performed using conventional graph search methods. We propose two modifications on SPTM. First, we train an energy-based graph connectivity function using contrastive predictive coding that admits stable training. Second, to allow zero-shot planning in new domains, we learn a conditional VAE model that generates images given a context describing the domain, and use these hallucinated samples for building the connectivity graph and planning. We show that this simple approach significantly outperform the SOTA VP methods, in terms of both plan interpretability and success rate when using the plan to guide a trajectory-following controller. Interestingly, our method can pick up non-trivial visual properties of objects, such as their geometry, and account for it in the plans.",[],[],"['Kara Liu', 'Thanard Kurutach', 'Christine Tung', 'Pieter Abbeel', 'Aviv Tamar']","['Berkeley AI Research, University of California, Berkeley', 'Berkeley AI Research, University of California, Berkeley', 'Berkeley AI Research, University of California, Berkeley', 'Berkeley AI Research, University of California, Berkeley', 'Berkeley AI Research, University of California, Berkeley and Technion']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525556,Transparency & Explainability,Estimation of Bounds on Potential Outcomes For Decision Making,"Estimation of individual treatment effects is commonly used as the basis for contextual decision making in fields such as healthcare, education, and economics. However, it is often sufficient for the decision maker to have estimates of upper and lower bounds on the potential outcomes of decision alternatives to assess risks and benefits. We show that, in such cases, we can improve sample efficiency by estimating simple functions that bound these outcomes instead of estimating their conditional expectations, which may be complex and hard to estimate. Our analysis highlights a trade-off between the complexity of the learning task and the confidence with which the learned bounds hold. Guided by these findings, we develop an algorithm for learning upper and lower bounds on potential outcomes which optimize an objective function defined by the decision maker, subject to the probability that bounds are violated being small. Using a clinical dataset and a well-known causality benchmark, we demonstrate that our algorithm outperforms baselines, providing tighter, more reliable bounds.",[],[],"['Maggie Makar', 'Fredrik Johansson', 'John Guttag', 'David Sontag']","['CSAIL, MIT', 'Chalmers University of Technology', 'CSAIL, MIT', 'CSAIL, MIT']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.5555/3524938.3525306,Transparency & Explainability,Retrieval Augmented Language Model Pre-Training,"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",[],[],"['Kelvin Guu', 'Kenton Lee', 'Zora Tung', 'Panupong Pasupat', 'Ming-Wei Chang']","['Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525592,Transparency & Explainability,Explainable k-Means and k-Medians Clustering,"Many clustering algorithms lead to cluster assignments that are hard to explain, partially because they depend on all the features of the data in a complicated way. To improve interpretability, we consider using a small decision tree to partition a data set into clusters, so that clusters can be characterized in a straightforward manner. We study this problem from a theoretical viewpoint, measuring cluster quality by the k-means and k-medians objectives. In terms of negative results, we show that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and any clustering based on a tree with k leaves must incur an Omega(log k) approximation factor compared to the optimal clustering. On the positive side, for two means/medians, we show that a single threshold cut can achieve a constant factor approximation, and we give nearly-matching lower bounds; for general k > 2, we design an efficient algorithm that leads to an O(k) approximation to the optimal k-medians and an O(k^2) approximation to the optimal k-means. Prior to our work, no algorithms were known with provable guarantees independent of dimension and input size.",[],[],"['Sanjoy Dasgupta', 'Nave Frost', 'Michal Moshkovitz', 'Cyrus Rashtchian']","['University of California, San Diego', 'Tel Aviv University', 'University of California, San Diego', 'University of California, San Diego']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.5555/3524938.3525447,Transparency & Explainability,Problems with Shapley-value-based explanations as feature importance measures,"Game-theoretic formulations of feature importance have become popular as a way to ""explain"" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.",[],[],"['I. Elizabeth Kumar', 'Suresh Venkatasubramanian', 'Carlos Scheidegger', 'Sorelle A. Friedler']","['School of Computing, University of Utah, Salt Lake City, UT', 'School of Computing, University of Utah, Salt Lake City, UT', 'Department of Computer Science, University of Arizona, Tucson, AZ,', 'Department of Computer Science, Haverford College, Haverford, PA']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525367,Transparency & Explainability,Semi-Supervised Learning with Normalizing Flows,"Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi supervised learning with normalizing flows, using a latent Gaussian mixture model. FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions.",[],[],"['Pavel Izmailov', 'Polina Kirichenko', 'Marc Finzi', 'Andrew Gordon Wilson']","['New York University', 'New York University', 'New York University', 'New York University']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525668,Transparency & Explainability,DeepCoDA: personalized interpretability for compositional health data,"Abstract Interpretability allows the domain-expert to directly evaluate the model's relevance and reliability, a practice that offers assurance and builds trust. In the healthcare setting, interpretable models should implicate relevant biological mechanisms independent of technical factors like data pre-processing. We define personalized interpretability as a measure of sample-specific feature attribution, and view it as a minimum requirement for a precision health model to justify its conclusions. Some health data, especially those generated by high-throughput sequencing experiments, have nuances that compromise precision health models and their interpretation. These data are compositional, meaning that each feature is conditionally dependent on all other features. We propose the Deep Compositional Data Analysis (DeepCoDA) framework to extend precision health modelling to high-dimensional compositional data, and to provide personalized interpretability through patient-specific weights. Our architecture maintains state-of-the-art performance across 25 real-world data sets, all while producing interpretations that are both personalized and fully coherent for compositional data.",[],[],"['Thomas P. Quinn', 'Dang Nguyen', 'Santu Rana', 'Sunil Gupta', 'Svetha Venkatesh']","['Applied Artificial Intelligence Institute, Deakin University, Geelong', 'Applied Artificial Intelligence Institute, Deakin University, Geelong', 'Applied Artificial Intelligence Institute, Deakin University, Geelong', 'Applied Artificial Intelligence Institute, Deakin University, Geelong', 'Applied Artificial Intelligence Institute, Deakin University, Geelong']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://dl.acm.org/doi/10.5555/3524938.3525443,Transparency & Explainability,A Sequential Self Teaching Approach for Improving Generalization in Sound Event Recognition,"An important problem in machine auditory perception is to recognize and detect sound events. In this paper, we propose a sequential self-teaching approach to learn sounds. Our main proposition is that it is harder to learn sounds in adverse situations such as from weakly labeled and/or noisy labeled data,  and in these situations a single stage of learning is not sufficient. Our proposal is a sequential stage-wise learning process that improves generalization capabilities of a given modeling system. We justify this method via technical results and on Audioset, the largest sound events dataset, our sequential learning approach can lead to up to 9% improvement in performance. A comprehensive evaluation also shows that the method leads to improved transferability of knowledge from previously trained models, thereby leading to improved generalization capabilities on transfer learning tasks.",[],[],"['Anurag Kumar', 'Vamsi Krishna Ithapu']","['Facebook Reality Labs, Redmond', 'Facebook Reality Labs, Redmond']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525832,Transparency & Explainability,Approximating Stacked and Bidirectional Recurrent Architectures with the Delayed Recurrent Neural Network,"Recent work has shown that topological enhancements to recurrent neural networks (RNNs) can increase their expressiveness and representational capacity. Two popular enhancements are stacked RNNs, which increases the capacity for learning non-linear functions, and bidirectional processing, which exploits acausal information in a sequence. In this work, we explore the delayed-RNN, which is a single-layer RNN that has a delay between the input and output. We prove that a weight-constrained version of the delayed-RNN is equivalent to a stacked-RNN. We also show that the delay gives rise to partial acausality, much like bidirectional networks. Synthetic experiments confirm that the delayed-RNN can mimic bidirectional networks, solving some acausal tasks similarly, and outperforming them in others. Moreover, we show similar performance to bidirectional networks in a real-world natural language processing task. These results suggest that delayed-RNNs can approximate topologies including stacked RNNs, bidirectional RNNs, and stacked bidirectional RNNs -- but with equivalent or faster runtimes for the delayed-RNNs.",[],[],"['Javier S. Turek', 'Shailee Jain', 'Vy A. Vo', 'Mihai Capotă', 'Alexander G. Huth', 'Theodore L. Willke']","['Intel Labs, Hillsboro, Oregon', 'Department of Computer Science, The University of Texas at Austin, Austin, Texas', 'Intel Labs, Hillsboro, Oregon', 'Intel Labs, Hillsboro, Oregon', 'Department of Computer Science and Department of Neuroscience, The University of Texas at Austin, Austin, Texas', 'Intel Labs, Hillsboro, Oregon']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525841,Transparency & Explainability,Born-again Tree Ensembles,"The use of machine learning algorithms in finance, medicine, and criminal justice can deeply impact human lives. As a consequence, research into interpretable machine learning has rapidly grown in an attempt to better control and fix possible sources of mistakes and biases. Tree ensembles, in particular, offer a good prediction quality in various domains, but the concurrent use of multiple trees reduces the interpretability of the ensemble. Against this background, we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble in its entire feature space. To find such a tree, we develop a dynamic-programming based algorithm that exploits sophisticated pruning and bounding rules to reduce the number of recursive calls. This algorithm generates optimal born-again trees for many datasets of practical interest, leading to classifiers which are typically simpler and more interpretable without any other form of compromise.",[],[],"['Thibaut Vidal', 'Maximilian Schiffer']","['Department of Computer Science, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro', 'TUM School of Management, Technical University of Munich, Munich']","['Brazil', 'Germany']"
https://dl.acm.org/doi/10.5555/3524938.3525696,Transparency & Explainability,Attentive Group Equivariant Convolutional Networks,"Although group convolutional networks are able to learn powerful representations based on symmetry patterns, they lack explicit means to learn meaningful relationships among them (e.g., relative positions and poses). In this paper, we present attentive group equivariant convolutions, a generalization of the group convolution, in which attention is applied during the course of convolution to accentuate meaningful symmetry combinations and suppress non-plausible, misleading ones. We indicate that prior work on visual attention can be described as special cases of our proposed framework and show empirically that our attentive group equivariant convolutional networks consistently outperform conventional group convolutional networks on benchmark image datasets. Simultaneously, we provide interpretability to the learned concepts through the visualization of equivariant attention maps.",[],[],"['David W. Romero', 'Erik J. Bekkers', 'Jakub M. Tomczak', 'Mark Hoogendoorn']","['Vrije Universiteit Amsterdam', 'University of Amsterdam, Th', 'Vrije Universiteit Amsterdam', 'Vrije Universiteit Amsterdam']","[None, 'Netherlands', None, None]"
https://dl.acm.org/doi/10.5555/3491440.3491780,Transparency & Explainability,Scalable Exact Inference in Multi-Output Gaussian Processes,"Multi-output Gaussian processes (MOGPs) leverage the flexibility and interpretability of GPs while capturing structure across outputs, which is desirable, for example, in spatio-temporal modelling. The key problem with MOGPs is their computational scaling $O(n^3 p^3)$, which is cubic in the number of both inputs $n$ (e.g., time points or locations) and outputs $p$. For this reason, a popular class of MOGPs assumes that the data live around a low-dimensional linear subspace, reducing the complexity to $O(n^3 m^3)$. However, this cost is still cubic in the dimensionality of the subspace $m$, which is still prohibitively expensive for many applications. We propose the use of a sufficient statistic of the data to accelerate inference and learning in MOGPs with orthogonal bases. The method achieves linear scaling in $m$ in practice, allowing these models to scale to large $m$ without sacrificing significant expressivity or requiring approximation. This advance opens up a wide range of real-world tasks and can be combined with existing GP approximations in a plug-and-play way. We demonstrate the efficacy of the method on various synthetic and real-world data sets.",[],[],"['Shibo Li', 'Wei Xing', 'Robert M. Kirby', 'Shandian Zhe']","['School of Computing, University of Utah', 'Scientific Computing and Imaging Institute, University of Utah', 'School of Computing, University of Utah and Scientific Computing and Imaging Institute', 'School of Computing, University of Utah']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525365,Transparency & Explainability,Fast Deterministic CUR Matrix Decomposition with Accuracy Assurance,"The deterministic CUR matrix decomposition is a low-rank approximation method to analyze a data matrix. It has attracted considerable attention due to its high interpretability, which results from the fact that the decomposed matrices consist of subsets of the original columns and rows of the data matrix. The subset is obtained by optimizing an objective function with sparsity-inducing norms via coordinate descent. However, the existing algorithms for optimization incur high computation costs. This is because coordinate descent iteratively updates all the parameters in the objective until convergence. This paper proposes a fast deterministic CUR matrix decomposition. Our algorithm safely skips unnecessary updates by efficiently evaluating the optimality conditions for the parameters to be zeros. In addition, we preferentially update the parameters that must be nonzeros. Theoretically, our approach guarantees the same result as the original approach. Experiments demonstrate that our algorithm speeds up the deterministic CUR while achieving the same accuracy.",[],[],"['Yasutoshi Ida', 'Sekitoshi Kanai', 'Yasuhiro Fujiwara', 'Tomoharu Iwata', 'Koh Takeuchi', 'Hisashi Kashima']","['NTT Software Innovation Center, Tokyo,  and Department of Intelligence Science and Technology, Kyoto University, Kyoto', 'NTT Software Innovation Center, Tokyo', 'NTT Communication Science Laboratories, Kyoto', 'NTT Communication Science Laboratories, Kyoto', 'Department of Intelligence Science and Technology, Kyoto University, Kyoto,  and RIKEN Center for Advanced Intelligence Project, Tokyo', 'Department of Intelligence Science and Technology, Kyoto University, Kyoto,  and RIKEN Center for Advanced Intelligence Project, Tokyo']","['Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan']"
https://dl.acm.org/doi/10.5555/3524938.3525174,Transparency & Explainability,Enhancing Simple Models by Exploiting What They Already Know,"There has been recent interest in improving performance of simple models for multiple reasons such as interpretability, robust learning from small data, deployment in memory constrained settings as well as environmental considerations. In this paper, we propose a novel method SRatio that can utilize information from high performing complex models (viz. deep neural networks, boosted trees, random forests) to reweight a training dataset for a potentially low performing simple model of much lower complexity such as a decision tree or a shallow network enhancing its performance. Our method also leverages the per sample hardness estimate of the simple model which is not the case with the prior works which primarily consider the complex model's confidences/predictions and is thus conceptually novel. Moreover, we generalize and formalize the concept of attaching probes to intermediate layers of a neural network to other commonly used classifiers and incorporate this into our method. The benefit of these contributions is witnessed in the experiments where on 6 UCI datasets and CIFAR-10 we outperform competitors in a majority (16 out of 27) of the cases and tie for best performance in the remaining cases. In fact, in a couple of cases, we even approach the complex model's performance. We also conduct further experiments to validate assertions and intuitively understand why our method works. Theoretically, we motivate our approach by showing that the weighted loss minimized by simple models using our weighting upper bounds the loss of the complex model.",[],[],"['Amit Dhurandhar', 'Karthikeyan Shanmugam', 'Ronny Luss']","['IBM Research, Yorktown Heights, NY', 'IBM Research, Yorktown Heights, NY', 'IBM Research, Yorktown Heights, NY']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525642,Transparency & Explainability,Performative Prediction,"When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.",[],[],"['Juan C. Perdomo', 'Tijana Zrnic', 'Celestine Mendler-Dünner', 'Moritz Hardt']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525816,Fairness & Bias,Student Specialization in Deep Rectified Networks With Finite Width and Input Dimension,"We consider a deep ReLU / Leaky ReLU student network trained from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). The student network is \emph{over-realized}: at each layer $l$, the number $n_l$ of student nodes is more than that ($m_l$) of teacher. Under mild conditions on dataset and teacher network, we prove that when the gradient is small at every data sample, each teacher node is \emph{specialized} by at least one student node \emph{at the lowest layer}. For two-layer network, such specialization can be achieved by training on any dataset of \emph{polynomial} size $\mathcal{O}( K^{5/2} d^3 \epsilon^{-1})$. until the gradient magnitude drops to $\mathcal{O}(\epsilon/K^{3/2}\sqrt{d})$. Here $d$ is the input dimension, $K = m_1 + n_1$ is the total number of neurons in the lowest layer of teacher and student. Note that we require a specific form of data augmentation and the sample complexity includes the additional data generated from augmentation. To our best knowledge, we are the first to give polynomial sample complexity for student specialization of training two-layer (Leaky) ReLU networks with finite depth and width in teacher-student setting, and finite complexity for the lowest layer specialization in multi-layer case, without parametric assumption of the input (like Gaussian). Our theory suggests that teacher nodes with large fan-out weights get specialized first when the gradient is still large, while others are specialized with small gradient, which suggests inductive bias in training. This shapes the stage of training as empirically observed in multiple previous works. Experiments on synthetic and CIFAR10 verify our findings. The code is released in \url{https://github.com/facebookresearch/luckmatters}.",[],[],['Yuandong Tian'],['Facebook AI Research'],[None]
https://dl.acm.org/doi/10.5555/3524938.3525039,Fairness & Bias,Adversarial Filters of Dataset Biases,"Large neural models have demonstrated human-level performance on language and vision benchmarks, while their performance degrades considerably on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting to spurious dataset biases. We investigate one recently proposed approach, AFLITE, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLITE, by situating it in the generalized framework for optimum bias reduction. We present extensive supporting evidence that AFLITE is broadly applicable for reduction of measurable dataset biases, and that models trained on the filtered datasets yield better generalization to out-of-distribution tasks. Finally, filtering results in a large drop in model performance (e.g., from 92% to 62% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.",[],[],"['Ronan Le Bras', 'Swabha Swayamdipta', 'Chandra Bhagavatula', 'Rowan Zellers', 'Matthew E. Peters', 'Ashish Sabharwal', 'Yejin Choi']","['Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence and Paul G. Allen School of Computer Science, University of Washington', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence and Paul G. Allen School of Computer Science, University of Washington']","[None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525207,Fairness & Bias,Revisiting Spatial Invariance with Low-Rank Local Connectivity,"Convolutional neural networks are among the most successful architectures in deep learning with this success at least partially attributable to the efficacy of spatial invariance as an inductive bias. Locally connected layers, which differ from convolutional layers only in their lack of spatial invariance, usually perform poorly in practice. However, these observations still leave open the possibility that some degree of relaxation of spatial invariance may yield a better inductive bias than either convolution or local connectivity. To test this hypothesis, we design a method to relax the spatial invariance of a network layer in a controlled manner; we create a \textit{low-rank} locally connected layer, where the filter bank applied at each position is constructed as a linear combination of basis set of filter banks with spatially varying combining weights. By varying the number of basis filter banks, we can control the degree of relaxation of spatial invariance. In experiments with small convolutional networks, we find that relaxing spatial invariance improves classification accuracy over both convolution and locally connected layers across MNIST, CIFAR-10, and CelebA datasets, thus suggesting that spatial invariance may be an overly restrictive prior.",[],[],"['Gamaleldin F. Elsayed', 'Prajit Ramachandran', 'Jonathon Shlens', 'Simon Kornblith']","['Google Research, Brain Team', 'Google Research, Brain Team', 'Google Research, Brain Team', 'Google Research, Brain Team']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525064,Fairness & Bias,Data preprocessing to mitigate bias: A maximum entropy based approach,"Data containing human or social attributes may over- or under-represent groups with respect to salient social attributes such as gender or race, which can lead to biases in downstream applications. This paper presents an algorithmic framework that can be used as a data preprocessing method towards mitigating such bias. Unlike prior work, it can efficiently learn distributions over large domains, controllably adjust the representation rates of protected groups and achieve target fairness metrics such as statistical parity, yet remains close to the empirical distribution induced by the given dataset. Our approach leverages the principle of maximum entropy – amongst all distributions satisfying a given set of constraints, we should choose the one closest in KL-divergence to a given prior. While maximum entropy distributions can succinctly encode distributions over large domains, they can be difficult to compute. Our main contribution is an instantiation of this framework for our set of constraints and priors, which encode our bias mitigation goals, and that runs in time polynomial in the dimension of the data. Empirically, we observe that samples from the learned distribution have desired representation rates and statistical rates, and when used for training a classifier incurs only a slight loss in accuracy while maintaining fairness properties.",[],[],"['L. Elisa Celis', 'Vijay Keswani', 'Nisheeth K. Vishnoi']","['Department of Statistics and Data Science, Yale University', 'Department of Statistics and Data Science, Yale University', 'Department of Computer Science, Yale University']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525711,Fairness & Bias,An Investigation of Why Overparameterization Exacerbates Spurious Correlations,"We study why overparameterization---increasing model size well beyond the point of zero training error---can hurt test error on minority groups despite improving average test error when there are spurious correlations in the data. Through simulations and experiments on two image datasets, we identify two key properties of the training data that drive this behavior: the proportions of majority versus minority groups, and the signal-to-noise ratio of the spurious correlations. We then analyze a linear setting and theoretically show how the inductive bias of models towards ``memorizing'' fewer examples can cause overparameterization to hurt. Our analysis leads to a counterintuitive approach of subsampling the majority group, which empirically achieves low minority error in the overparameterized regime, even though the standard approach of upweighting the minority fails. Overall, our results suggest a tension between using overparameterized models versus using all the training data for achieving low worst-group error.",[],[],"['Shiori Sagawa', 'Aditi Raghunathan', 'Pang Wei Koh', 'Percy Liang']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525212,Fairness & Bias,Identifying Statistical Bias in Dataset Replication,"Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models' ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for statistical bias, only an estimated 3.6% of the original 11.7% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available: https://git.io/data-rep-analysis.",[],[],"['Logan Engstrom', 'Andrew Ilyas', 'Shibani Santurkar', 'Dimitris Tsipras', 'Jacob Steinhardt', 'Aleksander Mądry']","['MIT', 'MIT', 'MIT', 'MIT', 'UC Berkeley', 'MIT']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525596,Fairness & Bias,Two Simple Ways to Learn Individual Fairness Metrics from Data,"Individual fairness is an intuitive definition of algorithmic fairness that addresses some of the drawbacks of group fairness. Despite its benefits, it depends on a task specific fair metric that encodes our intuition of what is fair and unfair for the ML task at hand, and the lack of a widely accepted fair metric for many ML tasks is the main barrier to broader adoption of individual fairness. In this paper, we present two simple ways to learn fair metrics from a variety of data types. We show empirically that fair training with the learned metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches.",[],[],"['Debarghya Mukherjee', 'Mikhail Yurochkin', 'Moulinath Banerjee', 'Yuekai Sun']","['Department of Statistics, University of Michigan', 'IBM Research, MIT-IBM Watson AI Lab.', 'Department of Statistics, University of Michigan', 'Department of Statistics, University of Michigan']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525964,Fairness & Bias,Learning the Valuations of a $k$-demand Agent,"We study problems where a learner aims to learn the valuations of an agent by observing which goods he buys under varying price vectors.  More specifically, we consider the case of a $k$-demand agent, whose valuation over the goods is additive when receiving up to $k$ goods, but who has no interest in receiving more than $k$ goods.  We settle the query complexity for the active-learning (preference elicitation) version, where the learner chooses the prices to post, by giving a {\em biased binary search} algorithm, generalizing the classical binary search procedure. We complement our query complexity upper bounds by lower bounds that match up to lower-order terms.  We also study the passive-learning version in which the learner does not control the prices, and instead they are sampled from some distribution.  We show that in the PAC model for passive learning, any {\em empirical risk minimizer} has a sample complexity that is optimal up to a factor of $\widetilde{O}(k)$.",[],[],"['Hanrui Zhang', 'Vincent Conitzer']","['Department of Computer Science, Duke University, Durham', 'Department of Computer Science, Duke University, Durham']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525708,Fairness & Bias,Bounding the fairness and accuracy of classifiers from population statistics,"We consider the study of a classification model whose properties are impossible to estimate using a validation set, either due to the absence of such a set or because access to the classifier, even as a black-box, is impossible. Instead, only aggregate statistics on the rate of positive predictions in each of several sub-populations are available, as well as the true rates of positive labels in each of these sub-populations.  We show that these aggregate statistics can be used to lower-bound the discrepancy of a classifier, which is a measure that balances inaccuracy and unfairness. To this end, we define a new measure of unfairness, equal to the fraction of the population on which the classifier behaves differently, compared to its global, ideally fair behavior, as defined by the measure of equalized odds.  We propose an efficient and practical procedure for finding the best possible lower bound on the discrepancy of the classifier, given the aggregate statistics, and demonstrate in experiments the empirical tightness of this lower bound, as well as its possible uses on various types of problems, ranging from estimating the quality of voting polls to measuring the effectiveness of patient identification from internet search queries. The code and data are available at https://github.com/sivansabato/bfa.",[],[],"['Sivan Sabato', 'Elad Yom-Tov']","['Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva,  and Microsoft Research, Herzelia', 'Microsoft Research, Herzelia']","['Israel', 'Israel']"
https://dl.acm.org/doi/10.5555/3524938.3525759,Fairness & Bias,On conditional versus marginal bias in multi-armed bandits,"The bias of the sample means of the arms in multi-armed bandits is an important issue in adaptive data analysis that has recently received considerable attention in the literature. Existing results relate in precise ways the sign and magnitude of the bias to various sources of data adaptivity, but do not apply to the conditional inference setting in which the sample means are computed only if some specific conditions are satisfied. In this paper, we characterize the sign of the conditional bias of monotone functions of the rewards, including the sample mean. Our results hold for arbitrary conditioning events and leverage natural monotonicity properties of the data collection policy. We further demonstrate, through several examples from sequential testing and best arm identification, that the sign of the conditional and marginal bias of the sample mean of an arm can be different, depending on the conditioning event. Our analysis offers new and interesting perspectives on the subtleties of assessing the bias in data adaptive settings.",[],[],"['Jaehyeok Shin', 'Alessandro Rinaldo', 'Aaditya Ramdas']","['Department of Statistics and Data Science, Carnegie Mellon University', 'Department of Statistics and Data Science, Carnegie Mellon University', 'Department of Statistics and Data Science, Carnegie Mellon University and Machine Learning Department, Carnegie Mellon University']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525868,Fairness & Bias,Loss Function Search for Face Recognition,"In face recognition, designing margin-based (\textit{e.g.}, angular, additive, additive angular margins) softmax loss functions plays an important role to learn discriminative features. However, these hand-crafted heuristic methods may be sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.",[],[],"['Xiaobo Wang', 'Shuo Wang', 'Cheng Chi', 'Shifeng Zhang', 'Tao Mei']","['JD AI Research', 'JD AI Research', 'Institute of Automation, Chinese Academy of Science', 'Institute of Automation, Chinese Academy of Science', 'JD AI Research']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525834,Fairness & Bias,StochasticRank: Global Optimization of Scale-Free Discrete Functions,"In this paper, we introduce a powerful and efficient framework for direct optimization of ranking metrics. The problem is ill-posed due to the discrete structure of the loss, and to deal with that, we introduce two important techniques: a stochastic smoothing and a novel gradient estimate based on partial integration. We also address the problem of smoothing bias and present a universal solution for a proper debiasing. To guarantee the global convergence of our method, we adopt a recently proposed Stochastic Gradient Langevin Boosting algorithm. Our algorithm is implemented as a part of the CatBoost gradient boosting library and outperforms the existing approaches on several learning-to-rank datasets. In addition to ranking metrics, our framework applies to any scale-free discrete loss function.",[],[],"['Aleksei Ustimenko', 'Liudmila Prokhorenkova']","['Yandex, Moscow, Russia', 'Yandex, Moscow, Russia and Moscow Institute of Physics and Technology, Dolgoprudny, Moscow Region, Russia and Higher School of Economics, Moscow, Russia']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525539,Fairness & Bias,Adversarial Nonnegative Matrix Factorization,"Nonnegative Matrix Factorization (NMF) has become an increasingly important research topic in machine learning. Despite all the practical success, most of existing NMF models are still vulnerable to adversarial attacks. To overcome this limitation, we propose a novel Adversarial NMF (ANMF) approach in which an adversary can exercise some control over the perturbed data generation process. Different from the traditional NMF models which focus on  either the regular input or certain types of noise, our model considers potential test adversaries that are beyond the pre-defined constraints, which can cope with various noises (or perturbations). We formulate the proposed model as a bilevel optimization problem and use Alternating Direction Method of Multipliers (ADMM) to solve it with convergence analysis. Theoretically, the robustness analysis of ANMF is established under mild conditions dedicating asymptotically unbiased prediction. Extensive experiments verify that ANMF is robust to a broad categories of perturbations, and achieves state-of-the-art performances on distinct real-world benchmark datasets.",[],[],"['Lei Luo', 'Yanfu Zhang', 'Heng Huang']","['JD Finance America Corporation, Mountain View, CA and Department of Electrical and Computer Engineering, University of Pittsburgh, PA', 'Department of Electrical and Computer Engineering, University of Pittsburgh, PA', 'JD Finance America Corporation, Mountain View, CA and Department of Electrical and Computer Engineering, University of Pittsburgh, PA']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525814,Fairness & Bias,Inductive Relation Prediction by Subgraph Reasoning,"The dominant paradigm for relation prediction in knowledge graphs involves learning and operating on latent representations (i.e., embeddings) of entities and relations. However, these embedding-based methods do not explicitly capture the compositional logical rules underlying the knowledge graph,  and they are limited to the transductive setting,  where the full set of entities must be known during training. Here, we propose a graph neural network based relation prediction framework,  GraIL, that reasons over local subgraph structures and has a strong inductive bias to learn entity-independent relational semantics.  Unlike embedding-based models, GraIL is naturally inductive and can generalize to unseen entities and graphs after training. We provide theoretical proof and strong empirical evidence that GraIL can rep-resent a useful subset of first-order logic and show that GraIL outperforms existing rule-induction baselines in the inductive setting. We also demonstrate significant gains obtained by ensembling GraIL with various knowledge graph embedding methods in the transductive setting, highlighting the complementary inductive bias of our method.",[],[],"['Komal K. Teru', 'Etienne G. Denis', 'William L. Hamilton']","['McGill University and Mila', 'McGill University and Mila', 'McGill University and Mila']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525680,Fairness & Bias,AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,"Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.",[],[],"['Esteban Real', 'Chen Liang', 'David R. So', 'Quoc V. Le']","['Google Brain/Google Research, Mountain View, CA', 'Google Brain/Google Research, Mountain View, CA', 'Google Brain/Google Research, Mountain View, CA', 'Google Brain/Google Research, Mountain View, CA']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525701,Fairness & Bias,Revisiting Training Strategies and Generalization Performance in Deep Metric Learning,"Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Under consistent comparison, DML objectives show much higher saturation than indicated by literature. Further based on our analysis, we uncover a correlation between the embedding space density and compression to the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets; code and a publicly accessible WandB-repo are available at https://github.com/Confusezius/RevisitingDeepMetricLearningPyTorch.",[],[],"['Karsten Roth', 'Timo Milbich', 'Samarth Sinha', 'Prateek Gupta', 'Björn Ommer', 'Joseph Paul Cohen']","['Mila, Université de Montréal and HCI/IWR, Heidelberg University', 'HCI/IWR, Heidelberg University', 'Mila, Université de Montréal and University of Toronto', 'Mila, Université de Montréal and The Alan Turing Institute, University of Oxford', 'HCI/IWR, Heidelberg University', 'Mila, Université de Montréal']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525714,Fairness & Bias,Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics,"Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions--demographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.",[],[],"['Debjani Saha', 'Candice Schumann', 'Duncan C. McElfresh', 'John P. Dickerson', 'Michelle L. Mazurek', 'Michael Carl Tschantz']","['University of Maryland, College Park, MD', 'University of Maryland, College Park, MD', 'University of Maryland, College Park, MD', 'University of Maryland, College Park, MD', 'University of Maryland, College Park, MD', 'ICSI, Berkeley, CA']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525048,Fairness & Bias,A Pairwise Fair and Community-preserving Approach to k-Center Clustering,"Clustering is a foundational problem in machine learning with numerous applications. As machine learning increases in ubiquity as a backend for automated systems, concerns about fairness arise. Much of the current literature on fairness deals with discrimination against protected classes in supervised learning (group fairness). We define a different notion of fair clustering wherein the probability that two points (or a community of points) become separated is bounded by an increasing function of their pairwise distance (or community diameter). We capture the situation where data points represent people who gain some benefit from being clustered together. Unfairness arises when certain points are deterministically separated, either arbitrarily or by someone who intends to harm them as in the case of gerrymandering election districts. In response, we formally define two new types of fairness in the clustering setting, pairwise fairness and community preservation. To explore the practicality of our fairness goals, we devise an approach for extending existing $k$-center algorithms to satisfy these fairness constraints. Analysis of this approach proves that reasonable approximations can be achieved while maintaining fairness. In experiments, we compare the effectiveness of our approach to classical $k$-center algorithms/heuristics and explore the tradeoff between optimal clustering and fairness.",[],[],"['Brian Brubach', 'Darshan Chakrabarti', 'John P. Dickerson', 'Samir Khuller', 'Aravind Srinivasan', 'Leonidas Tsepenekas']","['Department of Computer Science, University of Maryland, College Park, Maryland and Computer Science Department, Wellesley College, Wellesley, MA', 'School of Computer Science, Carnegie Mellon University, Pittsburgh, PA', 'Department of Computer Science, University of Maryland, College Park, Maryland', 'Department of Computer Science, Northwestern University, Evanston, IL', 'Department of Computer Science, University of Maryland, College Park, Maryland', 'Department of Computer Science, University of Maryland, College Park, Maryland']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525549,Fairness & Bias,Individual Fairness for k-Clustering,"We give a local search based algorithm for $k$-median and $k$-means (and more generally for any $k$-clustering with $\ell_p$ norm cost function) from the perspective of individual fairness. More precisely, for a point $x$ in a point set $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of radius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively, if a set of $k$ random points are chosen from $P$ as centers, every point $x\in P$ expects to have a center within radius $r(x)$. An individually fair clustering provides such a guarantee for every point $x\in P$. This notion of fairness was introduced in [Jung et al., 2019] where they showed how to get an approximately feasible $k$-clustering with respect to this fairness condition. In this work, we show how to get an approximately \emph{optimal} such fair $k$-clustering. The $k$-median ($k$-means) cost of our solution is within a constant factor of the cost of an optimal fair $k$-clustering, and our solution approximately satisfies the fairness condition (also within a constant factor).",[],[],"['Sepideh Mahabadi', 'Ali Vakilian']","['TTIC, Chicago, Illinois', 'University of Wisconsin, Madison, Wisconsin']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525721,Fairness & Bias,Spectral Subsampling MCMC for Stationary Time Series,"Bayesian inference using Markov Chain Monte Carlo (MCMC) on large datasets has developed rapidly in recent years. However, the underlying methods are generally limited to relatively simple settings where the data have specific forms of independence. We propose a novel technique for speeding up MCMC for time series data by efficient data subsampling in the frequency domain. For several challenging time series models, we demonstrate a speedup of up to two orders of magnitude while incurring negligible bias compared to MCMC on the full dataset. We also propose alternative control variates for variance reduction based on data grouping and coreset constructions.",[],[],"['Robert Salomone', 'Matias Quiroz', 'Robert Kohn', 'Mattias Villani', 'Minh-Ngoc Tran']","['UNSW Sydney', 'University of Technology Sydney', 'UNSW Sydney', 'Stockholm University and Linköping University', 'University of Sydney']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525341,Fairness & Bias,Black-Box Variational Inference as a Parametric Approximation to Langevin Dynamics,"Variational inference (VI) and Markov chain Monte Carlo (MCMC) are approximate posterior inference algorithms that are often said to have complementary strengths, with VI being fast but biased and MCMC being slower but asymptotically unbiased. In this paper, we analyze gradient-based MCMC and VI procedures and find theoretical and empirical evidence that these procedures are not as different as one might think. In particular, a close examination of the Fokker-Planck equation that governs the Langevin dynamics (LD) MCMC procedure reveals that LD implicitly follows a gradient flow that corresponds to an VI procedure based on optimizing a nonparametric normalizing flow. The evolution under gradient descent of real-world VI approximations that use tractable, parametric flows can thus be seen as an approximation to the evolution of a population of LD-MCMC chains. This result suggests that the transient bias of LD (due to the Markov chain not having burned in) may track that of VI (due to the optimizer not having converged), up to differences due to VI’s asymptotic bias and parameter geometry. Empirically, we find that the transient biases of these algorithms (and their momentum-accelerated counterparts) do evolve similarly. This suggests that practitioners with a limited time budget may get more accurate results by running an MCMC procedure (even if it is stopped before fully burning in) than a VI procedure, as long as the variance of the MCMC estimator can be dealt with (e.g., by running many parallel chains).",[],[],"['Matthew Hoffman', 'Yi-An Ma']","['Google Research', 'Google Research and Halicioğlu Data Science Institute, University of California, San Diego']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525788,Fairness & Bias,Doubly robust off-policy evaluation with shrinkage ,"We propose a new framework for designing estimators for off-policy evaluation in contextual bandits. Our approach is based on the asymptotically optimal doubly robust estimator, but we shrink the importance weights to minimize a bound on the mean squared error, which results in a better bias-variance tradeoff in finite samples. We use this optimization-based framework to obtain three estimators: (a) a weight-clipping estimator, (b) a new weight-shrinkage estimator, and (c) the first shrinkage-based estimator for combinatorial action sets. Extensive experiments in both standard and combinatorial bandit benchmark problems show that our estimators are highly adaptive and typically outperform state-of-the-art methods.",[],[],"['Yi Su', 'Maria Dimakopoulou', 'Akshay Krishnamurthy', 'Miroslav Dudík']","['Cornell University, Ithaca, NY', 'Netflix, Los Gatos, CA', 'Microsoft Research, New York, NY', 'Microsoft Research, New York, NY']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3524997,Fairness & Bias,Inductive-bias-driven Reinforcement Learning For Efficient Schedules in Heterogeneous Clusters,"The  problem  of  scheduling  of  workloads  onto heterogeneous processors (e.g., CPUs, GPUs, FPGAs) is of fundamental importance in modern data centers.  Current system schedulers rely on application/system-specific heuristics that have to be built on a case-by-case basis. Recent work has demonstrated ML techniques for automating the heuristic search by using black-box approaches which require significant training data and time, which  make  them  challenging  to  use  in practice. This paper presents Symphony, a scheduling framework that addresses the challenge in two ways:  (i)  a  domain-driven  Bayesian  reinforcement learning (RL) model for scheduling, which inherently models the resource dependencies identified from the system architecture; and (ii) a sampling-based technique to compute the gradients of a Bayesian model without performing full probabilistic  inference. Together,  these  techniques reduce both the amount of training data and the time required to produce scheduling policies that significantly outperform black-box approaches by up to 2.2×.",[],[],"['Subho S. Banerjee', 'Saurabh Jha', 'Zbigniew T. Kalbarczyk', 'Ravishankar K. Iyer']","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525731,Fairness & Bias,Discriminative Adversarial Search for Abstractive Summarization,"We introduce a novel approach for sequence decoding, Discriminative Adversarial Search (DAS), which has the desirable properties of alleviating the effects of exposure bias without requiring external metrics. Inspired by Generative Adversarial Networks (GANs), wherein a discriminator is used to improve the generator, our method differs from GANs in that the generator parameters are not updated at training time and the discriminator is used to drive sequence generation at inference time. We investigate the effectiveness of the proposed approach on the task of Abstractive Summarization: the results obtained show that a naive application of DAS improves over the state-of-the-art methods, with further gains obtained via discriminator retraining. Moreover, we show how DAS can be effective for cross-domain adaptation. Finally, all results reported are obtained without additional rule-based filtering strategies, commonly used by the best performing systems available: this indicates that DAS can effectively be deployed without relying on post-hoc modifications of the generated outputs.",[],[],"['Thomas Scialom', 'Paul-Alexis Dray', 'Sylvain Lamprier', 'Benjamin Piwowarski', 'Jacopo Staiano']","['reciTAL, Paris,  and Sorbonne Université, CNRS, LIP6, Paris', 'reciTAL, Paris', 'Sorbonne Université, CNRS, LIP6, Paris', 'Sorbonne Université, CNRS, LIP6, Paris', 'reciTAL, Paris']","['France', 'France', 'France', 'France', 'France']"
https://dl.acm.org/doi/10.5555/3524938.3525167,Fairness & Bias,Randomly Projected Additive Gaussian Processes for Regression,"Gaussian processes (GPs) provide flexible distributions over functions, with inductive biases controlled by a kernel. However, in many applications Gaussian processes can struggle with even moderate input dimensionality. Learning a low dimensional projection can help alleviate this curse of dimensionality, but introduces many trainable hyperparameters, which can be cumbersome, especially in the small data regime. We use additive sums of kernels for GP regression, where each kernel operates on a different random projection of its inputs. Surprisingly, we find that as the number of random projections increases, the predictive performance of this approach quickly converges to the performance of a kernel operating on the original full dimensional inputs, over a wide range of data sets, even if we are projecting into a single dimension. As a consequence, many problems can remarkably be reduced to one dimensional input spaces, without learning a transformation. We prove this convergence and its rate, and additionally propose a deterministic approach that converges more quickly than purely random projections. Moreover, we demonstrate our approach can achieve faster inference and improved predictive accuracy for high-dimensional inputs compared to kernels in the original input space.",[],[],"['Ian Delbridge', 'David Bindel', 'Andrew Gordon Wilson']","['Department of Computer Science, Cornell University, Ithaca, New York', 'Department of Computer Science, Cornell University, Ithaca, New York', 'Center for Data Science, New York University, New York City, New York']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525397,Fairness & Bias,Fair k-Centers via Maximum Matching,"The field of algorithms has seen a push for fairness, or the removal of inherent bias, in recent history. In data summarization, where a much smaller subset of a data set is chosen to represent the whole of the data, fairness can be introduced by guaranteeing each ""demographic group"" a specific portion of the representative subset. Specifically, this paper examines this fair variant of the k-centers problem, where a subset of the data with cardinality k is chosen to minimize distance to the rest of the data. Previous papers working on this problem presented both a 3-approximation algorithm with a super-linear runtime and a linear-time algorithm whose approximation factor is exponential in the number of demographic groups. This paper combines the best of each algorithm by presenting a linear-time algorithm with a guaranteed 3-approximation factor and provides empirical evidence of both the algorithm's runtime and effectiveness.",[],[],"['Matthew Jones', 'Huy Lê Nguyê˜n', 'Thy Nguyen']","['Khoury College of Computer Science, Northeastern University, Boston, Massachusetts', 'Khoury College of Computer Science, Northeastern University, Boston, Massachusetts', 'Khoury College of Computer Science, Northeastern University, Boston, Massachusetts']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525385,Fairness & Bias,Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation,"We present an approach for unsupervised domain adaptation—with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift—from a class-conditioned domain alignment perspective. Current methods for class-conditioned domain alignment aim to explicitly minimize a loss function based on pseudo-label estimations of the target domain. However, these methods suffer from pseudo-label bias in the form of error accumulation. We propose a method that removes the need for explicit optimization of model parameters from pseudo-labels. Instead, we present a sampling-based implicit alignment approach, where the sample selection is implicitly guided by the pseudo-labels. Theoretical analysis reveals the existence of a domain-discriminator shortcut in misaligned classes, which is addressed by the proposed approach to facilitate domain-adversarial learning. Empirical results and ablation studies confirm the effectiveness of the proposed approach, especially in the presence of within-domain class imbalance and between-domain class distribution shift.",[],[],"['Xiang Jiang', 'Qicheng Lao', 'Stan Matwin', 'Mohammad Havaei']","['Imagia,  and Dalhousie University', 'Imagia,  and Mila, Université de Montréal', 'Dalhousie University,  and Polish Academy of Sciences, Polan', 'Imagia']","['Canada', 'Canada', 'Canada', 'Canada']"
https://dl.acm.org/doi/10.5555/3524938.3525969,Fairness & Bias,Mix-n-Match : Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning,"This paper studies the problem of post-hoc calibration of machine learning classifiers. We introduce the following desiderata for uncertainty calibration: (a) accuracy-preserving, (b) data-efficient, and (c) high expressive power. We show that none of the existing methods satisfy all three requirements, and demonstrate how Mix-n-Match calibration strategies (i.e., ensemble and composition) can help achieve remarkably better data-efficiency and expressive power while provably maintaining the classification accuracy of the original classifier. Mix-n-Match strategies are generic in the sense that they can be used to improve the performance of any off-the-shelf calibrator. We also reveal potential issues in standard evaluation practices. Popular approaches (e.g., histogram-based expected calibration error (ECE)) may provide misleading results especially in small-data regime. Therefore, we propose an alternative data-efficient kernel density-based estimator for a reliable evaluation of the calibration performance and prove its asymptotically unbiasedness and consistency. Our approaches outperform state-of-the-art solutions on both the calibration as well as the evaluation tasks in most of the experimental settings. Our codes are available at https://github.com/zhang64- llnl/Mix-n-Match-Calibration.",[],[],"['Jize Zhang', 'Bhavya Kailkhura', 'T. Yong-Jin Han']","['Lawrence Livermore National Laboratories Livermore, CA', 'Lawrence Livermore National Laboratories Livermore, CA', 'Lawrence Livermore National Laboratories Livermore, CA']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525787,Fairness & Bias,Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks,"Adversarial training yields robust models against a specific threat model, e.g., $L_\infty$ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other $L_p$ norms, or larger perturbations. Our confidence-calibrated adversarial training (CCAT) tackles this problem by biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on $L_\infty$ adversarial examples, increases robustness against larger $L_\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing confidence. For each threat model, we use $7$ attacks with up to $50$ restarts and $5000$ iterations and report worst-case robust test error, extended to our confidence-thresholded setting, across all attacks.",[],[],"['David Stutz', 'Matthias Hein', 'Bernt Schiele']","['Max Planck Institute for Informatics, Saarbrücken', 'University of Tübingen, Tübingen', 'Max Planck Institute for Informatics, Saarbrücken']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3524984,Fairness & Bias,Constant Curvature Graph Convolutional Networks,"Interest has been rising lately towards methods representing data in non-Euclidean spaces, e.g. hyperbolic or spherical that provide specific inductive biases useful for certain real-world data properties, e.g. scale-free, hierarchical or cyclical. However, the popular graph neural networks are currently limited in modeling data only via Euclidean geometry and associated vector space operations. Here, we bridge this gap by proposing mathematically grounded generalizations of graph convolutional networks (GCN) to (products of) constant curvature spaces. We do this by i) introducing a unified formalism permitting a differentiable interpolation between all geometries of constant curvature irrespective of their sign, ii) leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Our class of models smoothly recover their Euclidean counterparts when the curvature goes to zero from either side. Empirically, we outperform Euclidean GCNs in the tasks of node classification and distortion minimization for symbolic data exhibiting non-Euclidean behavior, according to their discrete curvature.",[],[],"['Gregor Bachmann', 'Gary Bécigneul', 'Octavian-Eugen Ganea']","['Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich', 'Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525279,Fairness & Bias,The continuous categorical: a novel simplex-valued exponential family,"Simplex-valued data appear throughout statistics and machine learning, for example in the context of transfer learning and compression of deep networks. Existing models for this class of data rely on the Dirichlet distribution or other related loss functions; here we show these standard choices suffer systematically from a number of limitations, including bias and numerical issues that frustrate the use of flexible network models upstream of these distributions. We resolve these limitations by introducing a novel exponential family of distributions for modeling simplex-valued data – the continuous categorical, which arises as a nontrivial multivariate generalization of the recently discovered continuous Bernoulli. Unlike the Dirichlet and other typical choices, the continuous categorical results in a well-behaved probabilistic loss function that produces unbiased estimators, while preserving the mathematical simplicity of the Dirichlet. As well as exploring its theoretical properties, we introduce sampling methods for this distribution that are amenable to the reparameterization trick, and evaluate their performance. Lastly, we demonstrate that the continuous categorical outperforms standard choices empirically, across a simulation study, an applied example on multi-party elections, and a neural network compression task.",[],[],"['Elliott Gordon-Rodriguez', 'Gabriel Loaiza-Ganem', 'John P. Cunningham']","['Department of Statistics, Columbia University', 'Layer 6 AI', 'Department of Statistics, Columbia University']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525114,Fairness & Bias,Fair Generative Modeling via Weak Supervision,"Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6% over baselines for comparable image generation using generative adversarial networks.",[],[],"['Kristy Choi', 'Aditya Grover', 'Trisha Singh', 'Rui Shu', 'Stefano Ermon']","['Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University', 'Department of Statistics, Stanford University', 'Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525841,Fairness & Bias,Born-again Tree Ensembles,"The use of machine learning algorithms in finance, medicine, and criminal justice can deeply impact human lives. As a consequence, research into interpretable machine learning has rapidly grown in an attempt to better control and fix possible sources of mistakes and biases. Tree ensembles, in particular, offer a good prediction quality in various domains, but the concurrent use of multiple trees reduces the interpretability of the ensemble. Against this background, we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble in its entire feature space. To find such a tree, we develop a dynamic-programming based algorithm that exploits sophisticated pruning and bounding rules to reduce the number of recursive calls. This algorithm generates optimal born-again trees for many datasets of practical interest, leading to classifiers which are typically simpler and more interpretable without any other form of compromise.",[],[],"['Thibaut Vidal', 'Maximilian Schiffer']","['Department of Computer Science, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro', 'TUM School of Management, Technical University of Munich, Munich']","['Brazil', 'Germany']"
https://dl.acm.org/doi/10.5555/3524938.3525453,Fairness & Bias,Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics,"The overestimation bias is one of the major impediments to accurate off-policy learning. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method---Truncated Quantile Critics, TQC,---blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. Distributional representation and truncation allow for arbitrary granular overestimation control, while ensembling provides additional score improvements. TQC outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25% improvement on the most challenging Humanoid environment.",[],[],"['Arsenii Kuznetsov', 'Pavel Shvechikov', 'Alexander Grishin', 'Dmitry Vetrov']","['Samsung AI center, Moscow, Russia', 'Samsung AI center, Moscow, Russia and National Research University Higher School of Economics, Moscow, Russia', 'Samsung AI center, Moscow, Russia and Samsung- HSE Laboratory, National Research University Higher School of Economics, Moscow, Russia', 'Samsung AI center, Moscow, Russia and Samsung- HSE Laboratory, National Research University Higher School of Economics, Moscow, Russia']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525994,Fairness & Bias,Individual Calibration with Randomized Forecasting,"Machine learning applications often require calibrated predictions, e.g. a 90\% credible interval should contain the true outcome 90\% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if and only if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.",[],[],"['Shengjia Zhao', 'Tengyu Ma', 'Stefano Ermon']","['Computer Science Department, Stanford University', 'Computer Science Department, Stanford University', 'Computer Science Department, Stanford University']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525065,Fairness & Bias,Meta-learning with Stochastic Linear Bandits,"We investigate meta-learning procedures in the setting of stochastic linear bandits tasks. The goal is to select a learning algorithm which works well on average over a class of bandits tasks, that are sampled from a task-distribution. Inspired by recent work on learning-to-learn linear regression, we consider a class of bandit algorithms that implement a regularized version of the well-known OFUL algorithm, where the regularization is a square euclidean distance to a bias vector. We first study the benefit of the biased OFUL algorithm in terms of regret minimization. We then propose two strategies to estimate the bias within the learning-to-learn setting. We show both theoretically and experimentally, that when the number of tasks grows and the variance of the task-distribution is small, our strategies have a significant advantage over learning the tasks in isolation.",[],[],"['Leonardo Cella', 'Alessandro Lazaric', 'Massimiliano Pontil']","['University of Milan and Istituto Italiano di Tecnologia', 'Facebook AI Research', 'Istituto Italiano di Tecnologia']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525936,Fairness & Bias,Rethinking Bias-Variance Trade-off for Generalization of Neural Networks,"The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation of this by measuring the bias and variance of neural networks: while the bias is {\em monotonically decreasing} as in the classical theory, the variance is {\em unimodal} or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent in the recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.",[],[],"['Zitong Yang', 'Yaodong Yu', 'Chong You', 'Jacob Steinhardt', 'Yi Ma']","['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley and Department of Statistics, University of California, Berkeley.', 'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525201,Fairness & Bias,Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing,"A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.",[],[],"['Sanghamitra Dutta', 'Dennis Wei', 'Hazar Yueksel', 'Pin-Yu Chen', 'Sijia Liu', 'Kush R. Varshney']","['IBM Research and Department of Electrical and Computer Engineering, Carnegie Mellon University', 'IBM Research', 'IBM Research', 'IBM Research', 'IBM Research', 'IBM Research']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525002,Fairness & Bias,Frequency Bias in Neural Networks for Input of Non-Uniform Density,"Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias -- networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency $\kappa$, convergence at a point $x \in \S^{d-1}$ occurs in time $O(\kappa^d/p(x))$ where $p(x)$ denotes the local density at $x$. Specifically, for data in $\S^1$ we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.",[],[],"['Ronen Basri', 'Meirav Galun', 'Amnon Geifman', 'David Jacobs', 'Yoni Kasten', 'Shira Kritchman']","['Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot', 'Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot', 'Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot', 'Department of Computer Science, Univeristy of Maryland, College Park, MD', 'Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot', 'Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot']","['Israel', 'Israel', 'Israel', None, 'Israel', 'Israel']"
https://dl.acm.org/doi/10.5555/3524938.3524976,Fairness & Bias,Invertible generative models for inverse problems: mitigating representation error and dataset bias,"Trained generative models have shown remarkable performance as priors for inverse problems in imaging -- for example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.  Given a trained generative model, we study the empirical risk formulation of the desired inverse problem under a regularization that promotes high likelihood images, either directly by penalization or algorithmically by initialization. For compressive sensing, invertible priors can yield higher accuracy than sparsity priors across almost all undersampling ratios, and due to their lack of representation error, invertible priors can yield better reconstructions than GAN priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  We additionally compare performance for compressive sensing to unlearned methods, such as the deep decoder, and we establish theoretical bounds on expected recovery error in the case of a linear invertible model.",[],[],"['Muhammad Asim', 'Max Daniels', 'Oscar Leong', 'Ali Ahmed', 'Paul Hand']","['Department of Electrical Engineering, Information Technology University, Lahore', 'Department of Mathematics and Khoury College of Computer Sciences, Northeastern University, Boston, MA', 'Department of Computational and Applied Mathematics, Rice University, Houston, TX', 'Department of Electrical Engineering, Information Technology University, Lahore', 'Department of Mathematics and Khoury College of Computer Sciences, Northeastern University, Boston, MA']","['Pakistan', None, None, 'Pakistan', None]"
https://dl.acm.org/doi/10.5555/3524938.3525226,Fairness & Bias,Learning with Multiple Complementary Labels,"A complementary label (CL) simply indicates an incorrect class of an example, but learning with CLs results in multi-class classifiers that can predict the correct class. Unfortunately, the problem setting only allows a single CL for each example, which notably limits its potential since our labelers may easily identify multiple CLs (MCLs) to one example. In this paper, we propose a novel problem setting to allow MCLs for each example and two ways for learning with MCLs. In the first way, we design two wrappers that decompose MCLs into many single CLs, so that we could use any method for learning with CLs. However, the supervision information that MCLs hold is conceptually diluted after decomposition. Thus, in the second way, we derive an unbiased risk estimator; minimizing it processes each set of MCLs as a whole and possesses an estimation error bound. We further improve the second way into minimizing properly chosen upper bounds. Experiments show that the former way works well for learning with MCLs but the latter is even better.",[],[],"['Lei Feng', 'Takuo Kaneko', 'Bo Han', 'Gang Niu', 'Bo An', 'Masashi Sugiyama']","['School of Computer Science and Engineering, Nanyang Technological University', 'The University of Tokyo and RIKEN Center for Advanced Intelligence Project', 'Department of Computer Science,  Baptist University and RIKEN Center for Advanced Intelligence Projec', 'RIKEN Center for Advanced Intelligence Project', 'School of Computer Science and Engineering, Nanyang Technological University', 'RIKEN Center for Advanced Intelligence Project and The University of Tokyo']","['Singapore', None, 'Hong Kong', None, 'Singapore', None]"
https://dl.acm.org/doi/10.5555/3524938.3525169,Fairness & Bias,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,"Replica exchange Monte Carlo (reMC), also known as parallel tempering, is an important technique for accelerating the convergence of the conventional Markov Chain Monte Carlo (MCMC) algorithms. However, such a method requires the evaluation of the energy function based on the full dataset and is not scalable to big data. The na\""ive implementation of reMC in mini-batch settings introduces large biases, which cannot be directly extended to the stochastic gradient MCMC (SGMCMC), the standard sampling method for simulating from deep neural networks (DNNs). In this paper, we propose an adaptive replica exchange SGMCMC (reSGMCMC) to automatically correct the bias and study the corresponding properties. The analysis implies an acceleration-accuracy trade-off in the numerical discretization of a Markov jump process in a stochastic environment. Empirically, we test the algorithm through extensive experiments on various setups and obtain the state-of-the-art results on CIFAR10, CIFAR100, and SVHN in both supervised learning and semi-supervised learning tasks.",[],[],"['Wei Deng', 'Qi Feng', 'Liyao Gao', 'Faming Liang', 'Guang Lin']","['Purdue University, West Lafayette, IN', 'University of Southern California, Los Angeles, CA', 'Purdue University, West Lafayette, IN', 'Purdue University, West Lafayette, IN', 'Purdue University, West Lafayette, IN']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525374,Fairness & Bias,Debiased Sinkhorn barycenters,"Entropy regularization in optimal transport (OT) has been the driver of many recent interests for Wasserstein metrics and barycenters in machine learning. It allows to keep the appealing geometrical properties of the unregularized Wasserstein distance while having a significantly lower complexity thanks to Sinkhorn's algorithm. However, entropy brings some inherent smoothing bias, resulting for example in blurred barycenters. This side effect has prompted an increasing temptation in the community to settle for a slower algorithm such as log-domain stabilized Sinkhorn which breaks the parallel structure that can be leveraged on GPUs, or even go back to unregularized OT. Here we show how this bias is tightly linked to the reference measure that defines the entropy regularizer and propose debiased Sinkhorn barycenters that preserve the best of worlds: fast Sinkhorn-like iterations without entropy smoothing. Theoretically, we prove that this debiasing is perfect for Gaussian distributions with equal variance. Empirically, we illustrate the reduced blurring and the computational advantage.",[],[],"['Hicham Janati', 'Marco Cuturi', 'Alexandre Gramfort']","['Inria Saclay,  and CREST-ENSAE', 'Google Research, Brain team,  and CREST-ENSAE', 'CREST-ENSAE']","['France', 'France', 'France']"
https://dl.acm.org/doi/10.5555/3524938.3525950,Fairness & Bias,Label-Noise Robust Domain Adaptation,"Domain adaptation aims to correct the classifiers when faced with distribution shift between source (training) and target (test) domains. State-of-the-art domain adaptation methods make use of deep networks to extract domain-invariant representations. However, existing methods assume that all the instances in the source domain are correctly labeled; while in reality, it is unsurprising that we may obtain a source domain with noisy labels. In this paper, we are the first to comprehensively investigate how label noise could adversely affect existing domain adaptation methods in various scenarios. Further, we theoretically prove that there exists a method that can essentially reduce the side-effect of noisy source labels in domain adaptation. Specifically, focusing on the generalized target shift scenario, where both label distribution $P_Y$ and the class-conditional distribution $P_{X|Y}$ can change, we discover that the denoising Conditional Invariant Component (DCIC) framework can provably ensures (1) extracting invariant representations given examples with noisy labels in the source domain and unlabeled examples in the target domain and (2) estimating the label distribution in the target domain with no bias. Experimental results on both synthetic and real-world data verify the effectiveness of the proposed method.",[],[],"['Xiyu Yu', 'Tongliang Liu', 'Mingming Gong', 'Kun Zhang', 'Kayhan Batmanghelich', 'Dacheng Tao']","['Department of Computer Vision Technology (VIS), Baidu Incorporation', 'UBTECH Sydney AI Centre, The University of Sydney', 'School of Mathematics and Statistics, University of Melbourne', 'Department of Biomedical Informatics, University of Pittsburgh', 'Department of Philosophy, Carnegie Mellon University', 'UBTECH Sydney AI Centre, The University of Sydney']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525052,Fairness & Bias,DeBayes: a Bayesian Method for Debiasing Network Embeddings,"As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.",[],[],"['Maarten Buyl', 'Tijl De Bie']","['Department of Electronics and Information Systems, IDLab, Ghent University, Ghent', 'Department of Electronics and Information Systems, IDLab, Ghent University, Ghent']","['Belgium', 'Belgium']"
https://dl.acm.org/doi/10.5555/3524938.3524988,Fairness & Bias,Learning De-biased Representations with Biased Representations,"Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led to interesting advancement, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles), resulting in biased models that fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of a snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. We demonstrate the efficacy of our method across a variety of synthetic and real-world biases; our experiments show that the method discourages models from taking bias shortcuts, resulting in improved generalisation. Source code is available at https://github.com/clovaai/rebias.",[],[],"['Hyojin Bahng', 'Sanghyuk Chun', 'Sangdoo Yun', 'Jaegul Choo', 'Seong Joon Oh']","['Korea University', 'Clova AI Research, NAVER Corp.', 'Clova AI Research, NAVER Corp.', 'Graduate School of AI, KAIST', 'Clova AI Research, NAVER Corp.']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525528,Fairness & Bias,Too Relaxed to Be Fair,"We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.",[],[],"['Michael Lohaus', 'Michaël Perrot', 'Ulrike Von Luxburg']","['University of Tübingen,  and Max Planck Institute for Intelligent Systems, Tübingen', 'Max Planck Institute for Intelligent Systems, Tübingen,  and Univ Lyon, UJM-Saint-Etienne, CNRS, IOGS, LabHC UMR 5516, SAINT-ETIENNE, Franc', 'University of Tübingen,  and Max Planck Institute for Intelligent Systems, Tübingen']","['Germany', 'Germany', 'Germany']"
https://dl.acm.org/doi/10.5555/3524938.3525695,Fairness & Bias,Reverse-engineering deep ReLU networks,"The output of a neural network depends on its architecture and weights in a highly nonlinear way, and it is often assumed that a network's parameters cannot be recovered from its output. Here, we prove that, in fact, it is frequently possible to reconstruct the architecture, weights, and biases of a deep ReLU network by observing only its output. We leverage the fact that every ReLU network defines a piecewise linear function, where the boundaries between linear regions correspond to inputs for which some neuron in the network switches between inactive and active ReLU states. By dissecting the set of region boundaries into components associated with particular neurons, we show both theoretically and empirically that it is possible to recover the weights of neurons and their arrangement within the network, up to isomorphism.",[],[],"['David Rolnick', 'Konrad P. Körding']","['University of Pennsylvania, Philadelphia, PA', 'University of Pennsylvania, Philadelphia, PA']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525314,Fairness & Bias,Training Binary Neural Networks through Learning with Noisy Supervision,"This paper formalizes the binarization operations over neural networks from a learning perspective. In contrast to classical hand crafted rules (\eg hard thresholding) to binarize full-precision neurons, we propose to learn a mapping from full-precision neurons to the target binary ones. Each individual weight entry will not be binarized independently. Instead, they are taken as a whole to accomplish the binarization, just as they work together in generating convolution features. To help the training of the binarization mapping, the full-precision neurons after taking sign operations is regarded as some auxiliary supervision signal, which is noisy but still has valuable guidance.  An unbiased estimator is therefore introduced to mitigate the influence of the supervision noise. Experimental results on benchmark datasets indicate that the proposed binarization technique attains consistent improvements over baselines.",[],[],"['Kai Han', 'Yunhe Wang', 'Yixing Xu', 'Chunjing Xu', 'Enhua Wu', 'Chang Xu']","[""State Key Lab of Computer Science, Institute of Software, CAS & University of Chinese Academy of Sciences and Noah's Ark Lab, Huawei Technologies"", ""Noah's Ark Lab, Huawei Technologies"", ""Noah's Ark Lab, Huawei Technologies"", ""Noah's Ark Lab, Huawei Technologies"", 'State Key Lab of Computer Science, Institute of Software, CAS & University of Chinese Academy of Sciences and University of Macau', 'School of Computer Science, Faculty of Engineering, University of Sydney']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525669,Privacy & Data Governance,Fast and Private Submodular and $k$-Submodular Functions Maximization with Matroid Constraints,"The problem of maximizing nonnegative monotone submodular functions under a certain constraint has been intensively studied in the last decade, and a wide range of efficient approximation algorithms have been developed for this problem. Many machine learning problems, including data summarization and influence maximization, can be naturally modeled as the problem of maximizing monotone submodular functions. However, when such applications involve sensitive data about individuals, their privacy concerns should be addressed. In this paper, we study the problem of maximizing monotone submodular functions subject to matroid constraints in the framework of differential privacy. We provide $(1-\frac{1}{\mathrm{e}})$-approximation algorithm which improves upon the previous results in terms of approximation guarantee. This is done with an almost cubic number of function evaluations in our algorithm. Moreover, we study $k$-submodularity, a natural generalization of submodularity. We give the first $\frac{1}{2}$-approximation algorithm that preserves differential privacy for maximizing monotone $k$-submodular functions subject to matroid constraints. The approximation ratio is asymptotically tight and is obtained with an almost linear number of function evaluations.",[],[],"['Akbar Rafiey', 'Yuichi Yoshida']","['Department of Computing Science, Simon Fraser University, Burnaby', 'National Institute of Informatics, Tokyo']","['Canada', 'Japan']"
https://dl.acm.org/doi/10.5555/3524938.3525003,Privacy & Data Governance,Private Query Release Assisted by Public Data,"We study the problem of differentially private query release assisted by access to public data. In this problem, the goal is to answer a large class $\mathcal{H}$ of statistical queries with error no more than $\alpha$ using a combination of public and private samples. The algorithm is required to satisfy differential privacy only with respect to the private samples. We study the limits of this task in terms of the private and public sample complexities. Our upper and lower bounds on the private sample complexity have matching dependence on the dual VC-dimension of $\mathcal{H}$. For a large category of query classes, our bounds on the public sample complexity have matching dependence on $\alpha$.",[],[],"['Raef Bassily', 'Albert Cheu', 'Shay Moran', 'Aleksandar Nikolov', 'Jonathan Ullman', 'Zhiwei Steven Wu']","['Department of Computer Science & Engineering, The Ohio State University', 'Khoury College of Computer Sciences, Northeastern University', 'Google AI Princeton', 'Department of Computer Science, University of Toronto', 'Khoury College of Computer Sciences, Northeastern University', 'Department of Computer Science and Engineering, University of Minnesota']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525997,Privacy & Data Governance,Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth Expansion,"Datasets containing sensitive information are often sequentially analyzed by many algorithms and, accordingly, a fundamental question in differential privacy is concerned with how the overall privacy bound degrades under composition. To address this question, we introduce a family of analytical and sharp privacy bounds under composition using the Edgeworth expansion in the framework of the recently proposed $f$-differential privacy. In short, whereas the existing composition theorem, for example, relies on the central limit theorem, our new privacy bounds under composition gain improved tightness by leveraging the refined approximation accuracy of the Edgeworth expansion. Our approach is easy to implement and computationally efficient for any number of compositions. The superiority of these new bounds is confirmed by an asymptotic error analysis and an application to quantifying the overall privacy guarantees of noisy stochastic gradient descent used in training private deep neural networks.",[],[],"['Qinqing Zheng', 'Jinshuo Dong', 'Qi Long', 'Weijie Su']","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525181,Privacy & Data Governance,Optimal Differential Privacy Composition for Exponential Mechanisms,"Composition is one of the most important properties of differential privacy (DP), as it allows algorithm designers to build complex private algorithms from DP primitives. We consider precise composition bounds of the overall privacy loss for exponential mechanisms, one of the fundamental classes of mechanisms in DP. Exponential mechanism has also become a fundamental building block in private machine learning, e.g. private PCA and hyper-parameter selection. We give explicit formulations of the optimal privacy loss for both the adaptive and non-adaptive composition of exponential mechanism. For the non-adaptive setting in which each mechanism has the same privacy parameter, we give an efficiently computable formulation of the optimal privacy loss. In the adaptive case, we derive a recursive formula and an efficiently computable upper bound. These precise understandings about the problem lead to a 40\% saving of the privacy budget in a practical application. Furthermore, the algorithm-specific analysis shows a difference in privacy parameters of adaptive and non-adaptive composition, which was widely believed to not exist based on the evidence from general analysis.",[],[],"['Jinshuo Dong', 'David Durfee', 'Ryan Rogers']","['Applied Mathematics and Computational Sciences, University of Pennsylvania and Data Science Applied Research, LinkedIn', 'Data Science Applied Research, LinkedIn', 'Data Science Applied Research, LinkedIn']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525278,Privacy & Data Governance,Differentially Private Set Union,"We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe $U$ of items, possibly of infinite size, and a database $D$ of users. Each user $i$ contributes a subset $W_i \subseteq U$ of items. We want an ($\epsilon$,$\delta$)-differentially private Algorithm which outputs a subset $S \subset \cup_i W_i$ such that the size of $S$ is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, $n$-grams etc., from private text data belonging to users is an instance of the set union problem. In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.",[],[],"['Sivakanth Gopi', 'Pankaj Gulhane', 'Janardhan Kulkarni', 'Judy Hanwen Shen', 'Milad Shokouhi', 'Sergey Yekhanin']","['Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525357,Privacy & Data Governance,InstaHide: Instance-hiding Schemes for Private Distributed Learning,"How can multiple distributed entities train a shared deep net on their private data while protecting data privacy? This paper introduces InstaHide, a simple encryption of training images. Encrypted images can be used in standard deep learning pipelines (PyTorch, Federated Learning etc.) with no additional setup or infrastructure. The encryption has a minor effect on test accuracy (unlike differential privacy).Encryption consists of mixing the image with a set of other images (in the sense of Mixup data augmentation technique (Zhang et al., 2018)) followed by applying a random pixel-wise mask on the mixed image. Other contributions of this paper are: (a) Use of large public dataset of images (e.g. ImageNet) for mixing during encryption; this improves security. (b) Experiments demonstrating effectiveness in protecting privacy against known attacks while preserving model accuracy. (c) Theoretical analysis showing that successfully attacking privacy requires attackers to solve a difficult computational problem. (d) Demonstration that Mixup alone is insecure as (contrary to recent proposals), by showing some efficient attacks. (e) Release of a challenge dataset to allow design of new attacks.",[],[],"['Yangsibo Huang', 'Zhao Song', 'Kai Li', 'Sanjeev Arora']","['Princeton University', 'Princeton University and Institute for Advanced Study', 'Princeton University', 'Princeton University and Institute for Advanced Study']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525826,Privacy & Data Governance,Bayesian Differential Privacy for Machine Learning,"Traditional differential privacy is independent of the data distribution. However, this is not well-matched with the modern machine learning context, where models are trained on specific data. As a result, achieving meaningful privacy guarantees in ML often excessively reduces accuracy. We propose Bayesian differential privacy (BDP), which takes into account the data distribution to provide more practical privacy guarantees. We also derive a general privacy accounting method under BDP, building upon the well-known moments accountant. Our experiments demonstrate that in-distribution samples in classic machine learning datasets, such as MNIST and CIFAR-10, enjoy significantly stronger privacy guarantees than postulated by DP, while models maintain high classification accuracy.",[],[],"['Aleksei Triastcyn', 'Boi Faltings']","['Artificial Intelligence Lab, Ecole Polytechnique Fedérale de Lausanne, Lausanne', 'Artificial Intelligence Lab, Ecole Polytechnique Fedérale de Lausanne, Lausanne']","['Switzerland', 'Switzerland']"
https://dl.acm.org/doi/10.5555/3524938.3525164,Privacy & Data Governance,An end-to-end Differentially Private Latent Dirichlet Allocation Using a Spectral Algorithm,"We provide an end-to-end differentially private spectral algorithm for learning LDA, based on matrix/tensor decompositions, and establish theoretical guarantees on utility/consistency of the estimated model parameters. We represent the spectral algorithm as a computational graph. Noise can be injected along the edges of this graph to obtain differential privacy. We identify subsets of edges, named ``configurations'', such that adding noise to all edges in such a subset guarantees differential privacy of the end-to-end spectral algorithm. We characterize the sensitivity of the edges with respect to the input and thus estimate the amount of noise to be added to each edge for any required privacy level. We then characterize the utility loss  for each configuration as a function of injected noise.  Overall, by combining the sensitivity and utility characterization, we obtain an end-to-end differentially private spectral algorithm for LDA and identify which configurations outperform others under specific regimes. We are the first to achieve utility guarantees under a required level of differential privacy for learning in LDA. We additionally show that our method systematically outperforms differentially private variational inference.",[],[],"['Christopher DeCarolis', 'Mukul Ram', 'Seyed Esmaeili', 'Yu-Xiang Wang', 'Furong Huang']","['Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Computer Science, UC Santa Barbara', 'Department of Computer Science, University of Maryland']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525842,Privacy & Data Governance,Private Reinforcement Learning with PAC and Regret Guarantees,"Motivated by high-stakes decision-making domains like personalized medicine where user information is inherently sensitive, we design privacy preserving exploration policies for episodic reinforcement learning (RL). We first provide a meaningful privacy formulation using the notion of joint differential privacy (JDP)--a strong variant of differential privacy for settings where each user receives their own sets of output (e.g., policy recommendations). We then develop a private optimism-based learning algorithm that simultaneously achieves strong PAC and regret bounds, and enjoys a JDP guarantee. Our algorithm only pays for a moderate privacy cost on exploration: in comparison to the non-private bounds, the privacy parameter only appears in lower-order terms.  Finally, we present lower bounds on sample complexity and regret for reinforcement learning subject to JDP.",[],[],"['Giuseppe Vietri', 'Borja Balle', 'Akshay Krishnamurthy', 'Zhiwei Steven Wu']","['Department of Computer Science and Engineering, University of Minnesota', 'Deepmind', 'Microsoft Research', 'Department of Computer Science and Engineering, University of Minnesota']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3524944,Privacy & Data Governance,Context Aware Local Differential Privacy,"Local differential privacy (LDP) is a strong notion of privacy that often leads to a significant drop in utility. The original definition of LDP assumes that all the elements in the data domain are equally sensitive. However, in many real-life applications, some elements are more sensitive than others. We propose a context-aware framework for LDP that allows the privacy level to vary across the data domain, enabling system designers to place privacy constraints where they matter without paying the cost where they do not. For binary data domains, we provide a universally optimal privatization scheme and highlight its connections to Warner’s randomized response and Mangat’s improved response. Motivated by geo-location and web search applications, for k-ary data domains, we consider two special cases of context-aware LDP: block-structured LDP and high-low LDP. We study minimax discrete distribution estimation under both cases and provide communication-efficient, sample-optimal schemes, and information-theoretic lower bounds. We show, using worst-case analyses and experiments on Gowalla’s 3.6 million check-ins to 43,750 locations, that context-aware LDP achieves a far better accuracy under the same number of samples.",[],[],"['Jayadev Acharya', 'K. A. Bonawitz', 'Peter Kairouz', 'Daniel Ramage', 'Ziteng Sun']","['ECE, Cornell University, Ithaca, New York', 'Google, Seattle', 'Google, Seattle', 'Google, Seattle', 'ECE, Cornell University, Ithaca, New York']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525970,Privacy & Data Governance,Privately Learning Markov Random Fields,"We consider the problem of learning Markov Random Fields (including the prototypical example, the Ising model) under the constraint of differential privacy.  Our learning goals include both \emph{structure learning}, where we try to estimate the underlying graph structure of the model, as well as the harder goal of \emph{parameter learning}, in which we additionally estimate the parameter on each edge.  We provide algorithms and lower bounds for both problems under a variety of privacy constraints -- namely pure, concentrated, and approximate differential privacy. While non-privately, both learning goals enjoy roughly the same complexity, we show that this is not the case under differential privacy.  In particular, only structure learning under approximate differential privacy maintains the non-private logarithmic dependence on the dimensionality of the data, while a change in either the learning goal or the privacy notion would necessitate a polynomial dependence. As a result, we show that the privacy constraint imposes a strong separation between these two learning problems in the high-dimensional data regime.",[],[],"['Huanyu Zhang', 'Gautam Kamath', 'Janardhan Kulkarni', 'Zhiwei Steven Wu']","['School of Electrical and Computer Engineering, Cornell University', 'Cheriton School of Computer Science, University of Waterloo', 'Microsoft Research Redmond', 'Computer Science & Engineering, University of Minnesota']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525843,Privacy & Data Governance,New Oracle-Efficient Algorithms for Private Synthetic Data Release,"We present three new algorithms for constructing differentially private synthetic data---a sanitized version of a sensitive dataset that approximately preserves the answers to a large collection of statistical queries. All three algorithms are \emph{oracle-efficient} in the sense that they are computationally efficient when given access to an optimization oracle. Such an oracle can be implemented using many existing (non-private) optimization tools such as sophisticated integer program solvers. While the accuracy of the synthetic data is contingent on the oracle's optimization performance, the algorithms satisfy differential privacy even in the worst case. For all three algorithms, we provide theoretical guarantees for both accuracy and privacy. Through empirical evaluation, we demonstrate that our methods scale well with both the dimensionality of the data and the number of queries. Compared to the state-of-the-art method High-Dimensional Matrix Mechanism (McKenna et al.~VLDB 2018), our algorithms provide better accuracy in the large workload and high privacy regime (corresponding to low privacy loss $\eps$).",[],[],"['Giuseppe Vietri', 'Grace Tian', 'Mark Bun', 'Thomas Steinke', 'Zhiwei Steven Wu']","['Department of Computer Science and Engineering, University of Minnesota', 'Harvard University', 'Boston University', 'IBM Research, Almaden', 'Department of Computer Science and Engineering, University of Minnesota']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525748,Security,Learning for Dose Allocation in Adaptive Clinical Trials with Safety Constraints,"Phase I dose-finding trials are increasingly challenging as the relationship between efficacy and toxicity of new compounds (or combination of them) becomes more complex. Despite this, most commonly used methods in practice focus on identifying a Maximum Tolerated Dose (MTD) by learning only from toxicity events. We present a novel adaptive clinical trial methodology, called Safe Efficacy Exploration Dose Allocation (SEEDA), that aims at maximizing the cumulative efficacies while satisfying the toxicity safety constraint with high probability. We evaluate performance objectives that have operational meanings in practical clinical trials, including cumulative efficacy, recommendation/allocation success probabilities, toxicity violation probability, and sample efficiency. An extended SEEDA-Plateau algorithm that is tailored for the increase-then-plateau efficacy behavior of molecularly targeted agents (MTA) is also presented. Through numerical experiments using both synthetic and real-world datasets, we show that SEEDA outperforms state-of-the-art clinical trial designs by finding the optimal dose with higher success rate and fewer patients.",[],[],"['Cong Shen', 'Zhiyang Wang', 'Sofía S. Villar', 'Mihaela Van Der Schaar']","['University of Virginia', 'University of Pennsylvania', 'University of Cambridge', 'University of Cambridge,  and University of California, Los Angele']","[None, None, 'UK', 'UK']"
https://dl.acm.org/doi/10.5555/3524938.3525726,Security,Constrained Markov Decision Processes via Backward Value Functions,"Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world, undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight the computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks.",[],[],"['Harsh Satija', 'Philip Amortila', 'Joelle Pineau']","['Department of Computer Science, McGill University, Montreal,  and Mila Québec AI Institute and Facebook AI Research, Montrea', 'Department of Computer Science, McGill University, Montreal,  and Mila Québec AI Institut', 'Department of Computer Science, McGill University, Montreal,  and Mila Québec AI Institute and Facebook AI Research, Montrea']","['Canada', 'Canada', 'Canada']"
https://dl.acm.org/doi/10.5555/3524938.3525785,Security,Responsive Safety in Reinforcement Learning by PID Lagrangian Methods,"Lagrangian methods are widely used algorithms for constrained optimization problems, but their learning dynamics exhibit oscillations and overshoot which, when applied to safe reinforcement learning, leads to constraint-violating behavior during agent training.  We address this shortcoming by proposing a novel Lagrange multiplier update method that utilizes derivatives of the constraint function.  We take a controls perspective, wherein the traditional Lagrange multiplier update behaves as \emph{integral} control; our terms introduce \emph{proportional} and \emph{derivative} control, achieving favorable learning dynamics through damping and predictive measures.  We apply our PID Lagrangian methods in deep RL, setting a new state of the art in Safety Gym, a safe RL benchmark.  Lastly, we introduce a new method to ease controller tuning by providing invariance to the relative numerical scales of reward and cost.  Our extensive experiments demonstrate improved performance and hyperparameter robustness, while our algorithms remain nearly as simple to derive and implement as the traditional Lagrangian approach.",[],[],"['Adam Stooke', 'Joshua Achiam', 'Pieter Abbeel']","['University of California, Berkeley and OpenAI', 'University of California, Berkeley and OpenAI', 'University of California, Berkeley']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525590,Security,Confidence-Aware Learning for Deep Neural Networks,"Despite the power of deep neural networks for a wide range of tasks, an overconfident prediction issue has limited their practical use in many safety-critical applications. Many recent works have been proposed to mitigate this issue, but most of them require either additional computational costs in training and/or inference phases or customized architectures to output confidence estimates separately. In this paper, we propose a method of training deep neural networks with a novel loss function, named Correctness Ranking Loss, which regularizes class probabilities explicitly to be better confidence estimates in terms of ordinal ranking according to confidence. The proposed method is easy to implement and can be applied to the existing architectures without any modification. Also, it has almost the same computational costs for training as conventional deep classifiers and outputs reliable predictions by a single inference. Extensive experimental results on classification benchmark datasets indicate that the proposed method helps networks to produce well-ranked confidence estimates. We also demonstrate that it is effective for the tasks closely related to confidence estimation, out-of-distribution detection and active learning.",[],[],"['Jooyoung Moon', 'Jihyo Kim', 'Younghak Shin', 'Sangheum Hwang']","['Department of Data Science, Seoul National University of Science and Technology, Seoul, Republic of Korea', 'Department of Data Science, Seoul National University of Science and Technology, Seoul, Republic of Korea', 'LG CNS, Seoul, Republic of Korea', 'Department of Data Science, Seoul National University of Science and Technology, Seoul, Republic of Korea and Department of Industrial & Information Systems Engineering, Seoul National University of Science and Technology, Seoul, Republic of Korea']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525358,Security,Accelerated Stochastic Gradient-free and Projection-free Methods,"In the paper, we propose a class of accelerated stochastic gradient-free and projection-free (a.k.a., zeroth-order Frank-Wolfe) methods to solve the constrained stochastic and finite-sum nonconvex optimization. Specifically, we propose an accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW) method based on the variance reduced technique of SPIDER/SpiderBoost and a novel momentum accelerated technique. Moreover, under some mild conditions, we prove that the Acc-SZOFW has the function query complexity of $O(d\sqrt{n}\epsilon^{-2})$ for finding an $\epsilon$-stationary point in the finite-sum problem, which improves the exiting best result by a factor of $O(\sqrt{n}\epsilon^{-2})$, and has the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem, which improves the exiting best result by a factor of $O(\epsilon^{-1})$. To relax the large batches required in the Acc-SZOFW, we further propose a novel accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW*) based on a new variance reduced technique of STORM, which still reaches the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem without relying on any large batches. In particular, we present an accelerated framework of the Frank-Wolfe methods based on the proposed momentum accelerated technique. The extensive experimental results on black-box adversarial attack and robust black-box classification demonstrate the efficiency of our algorithms.",[],[],"['Feihu Huang', 'Lue Tao', 'Songcan Chen']","['College of Computer Science & Technology, Nanjing University of Aeronautics and Astronautics, Nanjing,  and MIIT Key Laboratory of Pattern Analysis & Machine Intelligenc', 'College of Computer Science & Technology, Nanjing University of Aeronautics and Astronautics, Nanjing,  and MIIT Key Laboratory of Pattern Analysis & Machine Intelligenc', 'College of Computer Science & Technology, Nanjing University of Aeronautics and Astronautics, Nanjing,  and MIIT Key Laboratory of Pattern Analysis & Machine Intelligenc']","['China', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3524938.3525929,Security,Randomized Smoothing of All Shapes and Sizes,"Randomized smoothing is the current state-of-the-art defense with provable robustness against $\ell_2$ adversarial attacks. Many works have devised new randomized smoothing schemes for other metrics, such as $\ell_1$ or $\ell_\infty$; however, substantial effort was needed to derive such new guarantees. This begs the question: can we find a general theory for randomized smoothing? We propose a novel framework for devising and analyzing randomized smoothing schemes, and validate its effectiveness in practice. Our theoretical contributions are: (1) we show that for an appropriate notion of ""optimal"", the optimal smoothing distributions for any ""nice"" norms have level sets given by the norm's *Wulff Crystal*; (2) we propose two novel and complementary methods for deriving provably robust radii for any smoothing distribution; and, (3) we show fundamental limits to current randomized smoothing techniques via the theory of *Banach space cotypes*. By combining (1) and (2), we significantly improve the state-of-the-art certified accuracy in $\ell_1$ on standard datasets. Meanwhile, we show using (3) that with only label statistics under random input perturbations, randomized smoothing cannot achieve nontrivial certified accuracy against perturbations of $\ell_p$-norm $\Omega(\min(1, d^{\frac{1}{p} - \frac{1}{2}}))$, when the input dimension $d$ is large. We provide code in github.com/tonyduan/rs4a.",[],[],"['Greg Yang', 'Tony Duan', 'J. Edward Hu', 'Hadi Salman', 'Ilya Razenshteyn', 'Jerry Li']","['Microsoft Research AI', 'Microsoft Research AI', 'Microsoft Research AI', 'Microsoft Research AI', 'Microsoft Research AI', 'Microsoft Research AI']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525771,Security,Second-Order Provable Defenses against Adversarial Attacks,"A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {\it any} perturbation of the input with a magnitude smaller than the certificate value, the classification output will provably remain unchanged. Computing exact robustness certificates for neural networks is difficult in general since it requires solving a non-convex optimization. In this paper, we provide computationally-efficient robustness certificates for neural networks with differentiable activation functions in two steps. First, we show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded (globally or locally), we can compute a robustness certificate in the $l_2$ norm efficiently using convex optimization. Second, we derive a computationally-efficient differentiable upper bound on the curvature of a deep network. We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness. Putting these results together leads to our proposed {\bf C}urvature-based {\bf R}obustness {\bf C}ertificate (CRC) and {\bf C}urvature-based {\bf R}obust {\bf T}raining (CRT). Our numerical results show that CRT leads to significantly higher certified robust accuracy compared to interval-bound propagation based training.",[],[],"['Sahil Singla', 'Soheil Feizi']","['Department of Computer Science, University of Maryland, College Park', 'Department of Computer Science, University of Maryland, College Park']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525338,Security,Parameterized Rate-Distortion Stochastic Encoder,"We propose a novel gradient-based tractable approach for the Blahut-Arimoto (BA) algorithm to compute the rate-distortion function where the BA algorithm is fully parameterized. This results in a rich and flexible framework to learn a new class of stochastic encoders, termed PArameterized RAte-DIstortion Stochastic Encoder (PARADISE). The framework can be applied to a wide range of settings from semi-supervised, multi-task to supervised and robust learning. We show that the training objective of PARADISE can be seen as a form of regularization that helps improve generalization. With an emphasis on robust learning we further develop a novel posterior matching objective to encourage smoothness on the loss function and show that PARADISE can significantly improve interpretability as well as robustness to adversarial attacks on the CIFAR-10 and ImageNet datasets. In particular, on the CIFAR-10 dataset, our model reduces standard and adversarial error rates in comparison to the state-of-the-art by 50% and 41%, respectively without the expensive computational cost of adversarial training.",[],[],"['Quan Hoang', 'Trung Le', 'Dinh Phung']","['Department of DSAI, Faculty of Information Technology, Monash University', 'Department of DSAI, Faculty of Information Technology, Monash University', 'Department of DSAI, Faculty of Information Technology, Monash University']","['Australia', 'Australia', 'Australia']"
https://dl.acm.org/doi/10.5555/3524938.3525933,Security,Interpolation between Residual and Non-Residual Networks,"Although ordinary differential equations (ODEs) provide insights for designing network architectures, its relationship with the non-residual convolutional neural networks (CNNs) is still unclear. In this paper, we present a novel ODE model by adding a damping term. It can be shown that the proposed model can recover both a ResNet and a CNN by adjusting an interpolation coefficient. Therefore, the damped ODE model provides a unified framework for the interpretation of residual and non-residual networks. The Lyapunov analysis reveals better stability of the proposed model, and thus yields robustness improvement of the learned networks. Experiments on a number of image classification benchmarks show that the proposed model substantially improves the accuracy of ResNet and ResNeXt over the perturbed inputs from both stochastic noise and adversarial attack methods. Moreover, the loss landscape analysis demonstrates the improved robustness of our method along the attack direction.",[],[],"['Zonghan Yang', 'Yang Liu', 'Chenglong Bao', 'Zuoqiang Shi']","['Institute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University', 'Institute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University', 'Yau Mathematical Sciences Center, Tsinghua University', 'Department of Mathematical Sciences, Tsinghua University']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525158,Security,Adversarial Attacks on Probabilistic Autoregressive Forecasting Models,"We develop an effective generation of adversarial attacks on neural models that output a sequence of probability distributions rather than a sequence of single values. This setting includes the recently proposed deep probabilistic autoregressive forecasting models that estimate the probability distribution of a time series given its past and achieve state-of-the-art results in a diverse set of application domains. The key technical challenge we address is how to effectively differentiate through the Monte-Carlo estimation of statistics of the output sequence joint distribution. Additionally, we extend prior work on probabilistic forecasting to the Bayesian setting which allows conditioning on future observations, instead of only on past observations. We demonstrate that our approach can successfully generate attacks with small input perturbations in two challenging tasks where robust decision making is crucial -- stock market trading and prediction of electricity consumption.",[],[],"['Raphaël Dang-Nhu', 'Gagandeep Singh', 'Pavol Bielik', 'Martin Vechev']","['Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich']","['Switzerland', 'Switzerland', 'Switzerland', 'Switzerland']"
https://dl.acm.org/doi/10.5555/3524938.3526000,Security,Robust Graph Representation Learning via Neural Sparsification,"Graph representation learning serves as the core of important prediction tasks, ranging from product recommendation to fraud detection. Real-life graphs usually have complex information in the local neighborhood, where each node is described by a rich set of features and connects to dozens or even hundreds of neighbors. Despite the success of neighborhood aggregation in graph neural networks, task-irrelevant information is mixed into nodes' neighborhood, making learned models suffer from sub-optimal generalization performance. In this paper, we present NeuralSparse, a supervised graph sparsification technique that improves generalization power by learning to remove potentially task-irrelevant edges from input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize sparsification processes, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance. Experimental results on both benchmark and private datasets show that NeuralSparse can yield up to 7.2% improvement in testing accuracy when working with existing graph neural networks on node classification tasks.",[],[],"['Cheng Zheng', 'Bo Zong', 'Wei Cheng', 'Dongjin Song', 'Jingchao Ni', 'Wenchao Yu', 'Haifeng Chen', 'Wei Wang']","['Department of Computer Science, University of California, Los Angeles, CA', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'Department of Computer Science, University of California, Los Angeles, CA']","[None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525700,Security,Certified Robustness to Label-Flipping Attacks via Randomized Smoothing,"Machine learning algorithms are known to be susceptible to data poisoning attacks, where an adversary manipulates the training data to degrade performance of the resulting classifier. In this work, we propose a strategy for building linear classifiers that are certifiably robust against a strong variant of label flipping, where each test example is targeted independently. In other words, for each test point, our classifier includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Our approach leverages randomized smoothing, a technique that has previously been used to guarantee---with high probability---test-time robustness to adversarial manipulation of the input to a classifier. We derive a variant which provides a deterministic, analytical bound, sidestepping the probabilistic certificates that traditionally result from the sampling subprocedure. Further, we obtain these certified bounds with minimal additional runtime complexity over standard classification and no assumptions on the train or test distributions. We generalize our results to the multi-class case, providing the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.",[],[],"['Elan Rosenfeld', 'Ezra Winston', 'Pradeep Ravikumar', 'J. Zico Kolter']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University and Bosch Center for AI']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525984,Security,Attacks Which Do Not Kill Training Make Adversarial Learning Stronger,"Adversarial training based on the minimax formulation is necessary for obtaining adversarial robustness of trained models. However, it is conservative or even pessimistic so that it sometimes hurts the natural generalization. In this paper, we raise a fundamental question—do we have to trade off natural generalization for adversarial robustness? We argue that adversarial training is to employ confident adversarial data for updating the current model. We propose a novel formulation of friendly adversarial training (FAT): rather than employing most adversarial data maximizing the loss, we search for least adversarial data (i.e., friendly adversarial data) minimizing the loss, among the adversarial data that are confidently misclassified. Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively—adversarial robustness can indeed be achieved without compromising the natural generalization.",[],[],"['Jingfeng Zhang', 'Xilie Xu', 'Bo Han', 'Gang Niu', 'Lizhen Cui', 'Masashi Sugiyama', 'Mohan Kankanhalli']","['School of Computing, National University of ', 'Taishan College, Shandong University, Jinan', 'Department of Computer Science, Hong Kong Baptist University, Hong Kong,  and RIKEN Center for Advanced Intelligence Project, Tokyo, Japa', 'RIKEN Center for Advanced Intelligence Project, Tokyo', 'School of Software & Joint SDU-NTU Centre for Artificial Intelligence Research, Shandong University, Jinan', 'RIKEN Center for Advanced Intelligence Project, Tokyo,  and Graduate School of Frontier Sciences, The University of Tokyo, Tokyo', 'School of Computing, National University of ']","['Singapore', 'China', 'China', 'Japan', 'China', 'Japan', 'Singapore']"
https://dl.acm.org/doi/10.5555/3524938.3525901,Security,Adversarial Robustness via Runtime Masking and Cleansing,"Deep neural networks are shown to be vulnerable to adversarial attacks. This motivates robust learning techniques, such as the adversarial training, whose goal is to learn a network that is robust against adversarial attacks.  However, the sample complexity of robust learning can be significantly larger than that of “standard” learning. In this paper, we propose improving the adversarial robustness of a network by leveraging the potentially large test data seen at runtime. We devise a new defense method, called runtime masking and cleansing (RMC), that adapts the network at runtime before making a prediction to dynamically mask network gradients and cleanse the model of the non-robust features inevitably learned during the training process due to the size limit of the training set. We conduct experiments on real-world datasets and the results demonstrate the effectiveness of RMC empirically.",[],[],"['Yi-Hsuan Wu', 'Chia-Hung Yuan', 'Shan-Hung Wu']","['Department of Computer Science, National Tsing Hua University, Taiwan', 'Department of Computer Science, National Tsing Hua University, Taiwan', 'Department of Computer Science, National Tsing Hua University, Taiwan']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525100,Security,On Breaking Deep Generative Model-based Defenses and Beyond,"Deep neural networks have been proven to be vulnerable to the so-called adversarial attacks. Recently there have been efforts to defend such attacks with deep generative models. These defense often predict by inverting the deep generative models rather than simple feedforward propagation. Such defenses are difficult to attack due to obfuscated gradient. In this work, we develop a new gradient approximation attack to break these defenses. The idea is to view the inversion phase as a dynamical system, through which we extract the gradient w.r.t the input by tracing its recent trajectory. An amortized strategy is further developed to accelerate the attack. Experiments show that our attack breaks state-of-the-art defenses (e.g DefenseGAN, ABS) much more effectively than other attacks. Additionally, our empirical results provide insights for understanding the weaknesses of deep generative model-based defenses.",[],[],"['Yanzhi Chen', 'Renjie Xie', 'Zhanxing Zhu']","['School of Informatics, The University of Edinburgh', 'School of Information Engineering, Southeast University', 'School of Mathematical Sciences, Peking University']","['UK', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3524938.3525707,Security,Adversarial Attacks on Copyright Detection Systems,"It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. As proof of concept, we describe a well-known music identification method and implement this system in the form of a neural net. We then attack this system using simple gradient methods and show that it is easily broken with white-box attacks. By scaling these perturbations up, we can create transfer attacks on industrial systems, such as the AudioTag copyright detector and YouTube's Content ID system, using perturbations that are audible but significantly smaller than a random baseline. Our goal is to raise awareness of the threats posed by adversarial examples in this space and to highlight the importance of hardening copyright detection systems to attacks.",[],[],"['Parsa Saadatpanah', 'Ali Shafahi', 'Tom Goldstein']","['University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525539,Security,Adversarial Nonnegative Matrix Factorization,"Nonnegative Matrix Factorization (NMF) has become an increasingly important research topic in machine learning. Despite all the practical success, most of existing NMF models are still vulnerable to adversarial attacks. To overcome this limitation, we propose a novel Adversarial NMF (ANMF) approach in which an adversary can exercise some control over the perturbed data generation process. Different from the traditional NMF models which focus on  either the regular input or certain types of noise, our model considers potential test adversaries that are beyond the pre-defined constraints, which can cope with various noises (or perturbations). We formulate the proposed model as a bilevel optimization problem and use Alternating Direction Method of Multipliers (ADMM) to solve it with convergence analysis. Theoretically, the robustness analysis of ANMF is established under mild conditions dedicating asymptotically unbiased prediction. Extensive experiments verify that ANMF is robust to a broad categories of perturbations, and achieves state-of-the-art performances on distinct real-world benchmark datasets.",[],[],"['Lei Luo', 'Yanfu Zhang', 'Heng Huang']","['JD Finance America Corporation, Mountain View, CA and Department of Electrical and Computer Engineering, University of Pittsburgh, PA', 'Department of Electrical and Computer Engineering, University of Pittsburgh, PA', 'JD Finance America Corporation, Mountain View, CA and Department of Electrical and Computer Engineering, University of Pittsburgh, PA']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525979,Security,Adaptive Reward-Poisoning Attacks against Reinforcement Learning,"In reward-poisoning attacks against reinforcement learning (RL), an attacker can perturb the environment reward $r_t$ into $r_t+\delta_t$ at each step, with the goal of forcing the RL agent to learn a nefarious policy. We categorize such attacks by the infinity-norm constraint on $\delta_t$: We provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe; we provide a corresponding upper threshold above which the attack is feasible. Feasible attacks can be further categorized as non-adaptive where $\delta_t$ depends only on $(s_t,a_t, s_{t+1})$, or adaptive where $\delta_t$ depends further on the RL agent's learning process at time $t$. Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$, whereas non-adaptive attacks require exponential steps. We provide a constructive proof that a Fast Adaptive Attack strategy achieves the polynomial rate. Finally, we show that empirically an attacker can find effective reward-poisoning attacks using state-of-the-art deep RL techniques.",[],[],"['Xuezhou Zhang', 'Yuzhe Ma', 'Adish Singla', 'Xiaojin Zhu']","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'Max Planck Institute for Software Systems', 'University of Wisconsin-Madison']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525548,Security,Adversarial Neural Pruning with Latent Vulnerability Suppression,"Despite the remarkable performance of deep neural networks on various computer vision tasks, they are known to be susceptible to adversarial perturbations, which makes it challenging to deploy them in real-world safety-critical applications. In this paper, we conjecture that the leading cause of adversarial vulnerability is the distortion in the latent feature space, and provide methods to suppress them effectively. Explicitly, we define \emph{vulnerability} for each latent feature and then propose a new loss for adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to minimize the feature-level vulnerability during training. We further propose a Bayesian framework to prune features with high vulnerability to reduce both vulnerability and loss on adversarial samples. We validate our \emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)} method on multiple benchmark datasets, on which it not only obtains state-of-the-art adversarial robustness but also improves the performance on clean examples, using only a fraction of the parameters used by the full network. Further qualitative analysis suggests that the improvements come from the suppression of feature-level vulnerability.",[],[],"['Divyam Madaan', 'Jinwoo Shin', 'Sung Ju Hwang']","['School of Computing, KAIST, South Korea', 'School of Electrical Engineering, KAIST, South Korea and Graduate School of AI, KAIST, South Korea', 'School of Computing, KAIST, South Korea and Graduate School of AI, KAIST, South Korea and AITRICS, South Korea']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525554,Security,Adversarial Robustness Against the Union of Multiple Perturbation Models,"Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers. While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks. However, these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union. In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against $\ell_\infty$, $\ell_2$, and $\ell_1$ attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 46.1% against the union of ($\ell_\infty$, $\ell_2$, $\ell_1$) perturbations with radius  = (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6% accuracy.",[],[],"['Pratyush Maini', 'Eric Wong', 'J. Zico Kolter']","['Department of Computer Science and Engineering, IIT Delhi', 'Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania', 'Computer Science Department, Carnegie Mellon University, Pittsburgh, Pennsylvania and Bosch Center for Artificial Intelligence, Pittsburgh, Pennsylvania']","['India', None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525094,Security,More Data Can Expand The Generalization Gap Between Adversarially Robust and Standard Models,"Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under $\ell_\infty$ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.",[],[],"['Lin Chen', 'Yifei Min', 'Mingrui Zhang', 'Amin Karbasi']","['Department of Electrical Engineering, Yale University', 'Department of Statistics and Data Science, Yale University', 'Department of Statistics and Data Science, Yale University', 'Department of Electrical Engineering, Yale University']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3524979,Security,Adversarial Learning Guarantees for Linear Hypotheses and Neural Networks,"Adversarial or test time robustness measures the susceptibility of a classifier to perturbations to the test input. While there has been a flurry of recent work on designing defenses against such perturbations, the theory of adversarial robustness is not well understood. In order to make progress on this, we focus on the problem of understanding generalization in adversarial settings, via the lens of Rademacher complexity. We give upper and lower bounds for the adversarial empirical Rademacher complexity of linear hypotheses with adversarial perturbations measured in $l_r$-norm for an arbitrary $r \geq 1$. We then extend our analysis to provide Rademacher complexity lower and upper bounds for a single ReLU unit. Finally, we give adversarial Rademacher complexity bounds for feed-forward neural networks with one hidden layer.",[],[],"['Pranjal Awasthi', 'Natalie S. Frank', 'Mehryar Mohri']","['Google Research and Rutgers University', 'Courant Institute of Math. Sciences', 'Google Research and Courant Institute of Math. Sciences']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525846,Security,Safe Reinforcement Learning in Constrained Markov Decision Processes,"Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a step-wise approach for optimizing safety and cumulative reward.  In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openly-available environment named GP-Safety-Gym, and the other simulates Mars surface exploration by using real observation data.",[],[],"['Akifumi Wachi', 'Yanan Sui']","['IBM Research AI, Tokyo', 'Tsinghua University, Beijing']","['Japan', 'China']"
https://dl.acm.org/doi/10.5555/3524938.3525047,Security,Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences,"Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.",[],[],"['Daniel S. Brown', 'Russell Coleman', 'Ravi Srinivasan', 'Scott Niekum']","['Computer Science Department, The University of Texas at Austin', 'Computer Science Department, The University of Texas at Austin and Applied Research Laboratories, The University of Texas at Austin', 'Applied Research Laboratories, The University of Texas at Austin', 'Computer Science Department, The University of Texas at Austin']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525385,Security,Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation,"We present an approach for unsupervised domain adaptation—with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift—from a class-conditioned domain alignment perspective. Current methods for class-conditioned domain alignment aim to explicitly minimize a loss function based on pseudo-label estimations of the target domain. However, these methods suffer from pseudo-label bias in the form of error accumulation. We propose a method that removes the need for explicit optimization of model parameters from pseudo-labels. Instead, we present a sampling-based implicit alignment approach, where the sample selection is implicitly guided by the pseudo-labels. Theoretical analysis reveals the existence of a domain-discriminator shortcut in misaligned classes, which is addressed by the proposed approach to facilitate domain-adversarial learning. Empirical results and ablation studies confirm the effectiveness of the proposed approach, especially in the presence of within-domain class imbalance and between-domain class distribution shift.",[],[],"['Xiang Jiang', 'Qicheng Lao', 'Stan Matwin', 'Mohammad Havaei']","['Imagia,  and Dalhousie University', 'Imagia,  and Mila, Université de Montréal', 'Dalhousie University,  and Polish Academy of Sciences, Polan', 'Imagia']","['Canada', 'Canada', 'Canada', 'Canada']"
https://dl.acm.org/doi/10.5555/3524938.3525787,Security,Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks,"Adversarial training yields robust models against a specific threat model, e.g., $L_\infty$ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other $L_p$ norms, or larger perturbations. Our confidence-calibrated adversarial training (CCAT) tackles this problem by biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on $L_\infty$ adversarial examples, increases robustness against larger $L_\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing confidence. For each threat model, we use $7$ attacks with up to $50$ restarts and $5000$ iterations and report worst-case robust test error, extended to our confidence-thresholded setting, across all attacks.",[],[],"['David Stutz', 'Matthias Hein', 'Bernt Schiele']","['Max Planck Institute for Informatics, Saarbrücken', 'University of Tübingen, Tübingen', 'Max Planck Institute for Informatics, Saarbrücken']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525357,Security,InstaHide: Instance-hiding Schemes for Private Distributed Learning,"How can multiple distributed entities train a shared deep net on their private data while protecting data privacy? This paper introduces InstaHide, a simple encryption of training images. Encrypted images can be used in standard deep learning pipelines (PyTorch, Federated Learning etc.) with no additional setup or infrastructure. The encryption has a minor effect on test accuracy (unlike differential privacy).Encryption consists of mixing the image with a set of other images (in the sense of Mixup data augmentation technique (Zhang et al., 2018)) followed by applying a random pixel-wise mask on the mixed image. Other contributions of this paper are: (a) Use of large public dataset of images (e.g. ImageNet) for mixing during encryption; this improves security. (b) Experiments demonstrating effectiveness in protecting privacy against known attacks while preserving model accuracy. (c) Theoretical analysis showing that successfully attacking privacy requires attackers to solve a difficult computational problem. (d) Demonstration that Mixup alone is insecure as (contrary to recent proposals), by showing some efficient attacks. (e) Release of a challenge dataset to allow design of new attacks.",[],[],"['Yangsibo Huang', 'Zhao Song', 'Kai Li', 'Sanjeev Arora']","['Princeton University', 'Princeton University and Institute for Advanced Study', 'Princeton University', 'Princeton University and Institute for Advanced Study']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525143,Security,Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack,"The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient obfuscation.",[],[],"['Francesco Croce', 'Matthias Hein']","['University of Tübingen', 'University of Tübingen']","['Germany', 'Germany']"
https://dl.acm.org/doi/10.5555/3524938.3525173,Security,Margin-aware Adversarial Domain Adaptation with Optimal Transport,"In this paper, we propose a new theoretical analysis of unsupervised domain adaptation that relates notions of large margin separation, adversarial learning and optimal transport. This analysis generalizes previous work on the subject by providing a bound on the target margin violation rate, thus reflecting a better control of the quality of separation between classes in the target domain than bounding the misclassification rate. The bound also highlights the benefit of a large margin separation on the source domain for adaptation and introduces an optimal transport (OT) based distance between domains that has the virtue of being task-dependent, contrary to other approaches. From the obtained theoretical results, we derive a novel algorithmic solution for domain adaptation that introduces a novel shallow OT-based adversarial approach and outperforms other OT-based DA baselines on several simulated and real-world classification tasks.",[],[],"['Sofien Dhouib', 'Ievgen Redko', 'Carole Lartizien']","['Univ Lyon, INSA-Lyon, Université Claude Bernard Lyon 1, UJM-Saint Etienne, CNRS, Inserm, CREATIS UMR 5220, U1206, LYON', 'Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d Optique Graduate School Laboratoire Hubert Curien UMR 5516, Saint-Etienne', 'Univ Lyon, INSA-Lyon, Université Claude Bernard Lyon 1, UJM-Saint Etienne, CNRS, Inserm, CREATIS UMR 5220, U1206, LYON']","['France', 'France', 'France']"
https://dl.acm.org/doi/10.5555/3524938.3525673,Security,Improving Robustness of Deep-Learning-Based Image Reconstruction,"Deep-learning-based methods for various applications have been shown vulnerable to adversarial examples.  Here we address the use of deep-learning networks as inverse problem solvers, which has generated much excitement and even adoption efforts by the main equipment vendors for medical imaging including computed tomography (CT) and MRI. However, the recent demonstration that such networks suffer from a similar vulnerability to adversarial attacks potentially undermines their future.  We propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. To this end, we introduce an auxiliary net-work to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we argue that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of in the signal-space used in previous work. We show for a linear reconstruction scheme that our min-max formulation results in a singular-value filter regularized solution, which suppresses the effect of adversarial examples.  Numerical experiments using the proposed min-max scheme confirm convergence to this solution.  We complement the theory by experiments on non-linear Compressive Sensing(CS) reconstruction by a deep neural network on two standard datasets, and, using anonymized clinical data, on a state-of-the-art published algorithm for low-dose x-ray CT reconstruction. We show a significant improvement in robustness over other methods for deep network-based reconstruction, by using the proposed approach.",[],[],"['Ankit Raj', 'Yoram Bresler', 'Bo Li']","['Coordinated Science Laboratory and Department of Electrical and Computer Engineering, University of Illinois at Urbana- Champaign', 'Coordinated Science Laboratory and Department of Electrical and Computer Engineering, University of Illinois at Urbana- Champaign', 'Department of Computer Science, UIUC']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525022,Security,Adversarial Robustness for Code,"Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code including -- finding and fixing bugs, code completion, decompilation, malware detection, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work, we explore this issue by: (i) instantiating adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks, and (iii) developing a set of novel techniques that enable training robust and accurate models of code.",[],[],"['Pavol Bielik', 'Martin Vechev']","['Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich']","['Switzerland', 'Switzerland']"
https://dl.acm.org/doi/10.5555/3524938.3525772,Security,FormulaZero: Distributionally Robust Online Adaptation via Offline Population Synthesis,"Balancing performance and safety is crucial to deploying autonomous vehicles in multi-agent environments. In particular, autonomous racing is a domain that penalizes safe but conservative policies, highlighting the need for robust, adaptive strategies. Current approaches either make simplifying assumptions about other agents or lack robust mechanisms for online adaptation. This work makes algorithmic contributions to both challenges. First, to generate a realistic, diverse set of opponents, we develop a novel method for self-play based on replica-exchange Markov chain Monte Carlo. Second, we propose a distributionally robust bandit optimization procedure that adaptively adjusts risk aversion relative to uncertainty in beliefs about opponents’ behaviors. We rigorously quantify the tradeoffs in performance and robustness when approximating these computations in real-time motion-planning, and we demonstrate our methods experimentally on autonomous vehicles that achieve scaled speeds comparable to Formula One racecars.",[],[],"['Aman Sinha', ""Matthew O'Kelly"", 'Hongrui Zheng', 'Rahul Mangharam', 'John Duchi', 'Russ Tedrake']","['Stanford University, Stanford, CA', 'University of Pennsylvania, Philadelphia, PA', 'University of Pennsylvania, Philadelphia, PA', 'University of Pennsylvania, Philadelphia, PA', 'Stanford University, Stanford, CA', 'Massachusetts Institute of Technology, Cambridge, Massachusetts']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525422,Security,Entropy Minimization In Emergent Languages,"There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel.  We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent's inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.",[],[],"['Eugene Kharitonov', 'Rahma Chaabouni', 'Diane Bouchacourt', 'Marco Baroni']","['Facebook AI Research, Paris', 'Facebook AI Research, Paris,  and Cognitive Machine Learning (ENS, EHESS, SL - CNRS, INRIA', 'Facebook AI Research, Paris', 'Facebook AI Research, Paris, France and Catalan Institute for Research and Advanced Studies, Barcelona']","['France', 'France', 'France', 'Spain']"