link,category,title,abstract,keywords,ccs_concepts,author_names,affiliations,author_countries
https://iclr.cc/virtual/2023/poster/11509,Transparency & Explainability,Expressive Monotonic Neural Networks,"The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce monotonic dependence, and is highly expressive. We show how the algorithm is used to train powerful, robust, and interpretable discriminators that achieve competitive performance compared to current state-of-the-art methods across various benchmarks, from social applications to the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider.","['Social Aspects of Machine Learning', 'lipschitz', 'monotonic']",[],"['Niklas Nolte', 'Ouail Kitouni', 'Mike Williams']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11585,Transparency & Explainability,Jointly Learning Visual and Auditory Speech Representations from Raw Data,"We present RAVEn, a self-supervised multi-modal approach to jointly learn visual and auditory speech representations. Our pre-training objective involves encoding masked inputs, and then predicting contextualised targets generated by slowly-evolving momentum encoders. Driven by the inherent differences between video and audio, our design is asymmetric w.r.t. the two modalities' pretext tasks: Whereas the auditory stream predicts both the visual and auditory targets, the visual one predicts only the auditory targets. We observe strong results in low- and high-resource labelled data settings when fine-tuning the visual and auditory encoders resulting from a single pre-training stage, in which the encoders are jointly trained. Notably, RAVEn surpasses all self-supervised methods on visual speech recognition (VSR) on LRS3, and combining RAVEn with self-training using only 30 hours of labelled data even outperforms a recent semi-supervised method trained on 90,000 hours of non-public data. At the same time, we achieve state-of-the-art results in the LRS3 low-resource setting for auditory speech recognition (as well as for VSR). Our findings point to the viability of learning powerful speech representations entirely from raw video and audio, i.e., without relying on handcrafted features. Code and models are available at https://github.com/ahaliassos/raven.","['Unsupervised and Self-supervised learning', 'lipreading', 'self-supervised learning', 'speech recognition']",[],"['Alexandros Haliassos', 'Pingchuan Ma', 'Rodrigo Mira', 'Stavros Petridis', 'Maja Pantic']","['Imperial College London', 'Facebook', 'Meta', 'Facebook', 'Facebook']",[]
https://iclr.cc/virtual/2023/poster/11477,Transparency & Explainability,Interpretable Debiasing of Vectorized Language Representations with Iterative Orthogonalization,"We propose a new mechanism to augment a word vector embedding representation that offers improved bias removal while retaining the key information—resulting in improved interpretability of the representation. Rather than removing the information associated with a concept that may induce bias, our proposed method identifies two concept subspaces and makes them orthogonal. The resulting representation has these two concepts uncorrelated. Moreover, because they are orthogonal, one can simply apply a rotation on the basis of the representation so that the resulting subspace corresponds with coordinates. This explicit encoding of concepts to coordinates works because they have been made fully orthogonal, which previous approaches do not achieve. Furthermore, we show that this can be extended to multiple subspaces. As a result, one can choose a subset of concepts to be represented transparently and explicitly, while the others are retained in the mixed but extremely expressive format of the representation.","['Deep Learning and representational learning', 'pre-trained contextualized embeddings', 'ethics', 'static embeddings', 'natural language processing', 'bias', 'debiasing', 'fairness']",[],"['Prince Osei Aboagye', 'Yan Zheng', 'Jack Shunn', 'Chin-Chia Michael Yeh', 'Junpeng Wang', 'Zhongfang Zhuang', 'Huiyuan Chen', 'Liang Wang', 'Wei Zhang', 'Jeff M. Phillips']","['University of Utah', 'VISA', 'University of Utah', 'VISA', 'VISA', 'VISA', 'VISA', 'VISA', 'VISA', 'University of Utah']",[]
https://iclr.cc/virtual/2023/poster/11080,Transparency & Explainability,HiT-MDP: Learning the SMDP option framework on MDPs with Hidden Temporal Embeddings,"The standard option framework is developed on the Semi-Markov Decision Process (SMDP) which is unstable to optimize and sample inefficient. To this end, we propose the Hidden Temporal MDP (HiT-MDP) and prove that the option-induced HiT-MDP is homomorphic equivalent to the option-induced SMDP. A novel transformer-based framework is introduced to learn options' embedding vectors (rather than conventional option tuples) on HiT-MDPs. We then derive a stable and sample efficient option discovering method under the maximum-entropy policy gradient framework. Extensive experiments on challenging Mujoco environments demonstrate HiT-MDP's efficiency and effectiveness: under widely used configurations, HiT-MDP achieves competitive, if not better, performance compared to the state-of-the-art baselines on all finite horizon and transfer learning environments. Moreover, HiT-MDP significantly outperforms all baselines on infinite horizon environments while exhibiting smaller variance, faster convergence, and better interpretability. Our work potentially sheds light on the theoretical ground of extending the option framework into a large-scale foundation model.","['Reinforcement Learning', 'reinforcement learning', 'markov decision process', 'Hiearchical Reinforcement Learning']",[],"['Chang Li', 'Dongjin Song', 'Dacheng Tao']","['University of Sydney', 'University of Connecticut', 'University of Sydney']",[]
https://iclr.cc/virtual/2023/poster/11475,Transparency & Explainability,Continuous-time identification of dynamic state-space models by deep subspace encoding,"Continuous-time (CT) modeling has proven to provide improved sample efficiency and interpretability in learning the dynamical behavior of physical systems compared to discrete-time (DT) models. However, even with numerous recent developments, the CT nonlinear state-space (NL-SS) model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, latent states, and general robustness. This paper presents a novel estimation method that addresses all these aspects and that can obtain state-of-the-art results on multiple benchmarks with compact fully connected neural networks capturing the CT dynamics. The proposed estimation method called the subspace encoder approach (SUBNET) ascertains these results by efficiently approximating the complete simulation loss by evaluating short simulations on subsections of the data, by using an encoder function to estimate the initial state for each subsection and a novel state-derivative normalization to ensure stability and good numerical conditioning of the training process. We prove that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and we show that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.","['Deep Learning and representational learning', 'Artificial Neural Networks', 'State-space', 'Continuous-time']",[],"['Gerben I. Beintema', 'Maarten Schoukens', 'Roland Toth']","['Eindhoven University of Technology', 'Eindhoven University of Technology', 'Delft University of Technology']",[]
https://iclr.cc/virtual/2023/poster/11150,Transparency & Explainability,AE-FLOW: Autoencoders with Normalizing Flows  for  Medical Images Anomaly Detection ,"Anomaly detection from medical images is an important task for clinical screening and diagnosis. In general, a large dataset of normal images are available while only few abnormal images can be collected in clinical practice. By mimicking the diagnosis process of radiologists, we attempt to tackle this problem by learning a tractable distribution of normal images and identify anomalies by differentiating the original image and the reconstructed normal image. More specifically, we propose a normalizing flow-based autoencoder for an efficient and tractable representation of normal medical images. The anomaly score consists of the likelihood originated from the normalizing  flow  and the reconstruction error of the autoencoder, which allows to identify the abnormality and provide an interpretability at both image and pixel levels. Experimental evaluation on two  medical images datasets showed that the proposed model outperformed the other approaches by a large margin, which validated the  effectiveness and robustness of the proposed method.","['Applications', 'Normalizing Flow', 'anomaly detection', 'Auto-encoder.']",[],"['Yuzhong Zhao', 'Qiaoqiao Ding', 'Xiaoqun Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University']",[]
https://iclr.cc/virtual/2023/poster/11220,Transparency & Explainability,DAVA: Disentangling Adversarial Variational Autoencoder,"The use of well-disentangled representations offers many advantages for downstream tasks, e.g. an increased sample efficiency, or better interpretability.However, the quality of disentangled interpretations is often highly dependent on the choice of dataset-specific hyperparameters, in particular the regularization strength.To address this issue, we introduce DAVA, a novel training procedure for variational auto-encoders. DAVA completely alleviates the problem of hyperparameter selection.We compare DAVA to models with optimal hyperparameters.Without any hyperparameter tuning, DAVA is competitive on a diverse range of commonly used datasets.Underlying DAVA, we discover a necessary condition for unsupervised disentanglement, which we call PIPE.We demonstrate the ability of PIPE to positively predict the performance of downstream models in abstract reasoning.We also thoroughly investigate correlations with existing supervised and unsupervised metrics. The code is available at https://github.com/besterma/dava.","['Unsupervised and Self-supervised learning', 'varational auto-encoder', 'Disentanglement learning', 'generative adversarial networks', 'curriculum learning']",[],"['Benjamin Estermann', 'Roger Wattenhofer']","['ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11341,Transparency & Explainability,Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small,"Research in mechanistic interpretability seeks to explain behaviors of ML models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task that requires logical reasoning: indirect object identification (IOI). Our explanation encompasses 28 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches including causal interventions and projections.To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior ""in the wild"" in a language model.  We evaluate the reliability of our explanation using three quantitative criteria - faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.","['Social Aspects of Machine Learning', 'transparency', 'interpretability', 'transformers', 'language models', 'Mechanistic Interpretability', 'Science of ML']",[],"['Kevin Ro Wang', 'Alexandre Variengien', 'Arthur Conmy', 'Buck Shlegeris', 'Jacob Steinhardt']","['Redwood Research', 'Conjecture', 'Google DeepMind', 'Redwood Research', 'University of California Berkeley']",[]
https://iclr.cc/virtual/2023/poster/11272,Transparency & Explainability,CUTS: Neural Causal Discovery from Irregular Time-Series Data,"Causal discovery from time-series data has been a central task in machine learning. Recently, Granger causality inference is gaining momentum due to its good explainability and high compatibility with emerging deep neural networks. However, most existing methods assume structured input data and degenerate greatly when encountering data with randomly missing entries or non-uniform sampling frequencies, which hampers their applications in real scenarios. To address this issue, here we present CUTS, a neural Granger causal discovery algorithm to jointly impute unobserved data points and build causal graphs, via plugging in two mutually boosting modules in an iterative framework: (i) Latent data prediction stage: designs a Delayed Supervision Graph Neural Network (DSGNN) to hallucinate and register unstructured data which might be of high dimension and with complex distribution; (ii) Causal graph fitting stage: builds a causal adjacency matrix with imputed data under sparse penalty. Experiments show that CUTS effectively infers causal graphs from irregular time-series data, with significantly superior performance to existing methods. Our approach constitutes a promising step towards applying causal discovery to real applications with non-ideal observations.","['Probabilistic Methods', 'graph neural networks', 'Granger causality', 'neural networks', 'causal discovery', 'time series']",[],"['Yuxiao Cheng', 'Runzhao Yang', 'Tingxiong Xiao', 'Zongren Li', 'Jinli Suo', 'Kunlun He', 'Qionghai Dai']","['Tsinghua University, Tsinghua University', 'Department of Automation, Tsinghua University', 'Tsinghua University, Tsinghua University', 'National University of Defense Technology', 'Tsinghua University, Tsinghua University', 'Chinese PLA General hospital', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/11290,Transparency & Explainability,Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models,"Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge:  entity,  dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC-Large with 770M parameters easily outperforms large language models that are 4-39x larger. In addition, KiC also exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models.","['Applications', 'Semi-parametric language model', 'text-to-text model', 'mixture-of-experts', 'natural language understanding']",[],"['Xiaoman Pan', 'Wenlin Yao', 'Hongming Zhang', 'Dian Yu', 'Dong Yu', 'Jianshu Chen']","['Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab Seattle', 'Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab']",[]
https://iclr.cc/virtual/2023/poster/11248,Transparency & Explainability,Global Explainability of GNNs via Logic Combination of Learned Concepts,"While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned.In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.","['Social Aspects of Machine Learning', 'concept learning', 'explainability', 'graph neural networks']",[],"['Steve Azzolin', 'Antonio Longa', 'Pietro Barbiero', 'Pietro Lio', 'Andrea Passerini']","['University of Trento', 'University of Trento', 'University of Cambridge', 'University of Cambridge', 'University of Trento']",[]
https://iclr.cc/virtual/2023/poster/11701,Transparency & Explainability,Characterizing the Influence of Graph Elements,"Influence function, a method from the robust statistics, measures the changes of model parameters or some functions about model parameters with respect to the removal or modification of training instances. It is an efficient and useful post-hoc method for studying the interpretability of machine learning models without the need of expensive model re-training. Recently, graph convolution networks (GCNs), which operate on graph data, have attracted a great deal of attention. However, there is no preceding research on the influence functions of GCNs to shed light on the effects of removing training nodes/edges from an input graph. Since the nodes/edges in a graph are interdependent in GCNs, it is challenging to derive influence functions for GCNs. To fill this gap, we started with the simple graph convolution (SGC) model that operates on an attributed graph, and formulated an influence function to approximate the changes of model parameters when a node or an edge is removed from an attributed graph. Moreover, we theoretically analyzed the error bound of the estimated influence of removing an edge. We experimentally validated the accuracy and effectiveness of our influence estimation function. In addition, we showed that the influence function of a SGC model could be used to estimate the impact of removing training nodes/edges on the test performance of the SGC without re-training the model. Finally, we demonstrated how to use influence functions to effectively guide the adversarial attacks on GCNs.","['General Machine Learning', 'influence functions', 'Interpretable Machine Learning', 'graph neural networks']",[],"['Zizhang Chen', 'Peizhao Li', 'Hongfu Liu', 'Pengyu Hong']","['Brandeis University', 'Brandeis University', 'Brandeis University', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/11654,Transparency & Explainability,A Differential Geometric View and Explainability of GNN on Evolving Graphs,"Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction. Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution. We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intuitiveness of the proposed method over the state-of-the-art methods.",['Social Aspects of Machine Learning'],[],"['Yazheng Liu', 'Xi Zhang', 'Sihong Xie']","['Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'HKUST-GZ']",[]
https://iclr.cc/virtual/2023/poster/12255,Transparency & Explainability,Bias Propagation in Federated Learning,"We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.","['Algorithmic Bias', 'fairness', 'federated learning']",[],"['Hongyan Chang', 'Reza Shokri']","['National University of Singapore', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/11071,Transparency & Explainability,Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees,"Current abstractive summarization models either suffer from a lack of clear interpretability or provide incomplete rationales by only highlighting parts of the source document. To this end, we propose the Summarization Program (SP), an interpretable modular framework consisting of an (ordered) list of binary trees, each encoding the step-by-step generative process of an abstractive summary sentence from the source document. A Summarization Program contains one root node per summary sentence, and a distinct tree connects each summary sentence (root node) to the document sentences (leaf nodes) from which it is derived, with the connecting nodes containing intermediate generated sentences. Edges represent different modular operations involved in summarization such as sentence fusion, compression, and paraphrasing. We first propose an efficient best-first search method over neural modules, SP-Search that identifies SPs for human summaries by directly optimizing for ROUGE scores. Next, using these programs as automatic supervision, we propose seq2seq models that generate Summarization Programs, which are then executed to obtain final summaries. We demonstrate that SP-Search effectively represents the generative process behind human summaries using modules that are typically faithful to their intended behavior. We also conduct a simulation study to show that Summarization Programs improve the interpretability of summarization models by allowing humans to better simulate model reasoning. Summarization Programs constitute a promising step toward interpretable and modular abstractive summarization, a complex task previously addressed primarily through blackbox end-to-end neural systems.",['Applications'],[],"['Swarnadeep Saha', 'Shiyue Zhang', 'Peter Hase', 'Mohit Bansal']","['Department of Computer Science, University of North Carolina, Chapel Hill', 'Bloomberg', 'University of North Carolina, Chapel Hill', 'University of North Carolina at Chapel Hill']",[]
https://iclr.cc/virtual/2023/poster/12170,Transparency & Explainability,Topology-aware Robust Optimization for Out-of-Distribution Generalization,"Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach, and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.","['Deep Learning and representational learning', 'out-of-distribution generalization', 'distributionally robust optimization']",[],"['Fengchun Qiao', 'Xi Peng']","['University of Delaware', 'University of Delaware']",[]
https://iclr.cc/virtual/2023/poster/11514,Transparency & Explainability,Compositional Law Parsing with Latent Random Functions,"Human cognition has compositionality. We understand a scene by decomposing the scene into different concepts (e.g., shape and position of an object) and learning the respective laws of these concepts, which may be either natural (e.g., laws of motion) or man-made (e.g., laws of a game). The automatic parsing of these laws indicates the model's ability to understand the scene, which makes law parsing play a central role in many visual tasks. This paper proposes a deep latent variable model for Compositional LAw Parsing (CLAP), which achieves the human-like compositionality ability through an encoding-decoding architecture to represent concepts in the scene as latent variables. CLAP employs concept-specific latent random functions instantiated with Neural Processes to capture the law of concepts. Our experimental results demonstrate that CLAP outperforms the baseline methods in multiple visual tasks such as intuitive physics, abstract visual reasoning, and scene representation. The law manipulation experiments illustrate CLAP's interpretability by modifying specific latent random functions on samples. For example, CLAP learns the laws of position-changing and appearance constancy from the moving balls in a scene, making it possible to exchange laws between samples or compose existing laws into novel laws.",['Generative models'],[],"['Fan Shi', 'Bin Li', 'Xiangyang Xue']","['Fudan University', 'Fudan University', 'Fudan University']",[]
https://iclr.cc/virtual/2023/poster/11337,Transparency & Explainability,ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning,"Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality — among other traits — by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.","['Applications', 'step-by-step reasoning', 'evaluation']",[],"['Olga Golovneva', 'Moya Peng Chen', 'Spencer Poff', 'Martin Corredor', 'Luke Zettlemoyer', 'Maryam Fazel-Zarandi', 'Asli Celikyilmaz']","['Facebook', 'Facebook', 'Facebook', 'Georgia Institute of Technology', 'University of Washington', 'FAIR - Meta', 'FAIR ']",[]
https://iclr.cc/virtual/2023/poster/11385,Transparency & Explainability,Progress measures for grokking via mechanistic interpretability,"Neural networks often exhibit emergent behavior in which qualitatively new capabilities that arise from scaling up the number of parameters, training data, or even the number of steps. One approach to understanding emergence is to find the continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. In this work, we argue that progress measures can be found via mechanistic interpretability---that is, by reverse engineering learned models into components and measuring the progress of each component over the course of training. As a case study, we study small transformers trained on a modular arithmetic tasks with emergent grokking behavior. We fully reverse engineer the algorithm learned by these networks, which uses discrete fourier transforms and trigonometric identities to convert addition to rotation about a circle. After confirming the algorithm via ablation, we then use our understanding of the algorithm to define progress measures that precede the grokking phase transition on this task. We see our result as demonstrating both that it is possible to fully reverse engineer trained networks, and that doing so can be invaluable to understanding their training dynamics.","['Social Aspects of Machine Learning', 'interpretability', 'circuits', 'grokking', 'progress measures', 'Mechanistic Interpretability']",[],"['Neel Nanda', 'Lawrence Chan', 'Tom Lieberum', 'Jess Smith', 'Jacob Steinhardt']","['Google DeepMind', 'University of California Berkeley', 'University of Amsterdam', 'University of Florida', 'University of California Berkeley']",[]
https://iclr.cc/virtual/2023/poster/11566,Transparency & Explainability,BSTT: A Bayesian Spatial-Temporal Transformer for Sleep Staging,"Sleep staging is helpful in assessing sleep quality and diagnosing sleep disorders. However, how to adequately capture the temporal and spatial relations of the brain during sleep remains a challenge. In particular, existing methods cannot adaptively infer spatial-temporal relations of the brain under different sleep stages. In this paper, we propose a novel Bayesian spatial-temporal relation inference neural network, named Bayesian spatial-temporal transformer (BSTT), for sleep staging. Our model is able to adaptively infer brain spatial-temporal relations during sleep for spatial-temporal feature modeling through a well-designed Bayesian relation inference component. Meanwhile, our model also includes a spatial transformer for extracting brain spatial features and a temporal transformer for capturing temporal features. Experiments show that our BSTT outperforms state-of-the-art baselines on ISRUC and MASS datasets. In addition, the visual analysis shows that the spatial-temporal relations obtained by BSTT inference have certain interpretability for sleep staging.","['Machine Learning for Sciences', 'Sleep Staging', 'bayesian deep learning', 'Spatial-Temporal Transformer']",[],"['Yuchen Liu', 'Ziyu Jia']","['Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences']",[]
https://iclr.cc/virtual/2023/poster/11038,Transparency & Explainability,Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes,"Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an ""interpretable-by-design"" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.","['Social Aspects of Machine Learning', 'Prototypes', 'deep reinforcement learning', 'User Study', 'Interpretable Machine Learning']",[],"['Eoin M. Kenny', 'Mycal Tucker', 'Julie Shah']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11163,Transparency & Explainability,This Looks Like It Rather Than That: ProtoKNN For Similarity-Based Classifiers,"Among research on the interpretability of deep learning models, the 'this looks like that' framework with ProtoPNet has attracted significant attention. By combining the strong power of deep learning models with the interpretability of case-based inference, ProtoPNet can achieve high accuracy while keeping its reasoning process interpretable. Many methods based on ProtoPNet have emerged to take advantage of this benefit, but despite their practical usefulness, they run into difficulty when utilizing similarity-based classifiers, e.g., in domains where unknown class samples exist. This is because ProtoPNet and its variants adopt the training process specific to linear classifiers, which allows the prototypes to represent useful image features for class recognition. Due to this difficulty, the effectiveness of similarity-based classifiers (e.g., k-nearest neighbor (KNN)) on the 'this looks like that' framework have not been sufficiently examined. To alleviate this problem, we propose ProtoKNN, an extension of ProtoPNet that adopts KNN classifiers. Extensive experiments on multiple open datasets demonstrate that the proposed method can achieve competitive results with a state-of-the-art method.","['Deep Learning and representational learning', 'deep learning', 'Fine-grained Image Classification', 'XAI', 'Inherently Interpretable Model', 'This Looks Like That Framework']",[],"['Yuki Ukai', 'Tsubasa Hirakawa', 'Takayoshi Yamashita', 'Hironobu Fujiyoshi']","['Chubu University', 'Chubu University', 'Chubu University', 'Chubu University']",[]
https://iclr.cc/virtual/2023/poster/11003,Transparency & Explainability,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.","['Applications', 'decision making', 'agent', 'reasoning', 'language model']",[],"['Shunyu Yao', 'Jeffrey Zhao', 'Dian Yu', 'Nan Du', 'Izhak Shafran', 'Karthik R Narasimhan', 'Yuan Cao']","['Princeton University', 'Google Brain', 'Google', 'Apple/AIML', 'Google', 'Princeton University', 'Google DeepMind']",[]
https://iclr.cc/virtual/2023/poster/11463,Transparency & Explainability,Temporal Dependencies in Feature Importance for Time Series Prediction,"Time series data introduces two key challenges for explainability methods: firstly, observations of the same feature over subsequent time steps are not independent, and secondly, the same feature can have varying importance to model predictions over time. In this paper, we propose Windowed Feature Importance in Time (WinIT), a feature removal based explainability approach to address these issues. Unlike existing feature removal explanation methods, WinIT explicitly accounts for the temporal dependence between different observations of the same feature in the construction of its importance score. Furthermore, WinIT captures the varying importance of a feature over time, by summarizing its importance over a window of past time steps. We conduct an extensive empirical study on synthetic and real-world data, compare against a wide range of leading explainability methods, and explore the impact of various evaluation strategies. Our results show that WinIT achieves significant gains over existing methods, with more consistent performance across different evaluation metrics.","['Social Aspects of Machine Learning', 'explainability', 'time series', 'recurrent']",[],"['Kin Kwan Leung', 'Jonathan Smith', 'Saba Zuberi', 'Maksims Volkovs']","['Layer 6 AI', 'Meta', 'Layer 6 AI', 'Layer6 AI']",[]
https://iclr.cc/virtual/2023/poster/11192,Transparency & Explainability,Short-Term Memory Convolutions,"The real-time processing of time series signals is a critical issue for many real-life applications. The idea of real-time processing is especially important in audio domain as the human perception of sound is sensitive to any kind of disturbance in perceived signals, especially the lag between auditory and visual modalities. The rise of deep learning (DL) models complicated the landscape of signal processing. Although they often have superior quality compared to standard DSP methods, this advantage is diminished by higher latency. In this work we propose novel method for minimization of inference time latency and memory consumption, called Short-Term Memory Convolution (STMC) and its transposed counterpart. The main advantage of STMC is the low latency comparable to long short-term memory (LSTM) networks. Furthermore, the training of STMC-based models is faster and more stable as the method is based solely on convolutional neural networks (CNNs). In this study we demonstrate an application of this solution to a U-Net model for a speech separation task and GhostNet model in acoustic scene classification (ASC) task. In case of speech separation we achieved a 5-fold reduction in inference time and a 2-fold reduction in latency without affecting the output quality. The inference time for ASC task was up to 4 times faster while preserving the original accuracy.",['Applications'],[],"['Grzegorz Stefański', 'Krzysztof Arendt', 'Paweł Daniluk', 'Bartłomiej Jasik', 'Artur Szumaczuk']","['Samsung', 'Samsung', 'Samsung', 'Samsung', 'Samsung']",[]
https://iclr.cc/virtual/2023/poster/11733,Transparency & Explainability,Visual Classification via Description from Large Language Models,"Vision-language models such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what the model ``thinks"" it is seeing to make its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.","['Applications', 'prompting', 'Multimodal', 'CLIP', 'GPT-3', 'large language models', 'zero-shot recognition', 'vision-language models']",[],"['Sachit Menon', 'Carl Vondrick']","['Columbia University', 'Columbia University']",[]
https://iclr.cc/virtual/2023/poster/12113,Transparency & Explainability,TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second,"We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods.TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass.TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior.This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures.On the $18$ datasets in the OpenML-CC18 suite that contain up to 1000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to $230\times$ speedup.This increases to a $5\,700\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML.We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.","['Deep Learning and representational learning', 'Bayesian prediction', 'Causal Reasoning', 'tabular data', 'Real-time Machine Learning', 'automl', 'Green AI']",[],"['Noah Hollmann', 'Samuel Müller', 'Katharina Eggensperger', 'Frank Hutter']","['Albert-Ludwigs-Universität Freiburg', 'University of Freiburg, Universität Freiburg', 'Eberhard-Karls-Universität Tübingen', 'University of Freiburg & Bosch']",[]
https://iclr.cc/virtual/2023/poster/12212,Transparency & Explainability,Image as Set of Points,"What is an image, and how to extract latent features? Convolutional Networks (ConvNets) consider an image as organized pixels in a rectangular shape and extract features via convolutional operation in a local region; Vision Transformers (ViTs) treat an image as a sequence of patches and extract features via attention mechanism in a global range. In this work, we introduce a straightforward and promising paradigm for visual representation, which is called Context Clusters. Context clusters (CoCs) view an image as a set of unorganized points and extract features via a simplified clustering algorithm. In detail, each point includes the raw feature (e.g., color) and positional information (e.g., coordinates), and a simplified clustering algorithm is employed to group and extract deep features hierarchically. Our CoCs are convolution- and attention-free, only relying on clustering algorithm for spatial interaction. Owing to the simple design, we show CoCs endow gratifying interpretability via the visualization of the clustering process.  Our CoCs aim at providing a new perspective on image and visual representation, which may enjoy broad applications in different domains and exhibit profound insights. Even though we are not targeting SOTA performance, COCs still achieve comparable or even better performance than ConvNets or ViTs on several benchmarks.","['Deep Learning and representational learning', 'representation', 'clustering', 'Context Cluster', 'Image Processing']",[],"['Xu Ma', 'Yuqian Zhou', 'Huan Wang', 'Can Qin', 'Bin Sun', 'Chang Liu', 'Yun Fu']","['Northeastern University', 'Adobe Systems', 'Northeastern University', 'SalesForce.com', 'Topazlabs LLC.', 'Northeastern University', 'Northeastern University']",[]
https://iclr.cc/virtual/2023/poster/12010,Transparency & Explainability,Visual Imitation Learning with Patch Rewards,"Visual imitation learning enables reinforcement learning agents to learn to behave from expert visual demonstrations such as videos or image sequences, without explicit, well-defined rewards. Previous reseaches either adopt supervised learning techniques or induce simple and coarse scalar rewards from pixels, neglecting the dense information contained in the image demonstrations.In this work, we propose to measure the expertise of various local regions of image samples, or called patches, and recover multi-dimensional patch rewards accordingly. Patch reward is a more precise rewarding characterization that serves as fine-grained expertise measurement and visual explainability tool.Specifically, we present Adversarial Imitation Learning with Patch Rewards (PatchAIL), which employs a patch-based discriminator to measure the expertise of different local parts from given images and provide patch rewards.The patch-based knowledge is also used to regularize the aggregated reward and stabilize the training.We evaluate our method on the standard pixel-based benchmark DeepMind Control Suite. The experiment results have demonstrated that PatchAIL outperforms baseline methods and provides valuable interpretations for visual demonstrations.","['Reinforcement Learning', 'reinforcement learning', 'imitation learning']",[],"['Minghuan Liu', 'Tairan He', 'Weinan Zhang', 'Shuicheng YAN', 'Zhongwen Xu']","['Shanghai Jiao Tong University', 'CMU, Carnegie Mellon University', 'Shanghai Jiao Tong University', 'National University of Singapore', 'DeepMind']",[]
https://iclr.cc/virtual/2023/poster/10885,Transparency & Explainability,SIMPLE: A Gradient Estimator for k-Subset Sampling,"$k$-subset sampling is ubiquitous in machine learning, enabling regularization and interpretability through sparsity. The challenge lies in rendering $k$-subset sampling amenable to end-to-end learning. This has typically involved relaxing the reparameterized samples to allow for backpropagation, but introduces both bias and variance. In this work, we fall back to discrete $k$-subset sampling on the forward pass. This is coupled with using the gradient with respect to the exact marginals, computed efficiently, as a proxy for the true gradient. We show that our gradient estimator exhibits lower bias and variance compared to state-of-the-art estimators. Empirical results show improved performance on learning to explain and sparse models benchmarks. We provide an algorithm for computing the exact ELBO for the $k$-subset distribution, obtaining significantly lower loss compared to state-of-the-art discrete sparse VAEs. All of our algorithms are exact and efficient.",['Deep Learning and representational learning'],[],"['Kareem Ahmed', 'Zhe Zeng', 'Mathias Niepert', 'Guy Van den Broeck']","['University of California, Los Angeles', 'University of California, Los Angeles', 'Universität Stuttgart', 'University of California, Los Angeles']",[]
https://iclr.cc/virtual/2023/poster/10716,Transparency & Explainability,Modeling content creator incentives on algorithm-curated platforms,"Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by modern algorithms including factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices—e.g., non-negative vs. unconstrained factorization—significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models like ours for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools for numerically finding equilibria in exposure games, and illustrate results of an audit on the MovieLens and LastFM datasets. Among else, we find that the strategically produced content exhibits strong dependence between algorithmic exploration and content diversity, and between model expressivity and bias towards gender-based user and creator groups.","['Theory', 'Nash equilibria', 'attention monetizing platforms', 'differentiable games', 'recommenders', 'exposure game', 'producer incentives']",[],"['Jiri Hron', 'Karl Krauth', 'Michael Jordan', 'Niki Kilbertus', 'Sarah Dean']","['Google', 'University of California Berkeley', 'University of California, Berkeley', 'Technische Universität München', 'Cornell University']",[]
https://iclr.cc/virtual/2023/poster/11637,Transparency & Explainability,Searching Lottery Tickets in Graph Neural Networks: A Dual Perspective,"Graph Neural Networks (GNNs) have shown great promise in various graph learning tasks. However, the computational overheads of fitting GNNs to large-scale graphs grow rapidly, posing obstacles to GNNs from scaling up to real-world applications. To tackle this issue, Graph Lottery Ticket (GLT) hypothesis articulates that there always exists a sparse subnetwork/subgraph with admirable performance in GNNs with random initialization. Such a pair of core subgraph and sparse subnetwork (called graph lottery tickets) can be uncovered by iteratively applying a novel sparsification method. While GLT provides new insights for GNN compression, it requires a full pretraining process to obtain graph lottery tickets, which is not universal and friendly to real-world applications. Moreover, the graph sparsification in GLT utilizes sampling techniques, which may result in massive information loss and aggregation failure. In this paper, we explore the searching of graph lottery tickets from a complementary perspective -- transforming a random ticket into a graph lottery ticket, which allows us to more comprehensively explore the relationships between the original network/graph and their sparse counterpart. To achieve this, we propose regularization-based network pruning and hierarchical graph sparsification, leading to our Dual Graph Lottery Ticket (DGLT) framework for a joint sparsification of network and graph. Compared to GLT, our DGLT helps achieve a triple-win situation of graph lottery tickets with high sparsity, admirable performance, and good explainability. More importantly, we rigorously prove that our model can eliminate noise and maintain reliable information in substructures using the graph information bottleneck theory. Extensive experimental results on various graph-related tasks validate the effectiveness of our framework.","['Deep Learning and representational learning', 'Graph information bottleneck', 'Dual Lottery Tickets Hypothesis', 'Lottery Tickets Hypothesis', 'Graph pooling']",[],"['Kun Wang', 'Yuxuan Liang', 'Pengkun Wang', 'Xu Wang', 'Pengfei Gu', 'Junfeng Fang', 'Yang Wang']","['University of Science and Technology of China', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China']",[]
https://iclr.cc/virtual/2023/poster/12080,Transparency & Explainability,Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint,"Deep learning has revolutionized human society, yet the black-box nature of deep neural networks hinders further application to reliability-demanded industries. In the attempt to unpack them, many works observe or impact internal variables to improve the comprehensibility and invertibility of the black-box models. However, existing methods rely on intuitive assumptions and lack mathematical guarantees. To bridge this gap, we introduce Bort, an optimizer for improving model explainability with boundedness and orthogonality constraints on model parameters, derived from the sufficient conditions of model comprehensibility and invertibility. We perform reconstruction and backtracking on the model representations optimized by Bort and observe a clear improvement in model explainability. Based on Bort, we are able to synthesize explainable adversarial samples without additional parameters and training. Surprisingly, we find Bort constantly improves the classification accuracy of various architectures including ResNet and DeiT on MNIST, CIFAR-10, and ImageNet. Code: https://github.com/zbr17/Bort.","['Optimization', 'optimizer.', 'neural network', 'explainable ai']",[],"['Borui Zhang', 'Wenzhao Zheng', 'Jie Zhou', 'Jiwen Lu']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/12100,Transparency & Explainability,Multivariate Time-series Imputation with Disentangled Temporal Representations,"Multivariate time series often faces the problem of missing value. Many time series imputation methods have been developed in the literature. However, these methods all rely on an entangled representation to model dynamics of time series, which may fail to fully exploit the multiple factors (e.g., periodic patterns) contained in the time series. Moreover, the entangled representation usually has no semantic meaning, and thus they often lack interpretability. In addition, many recent models are proposed to deal with the whole time series to capture cross-channel correlations and identify temporal dynamics, but they are not scalable to large-scale datasets. Different from existing approaches, we propose TIDER, a novel matrix factorization-based method with disentangled temporal representations that account for multiple factors, namely trend, seasonality, and local bias, to model complex dynamics. The learned disentanglement makes the imputation process more reliable and offers explainability for imputation results. Moreover, TIDER is scalable to large datasets. Empirical results show that our method not only outperforms existing approaches by notable margins on three real-world datasets, but also scales well to large datasets on which existing deep learning based methods struggle. Disentanglement validation experiments further demonstrate the robustness of our model in obtaining accurate and explainable disentangled components.","['Deep Learning and representational learning', 'multivariate time-series imputation', 'disentangled representation']",[],"['SHUAI LIU', 'Xiucheng Li', 'Gao Cong', 'Yile Chen', 'YUE JIANG']","['School of Computer Science and  Engineering, Nanyang Technological University', 'Harbin Institute of Technology', 'National Technological University', 'Nanyang Technological University', 'Nanyang Technological University']",[]
https://iclr.cc/virtual/2023/poster/12128,Transparency & Explainability,Visual Recognition with Deep Nearest Centroids,"We devise deep nearest centroids (DNC), a conceptually elegant yet surprisingly effective network for large-scale visual recognition, by revisiting Nearest Centroids, one of the most classic and simple classifiers. Current deep models learn the classifier in a fully parametric manner, ignoring the latent data structure and lacking simplicity and explainability. DNC instead conducts nonparametric, case-based reasoning; it utilizes sub-centroids of training samples to describe class distributions and clearly explains the classification as the proximity of test data and the class sub-centroids in the feature space. Due to the distance-based nature, the network output dimensionality is flexible, and all the learnable parameters are only for data embedding. That means all the knowledge learnt for ImageNet classification can be completely transferred for pixel recognition learning, under the ‘pre-training and fine-tuning’ paradigm. Apart from its nested simplicity and intuitive decision-making mechanism, DNC can even possess ad-hoc explainability when the sub-centroids are selected as actual training images that humans can view and inspect. Compared with parametric counterparts, DNC performs better on image classification (CIFAR-10, ImageNet) and greatly boots pixel recognition (ADE20K, Cityscapes), with improved transparency and fewer learnable parameters, using various network architectures (ResNet, Swin) and segmentation models (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights into related fields. Our code is available at https://github.com/ChengHan111/DNC.","['Deep Learning and representational learning', 'Nearest centroids classifier', 'Cased-base reasoning', 'Explainable neural networks', 'Image segmentation', 'image classification']",[],"['Wenguan Wang', 'Cheng Han', 'Tianfei Zhou', 'Dongfang Liu']","['Zhejiang University', 'Rochester Institute of Technology', 'Swiss Federal Institute of Technology', 'Rochester Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/12223,Transparency & Explainability,A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification,"Deep neural networks based on layer-stacking architectures have historically suffered from poor inherent interpretability. Meanwhile, symbolic probabilistic models function with clear interpretability, but how to combine them with neural networks to enhance their performance remains to be explored. In this paper, we try to marry these two systems for text classification via a structured language model. We propose a Symbolic-Neural model that can learn to explicitly predict class labels of text spans from a constituency tree without requiring any access to span-level gold labels. As the structured language model learns to predict constituency trees in a self-supervised manner, only raw texts and sentence-level labels are required as training data, which makes it essentially a general constituent-level self-interpretable classification model. Our experiments demonstrate that our approach could achieve good prediction accuracy in downstream tasks. Meanwhile, the predicted span labels are consistent with human rationales to a certain degree.","['Unsupervised and Self-supervised learning', 'Multiple instance learning', 'structured language model', 'recursive neural network', 'interpretability', 'natural language processing', 'text classification', 'unsupervised learning']",[],"['Xiang Hu', 'XinYu KONG', 'Kewei Tu']","['Alibaba Group', 'Beijing University of Posts and Telecommunications', 'ShanghaiTech University']",[]
https://iclr.cc/virtual/2023/poster/11415,Transparency & Explainability,Post-hoc Concept Bottleneck Models,"Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (``the bottleneck'') and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model ""sees"" in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via concept-level feedback can provide significant performance gains without using data from the target domain or model retraining.","['Social Aspects of Machine Learning', 'model editing', 'concepts', 'Concept Bottleneck Models', 'interpretability']",[],"['Mert Yuksekgonul', 'Maggie Wang', 'James Zou']","['Stanford University', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2023/poster/11389,Transparency & Explainability,Provably Auditing Ordinary Least Squares in Low Dimensions,"Auditing the stability of a machine learning model to small changes in the training procedure is critical for engendering trust in practical applications. For example, a model should not be overly sensitive to removing a small fraction of its training data. However, algorithmically validating this property seems computationally challenging, even for the simplest of models: Ordinary Least Squares (OLS) linear regression. Concretely, recent work defines the stability of a regression as the minimum number of samples that need to be removed so that rerunning the analysis overturns the conclusion (Broderick et al., 2020), specifically meaning that the sign of a particular coefficient of the OLS regressor changes. But the only known approach for estimating this metric, besides the obvious exponential-time algorithm, is a greedy heuristic that may produce severe overestimates and therefore cannot certify stability. We show that stability can be efficiently certified in the low-dimensional regime: when the number of covariates is a constant but the number of samples is large, there are polynomial-time algorithms for estimating (a fractional version of) stability, with provable approximation guarantees. Applying our algorithms to the Boston Housing dataset, we exhibit regression analyses where our estimator outperforms the greedy heuristic, and can successfully certify stability even in the regime where a constant fraction of the samples are dropped.","['Theory', 'stability', 'robustness', 'Linear Regression', 'ordinary least squares']",[],"['Ankur Moitra', 'Dhruv Rohatgi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11807,Transparency & Explainability,Fooling SHAP with Stealthily Biased Sampling,"SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups while remaining undetected. More precisely, experiments performed on real-world datasets showed that our attack could yield up to a 90\% relative decrease in amplitude of the sensitive feature attribution. These results highlight the manipulability of SHAP explanations and encourage auditors to treat them with skepticism.","['Social Aspects of Machine Learning', 'SHAP', 'Stealthily Sampling', 'explainability', 'robustness']",[],"['Gabriel Laberge', 'Ulrich Aïvodji', 'Satoshi Hara', 'Mario Marchand', 'Foutse Khomh']","['École Polytechnique de Montréal', 'École de technologie supérieure, Université du Québec', 'Osaka University', 'Laval university', 'École Polytechnique de Montréal, Université de Montréal']",[]
https://iclr.cc/virtual/2023/poster/11086,Transparency & Explainability,Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics,"Modern machine learning research relies on relatively few carefully curated datasets. Even in these datasets, and typically in `untidy' or raw data, practitioners are faced with significant issues of data quality and diversity which can be prohibitively labor intensive to address. Existing methods for dealing with these challenges tend to make strong assumptions about the particular issues at play, and often require a priori knowledge or metadata such as domain labels. Our work is orthogonal to these methods: we instead focus on providing a unified and efficient framework for Metadata Archaeology -- uncovering and inferring metadata of examples in a dataset. We curate different subsets of data that might exist in a dataset (e.g. mislabeled, atypical, or out-of-distribution examples) using simple transformations, and leverage differences in learning dynamics between these probe suites to infer metadata of interest. Our method is on par with far more sophisticated mitigation methods across different tasks: identifying and correcting mislabeled examples, classifying minority-group samples, prioritizing points relevant for training and enabling scalable human auditing of relevant examples.","['General Machine Learning', 'Loss trajectory', 'Metadata archaeology', 'Data auditing', 'Learning curves']",[],"['Shoaib Ahmed Siddiqui', 'Nitarshan Rajkumar', 'Tegan Maharaj', 'David Krueger', 'Sara Hooker']","['University of Cambridge', 'University of Cambridge', 'Toronto University', 'University of Cambridge', 'Cohere For AI']",[]
https://iclr.cc/virtual/2023/poster/11629,Transparency & Explainability,MultiViz: Towards Visualizing and Understanding Multimodal Models,"The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.","['Social Aspects of Machine Learning', 'visualization', 'representation learning', 'Interpretation', 'multimodal learning']",[],"['Paul Pu Liang', 'Yiwei Lyu', 'Gunjan Chhablani', 'Nihal Jain', 'Zihao Deng', 'Xingbo Wang', 'Louis-Philippe Morency', 'Ruslan Salakhutdinov']","['Carnegie Mellon University', 'University of Michigan - Ann Arbor', 'Georgia Institute of Technology', 'Amazon', 'University of Pennsylvania, University of Pennsylvania', 'Weill Cornell Medicine, Cornell University', 'Carnegie Mellon University', 'Department of Computer Science']",[]
https://iclr.cc/virtual/2023/poster/11224,Transparency & Explainability,Confidential-PROFITT: Confidential PROof of FaIr Training of Trees,"Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees.","['Social Aspects of Machine Learning', 'Zero-Knowledge Proof', 'Audit', 'fairness', 'confidentiality']",[],"['Ali Shahin Shamsabadi', 'Sierra Calanda Wyllie', 'Nicholas Franzese', 'Natalie Dullerud', 'Sébastien Gambs', 'Nicolas Papernot', 'Xiao Wang', 'Adrian Weller']","['Brave Software', 'University of Toronto', 'Northwestern University, Northwestern University', 'Stanford University', 'Université du Québec à Montréal', 'University of Toronto', 'Northwestern University', 'Alan Turing Institute']",[]
https://iclr.cc/virtual/2023/poster/11197,Transparency & Explainability,Understanding Neural Coding on Latent Manifolds by Sharing Features and Dividing Ensembles,"Systems neuroscience relies on two complementary views of neural data, characterized by single neuron tuning curves and analysis of population activity. These two perspectives combine elegantly in neural latent variable models that constrain the relationship between latent variables and neural activity, modeled by simple tuning curve functions. This has recently been demonstrated using Gaussian processes, with applications to realistic and topologically relevant latent manifolds. Those and previous models, however, missed crucial shared coding properties of neural populations. We propose $\textit{feature sharing}$ across neural tuning curves which significantly improves performance and helps optimization. We also propose a solution to the $\textit{ensemble detection}$ problem, where different groups of neurons, i.e., ensembles, can be modulated by different latent manifolds. Achieved through a soft clustering of neurons during training, this allows for the separation of mixed neural populations in an unsupervised manner. These innovations lead to more interpretable models of neural population activity that train well and perform better even on mixtures of complex latent manifolds. Finally, we apply our method on a recently published grid cell dataset, and recover distinct ensembles, infer toroidal latents and predict neural tuning curves in a single integrated modeling framework.","['Neuroscience and Cognitive Science', 'Latent Variable Models', 'grid cells', 'neural activity', 'neural ensemble detection', 'neuroscience', 'tuning curves']",[],"['Martin Bjerke', 'Kristopher T Jensen', 'Claudia Battistin', 'Benjamin Adric Dunn']","['Norwegian University of Science and Technology', 'University College London, University of London', 'Norwegian Institute of Technology', 'Norwegian Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11085,Transparency & Explainability,A probabilistic framework for task-aligned intra- and inter-area neural manifold estimation,"Latent manifolds provide a compact characterization of neural population activity and of shared co-variability across brain areas. Nonetheless, existing statistical tools for extracting neural manifolds face limitations in terms of interpretability of latents with respect to task variables, and can be hard to apply to datasets with no trial repeats. Here we propose a novel probabilistic framework that allows for interpretable partitioning of population variability within and across areas in the context of naturalistic behavior. Our approach for task aligned manifold estimation (TAME-GP) explicitly partitions variability into private and shared sources which can themselves be subdivided in task-relevant and task irrelevant components, uses a realistic Poisson noise model, and introduces temporal smoothing of latent trajectories in the form of a Gaussian Process prior. This TAME-GP graphical model allows for robust estimation of task-relevant variability in local population responses, and of shared co-variability between brain areas. We demonstrate the efficiency of our estimator on within model and biologically motivated simulated data. We also apply it to several datasets of neural population recordings during behavior. Overall, our results demonstrate the capacity of TAME-GP to capture meaningful intra- and inter-area neural variability with single trial resolution.","['Neuroscience and Cognitive Science', 'dimensionality reduction', 'inter-area interactions', 'neuroscience', 'Probabilistic Methods']",[],"['Edoardo Balzani', 'Jean-Paul G Noel', 'Pedro Herrero-Vidal', 'Dora Angelaki', 'Cristina Savin']","['New York University', 'New York University', 'New York University', 'New York University', 'New York University']",[]
https://iclr.cc/virtual/2023/poster/11054,Transparency & Explainability,Bayes-MIL: A New Probabilistic Perspective on Attention-based Multiple Instance Learning for Whole Slide Images,"Multiple instance learning (MIL) is a popular weakly-supervised learning model on the whole slide image (WSI) for AI-assisted pathology diagnosis. The recent advance in attention-based MIL allows the model to find its region-of-interest (ROI) for interpretation by learning the attention weights for image patches of WSI slides. However, we empirically find that the interpretability of some related methods is either untrustworthy as the principle of MIL is violated or unsatisfactory as the high-attention regions are not consistent with experts' annotations. In this paper, we propose Bayes-MIL to address the problem from a probabilistic perspective. The induced patch-level uncertainty is proposed as a new measure of MIL interpretability, which outperforms previous methods in matching doctors annotations. We design a slide-dependent patch regularizer (SDPR) for the attention, imposing constraints derived from the MIL assumption, on the attention distribution. SDPR explicitly constrains the model to generate correct attention values. The spatial information is further encoded by an approximate convolutional conditional random field (CRF), for better interpretability. Experimental results show Bayes-MIL outperforms the related methods in patch-level and slide-level metrics and provides much better interpretable ROI on several large-scale WSI datasets.","['Applications', 'Multiple instance learning', 'histopathology', 'medical imaging', 'bayesian neural network']",[],"['Yufei CUI', 'Ziquan Liu', 'Xiangyu Liu', 'Xue Liu', 'Tei-Wei Kuo', 'Chun Jason Xue', 'Antoni B. Chan']","[', McGill University', 'Queen Mary, University of London', 'SUN YAT-SEN UNIVERSITY', 'McGill University', 'National Taiwan University', 'Mohamed bin Zayed University of Artificial Intelligence', 'City University of Hong Kong']",[]
https://iclr.cc/virtual/2023/poster/10994,Transparency & Explainability,Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning,"Latent confounding has been a long-standing obstacle for causal reasoning from observational data. One popular approach is to model the data using acyclic directed mixed graphs (ADMGs), which describe ancestral relations between variables using directed and bidirected edges. However, existing methods using ADMGs are based on either linear functional assumptions or a discrete search that is complicated to use and lacks computational tractability for large datasets. In this work, we further extend the existing body of work and develop a novel gradient-based approach to learning an ADMG with nonlinear functional relations from observational data. We first show that the presence of latent confounding is identifiable under the assumptions of bow-free ADMGs with nonlinear additive noise models. With this insight, we propose a novel neural causal model based on autoregressive flows. This not only enables us to model complex causal relationships behind the data, but also estimate their functional relationships (hence treatment effects) simultaneously. We further validate our approach via experiments on both synthetic and real-world datasets, and demonstrate the competitive performance against relevant baselines.","['Probabilistic Methods', 'causal inference', 'causality', 'variational inference', 'structural equation model', 'causal discovery', 'latent confounders']",[],"['Matthew Ashman', 'Chao Ma', 'Agrin Hilmkil', 'Joel Jennings', 'Cheng Zhang']","['University of Cambridge', 'Microsoft', 'Chalmers University', 'Google Deepmind', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/10915,Transparency & Explainability,Interpretable Geometric Deep Learning via Learnable Randomness Injection,"Point cloud data is ubiquitous in scientific fields. Recently, geometric deep learning (GDL) has been widely applied to solve prediction tasks with such data. However, GDL models are often complicated and hardly interpretable, which poses concerns to scientists who are to deploy these models in scientific analysis and experiments. This work proposes a general mechanism, learnable randomness injection (LRI), which allows building inherently interpretable models based on general GDL backbones. LRI-induced models, once trained, can detect the points in the point cloud data that carry information indicative of the prediction label. We also propose four datasets from real scientific applications that cover the domains of high-energy physics and biochemistry to evaluate the LRI mechanism. Compared with previous post-hoc interpretation methods, the points detected by LRI align much better and stabler with the ground-truth patterns that have actual scientific meanings. LRI is grounded by the information bottleneck principle, and thus LRI-induced models are also more robust to distribution shifts between training and test scenarios. Our code and datasets are available at https://github.com/Graph-COM/LRI.","['Machine Learning for Sciences', 'geometric deep learning', 'Interpretation', 'graph neural networks']",[],"['Siqi Miao', 'Yunan Luo', 'Mia Liu', 'Pan Li']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Purdue University', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/12144,Transparency & Explainability,What shapes the loss landscape of self supervised learning?,"Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.","['Deep Learning and representational learning', 'collapse', 'self-supervised learning', 'loss landscape']",[],"['Liu Ziyin', 'Ekdeep Singh Lubana', 'Masahito Ueda', 'Hidenori Tanaka']","['The University of Tokyo', 'Center for Brain Science, Harvard University', 'The University of Tokyo', 'Harvard University, Harvard University']",[]
https://iclr.cc/virtual/2023/poster/11804,Transparency & Explainability,Causality Compensated Attention for Contextual Biased Visual Recognition,"Visual attention does not always capture the essential object representation desired for robust predictions. Attention modules tend to underline not only the target object but also the common co-occurring context that the module thinks helpful in the training. The problem is rooted in the confounding effect of the context leading to incorrect causalities between objects and predictions, which is further exacerbated by visual attention. In this paper, to learn causal object features robust for contextual bias, we propose a novel attention module named Interventional Dual Attention (IDA) for visual recognition. Specifically, IDA adopts two attention layers with multiple sampling intervention, which compensates the attention against the confounder context. Note that our method is model-agnostic and thus can be implemented on various backbones. Extensive experiments show our model obtains significant improvements in classification and detection with lower computation. In particular, we achieve the state-of-the-art results in multi-label classification on MS-COCO and PASCAL-VOC.","['Deep Learning and representational learning', 'object recognition', 'causal inference', 'Interventional Dual Attention', 'Confounding Context', 'attention mechanism']",[],"['Ruyang Liu', 'Jingjia Huang', 'Thomas H. Li', 'Ge Li']","['Peking University', 'ByteDance Inc.', 'AIIT, Peking University', 'Peking University Shenzhen Graduate School']",[]
https://iclr.cc/virtual/2023/poster/11158,Transparency & Explainability,A View From Somewhere: Human-Centric Face Representations,"Few datasets contain self-identified demographic information, inferring demographic information risks introducing additional biases, and collecting and storing data on sensitive attributes can carry legal risks. Besides, categorical demographic labels do not necessarily capture all the relevant dimensions of human diversity. We propose to implicitly learn a set of continuous face-varying dimensions, without ever asking an annotator to explicitly categorize a person. We uncover the dimensions by learning on A View From Somewhere (AVFS) dataset of 638,180 human judgments of face similarity. We demonstrate the utility of our learned embedding space for predicting face similarity judgments, collecting continuous face attribute values, attribute classification, and comparative dataset diversity auditing. Moreover, using a novel conditional framework, we show that an annotator's demographics influences the \emph{importance} they place on different attributes when judging similarity, underscoring the \emph{need} for diverse annotator groups to avoid biases. Data and code are available at \url{https://github.com/SonyAI/aviewfrom_somewhere}.","['Social Aspects of Machine Learning', 'annotator bias', 'computer vision', 'faces', 'diversity', 'similarity', 'cognitive', 'mental representations']",[],"['Jerone Andrews', 'Przemyslaw Joniak']","['Sony AI', 'The University of Tokyo']",[]
https://iclr.cc/virtual/2023/poster/11400,Transparency & Explainability,Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic,"Natural language inference (NLI) aims to determine the logical relationship between two sentences, such as Entailment, Contradiction, and Neutral. In recent years, deep learning models have become a prevailing approach to NLI, but they lack interpretability and explainability. In this work, we address the explainability of NLI by weakly supervised logical reasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our model first detects phrases as the semantic unit and aligns corresponding phrases in the two sentences. Then, the model predicts the NLI label for the aligned phrases, and induces the sentence label by fuzzy logic formulas. Our EPR is almost everywhere differentiable and thus the system can be trained end to end. In this way, we are able to provide explicit explanations of phrasal logical relationships in a weakly supervised manner. We further show that such reasoning results help textual explanation generation.","['Applications', 'natural language inference', 'Explainability and Interpretability', 'Neural Fuzzy Logic', 'Weakly Supervised Reasoning']",[],"['Zijun Wu', 'Zi Xuan Zhang', 'Atharva Naik', 'Zhijian Mei', 'Mauajama Firdaus', 'Lili Mou']","['University of Alberta', 'University of Alberta', 'CMU, Carnegie Mellon University', 'University of Alberta', 'University of Alberta', 'University of Alberta']",[]
https://iclr.cc/virtual/2023/poster/10889,Transparency & Explainability,Binding Language Models in Symbolic Languages,"Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at anonymized.","['Applications', 'large language model', 'code generation', 'neural symbolic', 'semantic parsing']",[],"['Zhoujun Cheng', 'Tianbao Xie', 'Peng Shi', 'Chengzu Li', 'Rahul Nadkarni', 'Yushi Hu', 'Caiming Xiong', 'Dragomir Radev', 'Mari Ostendorf', 'Luke Zettlemoyer', 'Noah A. Smith', 'Tao Yu']","['Shanghai Jiaotong University', 'the University of Hong Kong, University of Hong Kong', 'University of Waterloo', 'University of Cambridge', 'Department of Computer Science & Engineering, University of Washington', 'University of Washington', 'Salesforce Research', 'Yale University', 'University of Washington', 'University of Washington', 'University of Washington', 'The University of Hong Kong']",[]
https://iclr.cc/virtual/2023/poster/10814,Transparency & Explainability,GEASS: Neural causal feature selection for high-dimensional biological data,"Identifying nonlinear causal relationships in high-dimensional biological data is an important task. However, current neural network based causality detection approaches for such data suffer from poor interpretability and cannot scale well to the high dimensional regime. Here we present GEASS (Granger fEAture Selection of Spatiotemporal data), which identifies sparse Granger causality mechanisms of high dimensional spatiotemporal data by a single neural network. GEASS maximizes sparsity-regularized modified transfer entropy with a theoretical guarantee of recovering features with spatial/temporal Granger causal relationships. The sparsity regularization is achieved by a novel combinatorial stochastic gate layer to select sparse non-overlapping feature subsets. We demonstrate the efficacy of GEASS in several synthetic datasets and real biological data from single-cell RNA sequencing and spatial transcriptomics.","['Machine Learning for Sciences', 'feature selection', 'Granger causality', 'neural networks', 'spatial transcriptomics', 'single-cell genomics']",[],"['Mingze Dong', 'Yuval Kluger']","['Yale University', 'Yale University']",[]
https://iclr.cc/virtual/2023/poster/12201,Transparency & Explainability,ChiroDiff: Modelling chirographic data with Diffusion Models,"Generative modelling over continuous-time geometric constructs, a.k.a $chirographic\ data$ such as handwriting, sketches, drawings etc., have been accomplished through autoregressive distributions. Such strictly-ordered discrete factorization however falls short of capturing key properties of chirographic data -- it fails to build holistic understanding of the temporal concept due to one-way visibility (causality). Consequently, temporal data has been modelled as discrete token sequences of fixed sampling rate instead of capturing the true underlying concept. In this paper, we introduce a powerful model-class namely Denoising\ Diffusion\ Probabilistic\ Models or DDPMs for chirographic data that specifically addresses these flaws. Our model named ""ChiroDiff"", being non-autoregressive, learns to capture holistic concepts and therefore remains resilient to higher temporal sampling rate up to a good extent. Moreover, we show that many important downstream utilities (e.g. conditional sampling, creative mixing) can be flexibly implemented using ChiroDiff. We further show some unique use-cases like stochastic vectorization, de-noising/healing, abstraction are also possible with this model-class. We perform quantitative and qualitative evaluation of our framework on relevant datasets and found it to be better or on par with competing approaches.","['Generative models', 'generative model', 'chirographic data', 'diffusion model', 'Continuous-time']",[],"['Ayan Das', 'Yongxin Yang', 'Timothy Hospedales', 'Tao Xiang', 'Yi-Zhe Song']","['University of Surrey', 'Queen Mary University of London', 'University of Edinburgh', 'University of Surrey', 'University of Surrey']",[]
https://iclr.cc/virtual/2023/poster/12143,Transparency & Explainability,Interpretability with full complexity by constraining feature information,"Interpretability is a pressing issue for machine learning. Common approaches to interpretable machine learning constrain interactions between features of the input, sacrificing model complexity in order to render more comprehensible the effects of those features on the model's output. We approach interpretability from a new angle: constrain the information about the features without restricting the complexity of the model. We use the Distributed Information Bottleneck to optimally compress each feature so as to maximally preserve information about the output. The learned information allocation, by feature and by feature value, provides rich opportunities for interpretation, particularly in problems with many features and complex feature interactions. The central object of analysis is not a single trained model, but rather a spectrum of models serving as approximations that leverage variable amounts of information about the inputs. Information is allocated to features by their relevance to the output, thereby solving the problem of feature selection by constructing a learned continuum of feature inclusion-to-exclusion. The optimal compression of each feature---at every stage of approximation---allows fine-grained inspection of the distinctions among feature values that are most impactful for prediction. We develop a framework for extracting insight from the spectrum of approximate models and demonstrate its utility on a range of tabular datasets.",['Social Aspects of Machine Learning'],[],"['Kieran A. Murphy', 'Danielle Bassett']","['University of Pennsylvania', 'University of Pennsylvania']",[]
https://iclr.cc/virtual/2023/poster/10718,Transparency & Explainability,Concept-level Debugging of Part-Prototype Networks,"Part-prototype Networks (ProtoPNets) are concept-based classifiers designed to achieve the same performance as black-box models without compromising transparency. ProtoPNets compute predictions based on similarity to class-specific part-prototypes learned to recognize parts of training examples, making it easy to faithfully determine what examples are responsible for any target prediction and why. However, like other models, they are prone to picking up confounders and shortcuts from the data, thus suffering from compromised prediction accuracy and limited generalization. We propose ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a human supervisor, guided by the model’s explanations, supplies feedback in the form of what part-prototypes must be forgotten or kept, and the model is fine-tuned to align with this supervision. Our experimental evaluation shows that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the annotation cost. An online experiment with laypeople confirms the simplicity of the feedback requested to the users and the effectiveness of the collected feedback for learning confounder-free part-prototypes. ProtoPDebug is a promising tool for trustworthy interactive learning in critical applications, as suggested by a preliminary evaluation on a medical decision making task.","['Social Aspects of Machine Learning', 'debugging', 'part-prototype networks', 'concept-based models', 'self-explainable networks', 'explainability']",[],"['Stefano Teso', 'Katya Tentori', 'Fausto Giunchiglia', 'Andrea Passerini']","['University of Trento', 'University of Trento', 'University of Trento', 'University of Trento']",[]
https://iclr.cc/virtual/2023/poster/12197,Fairness & Bias,Fairness-aware Contrastive Learning with Partially Annotated Sensitive Attributes,"Learning high-quality representation is important and essential for visual recognition. Unfortunately, traditional representation learning suffers from fairness issues since the model may learn information of sensitive attributes. Recently, a series of studies have been proposed to improve fairness by explicitly decorrelating target labels and sensitive attributes. Most of these methods, however, rely on the assumption that fully annotated labels on target variable and sensitive attributes are available, which is unrealistic due to the expensive annotation cost. In this paper, we investigate a novel and practical problem of Fair Unsupervised Representation Learning with Partially annotated Sensitive labels (FURL-PS). FURL-PS has two key challenges: 1) how to make full use of the samples that are not annotated with sensitive attributes; 2) how to eliminate bias in the dataset without target labels. To address these challenges, we propose a general Fairness-aware Contrastive Learning (FairCL) framework consisting of two stages. Firstly, we generate contrastive sample pairs, which share the same visual information apart from sensitive attributes, for each instance in the original dataset. In this way, we construct a balanced and unbiased dataset. Then, we execute fair contrastive learning by closing the distance between representations of contrastive sample pairs. Besides, we also propose an unsupervised way to balance the utility and fairness of learned representations by feature reweighting. Extensive experimental results illustrate the effectiveness of our method in terms of fairness and utility, even with very limited sensitive attributes and serious data bias.","['Deep Learning and representational learning', 'Fair Representation Learning', 'semi-supervised learning', 'contrastive learning', 'data augmentation']",[],"['Fengda Zhang', 'Kun Kuang', 'Long Chen', 'Yuxuan Liu', 'Chao Wu', 'Jun Xiao']","['Nanyang Technological University', 'Zhejiang University', 'Hong Kong University of Science and Technology', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University']",[]
https://iclr.cc/virtual/2023/poster/11239,Fairness & Bias,"A law of adversarial risk, interpolation, and label noise","In supervised learning, it has been shown that label noise in the data can be interpolated without penalties on test accuracy.  We show that interpolating label noise induces adversarial vulnerability, and prove the first theorem showing the relationship between label noise and adversarial risk for any data distribution.  Our results are almost tight if we do not make any assumptions on the inductive bias of the learning algorithm. We then investigate how different components of this problem affect this result including properties of the distribution. We also discuss non-uniform label noise distributions; and prove a new theorem showing uniform label noise induces nearly as large an adversarial risk as the worst poisoning with the same noise rate.  Then, we provide theoretical and empirical evidence that uniform label noise is more harmful than typical real-world label noise.  Finally, we show how inductive biases amplify the effect of label noise and argue the need for future work in this direction.","['Social Aspects of Machine Learning', 'lower bound', 'label noise', 'robust machine learning', 'adversarial robustness']",[],"['Daniel Paleka', 'Amartya Sanyal']","['Department of Computer Science, ETHZ - ETH Zurich', 'Max-Planck Institute']",[]
https://iclr.cc/virtual/2023/poster/10984,Fairness & Bias,Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets,"Overparameterization in deep learning refers to settings where a trained Neural Network (NN) has representational capacity to fit the training data in many ways, some of which generalize well, while others do not. In the case of Recurrent Neural Networks (RNNs) there exists an additional layer of overparameterization, in the sense that a model may exhibit many solutions that generalize well for sequence lengths seen in training, some of which \emph{extrapolate} to longer sequences, while others do not. Numerous works studied the tendency of Gradient Descent (GD) to fit overparameterized NNs with solutions that generalize well. On the other hand, its tendency to fit overparameterized RNNs with solutions that extrapolate has been discovered only lately, and is far less understood. In this paper, we analyze the extrapolation properties of GD when applied to overparameterized linear RNNs. In contrast to recent arguments suggesting an implicit bias towards short-term memory, we provide theoretical evidence for learning low dimensional state spaces, which can also model long-term memory. Our result relies on a dynamical characterization showing that GD (with small step size and near zero initialization) strives to maintain a certain form of balancedness, as well as tools developed in the context of the \emph{moment problem} from statistics (recovery of discrete probability distribution from its moments). Experiments corroborate our theory, demonstrating extrapolation via learning low dimensional state spaces with both linear and non-linear RNNs.","['Theory', 'gradient descent', 'extrapolation', 'rnn', 'implicit bias']",[],"['Edo Cohen-Karlik', 'Itamar Menuhin-Gruman', 'Raja Giryes', 'Nadav Cohen', 'Amir Globerson']","['Tel Aviv University', 'Tel Aviv University', 'Tel Aviv University', 'School of Computer Science, Tel Aviv University', 'Tel Aviv University']",[]
https://iclr.cc/virtual/2023/poster/11282,Fairness & Bias,Unsupervised 3D Object Learning through Neuron Activity aware Plasticity,"We present an unsupervised deep learning model for 3D object classification. Conventional Hebbian learning, a well-known unsupervised model, suffers from loss of local features leading to reduced performance for tasks with complex geometric objects. We present a deep network with a novel Neuron Activity Aware (NeAW) Hebbian learning rule that dynamically switches the neurons to be governed by Hebbian learning or anti-Hebbian learning, depending on its activity. We analytically show that NeAW Hebbian learning relieves the bias in neuron activity, allowing more neurons to attend to the representation of the 3D objects. Empirical results show that the NeAW Hebbian learning outperforms other variants of Hebbian learning and shows higher accuracy over fully supervised models when training data is limited.","['Unsupervised and Self-supervised learning', 'hebbian learning']",[],"['Beomseok Kang', 'Biswadeep Chakraborty', 'Saibal Mukhopadhyay']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11097,Fairness & Bias,Amortised Invariance Learning for Contrastive Self-Supervision,"Contrastive self-supervised learning methods famously produce high quality transferable representations by learning invariances to different data augmentations. Invariances established during pre-training can be interpreted as strong inductive biases. However these may or may not be helpful, depending on if they match the invariance requirements of  downstream tasks or not. This has led to several attempts to learn task-specific invariances during pre-training, however, these methods are highly compute intensive and tedious to train. We introduce the notion of amortized invariance learning for contrastive self supervision. In the pre-training stage, we parameterize the feature extractor by differentiable invariance hyper-parameters that control the invariances encoded by the representation. Then, for any downstream task, both linear readout and task-specific invariance requirements can be efficiently and effectively learned by gradient-descent. We evaluate the notion of amortized invariances for contrastive learning over two different modalities: vision and audio, on two widely-used contrastive learning methods in vision: SimCLR and MoCo-v2 with popular architectures like ResNets and Vision Transformers, and SimCLR with ResNet-18 for audio. We show that our amortized features provide a reliable way to learn diverse downstream tasks with different invariance requirements, while using a single feature and avoiding task-specific pre-training. This provides an exciting perspective that opens up new horizons in the field of general purpose representation learning.",['Unsupervised and Self-supervised learning'],[],"['Ruchika Chavhan', 'Jan Stuehmer', 'Calum Heggan', 'Mehrdad Yaghoobi', 'Timothy Hospedales']","['University of Edinburgh, University of Edinburgh', 'Karlsruher Institut für Technologie', 'University of Edinburgh, University of Edinburgh', 'University of Edinburgh', 'University of Edinburgh']",[]
https://iclr.cc/virtual/2023/poster/11509,Fairness & Bias,Expressive Monotonic Neural Networks,"The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce monotonic dependence, and is highly expressive. We show how the algorithm is used to train powerful, robust, and interpretable discriminators that achieve competitive performance compared to current state-of-the-art methods across various benchmarks, from social applications to the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider.","['Social Aspects of Machine Learning', 'lipschitz', 'monotonic']",[],"['Niklas Nolte', 'Ouail Kitouni', 'Mike Williams']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11677,Fairness & Bias,In-sample Actor Critic for Offline Reinforcement Learning,"Offline reinforcement learning suffers from out-of-distribution issue and extrapolation error. Most methods penalize the out-of-distribution state-action pairs or regularize the trained policy towards the behavior policy but cannot guarantee to get rid of  extrapolation  error. We propose In-sample Actor Critic (IAC) which utilizes sampling-importance resampling to execute in-sample policy evaluation. IAC only uses the target Q-values of the actions in the dataset to evaluate the trained policy, thus avoiding extrapolation  error. The proposed method performs unbiased policy evaluation and has a lower variance than importance sampling in many cases. Empirical results show that IAC obtains competitive performance compared to the state-of-the-art methods on Gym-MuJoCo locomotion domains and much more challenging AntMaze domains.","['Reinforcement Learning', 'offline reinforcement learning']",[],"['Hongchang Zhang', 'Yixiu Mao', 'Boyuan Wang', 'Shuncheng He', 'Yi Xu', 'Xiangyang Ji']","['Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University, Tsinghua University', 'Dalian University of Technology', 'Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/11243,Fairness & Bias,The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks,"We study the type of solutions to which stochastic gradient descent converges when used to train a single hidden-layer multivariate ReLU network with the quadratic loss. Our results are based on a dynamical stability analysis. In the univariate case, it was shown that linearly stable minima correspond to network functions (predictors), whose second derivative has a bounded weighted $L^1$ norm. Notably, the bound gets smaller as the step size increases, implying that training with a large step size leads to `smoother' predictors. Here we generalize this result to the multivariate case, showing that a similar result applies to the Laplacian of the predictor. We demonstrate the tightness of our bound on the MNIST dataset, and show that it accurately captures the behavior of the solutions as a function of the step size. Additionally, we prove a depth separation result on the approximation power of ReLU networks corresponding to stable minima of the loss. Specifically, although shallow ReLU networks are universal approximators, we prove that stable shallow networks are not. Namely, there is a function that cannot be well-approximated by stable single hidden-layer ReLU networks trained with a non-vanishing step size. This is while the same function can be realized as a stable two hidden-layer ReLU network. Finally, we prove that if a function is sufficiently smooth (in a Sobolev sense) then it can be approximated arbitrarily well using shallow ReLU networks that correspond to stable solutions of gradient descent.","['Theory', 'depth separation', 'Approximation', 'implicit regularization', 'stability', 'hessian', 'implicit bias', 'dynamical systems']",[],"['Mor Shpigel Nacson', 'Rotem Mulayoff', 'Tomer Michaeli', 'Daniel Soudry']","['Technion, Technion', 'Technion', 'Technion, Technion', 'Technion - Israel Institute of Technology, Technion']",[]
https://iclr.cc/virtual/2023/poster/12204,Fairness & Bias,DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION,"Fully test-time adaptation aims at adapting a pre-trained model to the test stream during real-time inference, which is urgently required when the test distribution differs from the training distribution. Several efforts have been devoted to improving adaptation performance. However, we find that two unfavorable defects are concealed in the prevalent adaptation methodologies like test-time batch normalization (BN) and self-learning. First, we reveal that the normalization statistics in test-time BN are completely affected by the currently received test samples, resulting in inaccurate estimates. Second, we show that during test-time adaptation, the parameter update is biased towards some dominant classes. In addition to the extensively studied test stream with independent and class-balanced samples, we further observe that the defects can be exacerbated in more complicated test environments, such as (time) dependent or class-imbalanced data. We observe that previous approaches work well in certain scenarios while show performance degradation in others due to their faults. In this paper, we provide a plug-in solution called DELTA for Degradation-freE fuLly Test-time Adaptation, which consists of two components: (i) Test-time Batch Renormalization (TBR), introduced to improve the estimated normalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to address the class bias within optimization. We investigate various test-time adaptation methods on three commonly used datasets with four scenarios, and a newly introduced real-world dataset. DELTA can help them deal with all scenarios simultaneously, leading to SOTA performance.",['Deep Learning and representational learning'],[],"['Bowen Zhao', 'Chen Chen', 'Shu-Tao Xia']","['Tsinghua University', 'OPPO Research Institute', 'Shenzhen International Graduate School, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/12006,Fairness & Bias,"Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning","The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized reinforcement learning individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry across architectural modules to bias which skills are learnt. While asymmetry choice has a large influence on transferability, existing methods base their choice primarily on intuition in a domain-independent, potentially sub-optimal, manner. In this paper, we theoretically and empirically show the crucial expressivity-transferability trade-off of skills across sequential tasks, controlled by information asymmetry. Given this insight, we introduce Attentive Priors for Expressive and Transferable Skills (APES), a hierarchical KL-regularized method, heavily benefiting from both priors and hierarchy. Unlike existing approaches, APES automates the choice of asymmetry by learning it in a data-driven, domain-dependent, way based on our expressivity-transferability theorems. Experiments over complex transfer domains of varying levels of extrapolation and sparsity, such as robot block stacking, demonstrate the criticality of the correct asymmetric choice, with APES drastically outperforming previous methods.","['Reinforcement Learning', 'reinforcement learning', 'transfer learning', 'skills']",[],"['Sasha Salter', 'Kristian Hartikainen', 'Walter Goodwin', 'Ingmar Posner']","['Meta', 'University of Oxford', 'University of Oxford', 'University of Oxford']",[]
https://iclr.cc/virtual/2023/poster/11477,Fairness & Bias,Interpretable Debiasing of Vectorized Language Representations with Iterative Orthogonalization,"We propose a new mechanism to augment a word vector embedding representation that offers improved bias removal while retaining the key information—resulting in improved interpretability of the representation. Rather than removing the information associated with a concept that may induce bias, our proposed method identifies two concept subspaces and makes them orthogonal. The resulting representation has these two concepts uncorrelated. Moreover, because they are orthogonal, one can simply apply a rotation on the basis of the representation so that the resulting subspace corresponds with coordinates. This explicit encoding of concepts to coordinates works because they have been made fully orthogonal, which previous approaches do not achieve. Furthermore, we show that this can be extended to multiple subspaces. As a result, one can choose a subset of concepts to be represented transparently and explicitly, while the others are retained in the mixed but extremely expressive format of the representation.","['Deep Learning and representational learning', 'pre-trained contextualized embeddings', 'ethics', 'static embeddings', 'natural language processing', 'bias', 'debiasing', 'fairness']",[],"['Prince Osei Aboagye', 'Yan Zheng', 'Jack Shunn', 'Chin-Chia Michael Yeh', 'Junpeng Wang', 'Zhongfang Zhuang', 'Huiyuan Chen', 'Liang Wang', 'Wei Zhang', 'Jeff M. Phillips']","['University of Utah', 'VISA', 'University of Utah', 'VISA', 'VISA', 'VISA', 'VISA', 'VISA', 'VISA', 'University of Utah']",[]
https://iclr.cc/virtual/2023/poster/10765,Fairness & Bias,Rethinking Symbolic Regression: Morphology and Adaptability in the Context of Evolutionary Algorithms,"Symbolic Regression (SR) is the well-studied problem of finding closed-form analytical expressions that describe the relationship between variables in a measurement dataset. In this paper, we rethink SR from two perspectives: morphology and adaptability. Morphology: Current SR algorithms typically use several man-made heuristics to influence the morphology (or structure) of the expressions in the search space. These man-made heuristics may introduce unintentional bias and data leakage, especially with the relatively few equation-recovery benchmark problems available for evaluating SR approaches. To address this, we formulate a novel minimalistic approach, based on constructing a depth-aware mathematical language model trained on terminal walks of expression trees, as a replacement to these heuristics. Adaptability: Current SR algorithms tend to select expressions based on only a single fitness function (e.g., MSE on the training set). We promote the use of an adaptability framework in evolutionary SR which uses fitness functions that alternate across generations. This leads to robust expressions that perform well on the training set and are close to the true functional form. We demonstrate this by alternating fitness functions that quantify faithfulness to values (via MSE) and empirical derivatives (via a novel theoretically justified fitness metric coined MSEDI). Proof-of-concept: We combine these ideas into a minimalistic evolutionary SR algorithm that outperforms all benchmark and state of-the-art SR algorithms in problems with unknown constants added, which we claim are more reflective of SR performance for real-world applications. Our claim is then strengthened by reproducing the superior performance on real-world regression datasets from SRBench. For researchers interested in equation-recovery problems, we also propose a set of conventions that can be used to promote fairness in comparison across SR methods and to reduce unintentional bias.",['General Machine Learning'],[],"['Kei Sen Fong', 'Shelvia Wongso', 'Mehul Motani']","['National University of Singapore', 'National University of Singapore', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/11023,Fairness & Bias,How gradient estimator variance and bias impact learning in neural networks,"There is growing interest in understanding how real brains may approximate gradients and how gradients can be used to train neuromorphic chips. However, neither real brains nor neuromorphic chips can perfectly follow the loss gradient, so parameter updates would necessarily use gradient estimators that have some variance and/or bias. Therefore, there is a need to understand better how variance and bias in gradient estimators impact learning dependent on network and task properties. Here, we show that variance and bias can impair learning on the training data, but some degree of variance and bias in a gradient estimator can be beneficial for generalization. We find that the ideal amount of variance and bias in a gradient estimator are dependent on several properties of the network and task: the size and activity sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. As such, whether considering biologically-plausible learning algorithms or algorithms for training neuromorphic chips, researchers can analyze these properties to determine whether their approximation to gradient descent will be effective for learning given their network and task properties.","['Neuroscience and Cognitive Science', 'neural networks', 'Biologically-plausible learning', 'Imperfect gradient descent', 'Neuromorphic computing', 'computational neuroscience', 'Gradient approximation', 'learning and plasticity', 'credit assignment']",[],"['Arna Ghosh', 'Yuhan Helena Liu', 'Guillaume Lajoie', 'Blake Aaron Richards']","['McGill University', 'University of Washington', 'Mila, Quebec AI institute', 'McGill University']",[]
https://iclr.cc/virtual/2023/poster/11493,Fairness & Bias,Is Attention All That NeRF Needs?,"We present Generalizable NeRF Transformer (GNT), a transformer-based architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to render novel views on the fly from source views. While prior works on NeRFs optimize a scene representation by inverting a handcrafted rendering equation, GNT achieves neural representation and rendering that generalizes across scenes using transformers at two stages. (1) The view transformer leverages multi-view geometry as an inductive bias for attention-based scene representation, and predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views. (2) The ray transformer renders novel views using attention to decode the features from the view transformer along the sampled points during ray marching. Our experiments demonstrate that when optimized on a single scene, GNT can successfully reconstruct NeRF without an explicit rendering formula due to the learned ray renderer. When trained on multiple scenes, GNT consistently achieves state-of-the-art performance when transferring to unseen scenes and outperform all other methods by ~10% on average. Our analysis of the learned attention maps to infer depth and occlusion indicate that attention enables learning a physically-grounded rendering. Our results show the promise of transformers as a universal modeling tool for graphics. Please refer to our project page for video results: https://vita-group.github.io/GNT/","['Applications', 'transformer', 'neural rendering', 'Neural Radiance Field']",[],"['Mukund Varma T', 'Peihao Wang', 'Xuxi Chen', 'Tianlong Chen', 'Subhashini Venugopalan', 'Zhangyang Wang']","['University of California, San Diego', 'University of Texas, Austin', 'University of Texas at Austin', 'Massachusetts Institute of Technology', 'Google', 'University of Texas at Austin']",[]
https://iclr.cc/virtual/2023/poster/11059,Fairness & Bias,Using Language to Extend to Unseen Domains,"It is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply $\textit{verbalizing}$ the training domain (e.g.``photos of birds'') as well as domains we want to extend to but do not have data for (e.g.``paintings of birds'') can improve robustness. Using a multimodal model with a joint image and language embedding space, our method $\textit{LADS}$ learns a transformation of the image embeddings from the source domain to each target domain, while preserving task relevant information. Without using any images from the target domain, we show that over the $\textit{extended}$ domain containing both source and target, $\textit{LADS}$ outperforms standard fine-tuning and ensemble approaches over a suite of 4 benchmarks targeting domain adaptation and dataset bias.","['Applications', 'robust training', 'domain adaptation', 'vision and language']",[],"['Lisa Dunlap', 'Clara Mohri', 'Devin Guillory', 'Han Zhang', 'Trevor Darrell', 'Joseph E. Gonzalez', 'Aditi Raghunathan', 'Anna Rohrbach']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California Berkeley', 'Stanford University', 'Electrical Engineering & Computer Science Department', 'University of California - Berkeley', 'Carnegie Mellon University', 'Technische Universität Darmstadt']",[]
https://iclr.cc/virtual/2023/poster/11735,Fairness & Bias,Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions,"Pretrained language models have shown superior performance on many natural language processing tasks, yet they still struggle at multi-step formal reasoning tasks like grade school math problems. One key challenge of finetuning them to solve such math reasoning problems is that many existing datasets only contain one reference solution for each problem, despite the fact that there are often alternative solutions resembling different reasoning paths to the final answer. This way, the finetuned models are biased towards the limited reference solutions, which limits their generalization to unseen examples. To mitigate this issue, we propose to let the model perform sampling during training and learn from both self-sampled fully-correct solutions, which yield the correct answer upon execution, and partially-correct solutions, whose intermediate state matches an intermediate state of a known correct solution. We show that our use of self-sampled correct and partially-correct solutions can benefit learning and help guide the sampling process, leading to more efficient exploration of the solution space. Additionally, we explore various training objectives to support learning from multiple solutions per example and find they greatly affect the performance. Experiments on two math reasoning datasets show the effectiveness of our method compared to learning from a single reference solution with MLE, where we improve PASS@100 from 35.5% to 44.5% for GSM8K, and 27.6% to 36.2% PASS@80 for MathQA. Such improvements are also consistent across different model sizes.","['Applications', 'multi-target learning', 'large language models', 'mathematical reasoning', 'self-sampling']",[],"['Ansong Ni', 'Jeevana Priya Inala', 'Chenglong Wang', 'Alex Polozov', 'Christopher Meek', 'Dragomir Radev', 'Jianfeng Gao']","['Yale University', 'Microsoft', 'Microsoft', 'Google', 'University of Washington', 'Yale University', 'Microsoft Research']",[]
https://iclr.cc/virtual/2023/poster/11067,Fairness & Bias,Noise-Robust De-Duplication at Scale,"Identifying near duplicates within large, noisy text corpora has a myriad of applications that range from de-duplicating training datasets, reducing privacy risk, and evaluating test set leakage, to identifying reproduced news articles and literature within large corpora. Across these diverse applications, the overwhelming majority of work relies on $N$-grams. Limited efforts have been made to evaluate how well $N$-gram methods perform, in part because it is unclear how one could create an unbiased evaluation dataset for a massive corpus. This study uses the unique timeliness of historical news wires to create a 27,210 document dataset, with 122,876 positive duplicate pairs, for studying noise-robust de-duplication. The time-sensitivity of news makes comprehensive hand labelling feasible - despite the massive overall size of the corpus - as duplicates occur within a narrow date range. The study then develops and evaluates a range of de-duplication methods: hashing and $N$-gram overlap (which predominate in the literature), a contrastively trained bi-encoder, and a ``re-rank'' style approach combining a bi- and cross-encoder. The neural approaches significantly outperform hashing and $N$-gram overlap. We show that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours. We also apply our pre-trained model to the RealNews and patent portions of C4 (Colossal Clean Crawled Corpus), illustrating that a neural approach can identify many near duplicates missed by hashing, in the presence of various types of noise. The public release of our NEWS-COPY de-duplication dataset, codebase, and the pre-trained models will facilitate further research and applications.",['Applications'],[],"['Emily Silcock', ""Luca D'Amico-Wong"", 'Jinglin Yang']","['Department of Economics, Harvard University', 'Harvard University', 'University of California, Berkeley']",[]
https://iclr.cc/virtual/2023/poster/10733,Fairness & Bias,RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates,"Proximal splitting algorithms are well suited to solving large-scale nonsmooth optimization problems, in particular those arising in machine learning. We propose a new primal–dual algorithm, in which the dual update is randomized; equivalently, the proximity operator of one of the function in the problem is replaced by a stochastic oracle. For instance, some randomly chosen dual variables, instead of all, are updated at each iteration. Or, the proximity operator of a function is called with some small probability only. A nonsmooth variance-reduction technique is implemented so that the algorithm finds an exact minimizer of the general problem involving smooth and nonsmooth functions, possibly composed with linear operators. We derive linear convergence results in presence of strong convexity; these results are new even in the deterministic case, when our algorithms reverts to the recently proposed Primal–Dual Davis–Yin algorithm. Some randomized algorithms of the literature are also recovered as particular cases (e.g., Point-SAGA). But our randomization technique is general and encompasses many unbiased mechanisms beyond sampling and probabilistic updates, including compression. Since the convergence speed depends on the slowest among the primal and dual contraction mechanisms, the iteration complexity might remain the same when randomness is used. On the other hand, the computation complexity can be significantly reduced. Overall, randomness helps getting faster algorithms. This has long been known for stochastic-gradient-type algorithms, and our work shows that this fully applies in the more general primal–dual setting as well.","['Optimization', 'proximal splitting', 'proximity operator', 'randomized algorithm', 'optimization', 'stochastic algorithm']",[],"['Laurent Condat', 'Peter Richtárik']","['KAUST', 'King Abdullah University of Science and Technology (KAUST)']",[]
https://iclr.cc/virtual/2023/poster/10916,Fairness & Bias,GOGGLE: Generative Modelling for Tabular Data by Learning Relational Structure,"Deep generative models learn highly complex and non-linear representations to generate realistic synthetic data. While they have achieved notable success in computer vision and natural language processing, similar advances have been less demonstrable in the tabular domain. This is partially because generative modelling of tabular data entails a particular set of challenges, including heterogeneous relationships, limited number of samples, and difficulties in incorporating prior knowledge. Additionally, unlike their counterparts in image and sequence domain, deep generative models for tabular data almost exclusively employ fully-connected layers, which encode weak inductive biases about relationships between inputs. Real-world data generating processes can often be represented using relational structures, which encode sparse, heterogeneous relationships between variables. In this work, we learn and exploit relational structure underlying tabular data to better model variable dependence, and as a natural means to introduce regularization on relationships and include prior knowledge. Specifically, we introduce GOGGLE, an end-to-end message passing scheme that jointly learns the relational structure and corresponding functional relationships as the basis of generating synthetic samples. Using real-world datasets, we provide empirical evidence that the proposed method is effective in generating realistic synthetic data and exploiting domain knowledge for downstream tasks.","['Generative models', 'generative model', 'tabular data', 'synthetic data']",[],"['Tennison Liu', 'Zhaozhi Qian', 'Jeroen Berrevoets', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']",[]
https://iclr.cc/virtual/2023/poster/11425,Fairness & Bias,Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability,"Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\theta)$, is bounded by $2/\eta$, training is ""stable"" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen at al. (2021) detailed two important phenomena. The first, dubbed \emph{progressive sharpening}, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\eta$. The second, dubbed \emph{edge of stability}, is that the sharpness hovers at $2/\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call \emph{self-stabilization}, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows \emph{projected} gradient descent (PGD) under the constraint $S(\theta) \le 2/\eta$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability.","['Deep Learning and representational learning', 'optimization', 'implicit regularization', 'gradient descent', 'edge of stability', 'implicit bias']",[],"['Alex Damian', 'Eshaan Nichani', 'Jason D. Lee']","['Princeton University', 'Princeton University', 'Princeton University']",[]
https://iclr.cc/virtual/2023/poster/12237,Fairness & Bias,Delving into Semantic Scale Imbalance,"Model bias triggered by long-tailed data has been widely studied. However, measure based on the number of samples cannot explicate three phenomena simultaneously: (1) Given enough data, the classification performance gain is marginal with additional samples. (2) Classification performance decays precipitously as the number of training samples decreases when there is insufficient data. (3) Model trained on sample-balanced datasets still has different biases for different classes. In this work, we define and quantify the semantic scale of classes, which is equivalent to the feature diversity of classes. It is exciting to find experimentally that there is a marginal effect of semantic scale, which perfectly describes the first two phenomena. Further, the quantitative measurement of semantic scale imbalance is proposed, which can accurately reflect model bias on multiple datasets, even on sample-balanced data, revealing a novel perspective for the study of class imbalance. Due to the prevalence of semantic scale imbalance, we propose semantic-scale-balanced learning, including a general loss improvement scheme and a dynamic re-weighting training framework that overcomes the challenge of calculating semantic scales in real-time during iterations. Comprehensive experiments show that dynamic semantic-scale-balanced learning consistently enables the model to perform superiorly on large-scale long-tailed and non-long-tailed datasets, which is a good starting point for mitigating the prevalent but unnoticed model bias.","['Deep Learning and representational learning', 'imbalanced learning', 'Long-tailed distribution', 'model bias']",[],"['Yanbiao Ma', 'Licheng Jiao', 'Fang Liu', 'Yuxin Li', 'Shuyuan Yang', 'Xu Liu']","['Xidian University', ""Xi'an University of Electronic Science and Technology"", 'Xidian University', 'Xidian University', 'Xidian University', 'Xidian University']",[]
https://iclr.cc/virtual/2023/poster/12264,Fairness & Bias,Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization,"Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based on a parameter-efficient tuning schema without any training data. This leads to minimal generation quality loss and improved rectification performance with acceptable computational cost. Experimental results demonstrate that compared to several strong baselines, UDDIA achieves debiasing and detoxifying simultaneously and better balances efficiency and effectiveness, taking a further step towards practical ethical NLG.","['Debias', 'detoxify', 'language generation']",[],"['Zonghan Yang', 'Xiaoyuan Yi', 'Peng Li', 'Yang Liu', 'Xing Xie']","['Department of Computer Science and Technology, Tsinghua University', 'Microsoft', 'Tsinghua University', 'Tsinghua University', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/11668,Fairness & Bias,Predictive Inference with Feature Conformal Prediction,"Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Apart from experiments on existing predictive inference benchmarks, we also demonstrate the state-of-the-art performance of the proposed methods on \textit{large-scale} tasks such as ImageNet classification and Cityscapes image segmentation.","['Deep Learning and representational learning', 'conformal prediction', 'uncertainty']",[],"['Jiaye Teng', 'Chuan Wen', 'Dinghuai Zhang', 'Yoshua Bengio', 'Yang Gao', 'Yang Yuan']","['IIIS', 'Tsinghua University, Tsinghua University', 'Mila, University of Montreal', 'University of Montreal', 'Tsinghua University', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/11182,Fairness & Bias,Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection,"Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary prediction trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatiotemporal motion patterns of the subject-object compositions. Our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompt. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD.","['Applications', 'prompt tuning', 'Video Relation Detection']",[],"['Kaifeng Gao', 'Long Chen', 'Hanwang Zhang', 'Jun Xiao', 'Qianru Sun']","['Zhejiang University', 'Hong Kong University of Science and Technology', 'Nanyang Technological University', 'Zhejiang University', 'Singapore Management University']",[]
https://iclr.cc/virtual/2023/poster/11214,Fairness & Bias,Revisit Finetuning strategy for Few-Shot Learning to Transfer the Emdeddings,"Few-Shot Learning (FSL) aims to learn a simple and effective bias on limited novel samples. Recently, many methods have been focused on re-training a randomly initialized linear classifier to adapt it to the novel features extracted by the pre-trained feature extractor (called Linear-Probing-based methods). These methods typically assumed the pre-trained feature extractor was robust enough, i.e., finetuning was not needed, and hence the pre-trained feature extractor does not change on the novel samples. However, the unchanged pre-trained feature extractor will distort the features of novel samples because the robustness assumption may not hold, especially on the out-of-distribution samples. To extract the undistorted features, we designed Linear-Probing-Finetuning with Firth-Bias (LP-FT-FB) to yield an accurate bias on the limited samples for better finetuning the pre-trained feature extractor, providing stronger transferring ability. In LP-FT-FB, we further proposed inverse Firth Bias Reduction (i-FBR) to regularize the over-parameterized feature extractor on which FBR does not work well.The proposed i-FBR effectively alleviates the over-fitting problem of the feature extractor in the process of finetuning and helps extract undistorted novel features. To show the effectiveness of the designed LP-FT-FB, we conducted a lot of experiments on the commonly used FSL datasets under different backbones, including in-domain and cross-domain FSL tasks. The experimental results show that the proposed FT-LP-FB outperforms the SOTA FSL methods. The code is available at https://github.com/whzyf951620/LinearProbingFinetuningFirthBias.","['Deep Learning and representational learning', 'equivariance', 'finetuning', 'few-shot learning']",[],"['Heng Wang', 'Tan Yue', 'Xiang Ye', 'Bohan Li', 'Yong Li']","['Beijing University of Posts and Communications', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications']",[]
https://iclr.cc/virtual/2023/poster/10977,Fairness & Bias,Score-based Continuous-time Discrete Diffusion Models,"Score-based modeling through stochastic differential equations (SDEs) has provided a new perspective on diffusion models, and demonstrated superior performance on continuous data. However, the gradient of the log-likelihood function, \ie, the score function, is not properly defined for discrete spaces. This makes it non-trivial to adapt SDE with score functions to categorical data. In this paper, we extend diffusion models to discrete variables by introducing a stochastic jump process where the reverse process denoises via a continuous-time Markov chain. This formulation admits an analytical simulation during backward sampling. To learn the reverse process, we extend score matching to general categorical data, and show that an unbiased estimator can be obtained via simple matching of the conditional marginal distributions. We demonstrate the effectiveness of the proposed method on a set of synthetic and real-world music and image benchmarks.","['Generative models', 'discrete space diffusion', 'continuous-time diffusion', 'discrete score matching']",[],"['Haoran Sun', 'Lijun Yu', 'Bo Dai', 'Dale Schuurmans', 'Hanjun Dai']","['Google', 'School of Computer Science, Carnegie Mellon University', 'Georgia Institute of Technology', 'University of Alberta', 'Google Research']",[]
https://iclr.cc/virtual/2023/poster/11232,Fairness & Bias,FastFill: Efficient Compatible Model Update,"In many retrieval systems the original high dimensional data (e.g., images) is mapped to a lower dimensional feature through a learned embedding model. The task of retrieving the most similar data from a gallery set to a given query data is performed through similarity comparison on features. When the embedding model is updated, it might produce features that are not comparable/compatible with features already in the gallery computed with the old model. Subsequently, all features in the gallery need to be re-computed using the new embedding model -- a computationally expensive process called backfilling. Recently, compatible representation learning methods have been proposed to avoid back-filling. Despite their relative success, there is an inherent trade-off between new model performance and its compatibility with the old model. In this work, we introduce FastFill: a compatible model update process using feature alignment and policy based partial backfilling to promptly elevate retrieval performance. We show that previous backfilling strategies suffer from decreased performance and demonstrate the importance of both the training objective and the ordering in online partial backfilling. We propose a new training method for feature alignment between old and new embedding models using uncertainty estimation. Compared to previous works, we obtain significantly improved backfilling results on a variety of datasets: mAP on ImageNet (+4.4%), Places-365 (+2.7%), and VGG-Face2 (+1.3%). Further, we demonstrate that when updating a biased model with FastFill, the minority subgroup accuracy gap promptly vanishes with a small fraction of partial backfilling.","['Deep Learning and representational learning', 'Model Regression', 'Compatible Representation Learning', 'Image Retrieval']",[],"['Florian Jaeckle', 'Fartash Faghri', 'Ali Farhadi', 'Oncel Tuzel', 'Hadi Pouransari']","['University of Oxford', 'Apple', 'University of Washington', 'Apple', 'Apple']",[]
https://iclr.cc/virtual/2023/poster/11647,Fairness & Bias,Re-weighting Based Group Fairness Regularization via Classwise Robust Optimization,"Many existing group fairness-aware training methods aim to achieve the group fairness by either re-weighting underrepresented groups based on certain rules or using weakly approximated surrogates for the fairness metrics in the objective as regularization terms. Although each of the learning schemes has its own strength in terms of applicability or performance, respectively, it is difficult for any method in the either category to be considered as a gold standard since their successful performances are typically limited to specific cases. To that end, we propose a principled method, dubbed as FairDRO, which unifies the two learning schemes by incorporating a well-justified group fairness metric into the training objective using a classwise distributionally robust optimization (DRO) framework. We then develop an iterative optimization algorithm that minimizes the resulting objective by automatically producing the correct re-weights for each group. Our experiments show that FairDRO is scalable and easily adaptable to diverse applications, and consistently achieves the state-of-the-art performance on several benchmark datasets in terms of the accuracy-fairness trade-off, compared to recent strong baselines.","['Social Aspects of Machine Learning', 'DRO', 'Group Fairness']",[],"['Sangwon Jung', 'Taeeon Park', 'Sanghyuk Chun', 'Taesup Moon']","['Seoul National University', 'Seoul National University', 'NAVER AI Lab', 'Seoul National University']",[]
https://iclr.cc/virtual/2023/poster/11682,Fairness & Bias,ODAM: Gradient-based Instance-Specific Visual Explanations for Object Detection,"We propose the Gradient-weighted Object Detector Activation Mapping (Grad-ODAM), a visualized explanation technique for interpreting the predictions of object detectors. Utilizing the gradients of detector targets flowing into the intermediate feature maps, Grad-ODAM produces heat maps that show the influence of regions on the detector's decision. Compared to previous classification activation mapping works, Grad-ODAM generates instance-specific explanations rather than class-specific ones. We show that Grad-ODAM is applicable to both one-stage detectors such as FCOS and two-stage detectors such as Faster R-CNN, and produces higher-quality visual explanations than the state-of-the-art both effectively and efficiently. We next propose a training scheme, ODAM-Train, to improve the explanation ability on object discrimination of the detector through encouraging consistency between explanations for detections on the same object, and distinct explanations for detections on different objects. Based on the heat maps produced by Grad-ODAM with ODAM-Train, we propose ODAM-NMS, which considers the information of the model's explanation for each prediction to distinguish the duplicate detected objects. We present a detailed analysis of the visualized explanations of detectors and carry out extensive experiments to validate the effectiveness of the proposed ODAM.","['Deep Learning and representational learning', 'object detection', 'instance-specific visual explanation']",[],"['Chenyang ZHAO', 'Antoni B. Chan']","['City University of Hong Kong', 'City University of Hong Kong']",[]
https://iclr.cc/virtual/2023/poster/11670,Fairness & Bias,Measure the Predictive Heterogeneity,"As an intrinsic and fundamental property of big data, data heterogeneity exists in a variety of real-world applications, such as in agriculture, sociology, health care, etc. For machine learning algorithms, the ignorance of data heterogeneity will significantly hurt the generalization performance and the algorithmic fairness, since the prediction mechanisms among different sub-populations are likely to differ. In this work, we focus on the data heterogeneity that affects the prediction of machine learning models, and first formalize the Predictive Heterogeneity, which takes into account the model capacity and computational constraints. We prove that it can be reliably estimated from finite data with PAC bounds even in high dimensions. Additionally, we propose the Information Maximization (IM) algorithm, a bi-level optimization algorithm, to explore the predictive heterogeneity of data. Empirically, the explored predictive heterogeneity provides insights for sub-population divisions in agriculture, sociology, and object recognition, and leveraging such heterogeneity benefits the out-of-distribution generalization performance.","['Social Aspects of Machine Learning', 'predictive information', 'predictive heterogeneity', 'Data heterogeneity']",[],"['Jiashuo Liu', 'Jiayun Wu', 'Renjie Pi', 'Renzhe Xu', 'Xingxuan Zhang', 'Bo Li', 'Peng Cui']","['Tsinghua University, Tsinghua University', 'Computer Science, Tsinghua University', 'Hong Kong University of Science and Technology', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/10770,Fairness & Bias,A theoretical study of inductive biases in contrastive learning,"Understanding self-supervised learning is important but challenging. Previous theoretical works study the role of pretraining losses, and view neural networks as general black boxes. However, the recent work of [Saunshi et al.] argues that the model architecture --- a component largely ignored by previous works --- also has significant influences on the downstream performance of self-supervised learning. In this work, we provide the first theoretical analysis of self-supervised learning that incorporates the effect of inductive biases originating from the model class. In particular, we focus on contrastive learning --- a popular self-supervised learning method that is widely used in the vision domain. We show that when the model has limited capacity, contrastive representations would recover certain special clustering structures that are compatible with the model architecture, but ignore many other clustering structures in the data distribution. As a result, our theory can capture the more realistic setting where contrastive representations have much lower dimensionality than the number of clusters in the data distribution. We instantiate our theory on several synthetic data distributions, and provide empirical evidence to support the theory.","['Theory', 'theory of self-supervised learning', 'theory of contrastive learning']",[],"['Jeff Z. HaoChen', 'Tengyu Ma']","['Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2023/poster/11286,Fairness & Bias,Minimum Variance Unbiased N:M Sparsity for the Neural Gradients,"In deep learning, fine-grained N:M sparsity reduces the data footprint and bandwidth of a General Matrix multiply (GEMM) up to x2,  and doubles throughput by skipping computation of zero values. So far, it was mainly only used to prune weights to accelerate the forward and backward phases. We examine how this method can be used also for the neural gradients (i.e. loss gradients with respect to the intermediate neural layer outputs). To this end, we first establish a tensor-level optimality criteria. Previous works aimed to minimize the mean-square-error (MSE) of each pruned block. We show that while minimization of the MSE works fine for pruning the weights and activations, it catastrophically fails for the neural gradients. Instead, we show that accurate pruning of the neural gradients requires an unbiased minimum-variance pruning mask. We design such specialized masks, and find that in most cases, 1:2 sparsity is sufficient for training, and 2:4 sparsity is usually enough when this is not the case. Further, we suggest combining several such methods together in order to potentially speed up training even more. A reference implementation is supplied in the supplementary material.","['Deep Learning and representational learning', 'pruning', 'structured sparsity', 'acceleration', 'compression']",[],"['Brian Chmiel', 'Itay Hubara', 'Ron Banner', 'Daniel Soudry']","['Technion - Israel Institute of Technology, Technion', 'Technion, Technion', 'Intel', 'Technion - Israel Institute of Technology, Technion']",[]
https://iclr.cc/virtual/2023/poster/11168,Fairness & Bias,Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems,"Neural networks with physics-based inductive biases such as Lagrangian neural networks (LNNs), and Hamiltonian neural networks (HNNs) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle-based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph-based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyze the role of different inductive biases on the performance of GNODE. We show that similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newton’s third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of the model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by ∼4 orders of magnitude for a pendulum system, and ∼2 orders of magnitude for spring systems. These results suggest that NODE-based systems can give competitive performances with energy-conserving neural networks by employing appropriate inductive biases.","['Machine Learning for Sciences', 'Neural ODE', 'graph neural network', 'Graph Neural ODE', 'physical systems']",[],"['Suresh Bishnoi', 'Ravinder Bhattoo', 'Jayadeva Jayadeva', 'Sayan Ranu', 'N M Anoop Krishnan']","['Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi']",[]
https://iclr.cc/virtual/2023/poster/10948,Fairness & Bias,ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret,"Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promis- ing line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art—DREAM and neural fictitious self play (NFSP)—on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time.","['Theory', 'reinforcement learning', 'CFR', 'game theory', 'two-player zero-sum']",[],"['Stephen Marcus McAleer', 'Gabriele Farina', 'Marc Lanctot', 'Tuomas Sandholm']","['Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Google DeepMind', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2023/poster/11217,Fairness & Bias,Sampling-free Inference for Ab-Initio Potential Energy Surface Networks,"Recently, it has been shown that neural networks not only approximate the ground-state wave functions of a single molecular system well but can also generalize to multiple geometries. While such generalization significantly speeds up training, each energy evaluation still requires Monte Carlo integration which limits the evaluation to a few geometries. In this work, we address the inference shortcomings by proposing the Potential learning from ab-initio Networks (PlaNet) framework, in which we simultaneously train a surrogate model in addition to the neural wave function. At inference time, the surrogate avoids expensive Monte-Carlo integration by directly estimating the energy, accelerating the process from hours to milliseconds. In this way, we can accurately model high-resolution multi-dimensional energy surfaces for larger systems that previously were unobtainable via neural wave functions. Finally, we explore an additional inductive bias by introducing physically-motivated restricted neural wave function models. We implement such a function with several additional improvements in the new PESNet++ model. In our experimental evaluation, PlaNet accelerates inference by 7 orders of magnitude for larger molecules like ethanol while preserving accuracy. Compared to previous energy surface networks, PESNet++ reduces energy errors by up to 74%.","['Machine Learning for Sciences', 'graph neural networks', 'computational physics', 'self-supervised learning', 'molecules', 'Machine learning for science', 'online learning', 'self-generative learning']",[],"['Nicholas Gao', 'Stephan Günnemann']","['Technical University Munich', 'Technical University Munich']",[]
https://iclr.cc/virtual/2023/poster/11203,Fairness & Bias,That Label's got Style: Handling Label Style Bias for Uncertain Image Segmentation,"Segmentation uncertainty models predict a distribution over plausible segmentations for a given input, which they learn from the annotator variation in the training set. However, in practice these annotations can differ systematically in the way they are generated, for example through the use of different labeling tools. This results in datasets that contain both data variability and differing label styles. In this paper, we demonstrate that applying state-of-the-art segmentation uncertainty models on such datasets can lead to model bias caused by the different label styles. We present an updated modelling objective conditioning on labeling style for aleatoric uncertainty estimation, and modify two state-of-the-art-architectures for segmentation uncertainty accordingly. We show with extensive experiments that this method reduces label style bias, while improving segmentation performance, increasing the applicability of segmentation uncertainty models in the wild. We curate two datasets, with annotations in different label styles, which we will make publicly available along with our code upon publication.","['Deep Learning and representational learning', 'uncertainty quantification', 'segmentation']",[],"['Kilian Zepf', 'Eike Petersen', 'Jes Frellsen', 'Aasa Feragen']","['Technical University of Denmark', 'Technical University of Denmark', 'Technical University of Denmark', 'Technical University of Denmark']",[]
https://iclr.cc/virtual/2023/poster/10935,Fairness & Bias,Is Forgetting Less a Good Inductive Bias for Forward Transfer?,"One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetful representations lead to a better forward transfer suggesting a strong correlation between retaining past information and learning efficiency on new tasks. Further, we found less forgetful representations to be more diverse and discriminative compared to their forgetful counterparts.","['Deep Learning and representational learning', 'transfer learning', 'continual learning']",[],"['Jiefeng Chen', 'Timothy Nguyen', 'Dilan Gorur', 'Arslan Chaudhry']","['Amazon', 'Google', 'DeepMind', 'DeepMind']",[]
https://iclr.cc/virtual/2023/poster/11305,Fairness & Bias,Long-Tailed Partial Label Learning via Dynamic Rebalancing,"Real-world data usually couples the label ambiguity and heavy imbalance, challenging the algorithmic robustness of partial label learning (PLL) and long-tailed learning (LT). The straightforward combination of LT and PLL, i.e., LT-PLL, suffers from a fundamental dilemma: LT methods build upon a given class distribution that is unavailable in PLL, and the performance of PLL is severely influenced in long-tailed context. We show that even with the auxiliary of an oracle class prior, the state-of-the-art methods underperform due to an adverse fact that the constant rebalancing in LT is harsh to the label disambiguation in PLL. To overcome this challenge, we thus propose a dynamic rebalancing method, termed as RECORDS, without assuming any prior knowledge about the class distribution. Based on a parametric decomposition of the biased output, our method constructs a dynamic adjustment that is benign to the label disambiguation process and theoretically converges to the oracle class prior. Extensive experiments on three benchmark datasets demonstrate the significant gain of RECORDS compared with a range of baselines. The code is publicly available.","['Deep Learning and representational learning', 'long-tailed learning', 'partial label learning']",[],"['Feng Hong', 'Jiangchao Yao', 'Zhihan Zhou', 'Ya Zhang', 'Yanfeng Wang']","['Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']",[]
https://iclr.cc/virtual/2023/poster/11435,Fairness & Bias,PerFedMask: Personalized Federated Learning with Optimized Masking Vectors,"Recently, various personalized federated learning (FL) algorithms have been proposed to tackle data heterogeneity. To mitigate device heterogeneity, a common approach is to use masking.  In this paper, we first show that using random masking can lead to a bias in the obtained solution of the learning model. To this end, we propose a personalized FL algorithm with optimized masking vectors called PerFedMask. In particular, PerFedMask facilitates each device to obtain its optimized masking vector based on its computational capability before training.  Fine-tuning is performed after training. PerFedMask is a generalization of a recently proposed personalized FL algorithm, FedBABU (Oh et al., 2022). PerFedMask can be combined with other FL algorithms including HeteroFL (Diao et al., 2021) and Split-Mix FL (Hong et al., 2022). Results based on CIFAR-10 and CIFAR-100 datasets show that the proposed PerFedMask algorithm provides a higher test accuracy after fine-tuning and lower average number of trainable parameters when compared with six existing state-of-the-art FL algorithms in the literature. The codes are available at https://github.com/MehdiSet/PerFedMask.","['Deep Learning and representational learning', 'Personalized Federated Learning', 'Computational capability', 'Masking vectors', 'Data heterogeneity']",[],"['Mehdi Setayesh', 'Xiaoxiao Li', 'Vincent W.S. Wong']","['University of British Columbia', 'University of British Columbia', 'University of British Columbia']",[]
https://iclr.cc/virtual/2023/poster/11315,Fairness & Bias,Imbalanced Semi-supervised Learning with Bias Adaptive Classifier,"Pseudo-labeling has proven to be a promising semi-supervised learning (SSL) paradigm. Existing pseudo-labeling methods commonly assume that the class distributions of training data are balanced. However, such an assumption is far from realistic scenarios and thus severely limits the performance of current pseudo-labeling methods under the context of class-imbalance. To alleviate this problem, we design a bias adaptive classifier that targets the imbalanced SSL setups. The core idea is to automatically assimilate the training bias caused by class imbalance via the bias adaptive classifier, which is composed of a novel bias attractor and the original linear classifier. The bias attractor is designed as a light-weight residual network and learned through a bi-level learning framework, which enables the bias adaptive classifier to fit imbalanced training data, while the linear classifier can provide unbiased label prediction for each class. We conduct extensive experiments under various imbalanced semi-supervised setups, and the results demonstrate that our method can be applied to different pseudo-labeling models and is superior to current state-of-the-art methods.","['Deep Learning and representational learning', 'semi-supervised learning', 'weakly-supervised learning']",[],"['Renzhen Wang', 'Xixi Jia', 'Quanziang Wang', 'Yichen Wu', 'Deyu Meng']","[""Xi'an Jiaotong University"", 'Xidian University', ""Xi'an Jiaotong University"", 'City University of Hong Kong', '']",[]
https://iclr.cc/virtual/2023/poster/10869,Fairness & Bias,Equal Improvability: A New Fairness Notion Considering the Long-term Impact,"Devising a fair classifier that does not discriminate against different groups is an important problem in machine learning. Although researchers have proposed various ways of defining group fairness, most of them only focused on the immediate fairness, ignoring the long-term impact of a fair classifier under the dynamic scenario where each individual can improve its feature over time. Such dynamic scenarios happen in real world, e.g., college admission and credit loaning, where each rejected sample makes effort to change its features to get accepted afterwards. In this dynamic setting, the long-term fairness should equalize the samples’ feature distribution across different groups after the rejected samples make some effort to improve. In order to promote long-term fairness, we propose a new fairness notion called Equal Improvability (EI), which equalizes the potential acceptance rate of the rejected samples across different groups assuming a bounded level of effort will be spent by each rejected sample. We analyze the properties of EI and its connections with existing fairness notions. To find a classifier that satisfies the EI requirement, we propose and study three different approaches that solve EI regularized optimization problems. Through experiments on both synthetic and real datasets, we demonstrate that the proposed EI-regularized algorithms encourage us to find a fair classifier in terms of EI. Finally, we provide experimental results on dynamic scenarios which highlight the advantages of our EI metric in achieving the long-term fairness. Codes are available in anonymous GitHub repository.","['Social Aspects of Machine Learning', 'machine learning', 'Fairness and Bias in Artificial Intelligence']",[],"['Ozgur Guldogan', 'Yuchen Zeng', 'Jy-yong Sohn', 'Ramtin Pedarsani', 'Kangwook Lee']","['University of California, Santa Barbara', 'University of Wisconsin, Madison', 'Yonsei University', 'UC Santa Barbara', 'University of Wisconsin, Madison']",[]
https://iclr.cc/virtual/2023/poster/11069,Fairness & Bias,Hyperparameter Optimization through Neural Network Partitioning,"Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance --- especially in the presence of limited data. In this work, we propose a simple and efficient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into $K$ data shards and parameter partitions, respectively. Each partition is associated with and optimized only on specific data shards. Combining these partitions into subnetworks allows us to define the ""out-of-training-sample"" loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being significantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging.","['Deep Learning and representational learning', 'data-augmentation', 'marginal likelihood', 'hyperparameter optimization', 'invariances', 'federated learning']",[],"['Bruno Kacper Mlodozeniec', 'Matthias Reisser', 'Christos Louizos']","['University of Cambridge', 'Qualcomm Inc, QualComm', 'Qualcomm Inc, QualComm']",[]
https://iclr.cc/virtual/2023/poster/12161,Fairness & Bias,Mind the Gap: Offline Policy Optimization for Imperfect Rewards,"Reward function is essential in reinforcement learning (RL), serving as the guiding signal to incentivize agents to solve given tasks, however, is also notoriously difficult to design. In many cases, only imperfect rewards are available, which inflicts substantial performance loss for RL agents. In this study, we propose a unified offline policy optimization approach, \textit{RGM (Reward Gap Minimization)}, which can smartly handle diverse types of imperfect rewards. RGM is formulated as a bi-level optimization problem: the upper layer optimizes a reward correction term that performs visitation distribution matching w.r.t. some expert data; the lower layer solves a pessimistic RL problem with the corrected rewards. By exploiting the duality of the lower layer, we derive a tractable algorithm that enables sampled-based learning without any online interactions.  Comprehensive experiments demonstrate that RGM achieves superior performance to existing methods under diverse settings of imperfect rewards. Further, RGM can effectively correct wrong or inconsistent rewards against expert preference and retrieve useful information from biased rewards. Code is available at https://github.com/Facebear-ljx/RGM.","['Reinforcement Learning', 'offline policy optimization', 'reward gap', 'imperfect rewards']",[],"['Jianxiong Li', 'Xiao Hu', 'Haoran Xu', 'Jingjing Liu', 'Xianyuan Zhan', 'Qing-Shan Jia', 'Ya-Qin Zhang']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'University of Texas at Austin', 'Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'AIR, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/10749,Fairness & Bias,CASR: Generating Complex Sequences with Autoregressive Self-Boost Refinement,"There are sequence generation tasks where the best order to generate the target sequence is not left-to-right. For example, an answer to the Sudoku game, a structured code like s-expression, and even a logical natural language answer where the analysis may be generated after the decision. We define the target sequences of those tasks as complex sequences. Obviously, a complex sequence should be constructed with multiple logical steps, and has dependencies among each part of itself (e.g. decisions depend on analyses). It's a great challenge for the classic left-to-right autoregressive generation system to generate complex sequences. Current approaches improve one-pass left-to-right generation on NLG tasks by generating different heuristic intermediate sequences in multiple stages. However, for complex sequences, the heuristic rules to break down them may hurt performance, and increase additional exposure bias. To tackle these challenges, we propose a PLM-friendly autoregressive self-boost refinement framework, CASR. When training, CASR inputs the predictions generated by the model itself at the previous refinement step (instead of those produced by heuristic rules). To find an optimal design, we also discuss model architecture, parameter efficiency and initialization strategy. By evaluating CASR on Sudoku, WebQSP, MTOP and KVRET through controlled experiments and empirical studies, we find that CASR produces high-quality outputs. CASR also improves Accuracy on Sudoku (70.93% --> 97.28%) and achieves state-of-the-art performance on KVRET with Micro F1 score (67.88% --> 70.00%).","['Generative models', 'autoregressive generation', 'complex answers', 'self-boost refinement']",[],"['Hongwei Han', 'Mengyu Zhou', 'Shi Han', 'Xiu Li', 'Dongmei Zhang']","['Tsinghua University, Tsinghua University', 'Microsoft Research', 'Microsoft Research Asia', 'Tsinghua University', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/12255,Fairness & Bias,Bias Propagation in Federated Learning,"We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.","['Algorithmic Bias', 'fairness', 'federated learning']",[],"['Hongyan Chang', 'Reza Shokri']","['National University of Singapore', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/10687,Fairness & Bias,Human-Guided Fair Classification for Natural Language Processing,"Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. Finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models.","['Social Aspects of Machine Learning', 'Human Evaluation', 'Individual Fairness', 'style transfer', 'nlp', 'crowdsourcing']",[],"['Florian E. Dorner', 'Momchil Peychev', 'Nikola Konstantinov', 'Naman Goel', 'Elliott Ash', 'Martin Vechev']","['Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'ETH Zurich', 'INSAIT, Sofia University', 'University of Oxford', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11396,Fairness & Bias,Graph Neural Network-Inspired Kernels for Gaussian Processes in Semi-Supervised Learning,"Gaussian processes (GPs) are an attractive class of machine learning models because of their simplicity and flexibility as building blocks of more complex Bayesian models. Meanwhile, graph neural networks (GNNs) emerged recently as a promising class of models for graph-structured data in semi-supervised learning and beyond. Their competitive performance is often attributed to a proper capturing of the graph inductive bias. In this work, we introduce this inductive bias into GPs to improve their predictive performance for graph-structured data. We show that a prominent example of GNNs, the graph convolutional network, is equivalent to some GP when its layers are infinitely wide; and we analyze the kernel universality and the limiting behavior in depth. We further present a programmable procedure to compose covariance kernels inspired by this equivalence and derive example kernels corresponding to several interesting members of the GNN family. We also propose a computationally efficient approximation of the covariance matrix for scalable posterior inference with large-scale data. We demonstrate that these graph-based kernels lead to competitive classification and regression performance, as well as advantages in computation time, compared with the respective GNNs.","['Probabilistic Methods', 'semi-supervised learning', 'graph neural network', 'gaussian process']",[],"['Zehao Niu', 'Mihai Anitescu', 'Jie Chen']","['University of Chicago', 'University of Chicago', 'International Business Machines']",[]
https://iclr.cc/virtual/2023/poster/11504,Fairness & Bias,Part-Based Models Improve Adversarial Robustness,"We show that combining human prior knowledge with end-to-end learning can improve the robustness of deep neural networks by introducing a part-based model for object classification. We believe that the richer form of annotation helps guide neural networks to learn more robust features without requiring more samples or larger models. Our model combines a part segmentation model with a tiny classifier and is trained end-to-end to simultaneously segment objects into parts and then classify the segmented object. Empirically, our part-based models achieve both higher accuracy and higher adversarial robustness than a ResNet-50 baseline on all three datasets. For instance, the clean accuracy of our part models is up to 15 percentage points higher than the baseline's, given the same level of robustness. Our experiments indicate that these models also reduce texture bias and yield better robustness against common corruptions and spurious correlations. The code is publicly available at https://github.com/chawins/adv-part-model.","['Deep Learning and representational learning', 'computer vision', 'adversarial examples', 'part-based model', 'adversarial robustness']",[],"['Chawin Sitawarin', 'Kornrapat Pongmala', 'Yizheng Chen', 'Nicholas Carlini', 'David Wagner']","['University of California Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'Google', 'University of California Berkeley']",[]
https://iclr.cc/virtual/2023/poster/12000,Fairness & Bias,Truthful Self-Play,"We present a general framework for evolutionary learning to emergent unbiased state representation without any supervision. Evolutionary frameworks such as self-play converge to bad local optima in case of multi-agent reinforcement learning in non-cooperative partially observable environments with communication due to information asymmetry.  Our proposed framework is a simple modification of self-play inspired by mechanism design, also known as {\em reverse game theory}, to elicit truthful signals and make the agents cooperative. The key idea is to add imaginary rewards using the peer prediction method, i.e., a mechanism for evaluating the validity of information exchanged between agents in a decentralized environment. Numerical experiments with predator prey, traffic junction and StarCraft tasks demonstrate that the state-of-the-art performance of our framework.","['Reinforcement Learning', 'Imaginary Rewards', 'Comm-POSG']",[],['Shohei Ohsawa'],"['Daisy, inc.']",[]
https://iclr.cc/virtual/2023/poster/11500,Fairness & Bias,Matching receptor to odorant with protein language and graph neural networks,"Odor perception in mammals is triggered by interactions between volatile organic compounds and a subset of hundreds of proteins called olfactory receptors (ORs). Molecules activate these receptors in a complex combinatorial coding allowing mammals to discriminate a vast number of chemical stimuli. Recently, ORs have gained attention as new therapeutic targets following the discovery of their involvement in other physiological processes and diseases. To date, predicting molecule-induced activation for ORs is highly challenging since $43\%$ of ORs have no identified active compound. In this work, we combine [CLS] token from protBERT with a molecular graph and propose a tailored GNN architecture incorporating inductive biases from the protein-molecule binding. We abstract the biological process of protein-molecule activation as the injection of a molecule into a protein-specific environment. On a newly gathered dataset of $46$ $700$ OR-molecule pairs, this model outperforms state-of-the-art models on drug-target interaction prediction as well as standard GNN baselines. Moreover, by incorporating non-bonded interactions the model is able to work with mixtures of compounds. Finally, our predictions reveal a similar activation pattern for molecules within a given odor family, which is in agreement with the theory of combinatorial coding in olfaction.","['Machine Learning for Sciences', 'graph neural networks', 'Olfaction', 'protein-ligand binding', 'protein language modelling', 'olfactory receptors', 'Computational Biology']",[],"['Matej Hladiš', 'Maxence Lalis', 'Sebastien Fiorucci', 'Jérémie Topin']","['Institute of Chemistry in Nice, Université Côte d’Azur, France', ""Université Côte d'Azur"", ""Université Côte d'Azur"", ""Université Côte d'Azur""]",[]
https://iclr.cc/virtual/2023/poster/11687,Fairness & Bias,Feature Reconstruction From Outputs Can Mitigate Simplicity Bias in Neural Networks,"Deep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that \emph{Simplicity Bias} (SB) of DNNs -- bias towards learning only the simplest features -- is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/ complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To bridge the gap between these two lines of work, we first hypothesize and verify that while SB may not altogether preclude learning complex features, it amplifies simpler features over complex ones. Namely, simple features are replicated several times in the learned representations while complex features might not be replicated. This phenomenon, we term \emph{Feature  Replication  Hypothesis}, coupled with the \emph{Implicit Bias} of SGD to converge to maximum margin solutions in the feature space, leads the models to rely mostly on the simple features for classification. To mitigate this bias, we propose \emph{Feature Reconstruction Regularizer (FRR)} to ensure that the learned features can be reconstructed back from the logits. The use of \emph{FRR} in linear layer training (\emph{FRR-L}) encourages the use of more diverse features for classification. We further propose to finetune the full network by freezing the weights of the linear layer trained using \emph{FRR-L}, to refine the learned features, making them more suitable for classification. Using this simple solution, we demonstrate up to 15\% gains in OOD accuracy on the recently introduced semi-synthetic datasets with extreme distribution shifts. Moreover, we demonstrate noteworthy gains over existing SOTA methods on the standard OOD benchmark DomainBed as well.","['Deep Learning and representational learning', 'OOD generalization', 'deep learning', 'Out-of-distribution robustness', 'simplicity bias']",[],"['Sravanti Addepalli', 'Anshul Nasery', 'Venkatesh Babu Radhakrishnan', 'Praneeth Netrapalli', 'Prateek Jain']","['Google', 'University of Washington', 'Indian Institute of Science', 'Google', 'Google']",[]
https://iclr.cc/virtual/2023/poster/11344,Fairness & Bias,Soft Neighbors are Positive Supporters in Contrastive Visual Representation Learning,"Contrastive learning methods train visual encoders by comparing views (e.g., often created via a group of data augmentations on the same instance) from one instance to others. Typically, the views created from one instance are set as positive, while views from other instances are negative. This binary instance discrimination is studied extensively to improve feature representations in self-supervised learning. In this paper, we rethink the instance discrimination framework and find the binary instance labeling insufficient to measure correlations between different samples. For an intuitive example, given a random image instance, there may exist other images in a mini-batch whose content meanings are the same (i.e., belonging to the same category) or partially related (i.e., belonging to a similar category). How to treat the images that correlate similarly to the current image instance leaves an unexplored problem. We thus propose to support the current image by exploring other correlated instances (i.e., soft neighbors). We first carefully cultivate a candidate neighbor set, which will be further utilized to explore the highly-correlated instances. A cross-attention module is then introduced to predict the correlation score (denoted as positiveness) of other correlated instances with respect to the current one. The positiveness score quantitatively measures the positive support from each correlated instance, and is encoded into the objective for pretext training. To this end, our proposed method benefits in discriminating uncorrelated instances while absorbing correlated instances for SSL. We evaluate our soft neighbor contrastive learning method (SNCLR) on standard visual recognition benchmarks, including image classification, object detection, and instance segmentation. The state-of-the-art recognition performance shows that SNCLR is effective in improving feature representations from both ViT and CNN encoders.","['Deep Learning and representational learning', 'soft neighbors', 'visual correlation', 'contrastive learning']",[],"['Chongjian GE', 'Jiangliu Wang', 'Zhan Tong', 'Shoufa Chen', 'Yibing Song', 'Ping Luo']","['The University of Hong Kong', 'The Chinese University of Hong Kong', 'Ant Research', 'The University of Hong Kong', 'Fudan University', 'The University of Hong Kong']",[]
https://iclr.cc/virtual/2023/poster/11448,Fairness & Bias,FedFA:  Federated Feature Augmentation,"Federated learning is a distributed paradigm that allows multiple parties to collaboratively train deep models without exchanging the raw data. However, the data distribution among clients is naturally non-i.i.d., which leads to severe degradation of the learnt model. The primary goal of this paper is to develop a robust federated learning algorithm to address feature shift in clients’ samples, which can be caused by various factors, e.g., acquisition differences in medical imaging. To reach this goal, we propose FedFA to tackle federated learning from a dis- tinct perspective of federated feature augmentation. FedFA is based on a major insight that each client’s data distribution can be characterized by statistics (i.e., mean and standard deviation) of latent features; and it is likely to manipulate these local statistics globally, i.e., based on information in the entire federation, to let clients have a better sense of the underlying distribution and therefore alleviate local data bias. Based on this insight, we propose to augment each local feature statistic probabilistically based on a normal distribution, whose mean is the original statistic and variance quantifies the augmentation scope. Key to our approach is the determination of a meaningful Gaussian variance, which is accomplished by taking into account not only biased data of each individual client, but also underlying feature statistics characterized by all participating clients. We offer both theoretical and empirical justifications to verify the effectiveness of FedFA. Our code is available at https://github.com/tfzhou/FedFA.","['Deep Learning and representational learning', 'feature augmentation', 'federated learning']",[],"['Tianfei Zhou', 'Ender Konukoglu']","['Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich']",[]
https://iclr.cc/virtual/2023/poster/12048,Fairness & Bias,Vision Transformer Adapter for Dense Predictions,"This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. Code and models will be released at https://github.com/czczup/ViT-Adapter.","['Applications', 'Plain Vision Transformer', 'Dense Prediction', 'Adapter']",[],"['Zhe Chen', 'Yuchen Duan', 'Wenhai Wang', 'Junjun He', 'Tong Lu', 'Jifeng Dai', 'Yu Qiao']","['Nanjing University', 'Shanghai AI Laboratory', 'Shanghai AI Laboratory', 'Shanghai AI Laboratory', 'Nanjing University', 'Tsinghua University, Tsinghua University', 'Shanghai Aritifcal Intelligence Laboratory']",[]
https://iclr.cc/virtual/2023/poster/12111,Fairness & Bias,Disparate Impact in Differential Privacy from Gradient Misalignment,"As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.","['Social Aspects of Machine Learning', 'differential privacy', 'privacy', 'fairness']",[],"['Maria S. Esipova', 'Atiyeh Ashari Ghomi', 'Yaqiao Luo', 'Jesse C. Cresswell']","['University of Waterloo', 'Department of Computer Science, University of Toronto', 'Department of Computer Science, University of Toronto', 'Layer 6 AI']",[]
https://iclr.cc/virtual/2023/poster/11330,Fairness & Bias,Causal Estimation for Text Data with (Apparent) Overlap Violations,"Consider the problem of estimating the causal effect of some attribute of a text document; for example: what effect does writing a polite vs. rude email have on response time? To estimate a causal effect from observational data, we need to adjust for confounding aspects of the text that affect both the treatment and outcome---e.g., the topic or writing level of the text. These confounding aspects are unknown a priori, so it seems natural to adjust for the entirety of the text (e.g., using a transformer). However, causal identification and estimation procedures rely on the assumption of overlap: for all levels of the adjustment variables, there is randomness leftover so that every unit could have (not) received treatment. Since the treatment here is itself an attribute of the text, it is perfectly determined, and overlap is apparently violated. The purpose of this paper is to show how to handle causal identification and obtain robust causal estimation in the presence of apparent overlap violations. In brief, the idea is to use supervised representation learning to produce a data representation that preserves confounding information while eliminating information that is only predictive of the treatment. This representation then suffices for adjustment and satisfies overlap. Adapting results on non-parametric estimation, we show that this procedure shows robustness with respect to conditional outcome misestimation and yields a low-bias estimator that admits valid uncertainty quantification under weak conditions. Empirical results show reductions in bias and strong improvements in uncertainty quantification relative to the natural (transformer-based) baseline.",['Probabilistic Methods'],[],"['Lin Gui', 'Victor Veitch']","['University of Chicago', 'University of Chicago']",[]
https://iclr.cc/virtual/2023/poster/11499,Fairness & Bias,Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions,"The execution behavior of a program often depends on external resources, such as program inputs or file contents, and so the program cannot be run in isolation. Nevertheless, software developers benefit from fast iteration loops where automated tools identify errors as early as possible, even before programs can be compiled and run. This presents an interesting machine learning challenge: can we predict runtime errors in a ""static"" setting, where program execution is not possible? Here, we introduce a competitive programming dataset and task for predicting runtime errors, which we show is difficult for generic models like Transformers. We approach this task by developing an interpreter-inspired architecture with an inductive bias towards mimicking program executions, which models exception handling and ""learns to execute"" descriptions of external resources. Surprisingly, we show that the model can also predict the locations of errors, despite being trained only on labels indicating error presence or absence and kind. In total, we present a practical and difficult-yet-approachable challenge problem related to learning program execution behavior and we demonstrate promising new capabilities of interpreter-inspired machine learning models for code.","['Applications', 'graph neural networks', 'source code', 'recurrent networks', 'program analysis', 'attention mechanisms', 'program execution']",[],"['David Bieber', 'Rishab Goel', 'Dan Zheng', 'Hugo Larochelle', 'Daniel Tarlow']","['Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'Bloomberg LP', 'Google', 'Université de Montréal', '']",[]
https://iclr.cc/virtual/2023/poster/10939,Fairness & Bias,FunkNN: Neural Interpolation for Functional Generation,"Can we build continuous generative models which generalize across scales, can be evaluated at any coordinate, admit calculation of exact derivatives, and are conceptually simple? Existing MLP-based architectures generate worse samples than the grid-based generators with favorable convolutional inductive biases. Models that focus on generating images at different scales do better, but employ complex architectures not designed for continuous evaluation of images and derivatives.We take a signal-processing perspective and treat continuous signal generation as interpolation from samples. Indeed, correctly sampled discrete images contain all information about the low spatial frequencies. The question is then how to extrapolate the spectrum in a data-driven way while meeting the above design criteria. Our answer is FunkNN---a novel convolutional network which learns how to reconstruct continuous images at arbitrary coordinates and can be applied to any image dataset. Combined with a discrete generative model it becomes a functional generator which can act as a prior in continuous ill-posed inverse problems. We show that FunkNN generates high-quality continuous images and exhibits strong out-of-distribution performance thanks to its patch-based design. We further showcase its performance in several stylized inverse problems with exact spatial derivatives.",['Generative models'],[],"['AmirEhsan Khorashadizadeh', 'Anadi Chaman', 'Valentin Debarnot', 'Ivan Dokmanić']","['University of Basel', 'University of Illinois at Urbana Champaign', 'University of Basel', 'University of Basel']",[]
https://iclr.cc/virtual/2023/poster/10988,Fairness & Bias,Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions,"We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising.","['Theory', 'deep neural networks', 'representation cost', 'sparsity', 'implicit bias']",[],['Arthur Jacot'],"['NYU, New York University']",[]
https://iclr.cc/virtual/2023/poster/11047,Fairness & Bias,Agree to Disagree: Diversity through Disagreement for Better Transferability,"Gradient-based learning algorithms have an implicit \emph{simplicity bias} which in effect can limit the diversity of predictors being sampled by the learning procedure. This behavior can hinder the transferability of trained models by (i) favoring the learning of simpler but spurious features --- present in the training data but absent from the test data --- and (ii) by only leveraging a small subset of predictive features.  Such an effect is especially magnified when the test distribution does not exactly match the train distribution---referred to as the Out of Distribution (OOD) generalization problem. However, given only the training data, it is not always possible to apriori assess if a given feature is spurious or transferable. Instead, we advocate for learning an ensemble of models which capture a diverse set of predictive features. Towards this, we propose a new algorithm D-BAT (Diversity-By-disAgreement Training), which enforces agreement among the models on the training data, but disagreement on the OOD data. We show how D-BAT naturally emerges from the notion of generalized discrepancy, as well as demonstrate in multiple experiments how the proposed method can mitigate shortcut-learning, enhance uncertainty and OOD detection, as well as improve transferability.","['Deep Learning and representational learning', 'ensemble', 'OOD generalization', 'diversity']",[],"['Matteo Pagliardini', 'Martin Jaggi', 'François Fleuret', 'Sai Praneeth Karimireddy']","['Swiss Federal Institute of Technology Lausanne', 'EPFL', 'University of Geneva', 'University of California, Berkeley']",[]
https://iclr.cc/virtual/2023/poster/11125,Fairness & Bias,Strong inductive biases provably prevent harmless interpolation,"Classical wisdom suggests that estimators should avoid fitting noise to achieve good generalization. In contrast, modern overparameterized models can yield small test error despite interpolating noise — a phenomenon often called ""benign overfitting"" or ""harmless interpolation"". This paper argues that the degree to which interpolation is harmless hinges upon the strength of an estimator's inductive bias, i.e., how heavily the estimator favors solutions with a certain structure: while strong inductive biases prevent harmless interpolation, weak inductive biases can even require fitting noise to generalize well. Our main theoretical result establishes tight non-asymptotic bounds for high-dimensional kernel regression that reflect this phenomenon for convolutional kernels, where the filter size regulates the strength of the inductive bias. We further provide empirical evidence of the same behavior for deep neural networks with varying filter sizes and rotational invariance.","['Theory', 'generalization bounds', 'benign overfitting', 'non-parametric regression', 'high-dimensional statistics', 'deep learning theory']",[],"['Michael Aerni', 'Marco Milanta', 'Konstantin Donhauser', 'Fanny Yang']","['ETH Zurich', 'ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/12220,Fairness & Bias,Minimax Optimal Kernel Operator Learning via Multilevel Training,"Learning mappings between infinite-dimensional function spaces have achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators between infinite-dimensional function spaces.",['Machine Learning for Sciences'],[],"['Jikai Jin', 'Yiping Lu', 'Jose Blanchet', 'Lexing Ying']","['Stanford University', 'Northwestern University', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2023/poster/11545,Fairness & Bias,FaiREE: fair classification with finite-sample and distribution-free guarantee,"Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depend on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm which can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfying various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.","['Social Aspects of Machine Learning', 'classification', 'Algorithmic fairness', 'finite-sample', 'distribution-free']",[],"['Puheng Li', 'James Zou', 'Linjun Zhang']","['Stanford University', 'Stanford University', 'Rutgers University']",[]
https://iclr.cc/virtual/2023/poster/11108,Fairness & Bias,Emergence of Maps in the Memories of Blind Navigation Agents,"Animal navigation research posits that organisms build and maintain internal spa- tial representations, or maps, of their environment. We ask if machines – specifically, artificial intelligence (AI) navigation agents – also build implicit (or ‘mental’) maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent’s perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train ‘blind’ agents – with sensing limited to only egomotion and no other sensing of any kind – to perform PointGoal navigation (‘go to $\Delta$x, $\Delta$y’) via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments (∼95% success); (2) they utilize memory over long horizons (remembering ∼1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent ‘forgets’ exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation.","['Reinforcement Learning', 'Embodied AI', 'navigation', 'characterizing representations']",[],"['Erik Wijmans', 'Manolis Savva', 'Irfan Essa', 'Stefan Lee', 'Ari S. Morcos', 'Dhruv Batra']","['Apple', 'Simon Fraser University', 'Georgia Institute of Technology', 'Oregon State University', 'Meta AI (FAIR)', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/10684,Fairness & Bias,Learning Continuous Normalizing Flows For Faster Convergence To Target Distribution via Ascent Regularizations,"Normalizing flows (NFs) have been shown to be advantageous in modeling complex distributions and improving sampling efficiency for unbiased sampling. In this work, we propose a new class of continuous NFs, ascent continuous normalizing flows (ACNFs), that makes a base distribution  converge faster to a target distribution. As solving such a flow is non-trivial and barely possible, we propose a practical implementation to learn flexibly parametric ACNFs via ascent regularization and apply it in two learning cases: maximum likelihood learning for density estimation and minimizing reverse KL divergence for unbiased sampling and variational inference. The learned ACNFs demonstrate faster convergence towards the target distributions, therefore, achieving better density estimations, unbiased sampling and variational approximation at lower computational costs. Furthermore, the flows show to stabilize themselves to mitigate performance deterioration and are less sensitive to the choice of training flow length $T$.","['Probabilistic Methods', 'Normalizing flows', 'gradient flows', 'variational inference', 'density estimation', 'unbiased sampling']",[],"['Shuangshuang Chen', 'Sihao Ding', 'Yiannis Karayiannidis', 'Mårten Björkman']","['KTH Royal Institute of Technology, Stockholm, Sweden', 'Ohio State University', 'Chalmers University of Technology', 'KTH Royal Institute of Technology, Stockholm, Sweden']",[]
https://iclr.cc/virtual/2023/poster/12047,Fairness & Bias,Simplicial Embeddings in Self-Supervised Learning and Downstream Classification,"Simplicial Embeddings (SEM) are representations learned through self-supervised learning (SSL), wherein a representation is projected into $L$ simplices of $V$ dimensions each using a \texttt{softmax} operation. This procedure conditions the representation onto a constrained space during pretraining and imparts an inductive bias for group sparsity. For downstream classification, we formally prove that the SEM representation leads to better generalization than an unnormalized representation.Furthermore, we empirically demonstrate that SSL methods trained with SEMs have improved generalization on natural image datasets such as CIFAR-100 and ImageNet. Finally, when used in a downstream classification task, we show that SEM features exhibit emergent semantic coherence where small groups of learned features are distinctly predictive of semantically-relevant classes.","['Unsupervised and Self-supervised learning', 'pre-training', 'self-supervised learning', 'representation learning']",[],"['Samuel Lavoie', 'Christos Tsirigotis', 'Max Schwarzer', 'Ankit Vani', 'Michael Noukhovitch', 'Kenji Kawaguchi', 'Aaron Courville']","['University of Montreal', 'ServiceNow Research', 'University of Montreal', 'University of Montreal', 'University of Montreal', 'National University of Singapore', 'University of Montreal']",[]
https://iclr.cc/virtual/2023/poster/11931,Fairness & Bias,Multifactor Sequential Disentanglement via Structured Koopman Autoencoders,"Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disentangling abilities such as swapping of individual static factors between characters, and an incremental swap of disentangled factors from the source to the target. Moreover, we evaluate our method extensively on two factor standard benchmark tasks where we significantly improve over competing unsupervised approaches, and we perform competitively in comparison to weakly- and self-supervised state-of-the-art approaches. The code is available at https://github.com/azencot-group/SKD.","['Deep Learning and representational learning', 'Sequential Disentanglement', 'Koopman methods']",[],"['Nimrod Berman', 'Ilan Naiman', 'Omri Azencot']","['Ben-Gurion University of the Negev', 'Ben Gurion University of the Negev, Technion', 'Ben-Gurion University of the Negev']",[]
https://iclr.cc/virtual/2023/poster/11667,Fairness & Bias,Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models,"Studies on benign overfitting provide insights for the success of overparameterized deep learning models. In this work, we examine whether overfitting is truly benign in real-world classification tasks. We start with the observation that a ResNet model overfits benignly on Cifar10 but not benignly on ImageNet. To understand why benign overfitting fails in the ImageNet experiment, we theoretically analyze benign overfitting under a more restrictive setup where the number of parameters is not significantly larger than the number of data points. Under this mild overparameterization setup, our analysis identifies a phase change: unlike in the previous heavy overparameterization settings, benign overfitting can now fail in the presence of label noise. Our analysis explains our empirical observations, and is validated by a set of control experiments with ResNets. Our work highlights the importance of understanding implicit bias in underfitting regimes as a future direction.","['Theory', 'generalization', 'benign overfitting', 'implicit bias', 'mild overparameterization']",[],"['Kaiyue Wen', 'Jiaye Teng', 'Jingzhao Zhang']","['Tsinghua University, Tsinghua University', 'IIIS', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/11665,Fairness & Bias,Semi-Implicit Variational Inference via Score Matching,"Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching.  We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.","['Probabilistic Methods', 'Semi-implicit variational inference', 'denoising score matching', 'Minimax Optimization']",[],"['Longlin Yu', 'Cheng Zhang']","['Peking University', 'Peking University']",[]
https://iclr.cc/virtual/2023/poster/11611,Fairness & Bias,Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats,"Quantization of the weights and activations is one of the main methods to reduce the computational footprint of Deep Neural Networks (DNNs) training. Current methods enable 4-bit quantization of the forward phase. However, this constitutes only a third of the training process. Reducing the computational footprint of the entire training process requires the quantization of the neural gradients, i.e., the loss gradients with respect to the outputs of intermediate neural layers. Previous works separately showed that accurate 4-bit quantization of the neural gradients needs to (1) be unbiased and (2) have a log scale. However, no previous work aimed to combine both ideas, as we do in this work. Specifically, we examine the importance of having unbiased quantization in quantized neural network training, where to maintain it, and how to combine it with logarithmic quantization. Based on this, we suggest a $\textit{logarithmic unbiased quantization}$ (LUQ) method to quantize all both the forward and backward phase to 4-bit, achieving state-of-the-art results in 4-bit training without overhead. For example, in ResNet50 on ImageNet, we achieved a degradation of 1.1 %. We further improve this to degradation of only 0.32 % after three epochs of high precision fine-tunining, combined with a variance reduction method---where both these methods add overhead comparable to previously suggested methods. A reference implementation is supplied in the supplementary material.","['Deep Learning and representational learning', '4bit', 'compression', 'acceleration', 'quantization']",[],"['Brian Chmiel', 'Ron Banner', 'Elad Hoffer', 'Hilla Ben-Yaacov', 'Daniel Soudry']","['Technion - Israel Institute of Technology, Technion', 'Intel', 'Habana Labs (Intel)', 'Technion, Technion', 'Technion - Israel Institute of Technology, Technion']",[]
https://iclr.cc/virtual/2023/poster/11112,Fairness & Bias,Spectral Augmentation for Self-Supervised Learning on Graphs,"Graph contrastive learning (GCL), as an emerging self-supervised learning technique on graphs, aims to learn representations via instance discrimination. Its performance heavily relies on graph augmentation to reflect invariant patterns that are robust to small perturbations; yet it still remains unclear about what graph invariance GCL should capture. Recent studies mainly perform topology augmentations in a uniformly random manner in the spatial domain, ignoring its influence on the intrinsic structural properties embedded in the spectral domain. In this work, we aim to find a principled way for topology augmentations by exploring the invariance of graphs from the spectral perspective. We develop spectral augmentation which guides topology augmentations by maximizing the spectral change. Extensive experiments on both graph and node classification tasks demonstrate the effectiveness of our method in self-supervised representation learning. The proposed method also brings promising generalization capability in transfer learning, and is equipped with intriguing robustness property under adversarial attacks. Our study sheds light on a general principle for graph topology augmentation.","['Unsupervised and Self-supervised learning', 'graph self-supervised learning', 'graph spectral theory', 'graph augmentation']",[],"['Lu Lin', 'Jinghui Chen', 'Hongning Wang']","['Pennsylvania State University', 'Pennsylvania State University', 'Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/10995,Fairness & Bias,Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Approach,"Many machine learning problems today have multiple objective functions. They appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic multi-objective gradient correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the nonconvex setting. Simulations on multi-task supervised and reinforcement learning demonstrate the effectiveness of our method relative to the state-of-the-art methods.","['Optimization', 'machine learning', 'multi-objective optimization']",[],"['Heshan Devaka Fernando', 'Han Shen', 'Miao Liu', 'Subhajit Chaudhury', 'Keerthiram Murugesan', 'Tianyi Chen']","['Rensselaer Polytechnic Institute', 'Rensselaer Polytechnic Institute', 'International Business Machines', 'International Business Machines', 'International Business Machines', 'Rensselaer Polytechnic Institute']",[]
https://iclr.cc/virtual/2023/poster/11357,Fairness & Bias,Graph Signal Sampling for Inductive One-Bit Matrix Completion: a Closed-form Solution,"Inductive one-bit matrix completion is motivated by modern applications such as recommender systems, where new users would appear at test stage with the ratings consisting of only ones and no zeros. We propose a unified graph signal sampling framework which enjoys the benefits of graph signal analysis and processing. The key idea is to transform each user's ratings on the items to a function (signal) on the vertices of an item-item graph, then learn structural graph properties to recover the function from its values on certain vertices --- the problem of graph signal sampling. We propose a class of regularization functionals that takes into account discrete random label noise in the graph vertex domain, then develop the GS-IMC approach which biases the reconstruction towards functions that vary little between adjacent vertices for noise reduction. Theoretical result shows that accurate reconstructions can be achieved under mild conditions. For the online setting, we develop a Bayesian extension, i.e., BGS-IMC which considers continuous random Gaussian noise in the graph Fourier domain and builds upon a prediction-correction update algorithm to obtain the unbiased and minimum-variance reconstruction. Both GS-IMC and BGS-IMC have closed-form solutions and thus are highly scalable in large data. Experiments show that our methods achieve state-of-the-art performance on public benchmarks.","['General Machine Learning', 'graph signal sampling', 'inductive one-bit matrix completion']",[],"['Chao Chen', 'Haoyu Geng', 'Gang Zeng', 'zhaobing Han', 'Hua Chai', 'Xiaokang Yang', 'Junchi Yan']","['Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'Didi Research', 'Institute of Automation, Chinese Academy of Sciences', 'university  of tianjin of china, Tianjin University', 'Shanghai Jiao Tong University, China', 'Shanghai Jiao Tong University']",[]
https://iclr.cc/virtual/2023/poster/10698,Fairness & Bias,EquiMod: An Equivariance Module to Improve Visual Instance Discrimination,"Recent self-supervised visual representation methods are closing the gap with supervised learning performance. Most of these successful methods rely on maximizing the similarity between embeddings of related synthetic inputs created through data augmentations. This can be seen as a task that encourages embeddings to leave out factors modified by these augmentations, i.e. to be invariant to them. However, this only considers one side of the trade-off in the choice of the augmentations: they need to strongly modify the images to avoid simple solution shortcut learning (e.g. using only color histograms), but on the other hand, augmentations-related information may be lacking in the representations for some downstream tasks (e.g. literature shows that color is important for bird and flower classification). Few recent works proposed to mitigate this problem of using only an invariance task by exploring some form of equivariance to augmentations. This has been performed by learning additional embeddings space(s), where some augmentation(s) cause embeddings to differ, yet in a non-controlled way. In this work, we introduce EquiMod a generic equivariance module that structures the learned latent space, in the sense that our module learns to predict the displacement in the embedding space caused by the augmentations. We show that applying that module to state-of-the-art invariance models, such as BYOL and SimCLR, increases the performances on the usual CIFAR10 and ImageNet datasets. Moreover, while our model could collapse to a trivial equivariance, i.e. invariance, we observe that it instead automatically learns to keep some augmentations-related information beneficial to the representations.","['Deep Learning and representational learning', 'self-supervised learning', 'equivariance', 'contrastive learning', 'representation learning']",[],"['Alexandre DEVILLERS', 'Mathieu Lefort']","['Université Claude Bernard (Lyon I)', 'LIRIS, CNRS']",[]
https://iclr.cc/virtual/2023/poster/12091,Fairness & Bias,Causal Balancing for Domain Generalization,"While machine learning models rapidly advance the state-of-the-art on various real-world tasks, out-of-domain (OOD) generalization remains a challenging problem given the vulnerability of these models to spurious correlations. We propose a balanced mini-batch sampling strategy to transform a biased data distribution into a spurious-free balanced distribution, based on the invariance of the underlying causal mechanisms for the data generation process. We argue that the Bayes optimal classifiers trained on such balanced distribution are minimax optimal across a diverse enough environment space. We also provide an identifiability guarantee of the latent variable model of the proposed data generation process, when utilizing enough train environments. Experiments are conducted on DomainBed, demonstrating empirically that our method obtains the best performance across 20 baselines reported on the benchmark.","['Probabilistic Methods', 'causality', 'latent variable model', 'domain generalization']",[],"['Xinyi Wang', 'Michael Saxon', 'Jiachen Li', 'Hongyang Zhang', 'Kun Zhang', 'William Yang Wang']","['UC Santa Barbara', 'UC Santa Barbara', 'University of California, Santa Barbara', 'School of Computer Science, University of Waterloo', 'Carnegie Mellon University', 'UC Santa Barbara']",[]
https://iclr.cc/virtual/2023/poster/11165,Fairness & Bias,Imitating Human Behaviour with Diffusion Models,"Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment.","['Reinforcement Learning', 'Diffusion Models', 'generative models', 'imitation learning', 'behavioral cloning']",[],"['Tim Pearce', 'Tabish Rashid', 'Anssi Kanervisto', 'David Bignell', 'Mingfei Sun', 'Raluca Georgescu', 'Sergio Valcarcel Macua', 'Shan Zheng Tan', 'Ida Momennejad', 'Katja Hofmann', 'Sam Devlin']","['Research, Microsoft', 'Microsoft', 'Microsoft', 'Research, Microsoft', 'University of Manchester ', 'Microsoft', 'Microsoft', 'Research, Microsoft', 'Research, Microsoft', 'Microsoft', 'Microsoft Research']",[]
https://iclr.cc/virtual/2023/poster/10985,Fairness & Bias,Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules,"Work on fast weight programmers has demonstrated the effectiveness of key/value outer product-based learning rules for sequentially generating a weight matrix (WM) of a neural net (NN) by another NN or itself. However, the weight generation steps are typically not visually interpretable by humans, because the contents stored in the WM of an NN are not. Here we apply the same principle to generate natural images. The resulting fast weight painters (FPAs) learn to execute sequences of delta learning rules to sequentially generate images as sums of outer products of self-invented keys and values, one rank at a time, as if each image was a WM of an NN. We train our FPAs in the generative adversarial networks framework, and evaluate on various image datasets. We show how these generic learning rules can generate images with respectable visual quality without any explicit inductive bias for images. While the performance largely lags behind the one of specialised state-of-the-art image generators, our approach allows for visualising how synaptic learning rules iteratively produce complex connection patterns, yielding human-interpretable meaningful images. Finally, we also show that an additional convolutional U-Net (now popular in diffusion models) at the output of an FPA can learn one-step ""denoising"" of FPA-generated images to enhance their quality.Our code is public.","['Deep Learning and representational learning', 'image generation', 'learning rules', 'linear Transformers', 'gans', 'Fast Weight Programmers']",[],"['Kazuki Irie', 'Jürgen Schmidhuber']","['Harvard University', 'King Abdullah University of Science and Technology']",[]
https://iclr.cc/virtual/2023/poster/12155,Fairness & Bias,Distilling Cognitive Backdoor Patterns within an Image,"This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the ``minimal essence"" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor attacks.We also show that CD can potentially be applied to help detect potential biases from face datasets.Code is available at https://github.com/HanxunH/CognitiveDistillation.","['Deep Learning and representational learning', 'Backdoor defence', 'Backdoor sample detection']",[],"['Hanxun Huang', 'Xingjun Ma', 'Sarah Monazam Erfani']","['University of Melbourne', 'Fudan University', 'The University of Melbourne']",[]
https://iclr.cc/virtual/2023/poster/10773,Fairness & Bias,A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias,"Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to rely upon simple features, as SB has recently been shown to underlie several problems in robust generalization. Using a synthetic dataset, we demonstrate the susceptibility of existing protocols to SB. Given the strong effectiveness of LP+FT, we then propose modified linear probes that help mitigate SB, and lead to better initializations for subsequent FT. We verify the effectiveness of the proposed LP+FT variants for decreasing SB in a controlled setting, and their ability to improve OOD generalization and safety on three adaptation datasets.","['Deep Learning and representational learning', 'robustness', 'transfer learning', 'adaptation', 'data augmentation']",[],"['Puja Trivedi', 'Danai Koutra', 'Jayaraman J. Thiagarajan']","['University of Michigan', 'Amazon', 'Lawrence Livermore National Labs']",[]
https://iclr.cc/virtual/2023/poster/11697,Fairness & Bias,Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks,"The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information. Specifically, we propose two novel implementations: a Dynamic Permutation Network (DPN) and a Hyper Policy Network (HPN). The core idea is to build separate entity-wise PI input and PE output network modules to connect the entity-factored state space and action space in an end-to-end way. DPN achieves such connections by two separate module selection networks, which consistently assign the same input module to the same input entity (guarantee PI) and assign the same output module to the same entity-related output (guarantee PE). To enhance the representation capability, HPN replaces the module selection networks of DPN with hypernetworks to directly generate the corresponding module weights. Extensive experiments in SMAC, Google Research Football and MPE validate that the proposed methods significantly boost the performance and the learning efficiency of existing MARL algorithms. Remarkably, in SMAC, we achieve 100% win rates in almost all hard and super-hard scenarios (never achieved before).","['Reinforcement Learning', 'Multiagent Reinforcement Learning', 'Permutation Equivariance', 'permutation invariance']",[],"['Jianye HAO', 'Xiaotian Hao', 'Hangyu Mao', 'Weixun Wang', 'Yaodong Yang', 'Dong Li', 'YAN ZHENG', 'Zhen Wang']","['Tianjin University', 'university  of tianjin of china, Tianjin University', 'Sensetime Research', 'Tianjin University', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'Huawei Technologies Ltd.', 'Tianjin Unibersity, China', 'Northwestern Polytechnical University']",[]
https://iclr.cc/virtual/2023/poster/10759,Fairness & Bias,A Kernel Perspective of Skip Connections in Convolutional Networks,"Over-parameterized residual networks (ResNets) are amongst the most successful convolutional neural architectures for image processing. Here we study their properties through their Gaussian Process and Neural Tangent kernels. We derive explicit formulas for these kernels, analyze their spectra, and provide bounds on their implied condition numbers. Our results indicate that (1) with ReLU activation, the eigenvalues of these residual kernels decay polynomially at a similar rate compared to the same kernels when skip connections are not used, thus maintaining a similar frequency bias; (2) however, residual kernels are more locally biased. Our analysis further shows that the matrices obtained by these residual kernels yield favorable condition numbers at finite depths than those obtained without the skip connections, enabling therefore faster convergence of training with gradient descent.",['Theory'],[],"['Daniel Barzilai', 'Amnon Geifman', 'Meirav Galun', 'Ronen Basri']","['Weizmann Institute of Science', 'Weizmann Institute, Technion', 'Weizmann Institute', 'Meta Platforms Inc.']",[]
https://iclr.cc/virtual/2023/poster/11560,Fairness & Bias,Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent,"It is commonly believed that the implicit regularization of optimizers is needed for neural networks to generalize in the overparameterized regime. In this paper, we observe experimentally that this implicit regularization behavior is {\em generic}, i.e. it does not depend strongly on the choice of optimizer. We demonstrate this by training neural networks using several gradient-free optimizers, which do not benefit from properties that are often attributed to gradient-based optimizers.   This includes a guess-and-check optimizer that generates uniformly random parameter vectors until finding one that happens to achieve perfect train accuracy, and a zeroth-order Pattern Search optimizer that uses no gradient computations. In the low sample and few-shot regimes, where zeroth order optimizers are most computationally tractable, we find that these non-gradient optimizers achieve test accuracy comparable to SGD. The code to reproduce results can be found at https://github.com/Ping-C/optimizer .","['General Machine Learning', 'generalization', 'regularization']",[],"['Ping-yeh Chiang', 'Renkun Ni', 'David Yu Miller', 'Arpit Bansal', 'Jonas Geiping', 'Micah Goldblum', 'Tom Goldstein']","['University of Maryland, College Park', 'Department of Computer Science, University of Maryland, College Park', 'Department of Computer Science, University of Maryland, College Park', 'University of Maryland, College Park', 'ELLIS Institute Tübingen', 'New York University', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2023/poster/11494,Fairness & Bias,Stochastic No-regret Learning for General Games with Variance Reduction,"We show that a stochastic version of optimistic mirror descent (OMD), a variant of mirror descent with recency bias, converges fast in general games. More specifically, with our algorithm, the individual regret of each player vanishes at a speed of $O(1/T^{3/4})$ and the sum of all players' regret vanishes at a speed of $O(1/T)$, which is an improvement upon the $O(1/\sqrt{T})$ convergence rate of prior stochastic algorithms, where $T$ is the number of interaction rounds. Due to the advantage of stochastic methods in the computational cost, we significantly improve the time complexity over the deterministic algorithms to approximate coarse correlated equilibrium. To achieve lower time complexity, we equip the stochastic version of OMD in \cite{alacaoglu2021stochastic} with a novel low-variance Monte-Carlo estimator. Our algorithm extends previous works \cite{alacaoglu2021stochastic,carmon2019variance} from two-player zero-sum games to general games.","['Theory', 'game theory']",[],"['Yichi Zhou', 'Fang Kong', 'Shuai Li']","['Microsoft', 'Shanghai Jiao Tong University', 'John Hopcroft Center, Shanghai Jiao Tong University']",[]
https://iclr.cc/virtual/2023/poster/11082,Fairness & Bias,Efficient Conditionally Invariant Representation Learning,"We introduce the Conditional Independence Regression CovariancE (CIRCE), a measure of conditional independence for multivariate continuous-valued variables. CIRCE applies as a regularizer in settings where we wish to learn neural features $\varphi(X)$ of data $X$ to estimate a target $Y$, while being conditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are assumed to be continuous-valued but relatively low dimensional, whereas $X$ and its features may be complex and high dimensional. Relevant settings include domain-invariant learning, fairness, and causal learning. The procedure requires just a single ridge regression from $Y$ to kernelized features of $Z$, which can be done in advance. It is then only necessary to enforce independence of $\varphi(X)$ from residuals of this regression, which is possible with attractive estimation properties and consistency guarantees. By contrast, earlier measures of conditional feature dependence require multiple regressions for each step of feature learning, resulting in more severe bias and variance, and greater computational cost. When sufficiently rich features are used, we establish that CIRCE is zero if and only if $\varphi(X) \perp \!\!\! \perp Z \mid Y$. In experiments, we show superior performance to previous methods on challenging benchmarks, including learning conditionally invariant image features. Code for image data experiments is available at github.com/namratadeka/circe.","['Deep Learning and representational learning', 'kernel methods', 'conditional independence']",[],"['Roman Pogodin', 'Yazhe Li', 'Danica J. Sutherland', 'Victor Veitch', 'Arthur Gretton']","['McGill/Mila', 'University College London', 'University of British Columbia', 'University of Chicago', 'University College London']",[]
https://iclr.cc/virtual/2023/poster/11744,Fairness & Bias,Stable Target Field for Reduced Variance Score Estimation in Diffusion Models,"Diffusion models generate samples by reversing a fixed forward diffusion process. Despite already providing impressive empirical results, these diffusion models algorithms can be further improved by reducing the variance of the training targets in their denoising score-matching objective. We argue that the source of such variance lies in the handling of intermediate noise-variance scales, where multiple modes in the data affect the direction of reverse paths. We propose to remedy the problem by incorporating a reference batch which we use to calculate weighted conditional scores as more stable training targets. We show that the procedure indeed helps in the challenging intermediate regime by reducing (the trace of) the covariance of training targets. The new stable targets can be seen as trading bias for reduced variance, where the bias vanishes with increasing reference batch size. Empirically, we show that the new objective improves the image quality, stability, and training speed of various popular diffusion models across datasets with both general ODE and SDE solvers. When used in combination with EDM, our method yields a current SOTA FID of 1.90 with 35 network evaluations on the unconditional CIFAR-10 generation task. The code is available at https://github.com/Newbeeer/stf","['Generative models', 'generative model', 'variance reduction', 'score-based models', 'Diffusion Models']",[],"['Yilun Xu', 'Shangyuan Tong', 'Tommi Jaakkola']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11044,Fairness & Bias,The Lie Derivative for Measuring Learned Equivariance,"Equivariance guarantees that a model's predictions capture key symmetries in data. When an image is translated or rotated, an equivariant model's representation of that image will translate or rotate accordingly. The success of convolutional neural networks has historically been tied to translation equivariance directly encoded in their architecture. The rising success of vision transformers, which have no explicit architectural bias towards equivariance, challenges this narrative and suggests that augmentations and training data might also play a significant role in their performance. In order to better understand the role of equivariance in recent vision models, we apply the Lie derivative, a method for measuring equivariance with strong mathematical foundations and minimal hyperparameters. Using the Lie derivative, we study the equivariance properties of hundreds of pretrained models, spanning CNNs, transformers, and Mixer architectures. The scale of our analysis allows us to separate the impact of architecture from other factors like model size or training method. Surprisingly, we find that many violations of equivariance can be linked to spatial aliasing in ubiquitous network layers, such as pointwise non-linearities, and that as models get larger and more accurate they tend to display more equivariance, regardless of architecture. For example, transformers can be more equivariant than convolutional neural networks after training.",['Deep Learning and representational learning'],[],"['Nate Gruver', 'Marc Anton Finzi', 'Micah Goldblum', 'Andrew Gordon Wilson']","['New York University', 'Carnegie Mellon University', 'New York University', 'Cornell University']",[]
https://iclr.cc/virtual/2023/poster/11763,Fairness & Bias,Bridge the Inference Gaps of Neural Processes via Expectation Maximization,"The neural process (NP) is a family of computationally efficient models for learning distributions over functions. However, it suffers from under-fitting and shows suboptimal performance in practice. Researchers have primarily focused on incorporating diverse structural inductive biases, e.g. attention or convolution, in modeling. The topic of inference suboptimality and an analysis of the NP from the optimization objective perspective has hardly been studied in earlier work. To fix this issue, we propose a surrogate objective of the target log-likelihood of the meta dataset within the expectation maximization framework. The resulting model, referred to as the Self-normalized Importance weighted Neural Process (SI-NP), can learn a more accurate functional prior and has an improvement guarantee concerning the target log-likelihood. Experimental results show the competitive performance of SI-NP over other NPs objectives and illustrate that structural inductive biases, such as attention modules, can also augment our method to achieve SOTA performance.",['Probabilistic Methods'],[],"['Cheems Wang', 'Marco Federici', 'Herke van Hoof']","['Tsinghua University', 'University of Amsterdam', 'University of Amsterdam']",[]
https://iclr.cc/virtual/2023/poster/11733,Fairness & Bias,Visual Classification via Description from Large Language Models,"Vision-language models such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what the model ``thinks"" it is seeing to make its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.","['Applications', 'prompting', 'Multimodal', 'CLIP', 'GPT-3', 'large language models', 'zero-shot recognition', 'vision-language models']",[],"['Sachit Menon', 'Carl Vondrick']","['Columbia University', 'Columbia University']",[]
https://iclr.cc/virtual/2023/poster/12182,Fairness & Bias,E-CRF: Embedded Conditional Random Field for Boundary-caused Class Weights Confusion in Semantic Segmentation,"Modern semantic segmentation methods devote much effect to adjusting image feature representations to improve the segmentation performance in various ways, such as architecture design, attention mechnism, etc. However, almost all those methods neglect the particularity of class weights (in the classification layer) in segmentation models. In this paper, we notice that the class weights of categories that tend to share many adjacent boundary pixels lack discrimination, thereby limiting the performance. We call this issue Boundary-caused Class Weights Confusion (BCWC). We try to focus on this problem and propose a novel method named Embedded Conditional Random Field (E-CRF) to alleviate it. E-CRF innovatively fuses the CRF into the CNN network as an organic whole for more effective end-to-end optimization. The reasons are two folds. It utilizes CRF to guide the message passing between pixels in high-level features to purify the feature representation of boundary pixels, with the help of inner pixels belonging to the same object. More importantly, it enables optimizing class weights from both scale and direction during backpropagation. We make detailed theoretical analysis to prove it. Besides, superpixel is integrated into E-CRF and served as an auxiliary to exploit the local object prior for more reliable message passing. Finally, our proposed method yields impressive results on ADE20K, Cityscapes, and Pascal Context datasets.",[],[],"['Jie Zhu', 'Huabin Huang', 'Banghuai Li', 'Leye Wang']","['Peking University', 'Megvii Technology Inc.', 'Momenta', 'Peking University']",[]
https://iclr.cc/virtual/2023/poster/10703,Fairness & Bias,Online Bias Correction for Task-Free Continual Learning,"Task-free continual learning is the machine-learning setting where a model is trained online with data generated by a nonstationary stream. Conventional wisdom suggests that, in this setting, models are trained using an approach called experience replay, where the risk is computed both with respect to current stream observations and to a small subset of past observations. In this work, we explain both theoretically and empirically how experience replay biases the outputs of the model towards recent stream observations. Moreover, we propose a simple approach to mitigate this bias online, by changing how the output layer of the model is optimized. We show that our approach improves significantly the learning performance of experience-replay approaches over different datasets. Our findings suggest that, when performing experience replay, the output layer of the model should be optimized separately from the preceding layers.","['General Machine Learning', 'Task-Free Continual Learning']",[],"['Aristotelis Chrysakis', 'Marie-Francine Moens']","['KU Leuven', 'KU Leuven, KU Leuven']",[]
https://iclr.cc/virtual/2023/poster/10704,Fairness & Bias,Don’t fear the unlabelled: safe semi-supervised learning via debiasing,"Semi-supervised learning (SSL) provides an effective means of leveraging unlabelled data to improve a model’s performance. Even though the domain has received a considerable amount of attention in the past years, most methods present the common drawback of lacking theoretical guarantees. Our starting point is to notice that the estimate of the risk that most discriminative SSL methods minimise is biased, even asymptotically. This bias impedes the use of standard statistical learning theory and can hurt empirical performance. We propose a simple way of removing the bias. Our debiasing approach is straightforward to implement and applicable to most deep SSL methods.  We provide simple theoretical guarantees on the trustworthiness of these modified methods, without having to rely on the strong assumptions on the data distribution that SSL theory usually requires. In particular, we provide generalisation error bounds for the proposed methods. We evaluate debiased versions of different existing SSL methods, such as the Pseudo-label method and Fixmatch, and show that debiasing can compete with classic deep SSL techniques in various settings by providing better calibrated models. Additionally, we provide a theoretical explanation of the intuition of the popular SSL methods.  An implementation of a debiased version of Fixmatch is available athttps://github.com/HugoSchmutz/DeFixmatch","['Unsupervised and Self-supervised learning', 'semi-supervised learning', 'variance reduction', 'deep learning', 'control variate', 'asymptotic statistics', 'empirical risk minimisation']",[],"['Hugo Schmutz', 'Olivier HUMBERT', 'Pierre-Alexandre Mattei']","['INRIA', ""Université Côte d'Azur"", 'INRIA']",[]
https://iclr.cc/virtual/2023/poster/11452,Fairness & Bias,MEDFAIR: Benchmarking Fairness for Medical Imaging,"A multitude of work has shown that machine learning-based medical diagnosis systems can be biased against certain subgroups of people. This has motivated a growing number of bias mitigation algorithms that aim to address fairness issues in machine learning. However, it is difficult to compare their effectiveness in medical imaging for two reasons. First, there is little consensus on the criteria to assess fairness. Second, existing bias mitigation algorithms are developed under different settings, e.g., datasets, model selection strategies, backbones, and fairness metrics, making a direct comparison and evaluation based on existing results impossible. In this work, we introduce MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging. MEDFAIR covers eleven algorithms from various categories, ten datasets from different imaging modalities, and three model selection criteria. Through extensive experiments, we find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes; while in contrast, state-of-the-art bias mitigation algorithms do not significantly improve fairness outcomes over empirical risk minimization (ERM) in both in-distribution and out-of-distribution settings. We evaluate fairness from various perspectives and make recommendations for different medical application scenarios that require different ethical principles. Our framework provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning. Code is available at https://github.com/ys-zong/MEDFAIR.","['Social Aspects of Machine Learning', 'medical imaging', 'benchmark', 'fairness', 'Bias Mitigation']",[],"['Yongshuo Zong', 'Yongxin Yang', 'Timothy Hospedales']","['University of Edinburgh', 'Queen Mary University of London', 'University of Edinburgh']",[]
https://iclr.cc/virtual/2023/poster/11555,Fairness & Bias,Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States,"Graph Neural Networks (GNNs) are widely used deep learning models that can extract meaningful representations from graph datasets and achieve great success in many machine learning tasks. Among them, graph neural networks with iterative iterations like unfolded GNNs and implicit GNNs can effectively capture long-range dependencies in graphs and demonstrate superior performance on large graphs since they can mathematically ensure its convergence to some nontrivial solution after lots of aggregations. However, the aggregation time for such models costs a lot as they need to aggregate the full graph in each update. Such weakness limits the scalability of the implicit graph models. To tackle such limitations, we propose two unbiased stochastic proximal solvers inspired by the stochastic proximal gradient descent method and its variance reduction variant called USP and USP-VR solvers. From the point of stochastic optimization, we theoretically prove that our solvers are unbiased, which can converge to the same solution as the original solvers for unfolded GNNs and implicit GNNs. Furthermore, the computation complexities for unfolded GNNs and implicit GNNs with our proposed solvers are significantly less than their vanilla versions. Experiments on various large graph datasets show that our proposed solvers are more efficient and can achieve state-of-the-art performance.",['Deep Learning and representational learning'],[],"['Mingjie Li', 'Yifei Wang', 'Yisen Wang', 'Zhouchen Lin']","['Peking University', 'Massachusetts Institute of Technology', 'Peking University', 'Peking University']",[]
https://iclr.cc/virtual/2023/poster/11837,Fairness & Bias,A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning,"Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO's competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO","['Deep Learning and representational learning', 'Class-Incremental Learning']",[],"['Da-Wei Zhou', 'Qi-Wei Wang', 'Han-Jia Ye', 'De-Chuan Zhan']","['Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University']",[]
https://iclr.cc/virtual/2023/poster/11841,Fairness & Bias,Improved Convergence of Differential Private SGD with Gradient Clipping,"Differential private stochastic gradient descent (DP-SGD) with gradient clipping (DP-SGD-GC) is an effective optimization algorithm that can train machine learning models with a privacy guarantee. Despite the popularity of DP-SGD-GC, its convergence in unbounded domain without the Lipschitz continuous assumption is less-understood; existing analysis of DP-SGD-GC either impose additional assumptions or end up with an utility bound that involves an non-vanishing bias term. In this work, for smooth and unconstrained problems, we improve the current analysis and show that DP-SGD-GC can achieve a vanishing utility bound without any bias term. Furthermore, when the noise generated from subsampled gradients is light-tailed, we prove that DP-SGD-GC can achieve nearly the same utility bound as DP-SGD applies to the Lipschitz continuous objectives. As a by-product, we propose a new clipping technique, called value clipping, to mitigate the computational overhead caused by the classic gradient clipping. Experiments on standard benchmark datasets are conducted to support our analysis.",['Social Aspects of Machine Learning'],[],"['Huang Fang', 'Xiaoyun Li', 'Chenglin Fan', 'Ping Li']","['Baidu', 'Baidu', 'Baidu Research', 'Rutgers University']",[]
https://iclr.cc/virtual/2023/poster/11957,Fairness & Bias,Robust Fair Clustering: A Novel Fairness Attack and Defense Framework,"Clustering algorithms are widely used in many societal resource allocation applications, such as loan approvals and candidate recruitment, among others, and hence, biased or unfair model outputs can adversely impact individuals that rely on these applications. To this end, many $\textit{fair}$ clustering approaches have been recently proposed to counteract this issue. Due to the potential for significant harm, it is essential to ensure that fair clustering algorithms provide consistently fair outputs even under adversarial influence. However, fair clustering algorithms have not been studied from an adversarial attack perspective. In contrast to previous research, we seek to bridge this gap and conduct a robustness analysis against fair clustering by proposing a novel $\textit{black-box fairness attack}$. Through comprehensive experiments, we find that state-of-the-art models are highly susceptible to our attack as it can reduce their fairness performance significantly. Finally, we propose Consensus Fair Clustering (CFC), the first $\textit{robust fair clustering}$ approach that transforms consensus clustering into a fair graph partitioning problem, and iteratively learns to generate fair cluster outputs. Experimentally, we observe that CFC is highly robust to the proposed attack and is thus a truly robust fair clustering alternative.","['Unsupervised and Self-supervised learning', 'consensus clustering', 'Fairness Attack', 'Data Clustering', 'Fairness Defense']",[],"['Anshuman Chhabra', 'Peizhao Li', 'Prasant Mohapatra', 'Hongfu Liu']","['University of California, Davis', 'Brandeis University', 'University of South Florida', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/11032,Fairness & Bias,Verifying the Union of Manifolds Hypothesis for Image Data,"Deep learning has had tremendous success at learning low-dimensional representations of high-dimensional data. This success would be impossible if there was no hidden low-dimensional structure in data of interest; this existence is posited by the manifold hypothesis, which states that the data lies on an unknown manifold of low intrinsic dimension. In this paper, we argue that this hypothesis does not properly capture the low-dimensional structure typically present in image data. Assuming that data lies on a single manifold implies intrinsic dimension is identical across the entire data space, and does not allow for subregions of this space to have a different number of factors of variation. To address this deficiency, we consider the union of manifolds hypothesis, which states that data lies on a disjoint union of manifolds of varying intrinsic dimensions. We empirically verify this hypothesis on commonly-used image datasets, finding that indeed, observed data lies on a disconnected set and that intrinsic dimension is not constant. We also provide insights into the implications of the union of manifolds hypothesis in deep learning, both supervised and unsupervised, showing that designing models with an inductive bias for this structure improves performance across classification and generative modelling tasks. Our code is available at https://github.com/layer6ai-labs/UoMH.","['Deep Learning and representational learning', 'generative models', 'manifold hypothesis', 'geometry']",[],"['Bradley CA Brown', 'Anthony L. Caterini', 'Brendan Leigh Ross', 'Jesse C. Cresswell', 'Gabriel Loaiza-Ganem']","['University of Waterloo', 'Layer6', 'Layer 6 AI', 'Layer 6 AI', 'Layer 6 AI']",[]
https://iclr.cc/virtual/2023/poster/11426,Fairness & Bias,TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs,"Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for temporal logical rules learning. By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model. We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two benchmark datasets. We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results. In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different. In all these cases, TILP works much better than the state-of-the-art methods.",['Probabilistic Methods'],[],"['Yuan Yang', 'Faramarz Fekri', 'James Clayton Kerce']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11432,Fairness & Bias,Koopman Neural Operator Forecaster for Time-series with Temporal Distributional Shifts,"Temporal distributional shifts, with underlying dynamics changing over time, frequently occur in real-world time series and pose a fundamental challenge for deep neural networks (DNNs). In this paper, we propose a novel deep sequence model based on the Koopman theory for time series forecasting: Koopman Neural Forecaster (KNF) that leverages DNNs to learn the linear Koopman space and the coefficients of chosen measurement functions. KNF imposes appropriate inductive biases for improved robustness against distributional shifts, employing both a global operator to learn shared characteristics and a local operator to capture changing dynamics, as well as a specially-designed feedback loop to continuously update the learnt operators over time for rapidly varying behaviors. We demonstrate that KNF achieves superior performance compared to the alternatives, on multiple time series datasets that are shown to suffer from distribution shifts.","['Deep Learning and representational learning', 'Time series forecasting', 'Temporal distributional shifts', 'Koopman Theory']",[],"['Rui Wang', 'Yihe Dong', 'Sercan O Arik', 'Rose Yu']","['University of California, San Diego', 'Google', 'Google', 'University of California, San Diego']",[]
https://iclr.cc/virtual/2023/poster/11892,Fairness & Bias,Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models,"Deep generative models (DGMs) are data-eager because learning a complex model on limited data suffers from a large variance and easily overfits. Inspired by the classical perspective of the bias-variance tradeoff, we propose regularized deep generative model (Reg-DGM), which leverages a nontransferable pre-trained model to reduce the variance of generative modeling with limited data. Formally, Reg-DGM optimizes a weighted sum of a certain divergence and the expectation of an energy function, where the divergence is between the data and the model distributions, and the energy function is defined by the pre-trained model w.r.t. the model distribution. We analyze a simple yet representative Gaussian-fitting case to demonstrate how the weighting hyperparameter trades off the bias and the variance. Theoretically, we characterize the existence and the uniqueness of the global minimum of Reg-DGM in a non-parametric setting and prove its convergence with neural networks trained by gradient-based methods. Empirically, with various pre-trained feature extractors and a data-dependent energy function, Reg-DGM consistently improves the generation performance of strong DGMs with limited data and achieves competitive results to the state-of-the-art methods. Our implementation is available at https://github.com/ML-GSAI/Reg-ADA-APA.",['Generative models'],[],"['Hongtao Liu', 'Xiaodong Liu', 'Fan Bao', 'Weiran Shen', 'Chongxuan Li']","['Renmin University of China', 'Renmin University of China', 'Tsinghua University, Tsinghua University', 'Renmin University of China', 'Renmin University of China']",[]
https://iclr.cc/virtual/2023/poster/11297,Fairness & Bias,Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play,"Standard empirical risk minimization (ERM) training can produce deep neural network (DNN) models that are accurate on average but under-perform in under-represented population subgroups, especially when there are imbalanced group distributions in the long-tailed training data. Therefore, approaches that improve the accuracy - group robustness trade-off frontier of a DNN model (i.e. improving worst-group accuracy without sacrificing average accuracy, or vice versa) is of crucial importance.   Uncertainty-based active learning (AL) can potentially improve the frontier by preferentially sampling underrepresented subgroups to create a more balanced training dataset.  However, the quality of uncertainty estimates from modern DNNs tend to degrade in the presence of spurious correlations and dataset bias, compromising the effectiveness of AL for sampling tail groups. In this work, we propose Introspective Self-play (ISP), a simple approach to improve the uncertainty estimation of a deep neural network under dataset bias, by adding an auxiliary introspection task requiring a model to predict the bias for each data point in addition to the label. We show that ISP provably improves the bias-awareness of the model representation and the resulting uncertainty estimates. On two real-world tabular and language tasks,ISP serves as a simple “plug-in” for AL model training, consistently improving both the tail-group sampling rate and the final accuracy-fairness trade-off frontier of popular AL methods.","['Probabilistic Methods', 'active learning', 'spurious correlation', 'uncertainty quantification']",[],"['Jeremiah Zhe Liu', 'Krishnamurthy Dj Dvijotham', 'Jihyeon Lee', 'Quan Yuan', 'Balaji Lakshminarayanan', 'Deepak Ramachandran']","['Google Research', 'Google DeepMind', 'Google', 'Google', 'Google Brain', 'Google']",[]
https://iclr.cc/virtual/2023/poster/11801,Fairness & Bias,Flow Annealed Importance Sampling Bootstrap,"Normalizing flows are tractable density models that can approximate complicated target distributions, e.g. Boltzmann distributions of physical systems. However, current methods for training flows either suffer from mode-seeking behavior, use samples from the target generated beforehand by expensive MCMC methods, or use stochastic losses that have high variance. To avoid these problems, we augment flows with annealed importance sampling (AIS) and minimize the mass-covering $\alpha$-divergence with $\alpha=2$, which minimizes importance weight variance. Our method, Flow AIS Bootstrap (FAB), uses AIS to generate samples in regions where the flow is a poor approximation of the target, facilitating the discovery of new modes. We apply FAB to multimodal targets and show that we can approximate them very accurately where previous methods fail. To the best of our knowledge, we are the first to learn the Boltzmann distribution of the alanine dipeptide molecule using only the unnormalized target density, without access to samples generated via Molecular Dynamics (MD) simulations: FAB produces better results than training via maximum likelihood on MD samples while using 100 times fewer target evaluations. After reweighting the samples, we obtain unbiased histograms of dihedral angles that are almost identical to the ground truth.","['Machine Learning for Sciences', 'approximate inference', 'Boltzmann distribution', 'Boltzmann generator', 'Annealed Importance Sampling', 'Normalizing Flow']",[],"['Laurence Illing Midgley', 'Vincent Stimper', 'José Miguel Hernández-Lobato']","['University of Cambridge', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'University of Cambridge']",[]
https://iclr.cc/virtual/2023/poster/11322,Fairness & Bias,Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems,"Calibration is defined as the ratio of the average predicted click rate to the true click rate. The optimization of calibration is essential to many online advertising recommendation systems because it directly affects the downstream bids in ads auctions and the amount of money charged to advertisers. Despite its importance, calibration often suffers from a problem called “maximization bias”. Maximization bias refers to the phenomenon that the maximum of predicted values overestimates the true maximum. The problem is introduced because the calibration is computed on the set selected by the prediction model itself. It persists even if unbiased predictions are achieved on every datapoint and worsens when covariate shifts exist between the training and test sets. To mitigate this problem, we quantify maximization bias and propose a variance-adjusting debiasing (VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and practical as it is able to mitigate maximization bias problem under covariate shifts, without incurring additional online serving costs or compromising the ranking performance. We demonstrate the effectiveness of the proposed algorithm  using a state-of-the-art recommendation neural network model on a large-scale real-world dataset.","['Applications', 'distribution shifts', 'neural networks', 'recommendation system', 'calibration', 'computational advertisement', 'Maximization bias']",[],"['Yewen Fan', 'Nian Si', 'Kun Zhang']","['Carnegie Mellon University', 'University of Chicago', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2023/poster/12213,Fairness & Bias,Trainability Preserving Neural Pruning,"Many recent works have shown trainability plays a central role in neural network pruning -- unattended broken trainability can lead to severe under-performance and unintentionally amplify the effect of retraining learning rate, resulting in biased (or even misinterpreted) benchmark results. This paper introduces trainability preserving pruning (TPP), a scalable method to preserve network trainability against pruning, aiming for improved pruning performance and being more robust to retraining hyper-parameters (e.g., learning rate). Specifically, we propose to penalize the gram matrix of convolutional filters to decorrelate the pruned filters from the retained filters. In addition to the convolutional layers, per the spirit of preserving the trainability of the whole network, we also propose to regularize the batch normalization parameters (scale and bias). Empirical studies on linear MLP networks show that TPP can perform on par with the oracle trainability recovery scheme. On nonlinear ConvNets (ResNet56/VGG19) on CIFAR10/100, TPP outperforms the other counterpart approaches by an obvious margin. Moreover, results on ImageNet-1K with ResNets suggest that TPP consistently performs more favorably against other top-performing structured pruning approaches. Code: https://github.com/MingSun-Tse/TPP.","['Deep Learning and representational learning', 'kernel orthogonalization', 'trainability', 'neural network structured pruning']",[],"['Huan Wang', 'Yun Fu']","['Northeastern University', 'Northeastern University']",[]
https://iclr.cc/virtual/2023/poster/11520,Fairness & Bias,A Unified Algebraic Perspective on Lipschitz Neural Networks,"Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical solutions of a common semidefinite programming (SDP) condition.  We also prove that AOL biases the scaled weight to the ones which are close to the set of orthogonal matrices in a certain mathematical manner. Moreover, our algebraic condition, combined with the Gershgorin circle theorem, readily leads to new and diverse parameterizations for 1-Lipschitz network layers. Our approach, called SDP-based Lipschitz Layers (SLL), allows us to design non-trivial yet efficient generalization of convex potential layers.  Finally, the comprehensive set of experiments on image classification shows that SLLs outperform previous approaches on certified robust accuracy. Code is available at https://github.com/araujoalexandre/Lipschitz-SLL-Networks.","['Deep Learning and representational learning', 'Lipschitz neural networks', 'deep learning', 'robustness']",[],"['Alexandre Araujo', 'Aaron J Havens', 'Blaise Delattre', 'Alexandre Allauzen', 'Bin Hu']","['New York University', 'University of Illinois, Urbana Champaign', ', Université Paris-Dauphine (Paris IX)', 'Ecole supérieure de physique et chimie', 'University of Illinois, Urbana Champaign']",[]
https://iclr.cc/virtual/2023/poster/10686,Fairness & Bias,Encoding Recurrence into Transformers,"This paper novelly breaks down with ignorable loss an RNN layer into a sequence of simple RNNs, each of which can be further rewritten into a lightweight positional encoding matrix of a self-attention, named the Recurrence Encoding Matrix (REM). Thus, recurrent dynamics introduced by the RNN layer can be encapsulated into the positional encodings of a multihead self-attention, and this makes it possible to seamlessly incorporate these recurrent dynamics into a Transformer, leading to a new module, Self-Attention with Recurrence (RSA). The proposed module can leverage the recurrent inductive bias of REMs to achieve a better sample efficiency than its corresponding baseline Transformer, while the self-attention is used to model the remaining non-recurrent signals. The relative proportions of these two components are controlled by a data-driven gated mechanism, and the effectiveness of RSA modules are demonstrated by four sequential learning tasks.","['Deep Learning and representational learning', 'Recurrent models', 'gated mechanism', 'sample efficiency', 'transformers']",[],"['Kexin Lu', 'Yuxi CAI', 'Zhen Qin', 'Yanwen Fang', 'Guangjian Tian', 'Guodong Li']","['University of Hong Kong', 'University of Hong Kong', 'Huawei Technologies Ltd.', 'The University of Hong Kong', 'The Hong Kong Polytechnic University', 'The University of Hong Kong']",[]
https://iclr.cc/virtual/2023/poster/11870,Fairness & Bias,"FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy","Federated learning (FL) is an emerging distributed machine learning framework which jointly trains a global model via a large number of local devices with data privacy protections. Its performance suffers from the non-vanishing biases introduced by the local inconsistent optimal and the rugged client-drifts by the local over-fitting. In this paper, we propose a novel and practical method, FedSpeed, to alleviate the negative impacts posed by these problems.  Concretely, FedSpeed applies the prox-correction term on the current local updates to efficiently reduce the biases introduced by the prox-term, a necessary regularizer to maintain the strong local consistency. Furthermore, FedSpeed merges the vanilla stochastic gradient with a perturbation computed from an extra gradient ascent step in the neighborhood, thereby alleviating the issue of local over-fitting. Our theoretical analysis indicates that the convergence rate is related to both the communication rounds $T$ and local intervals $K$ with a tighter upper bound $\mathcal{O}(\frac{1}{T})$ if $K=\mathcal{O}(T)$.  Moreover, we conduct extensive experiments on the real-world dataset to demonstrate the efficiency of our proposed FedSpeed, which converges significantly faster and achieves the state-of-the-art (SOTA) performance on the general FL experimental settings than several baselines including FedAvg, FedProx, FedCM, FedAdam, SCAFFOLD, FedDyn, FedADMM, etc.","['Deep Learning and representational learning', 'federated learning']",[],"['Yan Sun', 'Li Shen', 'Tiansheng Huang', 'Liang Ding', 'Dacheng Tao']","['University of Sydney', 'JD Explore Academy', 'Georgia Institute of Technology', 'JD Explore Academy, JD.com Inc.', 'University of Sydney']",[]
https://iclr.cc/virtual/2023/poster/11967,Fairness & Bias,Everybody Needs Good Neighbours: An Unsupervised Locality-based Method for Bias Mitigation,"Learning models from human behavioural data often leads to outputs that are biased with respect to user demographics, such as gender or race. This effect can be controlled by explicit mitigation methods, but this typically presupposes access to demographically-labelled training data. Such data is often not available, motivating the need for unsupervised debiasing methods. To this end, we propose a new meta-algorithm for debiasing representation learning models, which combines the notions of data locality and accuracy of model fit, such that a supervised debiasing method can optimise fairness between neighbourhoods of poorly vs. well modelled instances as identified by our method. Results over five datasets, spanning natural language processing and structured data classification tasks, show that our technique recovers proxy labels that correlate with unknown demographic data, and that our method outperforms all unsupervised baselines, while also achieving competitive performance with state-of-the-art supervised methods which are given access to demographic labels.",['Social Aspects of Machine Learning'],[],"['Xudong Han', 'Timothy Baldwin', 'Trevor Cohn']","['University of Melbourne', 'Mohamed bin Zayed University of Artificial Intelligence', 'The University of Melbourne']",[]
https://iclr.cc/virtual/2023/poster/10967,Fairness & Bias,The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks,"In this work, we explore the maximum-margin bias of quasi-homogeneous neural networks trained with gradient flow on an exponential loss and past a point of separability. We introduce the class of quasi-homogeneous models, which is expressive enough to describe nearly all neural networks with homogeneous activations, even those with biases, residual connections, and normalization layers, while structured enough to enable geometric analysis of its gradient dynamics. Using this analysis, we generalize the existing results of maximum-margin bias for homogeneous networks to this richer class of models. We find that gradient flow implicitly favors a subset of the parameters, unlike in the case of a homogeneous model where all parameters are treated equally. We demonstrate through simple examples how this strong favoritism toward minimizing an asymmetric norm can degrade the robustness of quasi-homogeneous models. On the other hand, we conjecture that this norm-minimization discards, when possible, unnecessary higher-order parameters, reducing the model to a sparser parameterization. Lastly, by applying our theorem to sufficiently expressive neural networks with normalization layers, we reveal a universal mechanism behind the empirical phenomenon of Neural Collapse.","['Deep Learning and representational learning', 'homogeneous', 'classification', 'neural networks', 'gradient flow', 'neural collapse', 'margin', 'implicit regularization', 'maximum-margin', 'implicit bias', 'symmetry', 'robustness']",[],"['Daniel Kunin', 'Atsushi Yamamura', 'Chao Ma', 'Surya Ganguli']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2023/poster/11977,Fairness & Bias,Learning without Prejudices: Continual Unbiased Learning via Benign and Malignant Forgetting,"Although machine learning algorithms have achieved state-of-the-art status in image classification, recent studies have substantiated that the ability of the models to learn several tasks in sequence, termed continual learning (CL), often suffers from abrupt degradation of performance from previous tasks.  A large body of CL frameworks has been devoted to alleviating this issue. However, we observe that forgetting phenomena in CL are not always unfavorable, especially when there is bias (spurious correlation) in training data. We term such type of forgetting benign forgetting, and categorize detrimental forgetting as malignant forgetting. Based on this finding, our objective in this study is twofold:  (a) to discourage malignant forgetting by generating previous representations, and (b) encourage benign forgetting by employing contrastive learning in conjunction with feature-level augmentation. Extensive evaluations of biased experimental setups demonstrate that our proposed method, Learning without Prejudices, is effective for continual unbiased learning.","['Deep Learning and representational learning', 'unbiased learning', 'representation learning', 'continual learning']",[],"['Myeongho Jeon', 'Hyoje Lee', 'Yedarm Seong', 'Myungjoo Kang']","['Seoul National University', 'ICODE-LAB', 'Seoul National University', 'Seoul National University']",[]
https://iclr.cc/virtual/2023/poster/12021,Fairness & Bias,"Plateau in Monotonic Linear Interpolation --- A ""Biased"" View of Loss Landscape for Deep Networks","Monotonic linear interpolation (MLI) --- on the line connecting a random initialization with the minimizer it converges to, the loss and accuracy are monotonic --- is a phenomenon that is commonly observed in the training of neural networks. Such a  phenomenon may seem to suggest that optimization of neural networks is easy. In this paper, we show that the MLI property is not necessarily related to the hardness of optimization problems, and empirical observations on MLI for deep neural networks depend heavily on the biases. In particular, we show that interpolating both weights and biases linearly leads to very different influences on the final output, and when different classes have different last-layer biases on a deep network, there will be a long plateau in both the loss and accuracy interpolation (which existing theory of MLI cannot explain). We also show how the last-layer biases for different classes can be different even on a perfectly balanced dataset using a simple model. Empirically we demonstrate that similar intuitions hold on practical networks and realistic datasets.","['Theory', 'deep learning theory', 'loss landscape', 'monotonic linear interpolation']",[],"['Xiang Wang', 'Annie N. Wang', 'Mo Zhou', 'Rong Ge']","['Duke University', 'Duke University', 'Duke University', 'Duke University']",[]
https://iclr.cc/virtual/2023/poster/12028,Fairness & Bias,Discovering Informative and Robust Positives for Video Domain Adaptation,"Unsupervised domain adaptation for video recognition is challenging where the domain shift includes both spatial variations and temporal dynamics. Previous works have focused on exploring contrastive learning for cross-domain alignment. However, limited variations in intra-domain positives, false cross-domain positives, and false negatives hinder contrastive learning from fulfilling intra-domain discrimination and cross-domain closeness. This paper presents a non-contrastive learning framework without relying on negative samples for unsupervised video domain adaptation. To address the limited variations in intra-domain positives, we set unlabeled target videos as anchors and explored to mine ""informative intra-domain positives"" in the form of spatial/temporal augmentations and target nearest neighbors (NNs).To tackle the false cross-domain positives led by noisy pseudo-labels, we reversely set source videos as anchors and sample the synthesized target videos as ""robust cross-domain positives"" from an estimated target distribution, which are naturally more robust to the pseudo-label noise. Our approach is demonstrated to be superior to state-of-the-art methods through extensive experiments on several cross-domain action recognition benchmarks.","['Deep Learning and representational learning', 'Video Recognition', 'domain adaptation']",[],"['Chang Liu', 'Kunpeng Li', 'Michael Stopa', 'Jun Amano', 'Yun Fu']","['Northeastern University', 'Meta', 'PlayStation', 'Cornell University', 'Northeastern University']",[]
https://iclr.cc/virtual/2023/poster/12038,Fairness & Bias,Fair Attribute Completion on Graph with Missing Attributes,"Tackling unfairness in graph learning models is a challenging task, as the unfairness issues on graphs involve both attributes and topological structures. Existing work on fair graph learning simply assumes that attributes of all nodes are available for model training and then makes fair predictions. In practice, however, the attributes of some nodes might not be accessible due to missing data or privacy concerns, which makes fair graph learning even more challenging. In this paper, we propose FairAC, a fair attribute completion method, to complement missing information and learn fair node embeddings for graphs with missing attributes. FairAC adopts an attention mechanism to deal with the attribute missing problem and meanwhile, it mitigates two types of unfairness, i.e., feature unfairness from attributes and topological unfairness due to attribute completion. FairAC can work on various types of homogeneous graphs and generate fair embeddings for them and thus can be applied to most downstream tasks to improve their fairness performance. To our best knowledge, FairAC is the first method that jointly addresses the graph attribution completion and graph unfairness problems. Experimental results on benchmark datasets show that our method achieves better fairness performance with less sacrifice in accuracy, compared with the state-of-the-art methods of fair graph learning. Code is available at: https://github.com/donglgcn/FairAC.",['Deep Learning and representational learning'],[],"['Dongliang Guo', 'Zhixuan Chu', 'Sheng Li']","['University of Virginia, Charlottesville', 'Alibaba Group', 'University of Virginia, Charlottesville']",[]
https://iclr.cc/virtual/2023/poster/10885,Fairness & Bias,SIMPLE: A Gradient Estimator for k-Subset Sampling,"$k$-subset sampling is ubiquitous in machine learning, enabling regularization and interpretability through sparsity. The challenge lies in rendering $k$-subset sampling amenable to end-to-end learning. This has typically involved relaxing the reparameterized samples to allow for backpropagation, but introduces both bias and variance. In this work, we fall back to discrete $k$-subset sampling on the forward pass. This is coupled with using the gradient with respect to the exact marginals, computed efficiently, as a proxy for the true gradient. We show that our gradient estimator exhibits lower bias and variance compared to state-of-the-art estimators. Empirical results show improved performance on learning to explain and sparse models benchmarks. We provide an algorithm for computing the exact ELBO for the $k$-subset distribution, obtaining significantly lower loss compared to state-of-the-art discrete sparse VAEs. All of our algorithms are exact and efficient.",['Deep Learning and representational learning'],[],"['Kareem Ahmed', 'Zhe Zeng', 'Mathias Niepert', 'Guy Van den Broeck']","['University of California, Los Angeles', 'University of California, Los Angeles', 'Universität Stuttgart', 'University of California, Los Angeles']",[]
https://iclr.cc/virtual/2023/poster/10716,Fairness & Bias,Modeling content creator incentives on algorithm-curated platforms,"Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by modern algorithms including factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices—e.g., non-negative vs. unconstrained factorization—significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models like ours for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools for numerically finding equilibria in exposure games, and illustrate results of an audit on the MovieLens and LastFM datasets. Among else, we find that the strategically produced content exhibits strong dependence between algorithmic exploration and content diversity, and between model expressivity and bias towards gender-based user and creator groups.","['Theory', 'Nash equilibria', 'attention monetizing platforms', 'differentiable games', 'recommenders', 'exposure game', 'producer incentives']",[],"['Jiri Hron', 'Karl Krauth', 'Michael Jordan', 'Niki Kilbertus', 'Sarah Dean']","['Google', 'University of California Berkeley', 'University of California, Berkeley', 'Technische Universität München', 'Cornell University']",[]
https://iclr.cc/virtual/2023/poster/12055,Fairness & Bias,Estimating individual treatment effects under unobserved confounding using binary instruments,"Estimating conditional average treatment effects (CATEs) from observational data is relevant in many fields such as personalized medicine. However, in practice, the treatment assignment is usually confounded by unobserved variables and thus introduces bias. A remedy to remove the bias is the use of instrumental variables (IVs). Such settings are widespread in medicine (e.g., trials where the treatment assignment is used as binary IV). In this paper, we propose a novel, multiply robust machine learning framework, called MRIV, for estimating CATEs using binary IVs and thus yield an unbiased CATE estimator. Different from previous work for binary IVs, our framework estimates the CATE directly via a pseudo-outcome regression. (1)~We provide a theoretical analysis where we show that our framework yields multiple robust convergence rates: our CATE estimator achieves fast convergence even if several nuisance estimators converge slowly. (2)~We further show that our framework asymptotically outperforms state-of-the-art plug-in IV methods for CATE estimation, in the sense that it achieves a faster rate of convergence if the CATE is smoother than the individual outcome surfaces. (3)~We build upon our theoretical results and propose a tailored deep neural network architecture called MRIV-Net for CATE estimation using binary IVs. Across various computational experiments, we demonstrate empirically that our MRIV-Net achieves state-of-the-art performance. To the best of our knowledge, our MRIV is the first multiply robust machine learning framework tailored to estimating CATEs in the binary IV setting.","['Probabilistic Methods', 'instrumental variables', 'Causal machine learning', 'treatment effect estimation']",[],"['Dennis Frauen', 'Stefan Feuerriegel']","['Ludwig-Maximilians-Universität München', 'LMU Munich']",[]
https://iclr.cc/virtual/2023/poster/11845,Fairness & Bias,Temperature Schedules for self-supervised contrastive methods on long-tail data,"Most approaches for self-supervised learning (SSL) are optimised on curated balanced datasets, e.g. ImageNet, despite the fact that natural data usually exhibits long-tail distributions. In this paper, we analyse the behaviour of one of the most popular variants of SSL, i.e. contrastive methods, on imbalanced data. In particular, we investigate the role of the temperature parameter $\tau$ in the contrastive loss, by analysing the loss through the lens of average distance maximisation, and find that a large $\tau$ emphasises group-wise discrimination, whereas a small $\tau$ leads to a higher degree of instance discrimination. While $\tau$ has thus far been treated exclusively as a constant hyperparameter, in this work, we propose to employ a dynamic $\tau$ and show that a simple cosine schedule can yield significant improvements in the learnt representations. Such a schedule results in a constant `task switching' between an emphasis on instance discrimination and group-wise discrimination and thereby ensures that the model learns both group-wise features, as well as instance-specific details. Since frequent classes benefit from the former, while infrequent classes require the latter, we find this method to consistently improve separation between the classes in long-tail data without any additional computational cost.","['Unsupervised and Self-supervised learning', 'long-tail data', 'analysis', 'self-supervised learning', 'contrastive learning', 'temperature']",[],"['Anna Kukleva', 'Moritz Böhle', 'Bernt Schiele', 'Hilde Kuehne', 'Christian Rupprecht']","['Saarland Informatics Campus, Max-Planck Institute', 'Saarland Informatics Campus, Max-Planck Institute', 'Max Planck Institute for Informatics, Saarland Informatics Campus', 'Rheinische Friedrich-Wilhelms-Universität Bonn, Rheinische Friedrich-Wilhelms Universität Bonn', 'University of Oxford']",[]
https://iclr.cc/virtual/2023/poster/11716,Fairness & Bias,Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding,"Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to $t$-SNE can also be adopted in the SSCL setting, achieving significant improvement in both in-distribution and out-of-distribution generalization.","['Unsupervised and Self-supervised learning', 'theoretical understanding', 'stochastic neighbor embedding', 'contrastive learning']",[],"['Tianyang Hu', 'Zhili Liu', 'Fengwei Zhou', 'Wenjia Wang', 'Weiran Huang']","[""Huawei Noah's Ark Lab"", 'Hong Kong University of Science and Technology', 'Guangdong OPPO Mobile Telecommunications Corp., Ltd.', 'The Hong Kong University of Science and Technology', 'Shanghai Jiao Tong University']",[]
https://iclr.cc/virtual/2023/poster/12051,Fairness & Bias,Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors,"Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation methods. Existing DABP approaches mostly rely on knowledge distillation (KD) from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises and leads to degrading performance. To mitigate such bias, we propose a new strategy, \textit{divide-to-adapt}, that purifies cross-domain knowledge distillation by proper domain division. This is inspired by an observation we make for the first time in domain adaptation: the target domain usually contains easy-to-adapt and hard-to-adapt samples that have different levels of domain discrepancy w.r.t. the source domain, and deep models tend to fit easy-to-adapt samples first. Leveraging easy-to-adapt samples with less noise can help KD alleviate the negative effect of prediction noises from black-box predictors. In this sense, the target domain can be divided into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain at the early stage of training. Then the adaptation is achieved by semi-supervised learning. We further reduce distribution discrepancy between subdomains and develop weak-strong augmentation strategy to filter the predictor errors progressively. As such, our method is a simple yet effective solution to reduce error accumulation in cross-domain knowledge distillation for DABP. Moreover, we prove that the target error of DABP is bounded by the noise ratio of two subdomains, i.e., the confirmation bias, which provides the theoretical justifications for our method. Extensive experiments demonstrate our method achieves state of the art on all DABP benchmarks, outperforming the existing best approach by 7.0\% on VisDA-17, and is even comparable with the standard domain adaptation methods that use the source-domain data.","['Deep Learning and representational learning', 'model adaptation', 'transfer learning', 'black-box predictors']",[],"['Jianfei Yang', 'Xiangyu Peng', 'Kai Wang', 'Zheng Zhu', 'Jiashi Feng', 'Lihua Xie', 'Yang You']","['Nanyang Technological University', 'National University of Singapore', 'national university of singaore, National University of Singapore', 'PhiGent Robotics', 'ByteDance', 'Nanyang Technological University', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/11850,Fairness & Bias,"Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased","There is a recent trend of applying multi-agent reinforcement learning (MARL) to train an agent that can cooperate with humans in a zero-shot fashion without using any human data. The typical workflow is to first repeatedly run self-play (SP) to build a policy pool and then train the final adaptive policy against this pool. A crucial limitation of this framework is that every policy in the pool is optimized w.r.t. the environment reward function, which implicitly assumes that the testing partners of the adaptive policy will be precisely optimizing the same reward function as well. However, human objectives are often substantially biased according to their own preferences, which can differ greatly from the environment reward. We propose a more general framework, Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden reward functions in the self-play objective. By approximating the reward space as linear functions, HSP adopts an effective technique to generate an augmented policy pool with biased policies. We evaluate HSP on the Overcooked benchmark. Empirical results show that our HSP method produces higher rewards than baselines when cooperating with learned human models, manually scripted policies, and real humans. The HSP policy is also rated as the most assistive policy based on human feedback.","['Reinforcement Learning', 'multi-agent reinforcement learning', 'human-AI collaboration']",[],"['Chao Yu', 'Jiaxuan Gao', 'Weilin Liu', 'Botian Xu', 'Hao Tang', 'Jiaqi Yang', 'Yu Wang', 'Yi Wu']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'University of California Berkeley', 'Tsinghua University, Tsinghua University', 'Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/12100,Fairness & Bias,Multivariate Time-series Imputation with Disentangled Temporal Representations,"Multivariate time series often faces the problem of missing value. Many time series imputation methods have been developed in the literature. However, these methods all rely on an entangled representation to model dynamics of time series, which may fail to fully exploit the multiple factors (e.g., periodic patterns) contained in the time series. Moreover, the entangled representation usually has no semantic meaning, and thus they often lack interpretability. In addition, many recent models are proposed to deal with the whole time series to capture cross-channel correlations and identify temporal dynamics, but they are not scalable to large-scale datasets. Different from existing approaches, we propose TIDER, a novel matrix factorization-based method with disentangled temporal representations that account for multiple factors, namely trend, seasonality, and local bias, to model complex dynamics. The learned disentanglement makes the imputation process more reliable and offers explainability for imputation results. Moreover, TIDER is scalable to large datasets. Empirical results show that our method not only outperforms existing approaches by notable margins on three real-world datasets, but also scales well to large datasets on which existing deep learning based methods struggle. Disentanglement validation experiments further demonstrate the robustness of our model in obtaining accurate and explainable disentangled components.","['Deep Learning and representational learning', 'multivariate time-series imputation', 'disentangled representation']",[],"['SHUAI LIU', 'Xiucheng Li', 'Gao Cong', 'Yile Chen', 'YUE JIANG']","['School of Computer Science and  Engineering, Nanyang Technological University', 'Harbin Institute of Technology', 'National Technological University', 'Nanyang Technological University', 'Nanyang Technological University']",[]
https://iclr.cc/virtual/2023/poster/11833,Fairness & Bias,Evidential Uncertainty and Diversity Guided Active Learning for Scene Graph Generation,"Scene Graph Generation (SGG) has already shown its great potential in various downstream tasks, but it comes at the price of a prohibitively expensive annotation process. To reduce the annotation cost, we propose using Active Learning (AL) for sampling the most informative data. However, directly porting current AL methods to the SGG task poses the following challenges: 1) unreliable uncertainty estimates, and 2) data bias problems. To deal with these challenges, we propose EDAL (\textbf{E}vidential Uncertainty and \textbf{D}iversity Guided Deep \textbf{A}ctive \textbf{L}earning), a novel AL framework tailored for the SGG task. For challenge 1), we start with Evidential Deep Learning (EDL) coupled with a global relationship mining approach to estimate uncertainty, which can effectively overcome the perturbations of open-set relationships and background-relationships to obtain reliable uncertainty estimates. To address challenge 2), we seek the diversity-based method and design the Context Blocking Module (CBM) and Image Blocking Module (IBM) to alleviate context-level bias and image-level bias, respectively. Experiments show that our AL framework can approach the performance of a fully supervised SGG model with only about $10\%$ annotation cost. Furthermore, our ablation studies indicate that introducing AL into the SGG will face many challenges not observed in other vision tasks that are successfully overcome by our new modules.","['Applications', 'uncertainty estimation', 'active learning', 'Scene graph generation']",[],"['Shuzhou Sun', 'Shuaifeng Zhi', 'Janne Heikkila', 'Li Liu']","['University of Oulu', 'National University of Defense Technology', 'University of Oulu', 'National University of Defense Technology']",[]
https://iclr.cc/virtual/2023/poster/12129,Fairness & Bias,LPT: Long-tailed Prompt Tuning  for Image Classification,"For long-tailed classification tasks, most works often pretrain a big model on a large-scale (unlabeled) dataset, and then fine-tune the whole pretrained  model for  adapting to long-tailed data. Though promising, fine-tuning the whole pretrained model tends to suffer from high cost in computation and deployment of different models for different tasks, as well as weakened generalization capability for overfitting to certain features of long-tailed data. To alleviate these issues, we propose an effective Long-tailed Prompt Tuning (LPT) method for long-tailed classification tasks. LPT introduces several trainable prompts into a frozen pretrained model to adapt it to long-tailed data. For better effectiveness, we divide prompts into two groups: 1) a shared prompt for the whole long-tailed dataset to learn general features and to adapt a pretrained model into the target long-tailed domain; and 2) group-specific prompts to gather group-specific features for the samples which have similar features and also to empower the pretrained model with fine-grained discrimination ability. Then we design a two-phase training paradigm to learn these prompts. In the first phase, we train the shared prompt via conventional supervised prompt tuning to adapt a pretrained model to the desired long-tailed domain. In the second phase, we use the learnt shared prompt as query to select a small best matched set for a group of similar samples from the group-specific prompt set to dig the common features of these similar samples, and then optimize these prompts with a dual sampling strategy and the asymmetric Gaussian Clouded Logit loss. By only fine-tuning a few prompts while fixing the pretrained model, LPT can reduce training cost and deployment cost by storing a few prompts, and enjoys a strong generalization ability of the pretrained model. Experiments show that on various long-tailed benchmarks, with only $\sim$1.1\% extra trainable parameters, LPT achieves comparable or higher performance than previous whole model fine-tuning methods, and is more robust to domain-shift.",['Deep Learning and representational learning'],[],"['Bowen Dong', 'Pan Zhou', 'Shuicheng YAN', 'Wangmeng Zuo']","['Harbin Institute of Technology', 'National University of Singapore', 'National University of Singapore', 'Harbin Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11550,Fairness & Bias,Prompting GPT-3 To Be Reliable,"Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3’s reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM’s factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.","['Social Aspects of Machine Learning', 'prompting', 'GPT-3', 'biases', 'reliability', 'calibration', 'large language models', 'knowledge updating', 'robustness']",[],"['Chenglei Si', 'Zhe Gan', 'Zhengyuan Yang', 'Shuohang Wang', 'Jianfeng Wang', 'Jordan Lee Boyd-Graber', 'Lijuan Wang']","['Stanford University', 'Apple', 'Microsoft', 'Microsoft', 'Microsoft', 'University of Maryland, College Park', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/12184,Fairness & Bias,Stochastic Differentially Private and Fair Learning,"Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals’ health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term “stochastic"" refers to the fact that our proposed algorithm converges even when minibatches of data are used at each iteration (i.e. stochastic optimization). Our framework is flexible enough to permit different fairness notions, including demographic parity and equalized odds. In addition, our algorithm can be applied to non-binary classification tasks with multiple (non-binary) sensitive attributes. As a byproduct of our convergence analysis, we provide the first utility guarantee for a DP algorithm for solving nonconvex-strongly concave min-max problems. Our numerical experiments show that the proposed algorithm consistently offers significant performance gains over the state-of-the-art baselines, and can be applied to larger scale problems with non-binary target/sensitive attributes.","['Social Aspects of Machine Learning', 'differential privacy', 'stochastic optimization', 'Algorithmic fairness', 'private fair learning']",[],"['Andrew Lowy', 'Devansh Gupta', 'Meisam Razaviyayn']","['University of Southern California', 'Indraprastha Institute of Information Technology, Delhi', 'University of Southern California']",[]
https://iclr.cc/virtual/2023/poster/11221,Fairness & Bias,TDR-CL: Targeted Doubly Robust Collaborative Learning for Debiased Recommendations,"Bias is a common problem inherent in recommender systems, which is entangled with users' preferences and poses a great challenge to unbiased learning. For debiasing tasks, the doubly robust (DR) method and its variants show superior performance due to the double robustness property, that is, DR is unbiased when either imputed errors or learned propensities are accurate.However, our theoretical analysis reveals that DR usually has a large variance. Meanwhile, DR would suffer unexpectedly large bias and poor generalization caused by inaccurate imputed errors and learned propensities, which usually occur in practice. In this paper, we propose a principled approach that can effectively reduce the bias and variance simultaneously for existing DR approaches when the error  imputation model is misspecified. In addition, we further propose a novel semi-parametric collaborative learning approach that decomposes imputed errors into parametric and nonparametric parts and updates them collaboratively, resulting in more accurate predictions. Both theoretical analysis and experiments demonstrate the superiority of the proposed methods compared with existing debiasing methods.","['Applications', 'recommender system', 'Debias', 'bias', 'Doubly Robust']",[],"['Haoxuan Li', 'Yan Lyu', 'Chunyuan Zheng', 'Peng Wu']","['ByteDance Inc.', 'Peking University', 'Meituan', 'Beijing Technology and Business University']",[]
https://iclr.cc/virtual/2023/poster/11375,Fairness & Bias,On Accelerated Perceptrons and Beyond,"The classical Perceptron algorithm of Rosenblatt can be used to find a linear threshold function to correctly classify $n$ linearly separable data points, assuming the classes are separated by some margin $\gamma > 0$. A foundational result is that Perceptron converges after  $\Omega(1/\gamma^{2})$ iterations. There have been several recent works that managed to improve this rate by a quadratic factor, to $\Omega(\sqrt{\log n}/\gamma)$, with more sophisticated algorithms. In this paper, we unify these existing results under one framework by showing that they can all be described through the lens of solving min-max problems using modern acceleration techniques, mainly through \emph{optimistic} online learning.  We then show that the proposed framework also leads to improved results for a series of problems beyond the standard Perceptron setting. Specifically, a) for the margin maximization problem, we improve the state-of-the-art result from $O(\log t/t^2)$ to $O(1/t^2)$, where $t$ is the number of iterations; b) we provide the first result on identifying the implicit bias property of the classical Nesterov's accelerated gradient descent (NAG) algorithm, and show NAG can maximize the margin with an $O(1/t^2)$ rate; c) for the classical $p$-norm Perceptron problem, we provide an algorithm with $\Omega(\sqrt{(p-1)\log n}/\gamma)$ convergence rate, while existing algorithms suffer the $\Omega({(p-1)}/\gamma^2)$ convergence rate.","['Theory', 'margin maximization', 'game', 'implicit bias', 'optimistic online learning', 'Perceptron']",[],"['Guanghui Wang', 'Rafael Hanashiro', 'Etash Kumar Guha', 'Jacob Abernethy']","['Georgia Institute of Technology', 'Massachusetts Institute of Technology', 'RIKEN', 'Research, Google']",[]
https://iclr.cc/virtual/2023/poster/11040,Fairness & Bias,The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry,"Extensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model's performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems.","['Reinforcement Learning', 'reinforcement learning', 'Equivariant Learning', 'robotics']",[],"['Dian Wang', 'Jung Yeon Park', 'Neel Sortur', 'Lawson L.S. Wong', 'Robin Walters', 'Robert Platt']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University ', 'Northeastern University']",[]
https://iclr.cc/virtual/2023/poster/11807,Fairness & Bias,Fooling SHAP with Stealthily Biased Sampling,"SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups while remaining undetected. More precisely, experiments performed on real-world datasets showed that our attack could yield up to a 90\% relative decrease in amplitude of the sensitive feature attribution. These results highlight the manipulability of SHAP explanations and encourage auditors to treat them with skepticism.","['Social Aspects of Machine Learning', 'SHAP', 'Stealthily Sampling', 'explainability', 'robustness']",[],"['Gabriel Laberge', 'Ulrich Aïvodji', 'Satoshi Hara', 'Mario Marchand', 'Foutse Khomh']","['École Polytechnique de Montréal', 'École de technologie supérieure, Université du Québec', 'Osaka University', 'Laval university', 'École Polytechnique de Montréal, Université de Montréal']",[]
https://iclr.cc/virtual/2023/poster/11206,Fairness & Bias,StableDR: Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random,"In recommender systems, users always choose the favorite items to rate, which leads to data missing not at random and poses a great challenge for unbiased evaluation and learning of prediction models. Currently, the doubly robust (DR) methods have been widely studied and demonstrate superior performance. However, in this paper, we show that DR methods are unstable and have unbounded bias, variance, and generalization bounds to extremely small propensities. Moreover, the fact that DR relies more on extrapolation will lead to suboptimal performance. To address the above limitations while retaining double robustness, we propose a stabilized doubly robust (StableDR) learning approach with a weaker reliance on extrapolation. Theoretical analysis shows that StableDR has bounded bias, variance, and generalization error bound simultaneously under inaccurate imputed errors and arbitrarily small propensities. In addition, we propose a novel learning approach for StableDR that updates the imputation, propensity, and prediction models cyclically, achieving more stable and accurate predictions. Extensive experiments show that our approaches significantly outperform the existing methods.","['Applications', 'recommender system', 'Debias', 'bias', 'Doubly Robust']",[],"['Haoxuan Li', 'Chunyuan Zheng', 'Peng Wu']","['ByteDance Inc.', 'Meituan', 'Beijing Technology and Business University']",[]
https://iclr.cc/virtual/2023/poster/11661,Fairness & Bias,Generative Modelling with Inverse Heat Dissipation,"While diffusion models have shown great success in image generation, their noise-inverting generative process does not explicitly consider the structure of images, such as their inherent multi-scale nature. Inspired by diffusion models and the empirical success of coarse-to-fine modelling, we propose a new diffusion-like model that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image. We interpret the solution of the forward heat equation with constant additive noise as a variational approximation in the diffusion latent variable model. Our new model shows emergent qualitative properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images. Spectral analysis on natural images highlights connections to diffusion models and reveals an implicit coarse-to-fine inductive bias in them.","['Generative models', 'inductive bias', 'diffusion model', 'partial differential equation']",[],"['Severi Rissanen', 'Markus Heinonen', 'Arno Solin']","['Aalto University', 'Aalto University', 'Aalto University']",[]
https://iclr.cc/virtual/2023/poster/11631,Fairness & Bias,Blurring Diffusion Models,"Recently, Rissanen et al., (2022) have presented a new type of diffusion process for generative modeling based on heat dissipation, or blurring, as an alternative to isotropic Gaussian diffusion. Here, we show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise. In making this connection, we bridge the gap between inverse heat dissipation and denoising diffusion, and we shed light on the inductive bias that results from this modeling choice. Finally, we propose a generalized class of diffusion models that offers the best of both standard Gaussian denoising diffusion and inverse heat dissipation, which we call Blurring Diffusion Models.","['Generative models', 'generative model', 'blurring', 'diffusion']",[],"['Emiel Hoogeboom', 'Tim Salimans']","['Google', 'Google']",[]
https://iclr.cc/virtual/2023/poster/11627,Fairness & Bias,MARS: Meta-learning as Score Matching in the Function Space,"Meta-learning aims to extract useful inductive biases from a set of related datasets. In Bayesian meta-learning, this is typically achieved by constructing a prior distribution over neural network parameters. However, specifying families of computationally viable prior distributions over the high-dimensional neural network parameters is difficult. As a result, existing approaches resort to meta-learning restrictive diagonal Gaussian priors, severely limiting their expressiveness and performance. To circumvent these issues, we approach meta-learning through the lens of functional Bayesian neural network inference which views the prior as a stochastic process and performs inference in the function space. Specifically, we view the meta-training tasks as samples from the data-generating process and formalize meta-learning as empirically estimating the law of this stochastic process. Our approach can seamlessly acquire and represent complex prior knowledge by meta-learning the score function of the data-generating process marginals instead of parameter space priors. In a comprehensive benchmark, we demonstrate that our method achieves state-of-the-art performance in terms of predictive accuracy and substantial improvements in the quality of uncertainty estimates.","['Probabilistic Methods', 'bayesian neural networks', 'meta-learning', 'score estimation']",[],"['Krunoslav Lehman Pavasovic', 'Jonas Rothfuss', 'Andreas Krause']","['INRIA', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11147,Fairness & Bias,BALTO: fast tensor program optimization with diversity-based active learning,"Tensor program optimization (TPO) based on pre-trained models can effectively reduce the computing time of deep neural networks. However, training of such models is prohibitively expensive, which highly depends on a large-scale dataset and thus requires tremendous time-consuming performance measurements (more than 1 million) on target platforms. In this paper, we propose BALTO, a fast TPO approach with biased-diversity-based active learning, aiming at reducing much lower training costs under similar optimization accuracy.The key insight is that random sampling of existing approaches suffers from a heavy redundancy of low-performance programs, which incurs tremendous duplicated time-consuming measurements. Inspired by this, BALTO removes such redundancy by introducing active learning (AL) to TPO for a much lower training cost. However, applying AL with a brute-force way in BALTO can lead to an overestimation problem. To address this, we further propose a biased-diversity-based diversity scheme specially designed for BALTO. We compare BALTO against TenSet on $6$ typical hardware platforms over $2$ learning models. Experimental results show that, on average, BALTO only requires 5% of the total performance measurements of TenSet to achieve the same or higher model accuracy. Moreover, the optimized tensor programs even outperform that of TenSet by 1.06% due to higher model accuracy.",['Applications'],[],"['Jun Bi', 'Xiaqing Li', 'Qi Guo', 'Rui Zhang', 'Yuanbo Wen', 'Xing Hu', 'Zidong Du', 'Yifan Hao', 'Yunji Chen']","['University of Science and Technology of China', 'Institute of computing, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, CAS', 'Institute of Computing Technology, Chinese Academy of Sciences', ', Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences']",[]
https://iclr.cc/virtual/2023/poster/10888,Fairness & Bias,On the Sensitivity of Reward Inference to Misspecified Human Models,"Inferring reward functions from human behavior is at the center of value alignment – aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases in behavior that lead to arbitrarily large errors in the inferred reward. However, and arguably more importantly, we are also able to identify reasonable assumptions under which the reward inference error can be bounded linearly in the error in the human model. Finally, we verify our theoretical insights in discrete and continuous control tasks with simulated and human data.","['Reinforcement Learning', 'misspecification', 'reward learning', 'inverse reinforcement learning']",[],"['Joey Hong', 'Kush Bhatia', 'Anca Dragan']","['University of California Berkeley', 'Stanford University', 'University of California Berkeley']",[]
https://iclr.cc/virtual/2023/poster/11723,Fairness & Bias,LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation,"Graph neural network (GNN) is a powerful learning approach for graph-based recommender systems. Recently, GNNs integrated with contrastive learning have shown superior performance in recommendation with their data augmentation schemes, aiming at dealing with highly sparse data. Despite their success, most existing graph contrastive learning methods either perform stochastic augmentation (e.g., node/edge perturbation) on the user-item interaction graph, or rely on the heuristic-based augmentation techniques (e.g., user clustering) for generating contrastive views. We argue that these methods cannot well preserve the intrinsic semantic structures and are easily biased by the noise perturbation. In this paper, we propose a simple yet effective graph contrastive learning paradigm LightGCL that mitigates these issues impairing the generality and robustness of CL-based recommenders. Our model exclusively utilizes singular value decomposition for contrastive augmentation, which enables the unconstrained structural refinement with global collaborative relation modeling. Experiments conducted on several benchmark datasets demonstrate the significant improvement in performance of our model over the state-of-the-arts. Further analyses demonstrate the superiority of LightGCL's robustness against data sparsity and popularity bias. The source code of our model is available at https://github.com/HKUDS/LightGCL.","['Applications', 'recommender systems', 'contrastive learning', 'graph neural networks']",[],"['Xuheng Cai', 'Chao Huang', 'Lianghao Xia', 'Xubin Ren']","['University of Hong Kong', 'University of Hong Kong', 'University of Hong Kong', 'University of Hong Kong']",[]
https://iclr.cc/virtual/2023/poster/12104,Fairness & Bias,Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs,"Despite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.","['Machine Learning for Sciences', 'transformer networks', 'equivariant neural networks', 'graph neural networks', 'computational physics']",[],"['Yi-Lun Liao', 'Tess Smidt']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11414,Fairness & Bias,SimPer: Simple Self-Supervised Learning of Periodic Targets,"From human physiology to environmental evolution, important processes in nature often exhibit meaningful and strong periodic or quasi-periodic changes. Due to their inherent label scarcity, learning useful representations for periodic tasks with limited or no supervision is of great benefit. Yet, existing self-supervised learning (SSL) methods overlook the intrinsic periodicity in data, and fail to learn representations that capture periodic or frequency attributes. In this paper, we present SimPer, a simple contrastive SSL regime for learning periodic information in data. To exploit the periodic inductive bias, SimPer introduces customized augmentations, feature similarity measures, and a generalized contrastive loss for learning efficient and robust periodic representations. Extensive experiments on common real-world tasks in human behavior analysis, environmental sensing, and healthcare domains verify the superior performance of SimPer compared to state-of-the-art SSL methods, highlighting its intriguing properties including better data efficiency, robustness to spurious correlations, and generalization to distribution shifts.","['Unsupervised and Self-supervised learning', 'representation learning', 'self-supervised learning', 'Periodic targets', 'Periodic learning', 'Periodicity']",[],"['Yuzhe Yang', 'Xin Liu', 'Jiang Wu', 'Silviu Borac', 'Dina Katabi', 'Ming-Zher Poh', 'Daniel McDuff']","['Massachusetts Institute of Technology', 'Google', 'Google', 'Google', 'Massachusetts Institute of Technology', 'Cardiio, Inc.', 'Google']",[]
https://iclr.cc/virtual/2023/poster/12793,Fairness & Bias,"Principal Components Bias in Over-parameterized Linear Models, and its Manifestation in Deep Neural Networks","Recent work suggests that convolutional neural networks of different architectures learn to classify images in the same order. To understand this phenomenon, we revisit the over-parametrized deep linear network model. Our analysis reveals that, when the hidden layers are wide enough, the convergence rate of this model's parameters is exponentially faster along the directions of the larger principal components of the data, at a rate governed by the corresponding singular values. We term this convergence pattern the Principal Components bias (PC-bias). Empirically, we show how the PC-bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. We then compare our results to the simplicity bias, showing that both biases can be seen independently, and affect the order of learning in different ways. Finally, we discuss how the PC-bias may explain some benefits of early stopping and its connection to PCA, and why deep networks converge more slowly with random labels.",[],[],,,[]
https://iclr.cc/virtual/2023/poster/11224,Fairness & Bias,Confidential-PROFITT: Confidential PROof of FaIr Training of Trees,"Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees.","['Social Aspects of Machine Learning', 'Zero-Knowledge Proof', 'Audit', 'fairness', 'confidentiality']",[],"['Ali Shahin Shamsabadi', 'Sierra Calanda Wyllie', 'Nicholas Franzese', 'Natalie Dullerud', 'Sébastien Gambs', 'Nicolas Papernot', 'Xiao Wang', 'Adrian Weller']","['Brave Software', 'University of Toronto', 'Northwestern University, Northwestern University', 'Stanford University', 'Université du Québec à Montréal', 'University of Toronto', 'Northwestern University', 'Alan Turing Institute']",[]
https://iclr.cc/virtual/2023/poster/11039,Fairness & Bias,Distributed Extra-gradient with Optimal Complexity and Communication Guarantees,"We consider monotone variational inequality (VI) problems in multi-GPU  settings where multiple processors/workers/clients have access to local stochastic dual vectors. This setting  includes a broad range of important problems from distributed convex minimization to min-max and games. Extra-gradient, which is a de facto algorithm  for monotone VI problems, has not been designed to be communication-efficient. To this end, we propose a quantized generalized extra-gradient (Q-GenX), which is an unbiased and adaptive compression method tailored to solve VIs. We provide an adaptive step-size rule, which  adapts to the respective noise profiles at hand and achieve a fast rate of  ${\cal O}(1/T)$ under relative noise, and an order-optimal ${\cal O}(1/\sqrt{T})$ under absolute noise  and show distributed training accelerates convergence. Finally, we validate our theoretical results by providing real-world experiments and training generative adversarial networks on multiple GPUs.","['General Machine Learning', 'extra-gradient', 'Unbiased Quantization', 'Adaptive Sep-size', 'variational inequality']",[],"['Ali Ramezani-Kebrya', 'Kimon Antonakopoulos', 'Igor Krawczuk', 'Justin Deschenaux', 'Volkan Cevher']","['University of Oslo', 'EPFL - EPF Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'EPFL - EPF Lausanne']",[]
https://iclr.cc/virtual/2023/poster/11945,Fairness & Bias,Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes,"We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions  constructed via proximal causal inference, which is  formulated as  minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the dataset. To our best knowledge, \texttt{P3O} is the first provably efficient offline RL algorithm for POMDPs with a confounded dataset.",['Reinforcement Learning'],[],"['Miao Lu', 'Yifei Min', 'Zhaoran Wang', 'Zhuoran Yang']","['Stanford University', 'Yale University', 'Northwestern University', 'Yale University']",[]
https://iclr.cc/virtual/2023/poster/12144,Fairness & Bias,What shapes the loss landscape of self supervised learning?,"Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.","['Deep Learning and representational learning', 'collapse', 'self-supervised learning', 'loss landscape']",[],"['Liu Ziyin', 'Ekdeep Singh Lubana', 'Masahito Ueda', 'Hidenori Tanaka']","['The University of Tokyo', 'Center for Brain Science, Harvard University', 'The University of Tokyo', 'Harvard University, Harvard University']",[]
https://iclr.cc/virtual/2023/poster/11768,Fairness & Bias,Markup-to-Image Diffusion Models with Scheduled Sampling,"Building on recent advances in image generation, we present a fully data-driven approach to rendering markup into images. The approach is based on diffusion models, which parameterize the distribution of data using a sequence of denoising operations on top of a Gaussian noise distribution. We view the diffusion denoising process a sequential decision making process, and show that it exhibits compounding errors similar to exposure bias issues in imitation learning problems. To mitigate these issues, we adapt the scheduled sampling algorithm to diffusion training. We conduct experiments on four markup datasets: formulas (LaTeX), table layouts (HTML), sheet music (LilyPond), and molecular images (SMILES). These experiments each verify the effectiveness of diffusion and the use of scheduled sampling to fix generation issues. These results also show that the markup-to-image task presents a useful controlled compositional setting for diagnosing and analyzing generative image models.",['Applications'],[],"['Yuntian Deng', 'Noriyuki Kojima', 'Alexander M Rush']","['University of Waterloo', 'Cornell University', 'Cornell University']",[]
https://iclr.cc/virtual/2023/poster/10947,Fairness & Bias,Can Neural Networks Learn Implicit Logic from Physical Reasoning?,"Despite the success of neural network models in a range of domains, it remains an open question whether they can learn to represent abstract logical operators such as negation and disjunction. We test the hypothesis that neural networks without inherent inductive biases for logical reasoning can acquire an implicit representation of negation and disjunction. Here, implicit refers to limited, domain-specific forms of these operators, and work in psychology suggests these operators may be a precursor (developmentally and evolutionarily) to the type of abstract, domain-general logic that is characteristic of adult humans. To test neural networks, we adapt a test designed to diagnose the presence of negation and disjunction in animals and pre-verbal children, which requires inferring the location of a hidden object using constraints of the physical environment as well as implicit logic: if a ball is hidden in A or B, and shown not to be in A, can the subject infer that it is in B? Our results show that, despite the neural networks learning to track objects behind occlusion, they are unable to generalize to a task that requires implicit logic. We further show that models are unable to generalize to the test task even when they are trained directly on a logically identical (though visually dissimilar) task. However, experiments using transfer learning reveal that the models do recognize structural similarity between tasks which invoke the same logical reasoning pattern, suggesting that some desirable abstractions are learned, even if they are not yet sufficient to pass established tests of logical reasoning.","['Deep Learning and representational learning', 'logic', 'cognitive science', 'logical reasoning', 'developmental psychology', 'logical operators', 'representation learning', 'intuitive physics', 'physical reasoning']",[],"['Aaron Traylor', 'Roman Feiman', 'Ellie Pavlick']","['Brown University', 'Brown University', 'Brown University']",[]
https://iclr.cc/virtual/2023/poster/11800,Fairness & Bias,ManyDG: Many-domain Generalization for Healthcare Applications,"The vast amount of health data has been continuously collected for each patient, providing opportunities to support diverse healthcare predictive tasks such as seizure detection and hospitalization prediction. Existing models are mostly trained on other patients’ data and evaluated on new patients. Many of them might suffer from poor generalizability. One key reason can be overfitting due to the unique information related to patient identities and their data collection environments, referred to as patient covariates in the paper. These patient covariates usually do not contribute to predicting the targets but are often difficult to remove. As a result, they can bias the model training process and impede generalization. In healthcare applications, most existing domain generalization methods assume a small number of domains. In this paper, considering the diversity of patient covariates, we propose a new setting by treating each patient as a separate domain (leading to many domains). We develop a new domain generalization method ManyDG, that can scale to such many-domain problems. Our method identifies the patient do- main covariates by mutual reconstruction, and removes them via an orthogonal projection step. Extensive experiments show that ManyDG can boost the generalization performance on multiple real-world healthcare tasks (e.g., 3.7% Jaccard improvements on MIMIC drug recommendation) and support realistic but challenging settings such as insufficient data and continuous learning. The code is available at https://github.com/ycq091044/ManyDG.","['Machine Learning for Sciences', 'Healthcare', 'EEG', 'Patient covariate shift', 'EHR', 'domain generalization']",[],"['Chaoqi Yang', 'M Brandon Westover', 'Jimeng Sun']","['University of Illinois Urbana Champaign', 'Massachusetts General Hospital, Harvard University', 'Georgia Tech Research Corporation']",[]
https://iclr.cc/virtual/2023/poster/11117,Fairness & Bias,$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference,"In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art calibration-based methods under comparable few-shot scenario. 2) Beyond-Context: $k$NN Prompting can further scale up effectively with as many training data as are available, continually bringing substantial improvements. The scaling trend holds across 10 orders of magnitude ranging from 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B to 30B. It successfully bridges data scaling into model scaling, and brings new potentials for the gradient-free paradigm of LLM deployment. Code is publicly available at https://github.com/BenfengXu/KNNPrompting","['Deep Learning and representational learning', 'large language models', 'in-context learning', 'K Nearest Neighbors']",[],"['Benfeng Xu', 'Quan Wang', 'Zhendong Mao', 'Yajuan Lyu', 'Qiaoqiao She', 'Yongdong Zhang']","['University of Science and Technology of China', 'Beijing University of Posts and Telecommunications', 'University of Science and Technology of China', 'Baidu', 'Baidu', 'University of Science and Technology of China']",[]
https://iclr.cc/virtual/2023/poster/11804,Fairness & Bias,Causality Compensated Attention for Contextual Biased Visual Recognition,"Visual attention does not always capture the essential object representation desired for robust predictions. Attention modules tend to underline not only the target object but also the common co-occurring context that the module thinks helpful in the training. The problem is rooted in the confounding effect of the context leading to incorrect causalities between objects and predictions, which is further exacerbated by visual attention. In this paper, to learn causal object features robust for contextual bias, we propose a novel attention module named Interventional Dual Attention (IDA) for visual recognition. Specifically, IDA adopts two attention layers with multiple sampling intervention, which compensates the attention against the confounder context. Note that our method is model-agnostic and thus can be implemented on various backbones. Extensive experiments show our model obtains significant improvements in classification and detection with lower computation. In particular, we achieve the state-of-the-art results in multi-label classification on MS-COCO and PASCAL-VOC.","['Deep Learning and representational learning', 'object recognition', 'causal inference', 'Interventional Dual Attention', 'Confounding Context', 'attention mechanism']",[],"['Ruyang Liu', 'Jingjia Huang', 'Thomas H. Li', 'Ge Li']","['Peking University', 'ByteDance Inc.', 'AIIT, Peking University', 'Peking University Shenzhen Graduate School']",[]
https://iclr.cc/virtual/2023/poster/11260,Fairness & Bias,$\Lambda$-DARTS: Mitigating Performance Collapse by Harmonizing Operation Selection among Cells,"Differentiable neural architecture search (DARTS) is a popular method for neural architecture search (NAS), which performs cell-search and utilizes continuous relaxation to improve the search efficiency via gradient-based optimization. The main shortcoming of DARTS is performance collapse, where the discovered architecture suffers from a pattern of declining quality during search. Performance collapse has become an important topic of research, with many methods trying to solve the issue through either regularization or fundamental changes to DARTS.However, the weight-sharing framework used for cell-search in DARTS and the convergence of architecture parameters has not been analyzed yet. In this paper, we provide a thorough and novel theoretical and empirical analysis on DARTS and its point of convergence.We show that DARTS suffers from a specific structural flaw due to its weight-sharing framework that limits the convergence of DARTS to saturation points of the softmax function. This point of convergence gives an unfair advantage to layers closer to the output in choosing the optimal architecture, causing performance collapse. We then propose two new regularization terms that aim to prevent performance collapse by harmonizing operation selection via aligning gradients of layers. Experimental results on six different search spaces and three different datasets show that our method ($\Lambda$-DARTS) does indeed prevent performance collapse, providing justification for our theoretical analysis and the proposed remedy.",['Deep Learning and representational learning'],[],"['Sajad Movahedi', 'Melika Adabinejad', 'Ayyoob Imani', 'Arezou Keshavarz', 'Mostafa Dehghani', 'Azadeh Shakery', 'Babak N Araabi']","['University of Tehran, University of Tehran', 'University of Tehran, University of Tehran', 'Ludwig Maximilian University of Munich', 'Stanford University', 'Google DeepMind', 'University of Tehran, University of Tehran', 'University of Tehran, University of Tehran']",[]
https://iclr.cc/virtual/2023/poster/11471,Fairness & Bias,Learning to Segment from Noisy Annotations: A Spatial Correction Approach,"Noisy labels can significantly affect the performance of deep neural networks (DNNs). In medical image segmentation tasks, annotations are error-prone due to the high demand in annotation time and in the annotators' expertise. Existing methods mostly tackle label noise in classification tasks. Their independent-noise assumptions do not fit label noise in segmentation task. In this paper, we propose a novel noise model for segmentation problems that encodes spatial correlation and bias, which are prominent in segmentation annotations. Further, to mitigate such label noise, we propose a label correction method to recover true label progressively. We provide theoretical guarantees of the correctness of the proposed method. Experiments show that our approach outperforms current state-of-the-art methods on both synthetic and real-world noisy annotations.",['Deep Learning and representational learning'],[],"['Jiachen Yao', 'Songzhu Zheng', 'Prateek Prasanna', 'Chao Chen']","[', State University of New York at Stony Brook', 'Morgan Stanley', 'State University of New York, Stony Brook', 'State University of New York at Stony Brook']",[]
https://iclr.cc/virtual/2023/poster/10754,Fairness & Bias,Mid-Vision Feedback,"Feedback plays a prominent role in biological vision, where perception is modulated based on agents' evolving expectations and world model. We introduce a novel mechanism which modulates perception based on high level categorical expectations: Mid-Vision Feedback (MVF). MVF associates high level contexts with linear transformations. When a context is ""expected"" its associated linear transformation is applied over feature vectors in a mid level of a network. The result is that mid-level network representations are biased towards conformance with high level expectations, improving overall accuracy and contextual consistency. Additionally, during training mid-level feature vectors are biased through introduction of a loss term which increases the distance between feature vectors associated with different contexts. MVF is agnostic as to the source of contextual expectations, and can serve as a mechanism for top down integration of symbolic systems with deep vision architectures. We show the superior performance of MVF to post-hoc filtering for incorporation of contextual knowledge, and show superior performance of configurations using predicted context (when no context is known a priori) over configurations with no context awareness.",['Applications'],[],"['Eadom T Dessalene', 'Cornelia Fermuller', 'Yiannis Aloimonos']","['University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2023/poster/11445,Fairness & Bias,Agnostic Learning of General ReLU Activation Using Gradient Descent,"We provide a convergence analysis of gradient descent for the problem of agnostically learning a single ReLU function under Gaussian distributions. Unlike prior work that studies the setting of zero bias, we consider the more challenging scenario when the bias of the ReLU function is non-zero. Our main result establishes that starting from random initialization, in a polynomial number of iterations gradient descent outputs, with high probability, a ReLU function that achieves an error that is within a constant factor of the optimal i.e., it is guaranteed to achieve an error of $O(OPT)$, where $OPT$ is the error of the best ReLU function. This is a significant improvement over existing guarantees for gradient descent, which only guarantee  error of $O(\sqrt{d \cdot OPT})$ even in the zero-bias case  (Frei et al., 2020). We also provide finite sample guarantees, and obtain similar guarantees for a broader class of marginal distributions beyond Gaussians.","['Theory', 'agnostic learning', 'learning ReLU', 'global convergence', 'learning theory']",[],"['Pranjal Awasthi', 'Alex Tang', 'Aravindan Vijayaraghavan']","['Google', 'Northwestern University', 'Northwestern University']",[]
https://iclr.cc/virtual/2023/poster/10847,Fairness & Bias,Implicit Regularization for Group Sparsity,"We study the implicit regularization of gradient descent towards structured sparsity via a novel neural reparameterization, which we call a diagonally grouped linear neural network. We show the following intriguing property of our reparameterization: gradient descent over the squared regression loss, without any explicit regularization, biases towards solutions with a group sparsity structure. In contrast to many existing works in understanding implicit regularization, we prove that our training trajectory cannot be simulated by mirror descent. We analyze the gradient dynamics of the corresponding regression problem in the general noise setting and obtain minimax-optimal error rates. Compared to existing bounds for implicit sparse regularization using diagonal linear networks, our analysis with the new reparameterization shows improved sample complexity. In the degenerate case of size-one groups, our approach gives rise to a new algorithm for sparse linear regression. Finally, we demonstrate the efficacy of our approach with several numerical experiments.","['Optimization', 'implicit regularization', 'gradient descent', 'linear neural network', 'structured/group sparsity']",[],"['Jiangyuan Li', 'Thanh V Nguyen', 'Chinmay Hegde', 'Raymond K. W. Wong']","['Texas A&M University', 'Amazon', 'New York University', 'Texas A&M University']",[]
https://iclr.cc/virtual/2023/poster/11594,Fairness & Bias,Martingale Posterior Neural Processes,"A Neural Process (NP) estimates a stochastic process implicitly defined with neural networks given a stream of data, rather than pre-specifying priors already known, such as Gaussian processes. An ideal NP would learn everything from data without any inductive biases, but in practice, we often restrict the class of stochastic processes for the ease of estimation. One such restriction is the use of a finite-dimensional latent variable accounting for the uncertainty in the functions drawn from NPs. Some recent works show that this can be improved with more “data-driven” source of uncertainty such as bootstrapping. In this work, we take a different approach based on the martingale posterior, a recently developed alternative to Bayesian inference. For the martingale posterior, instead of specifying prior-likelihood pairs, a predictive distribution for future data is specified. Under specific conditions on the predictive distribution, it can be shown that the uncertainty in the generated future data actually corresponds to the uncertainty of the implicitly defined Bayesian posteriors. Based on this result, instead of assuming any form of the latent variables, we equip a NP with a predictive distribution implicitly defined with neural networks and use the corresponding martingale posteriors as the source of uncertainty. The resulting model, which we name as Martingale Posterior Neural Process (MPNP), is demonstrated to outperform baselines on various tasks.",['Probabilistic Methods'],[],"['Hyungi Lee', 'EungGu Yun', 'Giung Nam', 'Edwin Fong', 'Juho Lee']","['Korea Advanced Institute of Science & Technology', 'Saige Research', 'Korea Advanced Institute of Science & Technology', 'University of Hong Kong', 'KAIST']",[]
https://iclr.cc/virtual/2023/poster/11158,Fairness & Bias,A View From Somewhere: Human-Centric Face Representations,"Few datasets contain self-identified demographic information, inferring demographic information risks introducing additional biases, and collecting and storing data on sensitive attributes can carry legal risks. Besides, categorical demographic labels do not necessarily capture all the relevant dimensions of human diversity. We propose to implicitly learn a set of continuous face-varying dimensions, without ever asking an annotator to explicitly categorize a person. We uncover the dimensions by learning on A View From Somewhere (AVFS) dataset of 638,180 human judgments of face similarity. We demonstrate the utility of our learned embedding space for predicting face similarity judgments, collecting continuous face attribute values, attribute classification, and comparative dataset diversity auditing. Moreover, using a novel conditional framework, we show that an annotator's demographics influences the \emph{importance} they place on different attributes when judging similarity, underscoring the \emph{need} for diverse annotator groups to avoid biases. Data and code are available at \url{https://github.com/SonyAI/aviewfrom_somewhere}.","['Social Aspects of Machine Learning', 'annotator bias', 'computer vision', 'faces', 'diversity', 'similarity', 'cognitive', 'mental representations']",[],"['Jerone Andrews', 'Przemyslaw Joniak']","['Sony AI', 'The University of Tokyo']",[]
https://iclr.cc/virtual/2023/poster/11456,Fairness & Bias,SMART: Sentences as Basic Units for Text Evaluation,"Widely used evaluation metrics for text generation either do not work well with longer texts or fail to evaluate all aspects of text quality. In this paper, we introduce a new metric called SMART to mitigate such limitations. Specifically, we treat sentences as basic units of matching instead of tokens, and use a sentence matching function to soft-match candidate and reference sentences. Candidate sentences are also compared to sentences in the source documents to allow grounding (e.g., factuality) evaluation. Our results show that system-level correlations of our proposed metric with a model-based matching function outperforms all competing metrics on the SummEval summarization meta-evaluation dataset, while the same metric with a string-based matching function is competitive with current model-based metrics. The latter does not use any neural model, which is useful during model development phases where resources can be limited and fast evaluation is required. SMART also outperforms all factuality evaluation metrics on the TRUE benchmark. Finally, we also conducted extensive analyses showing that our proposed metrics work well with longer  summaries and are less biased towards specific models.","['Applications', 'summarization', 'evaluation']",[],"['Reinald Kim Amplayo', 'Peter J Liu', 'Yao Zhao', 'Shashi Narayan']","['Google', 'Google Brain', 'Google', 'Google']",[]
https://iclr.cc/virtual/2023/poster/11351,Fairness & Bias,Fairness and Accuracy under Domain Generalization,"As machine learning (ML) algorithms are increasingly used in high-stakes applications, concerns have arisen that they may be biased against certain social groups. Although many approaches have been proposed to make ML models fair, they typically rely on the assumption that data distributions in training and deployment are identical. Unfortunately, this is commonly violated in practice and a model that is fair during training may lead to an unexpected outcome during its deployment. Although the problem of designing robust ML models under dataset shifts has been widely studied, most existing works focus only on the transfer of accuracy. In this paper, we study the transfer of both fairness and accuracy under domain generalization where the data at test time may be sampled from never-before-seen domains. We first develop theoretical bounds on the unfairness and expected loss at deployment, and then derive sufficient conditions under which fairness and accuracy can be perfectly transferred via invariant representation learning. Guided by this, we design a learning algorithm such that fair ML models learned with training data still have high fairness and accuracy when deployment environments change. Experiments on real-world data validate the proposed algorithm.","['Social Aspects of Machine Learning', 'equal opportunity', 'equalized odds', 'js-divergence', 'regularization', 'invariant representation', 'domain generalization', 'fairness', 'accuracy']",[],"['Thai-Hoang Pham', 'Xueru Zhang', 'Ping Zhang']","['Ohio State University, Columbus', 'Ohio State University', 'The Ohio State University']",[]
https://iclr.cc/virtual/2023/poster/11518,Fairness & Bias,Mitigating Dataset Bias by Using Per-Sample Gradient,"The performance of deep neural networks is strongly influenced by the training dataset setup. In particular, when attributes with a strong correlation with the target attribute are present, the trained model can provide unintended prejudgments and show significant inference errors (i.e., the dataset bias problem). Various methods have been proposed to mitigate dataset bias, and their emphasis is on weakly correlated samples, called bias-conflicting samples. These methods are based on explicit bias labels provided by humans. However, such methods require human costs. Recently, several studies have sought to reduce human intervention by utilizing the output space values of neural networks, such as feature space, logits, loss, or accuracy. However, these output space values may be insufficient for the model to understand the bias attributes well. In this study, we propose a debiasing algorithm leveraging gradient called Per-sample Gradient-based Debiasing (PGD). PGD is comprised of three steps: (1) training a model on uniform batch sampling, (2) setting the importance of each sample in proportion to the norm of the sample gradient, and (3) training the model using importance-batch sampling, whose probability is obtained in step (2). Compared with existing baselines for various datasets, the proposed method showed state-of-the-art accuracy for the classification task. Furthermore, we describe theoretical understandings of how PGD can mitigate dataset bias.","['Deep Learning and representational learning', 'Gradient-norm based debiasing', 'dataset bias', 'debiasing']",[],"['Sumyeong Ahn', 'Seongyoon Kim', 'Se-Young Yun']","['Michigan State University', 'Korea Advanced Institute of Science & Technology', 'KAIST']",[]
https://iclr.cc/virtual/2023/poster/11181,Fairness & Bias,Unbiased Supervised Contrastive Learning,"Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss ($\epsilon$-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets including CIFAR10, CIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with $\epsilon$-SupInfoNCE, reaching state-of-the-art performance on a number of biased datasets, including real instances of biases ""in the wild"".","['Deep Learning and representational learning', 'supervised learning', 'deep learning', 'representation learning', 'neural networks', 'debiasing', 'contrastive learning']",[],"['Carlo Alberto Barbano', 'Benoit Dufumier', 'Enzo Tartaglione', 'Marco Grangetto', 'Pietro Gori']","['University of Turin', 'EPFL - EPF Lausanne', 'Télécom ParisTech', 'University of Turin', 'Telecom Paris']",[]
https://iclr.cc/virtual/2023/poster/11572,Fairness & Bias,Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation,"Skip connections and normalisation layers form two standard architectural components that are ubiquitous for the training of Deep Neural Networks (DNNs), but whose precise roles are poorly understood. Recent approaches such as Deep Kernel Shaping have made progress towards reducing our reliance on them, using insights from wide NN kernel theory to improve signal propagation in vanilla DNNs (which we define as networks without skips or normalisation). However, these approaches are incompatible with the self-attention layers present in transformers, whose kernels are intrinsically more complicated to analyse and control.  And so the question remains: \emph{is it possible to train deep vanilla transformers?} We answer this question in the affirmative by designing several approaches that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers. Our methods address various intricacies specific to signal propagation in transformers, including the interaction with positional encoding and causal masking. In experiments on WikiText-103 and C4, our approaches enable deep transformers without normalisation to train at speeds matching their standard counterparts, and deep vanilla transformers to reach the same performance as standard ones after about 5 times more iterations.","['Deep Learning and representational learning', 'self-attention', 'signal propagation', 'rank collapse', 'layer normalisation', 'residual connections', 'deep transformers', 'neural networks and kernels', 'positional encoding']",[],"['Bobby He', 'James Martens', 'Guodong Zhang', 'Aleksandar Botev', 'Andrew Brock', 'Samuel L Smith', 'Yee Whye Teh']","['Department of Computer Science, ETHZ - ETH Zurich', 'DeepMind', 'Department of Computer Science, University of Toronto', 'DeepMind', 'DeepMind', 'Google DeepMind', 'University of Oxford and DeepMind']",[]
https://iclr.cc/virtual/2023/poster/10767,Fairness & Bias,Learning to reason over visual objects,"A core component of human intelligence is the ability to identify abstract patterns inherent in complex, high-dimensional perceptual data, as exemplified by visual reasoning tasks such as Raven’s Progressive Matrices (RPM). Motivated by the goal of designing AI systems with this capacity, recent work has focused on evaluating whether neural networks can learn to solve RPM-like problems. Previous work has generally found that strong performance on these problems requires the incorporation of inductive biases that are specific to the RPM problem format, raising the question of whether such models might be more broadly useful. Here, we investigated the extent to which a general-purpose mechanism for processing visual scenes in terms of objects might help promote abstract visual reasoning. We found that a simple model, consisting only of an object-centric encoder and a transformer reasoning module, achieved state-of-the-art results on both of two challenging RPM-like benchmarks (PGM and I-RAVEN), as well as a novel benchmark with greater visual complexity (CLEVR-Matrices). These results suggest that an inductive bias for object-centric processing may be a key component of abstract visual reasoning, obviating the need for problem-specific inductive biases.",['Deep Learning and representational learning'],[],"['Shanka Subhra Mondal', 'Taylor Whittington Webb', 'Jonathan D. Cohen']","['Princeton University', 'University of California, Los Angeles', 'Princeton University']",[]
https://iclr.cc/virtual/2023/poster/11437,Fairness & Bias,Tuning Frequency Bias in Neural Network Training with Nonuniform Data,"Small generalization errors of over-parameterized neural networks (NNs) can be partially explained by the frequency biasing phenomenon, where gradient-based algorithms minimize the low-frequency misfit before reducing the high-frequency residuals. Using the Neural Tangent Kernel (NTK), one can provide a theoretically rigorous analysis for training where data are drawn from constant or piecewise-constant probability densities. Since most training data sets are not drawn from such distributions, we use the NTK model and a data-dependent quadrature rule to theoretically quantify the frequency biasing of NN training given fully nonuniform data. By replacing the loss function with a carefully selected Sobolev norm, we can further amplify, dampen, counterbalance, or reverse the intrinsic frequency biasing in NN training.","['Theory', 'neural tangent kernel', 'Sobolev norms', 'nonuniform', 'frequency bias', 'training', 'neural networks']",[],"['Annan Yu', 'Yunan Yang', 'Alex Townsend']","['Cornell University', 'Cornell University', 'Cornell University']",[]
https://iclr.cc/virtual/2023/poster/11440,Fairness & Bias,BigVGAN: A Universal Neural Vocoder with Large-Scale Training,"Despite recent progress in generative adversarial network (GAN)-based vocoders, where the model generates raw waveform conditioned on acoustic features, it is challenging to synthesize high-fidelity audio for numerous speakers across various recording environments. In this work, we present BigVGAN, a universal vocoder that generalizes well for various out-of-distribution scenarios without fine-tuning. We introduce periodic activation function and anti-aliased representation into the GAN generator, which brings the desired inductive bias for audio synthesis and significantly improves audio quality. In addition, we train our GAN vocoder at the largest scale up to 112M parameters, which is unprecedented in the literature. We identify and address the failure modes in large-scale GAN training for audio, while maintaining high-fidelity output without over-regularization. Our BigVGAN, trained only on clean speech (LibriTTS), achieves the state-of-the-art performance for various zero-shot (out-of-distribution) conditions, including unseen speakers, languages, recording environments, singing voices, music, and instrumental audio. We release our code and model at: https://github.com/NVIDIA/BigVGAN","['Applications', 'waveform generation', 'universal neural vocoder', 'speech synthesis', 'audio synthesis']",[],"['Sang-gil Lee', 'Wei Ping', 'Boris Ginsburg', 'Bryan Catanzaro', 'Sungroh Yoon']","['NVIDIA', 'NVIDIA', 'NVIDIA', 'University of California Berkeley', 'Seoul National University']",[]
https://iclr.cc/virtual/2023/poster/10729,Fairness & Bias,Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data ,"The implicit biases of gradient-based optimization algorithms are conjectured to be a major factor in the success of modern deep learning.  In this work, we investigate the implicit bias of gradient flow and gradient descent in two-layer fully-connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal, a common property of high-dimensional data.  For gradient flow, we leverage recent work on the implicit bias for homogeneous neural networks to show that asymptotically, gradient flow produces a neural network with rank at most two.  Moreover, this network is an $\ell_2$-max-margin solution (in parameter space), and has a linear decision boundary that corresponds to an approximate-max-margin linear predictor.  For gradient descent, provided the random initialization variance is small enough, we show that a single step of gradient descent suffices to drastically reduce the rank of the network, and that the rank remains small throughout training.  We provide experiments which suggest that a small initialization scale is important for finding low-rank neural networks with gradient descent.","['Deep Learning and representational learning', 'gradient descent', 'implicit bias', 'gradient flow', 'neural networks']",[],"['Spencer Frei', 'Gal Vardi', 'Peter Bartlett', 'Nathan Srebro', 'Wei Hu']","['University of California, Davis', 'Toyota Technological Institute at Chicago', 'University of California - Berkeley', 'Toyota Technological Institute at Chicago', 'University of Michigan - Ann Arbor']",[]
https://iclr.cc/virtual/2023/poster/10699,Fairness & Bias,Task-Aware Information Routing from Common Representation Space in Lifelong Learning,"Intelligent systems deployed in the real world suffer from catastrophic forgetting when exposed to a sequence of tasks. Humans, on the other hand, acquire, consolidate, and transfer knowledge between tasks that rarely interfere with the consolidated knowledge.  Accompanied by self-regulated neurogenesis, continual learning in the brain is governed by the rich set of neurophysiological processes that harbor different types of knowledge which are then integrated by the conscious processing. Thus, inspired by Global Workspace Theory of conscious information access in the brain, we propose TAMiL, a continual learning method that entails task-attention modules to capture task-specific information from the common representation space. We employ simple, undercomplete autoencoders to create a communication bottleneck between the common representation space and the global workspace, allowing only the task-relevant information to the global workspace, thereby greatly reducing task interference. Experimental results show that our method outperforms state-of-the-art rehearsal-based and dynamic sparse approaches and bridges the gap between fixed capacity and parameter isolation approaches while being scalable. We also show that our method effectively mitigates catastrophic forgetting while being well-calibrated with reduced task-recency bias.","['Deep Learning and representational learning', 'lifelong learning', 'representation learning', 'Global workspace theory', 'Task-specific attention', 'continual learning']",[],"['Prashant Shivaram Bhat', 'Bahram Zonooz', 'Elahe Arani']","['Eindhoven University of Technology', 'Eindhoven University of Technology', 'Eindhoven University of Technology']",[]
https://iclr.cc/virtual/2023/poster/12254,Fairness & Bias,Editing models with task arithmetic,"Changing how pre-trained models behave---e.g., improving their performance on a downstream task or mitigating biases learned during pre-training---is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around task vectors. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Moreover, task vectors can be added together to improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D"", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training.","['Deep Learning and representational learning', 'weight interpolation', 'pre-trained models', 'model patching', 'merging models', 'Fine-tuning', 'model editing', 'transfer learning']",[],"['Gabriel Ilharco', 'Marco Tulio Ribeiro', 'Mitchell Wortsman', 'Ludwig Schmidt', 'Hannaneh Hajishirzi', 'Ali Farhadi']","['Department of Computer Science, University of Washington', 'Microsoft', 'University of Washington, Seattle', 'University of Washington', 'University of Washington', 'University of Washington']",[]
https://iclr.cc/virtual/2023/poster/10908,Fairness & Bias,Learning Language Representations with Logical Inductive Bias,"Transformer architectures have achieved great success in solving natural language tasks, which learn strong language representations from large-scale unlabeled texts. In this paper, we seek to go further beyond and explore a new logical inductive bias for better language representation learning. Logic reasoning is known as a formal methodology to reach answers from given knowledge and facts. Inspired by such a view, we develop a novel neural architecture named FOLNet (First-Order Logic Network), to encode this new inductive bias. We construct a set of neural logic operators as learnable Horn clauses, which are further forward-chained into a fully differentiable neural architecture (FOLNet). Interestingly, we find that the self-attention module in transformers can be composed by two of our neural logic operators, which probably explains their strong reasoning performance. Our proposed FOLNet has the same input and output interfaces as other pretrained models and thus could be pretrained/finetuned by using similar losses. It also allows FOLNet to be used in a plug-and-play manner when replacing other pretrained models. With our logical inductive bias, the same set of ``logic deduction skills'' learned through pretraining are expected to be equally capable of solving diverse downstream tasks. For this reason, FOLNet learns language representations that have much stronger transfer capabilities. Experimental results on several language understanding tasks show that our pretrained FOLNet model outperforms the existing strong transformer-based approaches.","['Deep Learning and representational learning', 'language representation learning', 'pretraining', 'inductive bias', 'model architecture']",[],['Jianshu Chen'],['Tencent AI Lab'],[]
https://iclr.cc/virtual/2023/poster/11941,Fairness & Bias,Mega: Moving Average Equipped Gated Attention,"The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.  We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.","['Deep Learning and representational learning', 'exponential moving average', 'attention', 'Neural Architecture']",[],"['Xuezhe Ma', 'Chunting Zhou', 'Xiang Kong', 'Junxian He', 'Liangke Gui', 'Graham Neubig', 'Jonathan May', 'Luke Zettlemoyer']","['USC/ISI', 'Meta AI', 'Carnegie Mellon University', 'Hong Kong University of Science and Technology', 'School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University', 'University of Southern California', 'University of Washington']",[]
https://iclr.cc/virtual/2023/poster/11145,Privacy & Data Governance,On the Trade-Off between Actionable Explanations and the Right to be Forgotten,"As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the “right to be forgotten” which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a framework to identify a minimal subset of critical training points which, when removed, maximize the fraction of invalidated recourses.Using our framework, we empirically show that the removal of as little as 2 data instances from the training set can invalidate up to 95 percent of all recourses output by popular state-of-the-art algorithms. Thus, our work raises fundamental questions about the compatibility ofthe right to an actionable explanation'' in the context of theright to be forgotten'', while also providing constructive insights on the determining factors of recourse robustness.","['Social Aspects of Machine Learning', 'transparency', 'Counterfactual Explanations', 'Algorihtmic Recourse', 'interpretability', 'explainability']",[],"['Martin Pawelczyk', 'Tobias Leemann', 'Asia Biega', 'Gjergji Kasneci']","['Harvard University', 'Technische Universität München', 'Max Planck Institute for Security and Privacy', 'Technische Universität München']",[]
https://iclr.cc/virtual/2023/poster/10982,Privacy & Data Governance,Synthetic Data Generation of Many-to-Many Datasets via Random Graph Generation,"Synthetic data generation (SDG) has become a popular approach to release private datasets.In SDG, a generative model is fitted on the private real data, and samples drawn from the model are released as the protected synthetic data.While real-world datasets usually consist of multiple tables with potential \emph{many-to-many} relationships (i.e.~\emph{many-to-many datasets}), recent research in SDG mostly focuses on modeling tables \emph{independently} or only considers generating datasets with special cases of many-to-many relationships such as \emph{one-to-many}.In this paper, we first study challenges of building faithful generative models for many-to-many datasets, identifying limitations of existing methods.We then present a novel factorization for many-to-many generative models,  which leads to a scalable generation framework by combining recent results from random graph theory and representation learning.Finally, we extend the framework to establish the notion of $(\epsilon,\delta)$-differential privacy.Through a real-world dataset, we demonstrate that our method can generate synthetic datasets while preserving information within and across tables better than its closest competitor.","['Generative models', 'differential privacy', 'random graph generation', 'synthetic data generation']",[],"['Kai Xu', 'Georgi Ganev', 'Emile Joubert', 'Rees Davison', 'Olivier Van Acker', 'Luke Robinson']","['Amazon', 'University College London, University of London', 'University of Cape Town', 'University of Oxford', 'Birkbeck College, University of London', 'Hazy']",[]
https://iclr.cc/virtual/2023/poster/11012,Privacy & Data Governance,Federated Learning from Small Datasets,"Federated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning.","['Deep Learning and representational learning', 'small datasets', 'Distributed', 'daisy chain', 'sparse data', 'federated learning']",[],"['Michael Kamp', 'Jonas Fischer', 'Jilles Vreeken']","['Institute for AI in Medicine IKIM', 'Harvard TH Chan School of Public Health', 'CISPA Helmholtz Center for Information Security']",[]
https://iclr.cc/virtual/2023/poster/11368,Privacy & Data Governance,Federated Nearest Neighbor Machine Translation,"To protect user privacy and meet legal regulations, federated learning (FL) is attracting significant attention. Training neural machine translation (NMT) models with traditional FL algorithm (e.g., FedAvg) typically relies on multi-round model-based interactions. However, it is impractical and inefficient for machine translation tasks due to the vast communication overheads and heavy synchronization. In this paper, we propose a novel federated nearest neighbor (FedNN) machine translation framework that, instead of multi-round model-based interactions, leverages one-round memorization-based interaction to share knowledge across different clients to build low-overhead privacy-preserving systems. The whole approach equips the public NMT model trained on large-scale accessible data with a $k$-nearest-neighbor ($k$NN) classifier and integrates the external datastore constructed by private text data in all clients to form the final FL model.  A two-phase datastore encryption strategy is introduced to achieve privacy-preserving during this process.  Extensive experiments show that FedNN significantly reduces computational and communication costs compared with FedAvg, while maintaining promising performance in different FL settings.","['Applications', 'Memorization Augmentation', 'machine translation', 'federated learning']",[],"['Yichao Du', 'Zhirui Zhang', 'Bingzhe Wu', 'Lemao Liu', 'Tong Xu', 'Enhong Chen']","['University of Science and Technology of China', 'Tencent AI Lab', 'Peking University', 'Tencent AI Lab', 'University of Science and Technology of China', 'University of Science and Technology of China']",[]
https://iclr.cc/virtual/2023/poster/11519,Privacy & Data Governance,Efficient Model Updates for Approximate Unlearning of Graph-Structured Data,"With the adoption of recent laws ensuring the ``right to be forgotten'', the problem of machine unlearning has become of significant importance. This is particularly the case for graph-structured data, and learning tools specialized for such data, including graph neural networks (GNNs). This work introduces the first known approach for \emph{approximate graph unlearning} with provable theoretical guarantees. The challenges in addressing the problem are two-fold. First, there exist multiple different types of unlearning requests that need to be considered, including node feature, edge and node unlearning. Second, to establish provable performance guarantees, one needs to carefully evaluate the process of feature mixing during propagation. We focus on analyzing Simple Graph Convolutions (SGC) and their generalized PageRank (GPR) extensions, thereby laying the theoretical foundations for unlearning GNNs. Empirical evaluations of six benchmark datasets demonstrate excellent performance/complexity/privacy trade-offs of our approach compared to complete retraining and general methods that do not leverage graph information. For example, unlearning $200$ out of $1208$ training nodes of the Cora dataset only leads to a $0.1\%$ loss in test accuracy, but offers a $4$-fold speed-up compared to complete retraining with a $(\epsilon,\delta)=(1,10^{-4})$ ``privacy cost''. We also exhibit a $12\%$ increase in test accuracy for the same dataset when compared to unlearning methods that do not leverage graph information, with comparable time complexity and the same privacy guarantee.","['Social Aspects of Machine Learning', 'machine unlearning', 'graph unlearning', 'privacy']",[],"['Eli Chien', 'Chao Pan', 'Olgica Milenkovic']","['Georgia Institute of Technology', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign']",[]
https://iclr.cc/virtual/2023/poster/12111,Privacy & Data Governance,Disparate Impact in Differential Privacy from Gradient Misalignment,"As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.","['Social Aspects of Machine Learning', 'differential privacy', 'privacy', 'fairness']",[],"['Maria S. Esipova', 'Atiyeh Ashari Ghomi', 'Yaqiao Luo', 'Jesse C. Cresswell']","['University of Waterloo', 'Department of Computer Science, University of Toronto', 'Department of Computer Science, University of Toronto', 'Layer 6 AI']",[]
https://iclr.cc/virtual/2023/poster/11554,Privacy & Data Governance,Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection,"Deep neural networks have witnessed huge successes in many challenging prediction tasks and yet they often suffer from out-of-distribution (OoD) samples, misclassifying them with high confidence. Recent advances show promising OoD detection performance for centralized training, and however, OoD detection in federated learning (FL) is largely overlooked, even though many security sensitive applications such as autonomous driving and voice recognition authorization are commonly trained using FL for data privacy concerns. The main challenge that prevents previous state-of-the-art OoD detection methods from being incorporated to FL is that they require large amount of real OoD samples. However, in real-world scenarios, such large-scale OoD training data can be costly or even infeasible to obtain, especially for resource-limited local devices. On the other hand, a notorious challenge in FL is data heterogeneity where each client collects non-identically and independently distributed (non-iid) data. We propose to take advantage of such heterogeneity and turn the curse into a blessing that facilitates OoD detection in FL. The key is that for each client, non-iid data from other clients (unseen external classes) can serve as an alternative to real OoD samples. Specifically, we propose a novel Federated Out-of-Distribution Synthesizer (FOSTER), which learns a class-conditional generator to synthesize virtual external-class OoD samples, and maintains data confidentiality and communication efficiency required by FL. Experimental results show that our method outperforms the state-of-the-art by 2.49%, 2.88%, 1.42% AUROC, and 0.01%, 0.89%, 1.74% ID accuracy, on CIFAR-10, CIFAR-100, and STL10, respectively.","['Deep Learning and representational learning', 'out-of-distribution detection', 'Heterogeneity', 'federated learning']",[],"['Shuyang Yu', 'Junyuan Hong', 'Haotao Wang', 'Zhangyang Wang', 'Jiayu Zhou']","['Michigan State University', 'University of Texas at Austin', 'University of Texas, Austin', 'University of Texas at Austin', 'Michigan State University']",[]
https://iclr.cc/virtual/2023/poster/10724,Privacy & Data Governance,Asynchronous Distributed Bilevel Optimization,"Bilevel optimization plays an essential role in many machine learning tasks, ranging from hyperparameter optimization to meta-learning. Existing studies on bilevel optimization, however, focus on either centralized or synchronous distributed setting. The centralized bilevel optimization approaches require collecting massive amount of data to a single server, which inevitably incur significant communication expenses and may give rise to data privacy risks. Synchronous distributed bilevel optimization algorithms, on the other hand, often face the straggler problem and will immediately stop working if a few workers fail to respond. As a remedy, we propose Asynchronous Distributed Bilevel Optimization (ADBO) algorithm. The proposed ADBO can tackle bilevel optimization problems with both nonconvex upper-level and lower-level objective functions, and its convergence is theoretically guaranteed. Furthermore, it is revealed through theoretic analysis that the iteration complexity of ADBO to obtain the $\epsilon$-stationary point is upper bounded by $\mathcal{O}(\frac{1}{{{\epsilon ^2}}})$. Thorough empirical studies on public datasets have been conducted to elucidate the effectiveness and efficiency of the proposed ADBO.",['Optimization'],[],"['Yang Jiao', 'Tiancheng Wu', 'Dongjin Song', 'Chengtao Jian']","['Tongji University', 'Tongji University', 'University of Connecticut', 'Tongji University']",[]
https://iclr.cc/virtual/2023/poster/11725,Privacy & Data Governance,Progressive Voronoi Diagram Subdivision Enables Accurate Data-free Class-Incremental Learning,"Data-free Class-incremental Learning (CIL) is a challenging problem because rehearsing data from previous phases is strictly prohibited, causing catastrophic forgetting of Deep Neural Networks (DNNs). In this paper, we present \emph{iVoro}, a novel framework derived from computational geometry. We found Voronoi Diagram (VD), a classical model for space subdivision, is especially powerful for solving the CIL problem, because VD itself can be constructed favorably in an incremental manner -- the newly added sites (classes) will only affect the proximate classes, making the non-contiguous classes hardly forgettable. Furthermore, we bridge DNN and VD using Power Diagram Reduction, and show that the VD structure can be progressively refined along the phases using a divide-and-conquer algorithm. Moreover, our VD construction is not restricted to the deep feature space, but is also applicable to multiple intermediate feature spaces, promoting VD to be multilayer VD that efficiently captures multi-grained features from DNN. Importantly, \emph{iVoro} is also capable of handling uncertainty-aware test-time Voronoi cell assignment and has exhibited high correlations between geometric uncertainty and predictive accuracy (up to ${\sim}0.9$). Putting everything together, \emph{iVoro} achieves up to $25.26\%$, $37.09\%$, and $33.21\%$ improvements on CIFAR-100, TinyImageNet, and ImageNet-Subset, respectively, compared to the state-of-the-art non-exemplar CIL approaches. In conclusion, \emph{iVoro} enables highly accurate, privacy-preserving, and geometrically interpretable CIL that is particularly useful when cross-phase data sharing is forbidden, e.g. in medical applications.","['Deep Learning and representational learning', 'Voronoi Diagram', 'Computational Geometry']",[],"['Chunwei Ma', 'Zhanghexuan Ji', 'Ziyun Huang', 'Yan Shen', 'Mingchen Gao', 'Jinhui Xu']","['State University of New York, Buffalo', 'Johnson & Johnson MedTech LCI/WWDA', 'Pennsylvania State University, Erie', 'State University of New York, Buffalo', 'University at Buffalo, SUNY', 'State University of New York, Buffalo']",[]
https://iclr.cc/virtual/2023/poster/11920,Privacy & Data Governance,Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses,"This paper studies federated learning (FL)—especially cross-silo FL—with data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) has data from different people (e.g. patients) and must maintain the privacy of each person’s data (e.g. medical record), even if the server or other silos act as adversarial eavesdroppers. This requirement motivates the study of Inter-Silo Record-Level Differential Privacy (ISRL-DP), which requires silo $i$’s communications to satisfy record/item-level differential privacy (DP). ISRL-DP ensures that the data of each person (e.g. patient) in silo $i$ (e.g. hospital $i$) cannot be leaked. ISRL-DP is different from well-studied privacy notions. Central and user-level DP assume that people trust the server/other silos. On the other end of the spectrum, local DP assumes that people do not trust anyone at all (even their own silo). Sitting between central and local DP, ISRL-DP makes the realistic assumption (in cross-silo FL) that people trust their own silo, but not the server or other silos. In this work, we provide tight (up to logarithms) upper and lower bounds for ISRL-DP FL with convex/strongly convex loss functions and homogeneous (i.i.d.) silo data. Remarkably, we show that similar bounds are attainable for smooth losses with arbitrary heterogeneous silo data distributions, via an accelerated ISRL-DP algorithm. We also provide tight upper and lower bounds for ISRL-DP federated empirical risk minimization, and use acceleration to attain the optimal bounds in fewer rounds of communication than the state-of-the-art. Finally, with a secure “shuffler” to anonymize silo messages (but without a trusted server), our algorithm attains the optimal central DP rates under more practical trust assumptions. Numerical experiments show favorable privacy-accuracy tradeoffs for our algorithm in classification and regression tasks.","['Social Aspects of Machine Learning', 'private optimization', 'stochastic convex optimization', 'cross-silo federated learning', 'differential privacy', 'distributed optimization', 'federated learning']",[],"['Andrew Lowy', 'Meisam Razaviyayn']","['University of Southern California', 'University of Southern California']",[]
https://iclr.cc/virtual/2023/poster/11949,Privacy & Data Governance,Dataless Knowledge Fusion by Merging Weights of Language Models,"Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a data-less knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.","['Applications', 'model merging', 'weight merging']",[],"['Xisen Jin', 'Xiang Ren', 'Daniel Preotiuc-Pietro', 'Pengxiang Cheng']","['University of Southern California', 'University of Southern California', 'Bloomberg', 'Bloomberg']",[]
https://iclr.cc/virtual/2023/poster/11384,Privacy & Data Governance,Distributed Differential Privacy in Multi-Armed Bandits,"We consider the standard $K$-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server.  Under this trust model, previous work largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves ($\epsilon,\delta$) or approximate-DP guarantee by sacrificing an additive $O\!\left(\!\frac{K\log T\sqrt{\log(1/\delta)}}{\epsilon}\!\right)\!$ factor in $T$-step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger ($\epsilon,0$) or pure-DP guarantee under the widely used central trust model is only $\Theta\!\left(\!\frac{K\log T}{\epsilon}\!\right)\!$, where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We also show that our algorithm, when instantiated with Skellam noise and the secure protocol, ensures \emph{R\'{e}nyi differential privacy} -- a stronger notion than approximate DP -- under distributed trust model with a privacy cost of $O\!\left(\!\frac{K\sqrt{\log T}}{\epsilon}\!\right)\!$. Finally, as a by-product of our techniques, we also recover the best-known regret bounds for bandits under central and local models while using only \emph{discrete privacy noise}, which can avoid the privacy leakage due to floating point arithmetic of continuous noise on finite computers.","['Reinforcement Learning', 'differential privacy', 'Multi-armed Bandits']",[],"['Sayak Ray Chowdhury', 'Xingyu Zhou']","['Microsoft', 'Wayne State University']",[]
https://iclr.cc/virtual/2023/poster/11870,Privacy & Data Governance,"FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy","Federated learning (FL) is an emerging distributed machine learning framework which jointly trains a global model via a large number of local devices with data privacy protections. Its performance suffers from the non-vanishing biases introduced by the local inconsistent optimal and the rugged client-drifts by the local over-fitting. In this paper, we propose a novel and practical method, FedSpeed, to alleviate the negative impacts posed by these problems.  Concretely, FedSpeed applies the prox-correction term on the current local updates to efficiently reduce the biases introduced by the prox-term, a necessary regularizer to maintain the strong local consistency. Furthermore, FedSpeed merges the vanilla stochastic gradient with a perturbation computed from an extra gradient ascent step in the neighborhood, thereby alleviating the issue of local over-fitting. Our theoretical analysis indicates that the convergence rate is related to both the communication rounds $T$ and local intervals $K$ with a tighter upper bound $\mathcal{O}(\frac{1}{T})$ if $K=\mathcal{O}(T)$.  Moreover, we conduct extensive experiments on the real-world dataset to demonstrate the efficiency of our proposed FedSpeed, which converges significantly faster and achieves the state-of-the-art (SOTA) performance on the general FL experimental settings than several baselines including FedAvg, FedProx, FedCM, FedAdam, SCAFFOLD, FedDyn, FedADMM, etc.","['Deep Learning and representational learning', 'federated learning']",[],"['Yan Sun', 'Li Shen', 'Tiansheng Huang', 'Liang Ding', 'Dacheng Tao']","['University of Sydney', 'JD Explore Academy', 'Georgia Institute of Technology', 'JD Explore Academy, JD.com Inc.', 'University of Sydney']",[]
https://iclr.cc/virtual/2023/poster/10727,Privacy & Data Governance,Individual Privacy Accounting with Gaussian Differential Privacy,"Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual losses in a principled manner, we need a privacy accountant for adaptive compositions of mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the Rényi differential privacy by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on determining a certain supermartingale for the hockey-stick divergence and on extending the Rényi divergence-based fully adaptive composition results by Feldman and Zrnic (2021). We also consider measuring the individual  $(\varepsilon,\delta)$-privacy losses using the so-called privacy loss distributions. Using the Blackwell theorem, we can then use the results of Feldman and Zrnic (2021) to construct an approximative individual $(\varepsilon,\delta)$-accountant. We also show how to speed up the FFT-based individual DP accounting using the Plancherel theorem.","['Social Aspects of Machine Learning', 'differential privacy', 'gaussian differential privacy', 'privacy accounting', 'fully adaptive compositions', 'individual privacy loss']",[],"['Antti Koskela', 'Marlon Tobaben', 'Antti Honkela']","['Nokia Bell Labs', 'University of Helsinki', 'University of Helsinki']",[]
https://iclr.cc/virtual/2023/poster/12051,Privacy & Data Governance,Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors,"Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation methods. Existing DABP approaches mostly rely on knowledge distillation (KD) from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises and leads to degrading performance. To mitigate such bias, we propose a new strategy, \textit{divide-to-adapt}, that purifies cross-domain knowledge distillation by proper domain division. This is inspired by an observation we make for the first time in domain adaptation: the target domain usually contains easy-to-adapt and hard-to-adapt samples that have different levels of domain discrepancy w.r.t. the source domain, and deep models tend to fit easy-to-adapt samples first. Leveraging easy-to-adapt samples with less noise can help KD alleviate the negative effect of prediction noises from black-box predictors. In this sense, the target domain can be divided into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain at the early stage of training. Then the adaptation is achieved by semi-supervised learning. We further reduce distribution discrepancy between subdomains and develop weak-strong augmentation strategy to filter the predictor errors progressively. As such, our method is a simple yet effective solution to reduce error accumulation in cross-domain knowledge distillation for DABP. Moreover, we prove that the target error of DABP is bounded by the noise ratio of two subdomains, i.e., the confirmation bias, which provides the theoretical justifications for our method. Extensive experiments demonstrate our method achieves state of the art on all DABP benchmarks, outperforming the existing best approach by 7.0\% on VisDA-17, and is even comparable with the standard domain adaptation methods that use the source-domain data.","['Deep Learning and representational learning', 'model adaptation', 'transfer learning', 'black-box predictors']",[],"['Jianfei Yang', 'Xiangyu Peng', 'Kai Wang', 'Zheng Zhu', 'Jiashi Feng', 'Lihua Xie', 'Yang You']","['Nanyang Technological University', 'National University of Singapore', 'national university of singaore, National University of Singapore', 'PhiGent Robotics', 'ByteDance', 'Nanyang Technological University', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/12184,Privacy & Data Governance,Stochastic Differentially Private and Fair Learning,"Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals’ health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term “stochastic"" refers to the fact that our proposed algorithm converges even when minibatches of data are used at each iteration (i.e. stochastic optimization). Our framework is flexible enough to permit different fairness notions, including demographic parity and equalized odds. In addition, our algorithm can be applied to non-binary classification tasks with multiple (non-binary) sensitive attributes. As a byproduct of our convergence analysis, we provide the first utility guarantee for a DP algorithm for solving nonconvex-strongly concave min-max problems. Our numerical experiments show that the proposed algorithm consistently offers significant performance gains over the state-of-the-art baselines, and can be applied to larger scale problems with non-binary target/sensitive attributes.","['Social Aspects of Machine Learning', 'differential privacy', 'stochastic optimization', 'Algorithmic fairness', 'private fair learning']",[],"['Andrew Lowy', 'Devansh Gupta', 'Meisam Razaviyayn']","['University of Southern California', 'Indraprastha Institute of Information Technology, Delhi', 'University of Southern California']",[]
https://iclr.cc/virtual/2023/poster/11362,Privacy & Data Governance,Machine Unlearning of Federated Clusters,"Federated clustering (FC) is an unsupervised learning problem that arises in a number of practical applications, including personalized recommender and healthcare systems. With the adoption of recent laws ensuring the ""right to be forgotten"", the problem of machine unlearning for FC methods has become of significant importance. We introduce, for the first time, the problem of machine unlearning for FC, and propose an efficient unlearning mechanism for a customized secure FC framework. Our FC framework utilizes special initialization procedures that we show are well-suited for unlearning. To protect client data privacy, we develop the secure compressed multiset aggregation (SCMA) framework that addresses sparse secure federated learning (FL) problems encountered during clustering as well as more general problems. To simultaneously facilitate low communication complexity and secret sharing protocols, we integrate Reed-Solomon encoding with special evaluation points into our SCMA pipeline, and prove that the client communication cost is logarithmic in the vector dimension. Additionally, to demonstrate the benefits of our unlearning mechanism over complete retraining, we provide a theoretical analysis for the unlearning performance of our approach. Simulation results show that the new FC framework exhibits superior clustering performance compared to previously reported FC baselines when the cluster sizes are highly imbalanced. Compared to completely retraining K-means++ locally and globally for each removal request, our unlearning procedure offers an average speed-up of roughly 84x across seven datasets. Our implementation for the proposed method is available at https://github.com/thupchnsky/mufc.","['Social Aspects of Machine Learning', 'machine unlearning', 'secure aggregation', 'federated clustering', 'federated learning']",[],"['Chao Pan', 'Jin Sima', 'Saurav Prakash', 'Vishal Rana', 'Olgica Milenkovic']","['University of Illinois, Urbana Champaign', 'University of Illinois Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana-Champaign', 'University of Illinois, Urbana Champaign']",[]
https://iclr.cc/virtual/2023/poster/11224,Privacy & Data Governance,Confidential-PROFITT: Confidential PROof of FaIr Training of Trees,"Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees.","['Social Aspects of Machine Learning', 'Zero-Knowledge Proof', 'Audit', 'fairness', 'confidentiality']",[],"['Ali Shahin Shamsabadi', 'Sierra Calanda Wyllie', 'Nicholas Franzese', 'Natalie Dullerud', 'Sébastien Gambs', 'Nicolas Papernot', 'Xiao Wang', 'Adrian Weller']","['Brave Software', 'University of Toronto', 'Northwestern University, Northwestern University', 'Stanford University', 'Université du Québec à Montréal', 'University of Toronto', 'Northwestern University', 'Alan Turing Institute']",[]
https://iclr.cc/virtual/2023/poster/10697,Privacy & Data Governance,Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning,"Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy.  Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free.  Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it).  We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \DPFEDREP\ converges to a ball centered around the \emph{global optimal} solution at a linear rate, and the radius of the ball is proportional to the reciprocal of the privacy budget.  With this novel utility analysis, we improve the SOTA utility-privacy trade-off for this problem by a factor of $\sqrt{d}$, where $d$ is the input dimension.  We empirically evaluate our method with the image classification task on CIFAR10, CIFAR100, and EMNIST, and observe a significant performance improvement over the prior work under the same small privacy budget. The code can be found in this link, https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.","['Social Aspects of Machine Learning', 'differential privacy', 'representation learning', 'federated learning']",[],"['Zebang Shen', 'Jiayuan Ye', 'Anmin Kang', 'Hamed Hassani', 'Reza Shokri']","['Department of Computer Science, ETHZ - ETH Zurich', 'National University of Singapore', 'National University of Singapore', 'University of Pennsylvania', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/12127,Privacy & Data Governance,Easy Differentially Private Linear Regression,"Linear regression is a fundamental tool for statistical analysis. This has motivated the development of linear regression methods that also satisfy differential privacy and thus guarantee that the learned model reveals little about any one data point used to construct it. However, existing differentially private solutions assume that the end user can easily specify good data bounds and hyperparameters. Both present significant practical obstacles. In this paper, we study an algorithm which uses the exponential mechanism to select a model with high Tukey depth from a collection of non-private regression models. Given $n$ samples of $d$-dimensional data used to train $m$ models, we construct an efficient analogue using an approximate Tukey depth that runs in time $O(d^2n + dm\log(m))$. We find that this algorithm obtains strong empirical performance in the data-rich setting with no data bounds or hyperparameter selection required.","['Social Aspects of Machine Learning', 'differential privacy', 'Linear Regression']",[],"['Kareem Amin', 'Matthew Joseph', 'Mónica Ribero', 'Sergei Vassilvitskii']","['Google', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2023/poster/10740,Privacy & Data Governance,Regression with Label Differential Privacy,"We study the task of training regression models with the guarantee oflabeldifferential privacy (DP). Based on a global prior distribution of label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a ""randomized response on bins"", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.","['Social Aspects of Machine Learning', 'regression', 'label differential privacy']",[],"['Badih Ghazi', 'Pritish Kamath', 'Ravi Kumar', 'Ethan Jacob Leeman', 'Pasin Manurangsi', 'Avinash V Varadarajan', 'Chiyuan Zhang']","['Google', 'Google Research', 'Google', 'Google', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2023/poster/11240,Security,Learning ReLU networks to high uniform accuracy is intractable,"Statistical learning theory provides bounds on the necessary number of training samples needed to reach a prescribed accuracy in a learning problem formulated over a given target class. This accuracy is typically measured in terms of a generalization error, that is, an expected value of a given loss function. However, for several applications --- for example in a security-critical context or for problems in the computational sciences --- accuracy in this sense is not sufficient. In such cases, one would like to have guarantees for high accuracy on every input value, that is, with respect to the uniform norm. In this paper we precisely quantify the number of training samples needed for any conceivable training algorithm to guarantee a given uniform accuracy on any learning problem formulated over target classes containing (or consisting of) ReLU neural networks of a prescribed architecture. We prove that, under very general assumptions, the minimal number of training samples for this task scales exponentially both in the depth and the input dimension of the network architecture.","['Theory', 'Teacher-Student Learning', 'hardness results', 'Sample complexity', 'ReLU networks', 'learning theory']",[],"['Julius Berner', 'Philipp Grohs', 'Felix Voigtlaender']","['University of Vienna', 'University of Vienna', 'Katholische Universität Eichstätt-Ingolstadt']",[]
https://iclr.cc/virtual/2023/poster/11045,Security,Effective passive membership inference attacks in federated learning against overparameterized models,"This work considers the challenge of performing membership inference attacks in a federated learning setting ---for image classification--- where an adversary can only observe the communication between the central node and a single client (a passive white-box attack). Passive attacks are one of the hardest-to-detect attacks, since they can be performed without modifying how the behavior of the central server or its clients, and assumesno access to private data instances. The key insight of our method is empirically observing that, near parameters that generalize well in test, the gradient of large overparameterized neural network models statistically behave like high-dimensional independent isotropic random vectors.  Using this insight, we devise two attacks that are often little impacted by existing and proposed defenses. Finally, we validated the hypothesis that our attack depends on the overparametrization by showing that increasing the level of overparametrization (without changing the neural network architecture) positively correlates with our attack effectiveness.","['Deep Learning and representational learning', 'membership inference attack', 'neural networks', 'image classification', 'overparameterization', 'federated learning']",[],"['Jiacheng Li', 'Ninghui Li', 'Bruno Ribeiro']","['Purdue University', 'Purdue University', 'Purdue University']",[]
https://iclr.cc/virtual/2023/poster/11157,Security,CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning,"Federated Learning (FL) is a setting for training machine learning models in distributed environments where the clients do not share their raw data but instead send model updates to a server. However, model updates can be subject to attacks and leak private information. Differential Privacy (DP) is a leading mitigation strategy which involves adding noise to clipped model updates, trading off performance for strong theoretical privacy guarantees. Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice. In this paper, we aim to achieve a tighter measurement of the model exposure by considering a realistic threat model. We propose a novel method, CANIFE, that uses canaries - carefully crafted samples by a strong adversary to evaluate the empirical privacy of a training round. We apply this attack to vision models trained on CIFAR-10 and CelebA and to language models trained on Sent140 and Shakespeare. In particular, in realistic FL scenarios, we demonstrate that the empirical per-round epsilon obtained with CANIFE is 4 -- 5$\times$ lower than the theoretical bound.","['Social Aspects of Machine Learning', 'Model Auditing', 'Empirical Privacy', 'membership inference attack', 'differential privacy', 'federated learning']",[],"['Samuel Maddock', 'Alexandre Sablayrolles', 'Pierre Stock']","['University of Warwick', 'Facebook', 'Facebook']",[]
https://iclr.cc/virtual/2023/poster/11267,Security,"Inequality phenomenon in $l_{\infty}$-adversarial training, and its unrealized threats","The appearance of adversarial examples raises attention from both academia and industry. Along with the attack-defense arms race, adversarial training is the most effective against adversarial examples.However, we find inequality phenomena occur during the $l_{\infty}$-adversarial training, that few features dominate the prediction made by the adversarially trained model. We systematically evaluate such inequality phenomena by extensive experiments and find such phenomena become more obvious when performing adversarial training with increasing adversarial strength (evaluated by $\epsilon$). We hypothesize such inequality phenomena make $l_{\infty}$-adversarially trained model less reliable than the standard trained model when few ``important features"" are influenced. To validate our hypothesis, we proposed two simple attacks that either perturb or replace important features with noise or occlusion. Experiments show that $l_{\infty}$-adversarially trained model can be easily attacked when the few important features are influenced. Our work shed light on the limitation of the practicality of $l_{\infty}$-adversarial training.","['Social Aspects of Machine Learning', 'adversarial training', 'Adversarial feature represenation', 'adversarial robustness']",[],"['Ranjie Duan', 'YueFeng Chen', 'Yao Zhu', 'Xiaojun Jia', 'Rong Zhang', ""Hui Xue'""]","['Alibaba Group', 'Alibaba Group', 'Zhejiang University', ', Chinese Academy of Sciences', 'Huazhong University of Science and Technology', 'Zhejiang University, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/10910,Security,Empowering Graph Representation Learning with Test-Time Graph Transformation,"As powerful tools for representation learning on graphs, graph neural networks (GNNs) have facilitated various applications from drug discovery to recommender systems. Nevertheless, the effectiveness of GNNs is immensely challenged by issues related to data quality, such as distribution shift, abnormal features and adversarial attacks. Recent efforts have been made on tackling these issues from a modeling perspective which requires additional cost of changing model architectures or re-training model parameters. In this work, we provide a data-centric view to tackle these issues and propose a graph transformation framework named GTrans which adapts and refines graph data at test time to achieve better performance. We provide theoretical analysis on the design of the framework and discuss why adapting graph data works better than adapting the model. Extensive experiments have demonstrated the effectiveness of GTrans on three distinct scenarios for eight benchmark datasets where suboptimal data is presented. Remarkably, GTrans performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings.","['Deep Learning and representational learning', 'out-of-distribution generalization', 'graph neural networks', 'adversarial robustness']",[],"['Wei Jin', 'Tong Zhao', 'Jiayuan Ding', 'Yozen Liu', 'Jiliang Tang', 'Neil Shah']","['Emory University', 'Snap Inc.', 'University of Southern California', 'Snap Inc.', 'Michigan State University', 'Snap Inc.']",[]
https://iclr.cc/virtual/2023/poster/11208,Security,Defending against Adversarial Audio  via Diffusion Model,"Deep learning models have been widely used in commercial acoustic systems in recent years. However, adversarial audio examples can cause abnormal behaviors for those acoustic systems, while being hard for humans to perceive. Various methods, such as transformation-based defenses and adversarial training, have been proposed to protect acoustic systems from adversarial attacks, but they are less effective against adaptive attacks. Furthermore, directly applying the methods from the image domain can lead to suboptimal results because of the unique properties of audio data. In this paper, we propose an adversarial purification-based defense pipeline, AudioPure, for acoustic systems via off-the-shelf diffusion models. Taking advantage of the strong generation ability of diffusion models, AudioPure first adds a small amount of noise to the adversarial audio and then runs the reverse sampling step to purify the noisy audio and recover clean audio. AudioPure is a plug-and-play method that can be directly applied to any pretrained classifier without any fine-tuning or re-training. We conduct extensive experiments on the speech command recognition task to evaluate the robustness of AudioPure. Our method is effective against diverse adversarial attacks (e.g. L2 or L∞-norm). It outperforms the existing methods under both strong adaptive white-box and black-box attacks bounded by L2 or L∞-norm (up to +20% in robust accuracy). Besides, we also evaluate the certified robustness for perturbations bounded by L2-norm via randomized smoothing. Our pipeline achieves a higher certified accuracy than baselines.","['Social Aspects of Machine Learning', 'Adversarial attack and defense', 'Diffusion Models', 'speech recognition', 'AI security']",[],"['Shutong Wu', 'Jiongxiao Wang', 'Wei Ping', 'Weili Nie', 'Chaowei Xiao']","['University of Wisconsin - Madison', 'University of Wisconsin - Madison', 'NVIDIA', 'NVIDIA', 'University of Wisconsin - Madison']",[]
https://iclr.cc/virtual/2023/poster/11253,Security,Neural-based classification rule learning for sequential data,"Discovering interpretable patterns for classification of sequential data is of key importance for a variety of fields, ranging from genomics to fraud detection or more generally interpretable decision-making.In this paper, we propose a novel differentiable fully interpretable method to discover both local and global patterns (i.e. catching a relative or absolute temporal dependency) for rule-based binary classification.It consists of a convolutional binary neural network with an interpretable neural filter and a training strategy based on dynamically-enforced sparsity.We demonstrate the validity and usefulness of the approach on synthetic datasets and on an open-source peptides dataset.Key to this end-to-end differentiable method is that the expressive patterns used in the rules are learned alongside the rules themselves.","['Social Aspects of Machine Learning', 'classification rule learning', 'binary neural network', 'interpretable AI', 'sequential data']",[],"['Marine Collery', 'Philippe Bonnard', 'François FAGES', 'Remy Kusters']","['International Business Machines', 'Ecole Nationale Supérieure en Electrotechnique, Electronique, Informatique et Hydraulique de Toulouse', 'INRIA', 'International Business Machines']",[]
https://iclr.cc/virtual/2023/poster/11256,Security,Out-of-Distribution Detection based on In-Distribution Data Patterns Memorization with Modern Hopfield Energy,"Out-of-Distribution (OOD) detection is essential for safety-critical applications of deep neural networks. OOD detection is challenging since DNN models may produce very high logits value even for OOD samples. Hence, it is of great difficulty to discriminate OOD data by directly adopting Softmax on output logits as the confidence score. Differently, we detect the OOD sample with Hopfield energy in a store-then-compare paradigm. In more detail, penultimate layer outputs on the training set are considered as the representations of in-distribution (ID) data. Thus they can be transformed into stored patterns that serve as anchors to measure the discrepancy of unseen data for OOD detection. Starting from the energy function defined in Modern Hopfield Network for the discrepancy score calculation, we derive a simplified version SHE with theoretical analysis. In SHE, we utilize only one stored pattern to present each class, and these patterns can be obtained by simply averaging the penultimate layer outputs of training samples within this class. SHE has the advantages of hyperparameterfreeand high computational efficiency. The evaluations of nine widely-used OOD datasets show the promising performance of such a simple yet effective approach and its superiority over State-of-the-Art models. Code is available at https://github.com/zjs975584714/SHE ood detection.","['Deep Learning and representational learning', 'out-of-distribution detection', 'Hopfield Energy', 'Hyperparameter-Free']",[],"['Jinsong Zhang', 'Qiang Fu', 'Xu Chen', 'Lun Du', 'Zelin Li', 'Gang Wang', 'xiaoguang Liu', 'Shi Han', 'Dongmei Zhang']","['Research, Microsoft', 'Microsoft', 'Microsoft', 'Microsoft Research Asia', 'Research, Microsoft', 'Nankai University', 'Nankai University', 'Microsoft Research Asia', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/11701,Security,Characterizing the Influence of Graph Elements,"Influence function, a method from the robust statistics, measures the changes of model parameters or some functions about model parameters with respect to the removal or modification of training instances. It is an efficient and useful post-hoc method for studying the interpretability of machine learning models without the need of expensive model re-training. Recently, graph convolution networks (GCNs), which operate on graph data, have attracted a great deal of attention. However, there is no preceding research on the influence functions of GCNs to shed light on the effects of removing training nodes/edges from an input graph. Since the nodes/edges in a graph are interdependent in GCNs, it is challenging to derive influence functions for GCNs. To fill this gap, we started with the simple graph convolution (SGC) model that operates on an attributed graph, and formulated an influence function to approximate the changes of model parameters when a node or an edge is removed from an attributed graph. Moreover, we theoretically analyzed the error bound of the estimated influence of removing an edge. We experimentally validated the accuracy and effectiveness of our influence estimation function. In addition, we showed that the influence function of a SGC model could be used to estimate the impact of removing training nodes/edges on the test performance of the SGC without re-training the model. Finally, we demonstrated how to use influence functions to effectively guide the adversarial attacks on GCNs.","['General Machine Learning', 'influence functions', 'Interpretable Machine Learning', 'graph neural networks']",[],"['Zizhang Chen', 'Peizhao Li', 'Hongfu Liu', 'Pengyu Hong']","['Brandeis University', 'Brandeis University', 'Brandeis University', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/10897,Security,TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization,"Robustness evaluation against adversarial examples has become increasingly important to unveil the trustworthiness of the prevailing deep models in natural language processing (NLP). However, in contrast to the computer vision domain where the first-order projected gradient descent (PGD) is used as the benchmark approach to generate adversarial examples for robustness evaluation, there lacks a principled first-order gradient-based robustness evaluation framework in NLP. The emerging optimization challenges lie in 1) the discrete nature of textual inputs together with the strong coupling between the perturbation location and the actual content, and 2) the additional constraint that the perturbed text should be fluent and achieve a low perplexity under a language model. These challenges make the development of PGD-like NLP attacks difficult. To bridge the gap, we propose TextGrad, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP. Specifically, we address the aforementioned challenges in a unified optimization framework. And we develop an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables and leverage an effective sampling method to establish an accurate mapping from the continuous optimization variables to the discrete textual perturbations. Moreover, as a first-order attack generation method, TextGrad can be baked into adversarial training to further improve the robustness of NLP models. Extensive experiments are provided to demonstrate the effectiveness of TextGrad not only in attack generation for robustness evaluation but also in adversarial defense. From the attack perspective, we show that TextGrad achieves remarkable improvements in both the attack success rate and the perplexity score over five state-of-the-art baselines. From the defense perspective, TextGrad-enabled adversarial training yields the most robust NLP model against a wide spectrum of NLP attacks.",['Applications'],[],"['Bairu Hou', 'Jinghan Jia', 'Yihua Zhang', 'Guanhua Zhang', 'Yang Zhang', 'Sijia Liu', 'Shiyu Chang']","['University of California, Santa Barbara', 'Michigan State University', 'Michigan State University', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'International Business Machines', 'Michigan State University', 'UC Santa Barbara']",[]
https://iclr.cc/virtual/2023/poster/11377,Security,Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication,"Communication is important in many multi-agent reinforcement learning (MARL) problems for agents to share information and make good decisions. However, when deploying trained communicative agents in a real-world application where noise and potential attackers exist, the safety of communication-based policies becomes a severe issue that is underexplored. Specifically, if communication messages are manipulated by malicious attackers, agents relying on untrustworthy communication may take unsafe actions that lead to catastrophic consequences. Therefore, it is crucial to ensure that agents will not be misled by corrupted communication, while still benefiting from benign communication. In this work, we consider an environment with $N$ agents, where the attacker may arbitrarily change the communication from any $C<\frac{N-1}{2}$ agents to a victim agent. For this strong threat model, we propose a certifiable defense by constructing a message-ensemble policy that aggregates multiple randomly ablated message sets. Theoretical analysis shows that this message-ensemble policy can utilize benign communication while being certifiably robust to adversarial communication, regardless of the attacking algorithm. Experiments in multiple environments verify that our defense significantly improves the robustness of trained policies against various types of attacks.","['Reinforcement Learning', 'reinforcement learning', 'adversarial attack', 'adversarial communication', 'certifiable robustness', 'multi-agent system']",[],"['Yanchao Sun', 'Ruijie Zheng', 'Parisa Hassanzadeh', 'Yongyuan Liang', 'Soheil Feizi', 'Sumitra Ganesh', 'Furong Huang']","['J.P. Morgan AI Research', 'Department of Computer Science, University of Maryland, College Park', 'J.P. Morgan Chase', 'SUN YAT-SEN UNIVERSITY', 'University of Maryland, College Park', 'J.P. Morgan Chase', 'Department of Computer Science, University of Maryland']",[]
https://iclr.cc/virtual/2023/poster/11924,Security,Modeling Multimodal Aleatoric Uncertainty in Segmentation with Mixture of Stochastic Experts,"Equipping predicted segmentation with calibrated uncertainty is essential for safety-critical applications. In this work, we focus on capturing the data-inherent uncertainty (aka aleatoric uncertainty) in segmentation, typically when ambiguities exist in input images. Due to the high-dimensional output space and potential multiple modes in segmenting ambiguous images, it remains challenging to predict well-calibrated uncertainty for segmentation. To tackle this problem, we propose a novel mixture of stochastic experts (MoSE) model, where each expert network estimates a distinct mode of the aleatoric uncertainty and a gating network predicts the probabilities of an input image being segmented in those modes. This yields an efficient two-level uncertainty representation. To learn the model, we develop a Wasserstein-like loss that directly minimizes the distribution distance between the MoSE and ground truth annotations. The loss can easily integrate traditional segmentation quality measures and be efficiently optimized via constraint relaxation. We validate our method on the LIDC-IDRI dataset and a modified multimodal Cityscapes dataset. Results demonstrate that our method achieves the state-of-the-art or competitive performance on all metrics.","['Applications', 'Stochastic Segmentation', 'Multiple Annotations', 'Aleatoric Uncertainty', 'semantic segmentation']",[],"['Zhitong Gao', 'Yucong Chen', 'Chuyu Zhang', 'Xuming He']","['ShanghaiTech University', 'Xiaohongshu', 'ShanghaiTech University', 'ShanghaiTech University']",[]
https://iclr.cc/virtual/2023/poster/11891,Security,Towards Robustness Certification Against Universal Perturbations,"In this paper, we investigate the problem of certifying neural network robustness against universal perturbations (UPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing robustness certification methods aim to provide robustness guarantees for each sample with respect to the worst-case perturbations given a neural network. However, those sample-wise bounds will be loose when considering the UP threat model as they overlook the important constraint that the perturbation should be shared across all samples. We propose a method based on a combination of linear relaxation-based perturbation analysis and Mixed Integer Linear Programming to establish the first robust certification method for UP. In addition, we develop a theoretical framework for computing error bounds on the entire population using the certification results from a randomly sampled batch. Aside from an extensive evaluation of the proposed certification, we further show how the certification facilitates efficient comparison of robustness among different models or efficacy among different universal adversarial attack defenses and enables accurate detection of backdoor target classes.","['Social Aspects of Machine Learning', 'poisoning attack', 'adversarial attack', 'certified robustness', 'backdoor attack', 'Universal Perturbation']",[],"['Yi Zeng', 'Zhouxing Shi', 'Ming Jin', 'Feiyang Kang', 'Lingjuan Lyu', 'Cho-Jui Hsieh', 'Ruoxi Jia']","['Virginia Tech', 'University of California, Los Angeles', 'Virginia Tech', 'Virginia Polytechnic Institute and State University', 'Sony Research', 'Google', 'Virginia Tech']",[]
https://iclr.cc/virtual/2023/poster/11430,Security,Revisiting the Assumption of Latent Separability for Backdoor Defenses,"Recent studies revealed that deep learning is susceptible to backdoor poisoning attacks. An adversary can embed a hidden backdoor into a model to manipulate its predictions by only modifying a few training data, without controlling the training process. Currently, a tangible signature has been widely observed across a diverse set of backdoor poisoning attacks --- models trained on a poisoned dataset tend to learn separable latent representations for poison and clean samples. This latent separation is so pervasive that a family of backdoor defenses directly take it as a default assumption (dubbed latent separability assumption), based on which to identify poison samples via cluster analysis in the latent space. An intriguing question consequently follows: is the latent separation unavoidable for backdoor poisoning attacks? This question is central to understanding whether the assumption of latent separability provides a reliable foundation for defending against backdoor poisoning attacks. In this paper, we design adaptive backdoor poisoning attacks to present counter-examples against this assumption. Our methods include two key components: (1) a set of trigger-planted samples correctly labeled to their semantic classes (other than the target class) that can regularize backdoor learning; (2) asymmetric trigger planting strategies that help to boost attack success rate (ASR) as well as to diversify latent representations of poison samples. Extensive experiments on benchmark datasets verify the effectiveness of our adaptive attacks in bypassing existing latent separation based backdoor defenses. Moreover, our attacks still maintain a high attack success rate with negligible clean accuracy drop. Our studies call for defense designers to take caution when leveraging latent separation as an assumption in their defenses. Our codes are available at https://github.com/Unispac/Circumventing-Backdoor-Defenses.","['Social Aspects of Machine Learning', 'Backdoor Attacks']",[],"['Xiangyu Qi', 'Tinghao Xie', 'Yiming Li', 'Saeed Mahloujifar', 'Prateek Mittal']","['Princeton University', 'Princeton University', 'Zhejiang University', 'Meta', 'Princeton University']",[]
https://iclr.cc/virtual/2023/poster/11872,Security,Instance-wise Batch Label Restoration via Gradients in Federated Learning,"Gradient inversion attacks have posed a serious threat to the privacy of federated learning. The attacks search for the optimal pair of input and label best matching the shared gradients and the search space of the attacks can be reduced by pre-restoring labels. Recently, label restoration technique allows for the extraction of labels from gradients analytically, but even the state-of-the-art remains limited to identify the presence of categories (i.e., the class-wise label restoration). This work considers the more real-world settings, where there are multiple instances of each class in a training batch. An analytic method is proposed to perform instance-wise batch label restoration from only the gradient of the final layer. On the basis of the approximate recovered class-wise embeddings and post-softmax probabilities, we establish linear equations of the gradients, probabilities and labels to derive the Number of Instances (NoI) per class by the Moore-Penrose pseudoinverse algorithm. Our experimental evaluations reach over 99% Label existence Accuracy (LeAcc) and exceed 96% Label number Accuracy (LnAcc) in most cases on three image datasets and four classification models. The two metrics are used to evaluate class-wise and instance-wise label restoration accuracy, respectively. And the recovery is made feasible even with a batch size of 4096 and partially negative activations (e.g., Leaky ReLU and Swish). Furthermore, we demonstrate that our method facilitates the existing gradient inversion attacks by exploiting the recovered labels, with an increase of 6-7 in PSNR on both MNIST and CIFAR100. Our code isavailable at https://github.com/BUAA-CST/iLRG.","['Deep Learning and representational learning', 'batch label restoration', 'gradient inversion attack.', 'federated learning']",[],"['Kailang Ma', 'Yu Sun', 'Jian Cui', 'Dawei Li', 'Zhenyu Guan', 'Jianwei Liu']","['Beijing University of Aeronautics and Astronautics', 'School of Cyber Science and Technology, Beihang University', 'School of Cyber Science and Technology, Beihang University ', 'Beijing University of Aeronautics and Astronautics', 'Beijing University of Aeronautics and Astronautics', 'School of Cyber Science and Technology, Beihang University']",[]
https://iclr.cc/virtual/2023/poster/10721,Security,Zeroth-Order Optimization with Trajectory-Informed Derivative Estimation,"Zeroth-order (ZO) optimization, in which the derivative is unavailable, has recently succeeded in many important machine learning applications. Existing algorithms rely on finite difference (FD) methods for derivative estimation and gradient descent (GD)-based approaches for optimization. However, these algorithms suffer from query inefficiency because many additional function queries are required for derivative estimation in their every GD update, which typically hinders their deployment in real-world applications where every function query is expensive. To this end, we propose a trajectory-informed derivative estimation method which only employs the optimization trajectory (i.e., the history of function queries during optimization) and hence can eliminate the need for additional function queries to estimate a derivative. Moreover, based on our derivative estimation, we propose the technique of dynamic virtual updates, which allows us to reliably perform multiple steps of GD updates without reapplying derivative estimation. Based on these two contributions, we introduce the zeroth-order optimization with trajectory-informed derivative estimation (ZoRD) algorithm for query-efficient ZO optimization. We theoretically demonstrate that our trajectory-informed derivative estimation and our ZoRD algorithm improve over existing approaches, which is then supported by our real-world experiments such as black-box adversarial attack, non-differentiable metric optimization, and derivative-free reinforcement learning.","['Optimization', 'finite difference', 'zeroth-order optimization', 'derivative estimation']",[],"['Yao Shu', 'Zhongxiang Dai', 'Weicong Sng', 'Arun Verma', 'Patrick Jaillet', 'Bryan Kian Hsiang Low']","['Tencent AI', 'Massachusetts Institute of Technology', 'National University of Singapore', 'National University of Singapore', 'Massachusetts Institute of Technology', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/11070,Security,"Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow","Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neural networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness under certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part of the global minimum. By relating conserved quantities to convergence rate and sharpness of the minimum, we provide insights on how initialization impacts convergence and generalizability.","['Optimization', 'lie group', 'gradient flow', 'flat minima', 'symmetry', 'Lie algebra', 'conserved quantity']",[],"['Bo Zhao', 'Iordan Ganev', 'Robin Walters', 'Rose Yu', 'Nima Dehmamy']","['University of California, San Diego', 'Institute for Computing and Information Sciences, Radboud University Nijmegen, Radboud University', 'Northeastern University ', 'University of California, San Diego', 'International Business Machines']",[]
https://iclr.cc/virtual/2023/poster/11491,Security,Calibrating Transformers via Sparse Gaussian Processes,"Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer’s success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.","['Probabilistic Methods', 'gaussian processes', 'transformers', 'uncertainty estimation', 'variational inference', 'bayesian neural networks']",[],"['Wenlong Chen', 'Yingzhen Li']","['Imperial College London, Imperial College London', 'Imperial College London']",[]
https://iclr.cc/virtual/2023/poster/11495,Security,The Dark Side of AutoML: Towards Architectural Backdoor Search,"This paper asks the intriguing question: is it possible to exploit neural architecture search (NAS) as a new attack vector to launch previously improbable attacks? Specifically, we present EVAS, a new attack that leverages NAS to find neural architectures with inherent backdoors and exploits such vulnerability using input-aware triggers. Compared with existing attacks, EVAS demonstrates many interesting properties: (i) it does not require polluting training data or perturbing model parameters; (ii) it is agnostic to downstream fine-tuning or even re-training from scratch; (iii) it naturally evades defenses that rely on inspecting model parameters or training data. With extensive evaluation on benchmark datasets, we show that EVAS features high evasiveness, transferability, and robustness, thereby expanding the adversary's design spectrum. We further characterize the mechanisms underlying EVAS, which are possibly explainable by architecture-level ``shortcuts'' that recognize trigger patterns. This work showcases that NAS can be exploited in a harmful way to find architectures with inherent backdoor vulnerability. The code is available at https://github.com/ain-soph/nas_backdoor.","['Deep Learning and representational learning', 'neural architecture search', 'backdoor attack and defense']",[],"['Ren Pang', 'Changjiang Li', 'Zhaohan Xi', 'Shouling Ji', 'Ting Wang']","['Pennsylvania State University', 'State University of New York at Stony Brook', 'Pennsylvania State University', 'Zhejiang University', 'State University of New York at Stony Brook']",[]
https://iclr.cc/virtual/2023/poster/12258,Security,Guiding Safe Exploration with Weakest Preconditions,"In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions.","['reinforcement learning', 'safe learning', 'Safe exploration']",[],"['Greg Anderson', 'Swarat Chaudhuri', 'Isil Dillig']","['Reed College', 'University of Texas at Austin', 'University of Texas, Austin']",[]
https://iclr.cc/virtual/2023/poster/11490,Security,Perfectly Secure Steganography Using Minimum Entropy Coupling,"Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning. While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques. In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)'s information theoretic-model of steganography if and only if it is induced by a coupling. Furthermore, we show that, among perfectly secure procedures, a procedure is maximally efficient if and only if it is induced by a minimum entropy coupling. These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees with non-trivial efficiency; additionally, these algorithms are highly scalable. To provide empirical validation, we compare a minimum entropy coupling-based approach to three modern baselines---arithmetic coding, Meteor, and adaptive dynamic grouping---using GPT-2, WaveRNN, and Image Transformer as communication channels. We find that the minimum entropy coupling-based approach achieves superior encoding efficiency, despite its stronger security constraints. In aggregate, these results suggest that it may be natural to view information-theoretic steganography through the lens of minimum entropy coupling.","['Applications', 'Minimum Entropy Coupling', 'Information-Theoretic Steganography']",[],"['Christian Schroeder de Witt', 'Samuel Sokota', 'J Zico Kolter', 'Jakob Nicolaus Foerster', 'Martin Strohmeier']","['University of Oxford', 'Carnegie Mellon University', 'Carnegie Mellon University', 'University of Oxford, University of Oxford', 'armasuisse Science & Technology']",[]
https://iclr.cc/virtual/2023/poster/11166,Security,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 46 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.","['Social Aspects of Machine Learning', 'System 2', 'logical reasoning', 'reasoning', 'interpretability', 'language models', 'neural symbolic', 'large language models', 'Neuro-Symbolic']",[],"['Antonia Creswell', 'Murray Shanahan', 'Irina Higgins']","['DeepMind', 'Google', 'DeepMind']",[]
https://iclr.cc/virtual/2023/poster/11551,Security,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,"Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems.  Traditional adversarial training method requires ground-truth labels and hence cannot be directly applied to self-supervised MDE that does not have depth ground truth. Some self-supervised model hardening technique (e.g., contrastive learning) ignores the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using the depth ground truth. We improve adversarial robustness against physical-world attacks using $L_0$-norm-bounded perturbation in training. We compare our method with supervised learning-based and contrastive learning-based methods that are tailored for MDE. Results on two representative MDE networks show that we achieve better robustness against various adversarial attacks with nearly no benign performance degradation.","['Unsupervised and Self-supervised learning', 'adversarial training', 'Self-supervised Learning.', 'adversarial attack', 'Monocular Depth Estimation']",[],"['Zhiyuan Cheng', 'James Chenhao Liang', 'Guanhong Tao', 'Dongfang Liu', 'Xiangyu Zhang']","['Purdue University', 'Rochester Institute of Technology', 'Purdue University', 'Rochester Institute of Technology', ', Purdue University']",[]
https://iclr.cc/virtual/2023/poster/11554,Security,Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection,"Deep neural networks have witnessed huge successes in many challenging prediction tasks and yet they often suffer from out-of-distribution (OoD) samples, misclassifying them with high confidence. Recent advances show promising OoD detection performance for centralized training, and however, OoD detection in federated learning (FL) is largely overlooked, even though many security sensitive applications such as autonomous driving and voice recognition authorization are commonly trained using FL for data privacy concerns. The main challenge that prevents previous state-of-the-art OoD detection methods from being incorporated to FL is that they require large amount of real OoD samples. However, in real-world scenarios, such large-scale OoD training data can be costly or even infeasible to obtain, especially for resource-limited local devices. On the other hand, a notorious challenge in FL is data heterogeneity where each client collects non-identically and independently distributed (non-iid) data. We propose to take advantage of such heterogeneity and turn the curse into a blessing that facilitates OoD detection in FL. The key is that for each client, non-iid data from other clients (unseen external classes) can serve as an alternative to real OoD samples. Specifically, we propose a novel Federated Out-of-Distribution Synthesizer (FOSTER), which learns a class-conditional generator to synthesize virtual external-class OoD samples, and maintains data confidentiality and communication efficiency required by FL. Experimental results show that our method outperforms the state-of-the-art by 2.49%, 2.88%, 1.42% AUROC, and 0.01%, 0.89%, 1.74% ID accuracy, on CIFAR-10, CIFAR-100, and STL10, respectively.","['Deep Learning and representational learning', 'out-of-distribution detection', 'Heterogeneity', 'federated learning']",[],"['Shuyang Yu', 'Junyuan Hong', 'Haotao Wang', 'Zhangyang Wang', 'Jiayu Zhou']","['Michigan State University', 'University of Texas at Austin', 'University of Texas, Austin', 'University of Texas at Austin', 'Michigan State University']",[]
https://iclr.cc/virtual/2023/poster/11498,Security,Boosting Adversarial Transferability using Dynamic Cues,"The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\eg, DINO), and Vision-language models (\eg, CLIP) to black-box \emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnable transformation that allows optimizing for temporal gradients during an adversarial attack to fool the motion dynamics. Specifically, we introduce spatial (image) and temporal (video) cues within the same source model through task-specific prompts. Attacking such prompts maximizes the adversarial transferability from image-to-video and image-to-image models using the attacks designed for image models. As an example, an iterative attack launched from image model Deit-B with temporal prompts reduces generalization (top1 \% accuracy) of a video model by 35\% on Kinetics-400. Our approach also improves adversarial transferability to image models by 9\% on ImageNet w.r.t the current state-of-the-art approach. Our attack results indicate that the attacker does not need specialized architectures, \eg, divided space-time attention, 3D convolutions, or multi-view convolution networks for different data modalities. Image models are effective surrogates to optimize an adversarial attack to fool black-box models in a changing environment over time. Code is available at \url{https://bit.ly/3Xd9gRQ}","['Social Aspects of Machine Learning', 'prompt learning', 'Transferability', 'adversarial attacks', 'Dynamic video modeling']",[],"['Muzammal Naseer', 'Ahmad Mahmood', 'Salman Khan', 'Fahad Khan']","['Mohamed bin Zayed University of Artificial Intelligence', 'ETHZ - ETH Zurich', 'Mohamed bin Zayed University of Artificial Intelligence', 'Inception Institute of Artificial Intelligence']",[]
https://iclr.cc/virtual/2023/poster/11112,Security,Spectral Augmentation for Self-Supervised Learning on Graphs,"Graph contrastive learning (GCL), as an emerging self-supervised learning technique on graphs, aims to learn representations via instance discrimination. Its performance heavily relies on graph augmentation to reflect invariant patterns that are robust to small perturbations; yet it still remains unclear about what graph invariance GCL should capture. Recent studies mainly perform topology augmentations in a uniformly random manner in the spatial domain, ignoring its influence on the intrinsic structural properties embedded in the spectral domain. In this work, we aim to find a principled way for topology augmentations by exploring the invariance of graphs from the spectral perspective. We develop spectral augmentation which guides topology augmentations by maximizing the spectral change. Extensive experiments on both graph and node classification tasks demonstrate the effectiveness of our method in self-supervised representation learning. The proposed method also brings promising generalization capability in transfer learning, and is equipped with intriguing robustness property under adversarial attacks. Our study sheds light on a general principle for graph topology augmentation.","['Unsupervised and Self-supervised learning', 'graph self-supervised learning', 'graph spectral theory', 'graph augmentation']",[],"['Lu Lin', 'Jinghui Chen', 'Hongning Wang']","['Pennsylvania State University', 'Pennsylvania State University', 'Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/10995,Security,Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Approach,"Many machine learning problems today have multiple objective functions. They appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic multi-objective gradient correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the nonconvex setting. Simulations on multi-task supervised and reinforcement learning demonstrate the effectiveness of our method relative to the state-of-the-art methods.","['Optimization', 'machine learning', 'multi-objective optimization']",[],"['Heshan Devaka Fernando', 'Han Shen', 'Miao Liu', 'Subhajit Chaudhury', 'Keerthiram Murugesan', 'Tianyi Chen']","['Rensselaer Polytechnic Institute', 'Rensselaer Polytechnic Institute', 'International Business Machines', 'International Business Machines', 'International Business Machines', 'Rensselaer Polytechnic Institute']",[]
https://iclr.cc/virtual/2023/poster/12155,Security,Distilling Cognitive Backdoor Patterns within an Image,"This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the ``minimal essence"" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor attacks.We also show that CD can potentially be applied to help detect potential biases from face datasets.Code is available at https://github.com/HanxunH/CognitiveDistillation.","['Deep Learning and representational learning', 'Backdoor defence', 'Backdoor sample detection']",[],"['Hanxun Huang', 'Xingjun Ma', 'Sarah Monazam Erfani']","['University of Melbourne', 'Fudan University', 'The University of Melbourne']",[]
https://iclr.cc/virtual/2023/poster/11683,Security,Toward Adversarial Training on Contextualized Language Representation,"Beyond the success story of adversarial training (AT) in the recent text domain on top of pre-trained language models (PLMs), our empirical study showcases the inconsistent gains from AT on some tasks, e.g. commonsense reasoning, named entity recognition. This paper investigates AT from the perspective of the contextualized language representation outputted by PLM encoders. We find the current AT attacks lean to generate sub-optimal adversarial examples that can fool the decoder part but have a minor effect on the encoder. However, we find it necessary to effectively deviate the latter one to allow AT to gain. Based on the observation, we propose simple yet effective \textit{Contextualized representation-Adversarial Training} (CreAT), in which the attack is explicitly optimized to deviate the contextualized representation of the encoder. It allows a global optimization of adversarial examples that can fool the entire model. We also find CreAT gives rise to a better direction to optimize the adversarial examples, to let them less sensitive to hyperparameters. Compared to AT, CreAT produces consistent performance gains on a wider range of tasks and is proven to be more effective for language pre-training where only the encoder part is kept for downstream tasks. We achieve the new state-of-the-art performances on a series of challenging benchmarks, e.g. AdvGLUE (59.1 $ \rightarrow $ 61.1), HellaSWAG (93.0  $ \rightarrow $ 94.9), ANLI (68.1  $ \rightarrow $ 69.3).","['Applications', 'adversarial training', 'Pre-Trained Language Model']",[],"['Hongqiu Wu', 'Yongxiang Liu', 'Hanwen Shi', 'hai zhao', 'Min Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'Harbin Institute of Technology, Shenzhen']",[]
https://iclr.cc/virtual/2023/poster/10773,Security,A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias,"Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to rely upon simple features, as SB has recently been shown to underlie several problems in robust generalization. Using a synthetic dataset, we demonstrate the susceptibility of existing protocols to SB. Given the strong effectiveness of LP+FT, we then propose modified linear probes that help mitigate SB, and lead to better initializations for subsequent FT. We verify the effectiveness of the proposed LP+FT variants for decreasing SB in a controlled setting, and their ability to improve OOD generalization and safety on three adaptation datasets.","['Deep Learning and representational learning', 'robustness', 'transfer learning', 'adaptation', 'data augmentation']",[],"['Puja Trivedi', 'Danai Koutra', 'Jayaraman J. Thiagarajan']","['University of Michigan', 'Amazon', 'Lawrence Livermore National Labs']",[]
https://iclr.cc/virtual/2023/poster/11068,Security,Few-shot Backdoor Attacks via Neural Tangent Kernels,"In a backdoor attack, an attacker injects corrupted examples into the training set. The goal of the attacker is to cause the final trained model to predict the attacker's desired target label when a predefined trigger is added to test inputs. Central to these attacks is the trade-off between the success rate of the attack and the number of corrupted training examples injected. We pose this attack as a novel bilevel optimization problem: construct strong poison examples that maximize the attack success rate of the trained model. We use neural tangent kernels to approximate the training dynamics of the model being attacked and automatically learn strong poison examples. We experiment on subclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt architectures on periodic and patch trigger attacks and show that NTBA-designed poisoned examples achieve, for example, an attack success rate of  90% with ten times smaller number of poison examples injected compared to the baseline. We provided an interpretation of the NTBA-designed attacks using the analysis of kernel linear regression. We further demonstrate a vulnerability in overparametrized deep neural networks, which is revealed by the shape of the neural tangent kernel.","['Deep Learning and representational learning', 'neural tangent kernel', 'kernel regression', 'robust machine learning', 'backdoor attack', 'Data Poisoning']",[],"['Jonathan Hayase', 'Sewoong Oh']","['University of Washington', 'University of Washington']",[]
https://iclr.cc/virtual/2023/poster/11711,Security,ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation,"Adversarial attack aims to generate deceptive inputs to fool a machine learning model. In deep learning, an adversarial input created for a specific neural network can also trick other neural networks. This intriguing property is known as black-box transferability of adversarial examples. To improve black-box transferability, a previously proposed method called Intermediate Level Attack (ILA) fine-tunes an adversarial example by maximizing its perturbation on an intermediate layer of the source model. Meanwhile, it has been shown that simple image transformations can also enhance attack transferability. Based on these two observations, we propose ILA-DA, which employs three novel augmentation techniques to enhance ILA. Specifically, we propose (1) an automated way to apply effective image transformations, (2) an efficient reverse adversarial update technique, and (3) an attack interpolation method to create more transferable adversarial examples. Shown by extensive experiments, ILA-DA greatly outperforms ILA and other state-of-the-art attacks by a large margin. On ImageNet, we attain an average attack success rate of 84.5%, which is 19.5% better than ILA and 4.7% better than the previous state-of-the-art across nine undefended models. For defended models, ILA-DA also leads existing attacks and provides further gains when incorporated into more advanced attack methods.","['Deep Learning and representational learning', 'adversarial examples', 'data augmentation', 'Adversarial Transferability']",[],"['Chiu-Wai Yan', 'Tsz-Him Cheung', 'Dit-Yan Yeung']","['Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology']",[]
https://iclr.cc/virtual/2023/poster/11111,Security,MACTA: A Multi-agent Reinforcement Learning Approach for Cache Timing Attacks and Detection,"Security vulnerabilities in computer systems raise serious concerns as computers process an unprecedented amount of private and sensitive data today. Cache timing attacks (CTA) pose an important practical threat as they can effectively breach many protection mechanisms in today’s systems. However, the current detection techniques for cache timing attacks heavily rely on heuristics and expert knowledge, which can lead to brittleness and the inability to adapt to new attacks. To mitigate the CTA threat, we propose MACTA, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train both attackers and detectors. Following best practices, we develop a realistic simulated MARL environment, MA-AUTOCAT, which enables training and evaluation of cache-timing attackers and detectors. Our empirical results suggest that MACTA is an effective solution without any manual input from security experts. MACTA detectors can generalize to a heuristic attack not exposed in training with a 97.8% detection rate and reduce the attack bandwidth of adaptive attackers by 20% on average. In the meantime, MACTA attackers are qualitatively more effective than other attacks studied, and the average evasion rate of MACTA attackers against an unseen state-of-the-art detector can reach up to 99%. Furthermore, we found that agents equipped with a Transformer encoder can learn effective policies in situations when agents with multi-layer perceptron encoders do not in this environment, suggesting the potential of Transformer structures in CTA problems.","['Applications', 'multi-agent reinforcement learning', 'game theory', 'security']",[],"['Jiaxun Cui', 'Xiaomeng Yang', 'Mulong Luo', 'Geunbae Lee', 'Peter Stone', 'Hsien-Hsin S. Lee', 'Benjamin Lee', 'G. Edward Suh', 'Wenjie Xiong', 'Yuandong Tian']","['The University of Texas at Austin', 'Meta', 'Cornell University', 'Virginia Polytechnic Institute and State University', 'Sony AI', 'Intel', 'University of Pennsylvania, University of Pennsylvania', 'Meta AI', 'Virginia Polytechnic Institute and State University', 'Meta AI (FAIR)']",[]
https://iclr.cc/virtual/2023/poster/11720,Security,Spiking Convolutional Neural Networks for Text Classification,"Spiking neural networks (SNNs) offer a promising pathway to implement deep neural networks (DNNs) in a more energy-efficient manner since their neurons are sparsely activated and inferences are event-driven. However, there have been very few works that have demonstrated the efficacy of SNNs in language tasks partially because it is non-trivial to represent words in the forms of spikes and to deal with variable-length texts by SNNs. This work presents a ""conversion + fine-tuning'' two-step method for training SNN for text classification and proposes a simple but effective way to encode pre-trained word embeddings as spike trains. We show empirically that after further fine-tuning with surrogate gradients, the converted SNNs achieve comparable results to their DNN counterparts across multiple datasets for Both English and Chinese. We also demonstrate that such SNNs are more robust against adversarial attacks than DNNs.","['General Machine Learning', 'spiking neural networks', 'Training Method', 'text classification']",[],"['Changze Lv', 'Jianhan Xu']","['Fudan University', 'Fudan University']",[]
https://iclr.cc/virtual/2023/poster/11087,Security,Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness,"While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We conducted chronic, large-scale multi-electrode recordings  across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model ""IT"" representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models' IT-likeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains.","['Deep Learning and representational learning', 'computer vision', 'Primate Vision', 'Inferior Temporal Cortex', 'Behavioral Alignment', 'adversarial robustness']",[],"['Joel Dapello', 'Kohitij Kar', 'Martin Schrimpf', 'Robert Baldwin Geary', 'Michael Ferguson', 'David Daniel Cox', 'James J. DiCarlo']","['Altos Labs', 'Massachusetts Institute of Technology', 'EPFL - EPF Lausanne', 'Harvard University', 'Massachusetts Institute of Technology', 'International Business Machines', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11370,Security,ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs,"Solving combinatorial optimization (CO) on graphs has been attracting increasing interests from the machine learning community whereby data-driven approaches were recently devised to go beyond traditional manually-designated algorithms. In this paper, we study the robustness of a combinatorial solver as a blackbox regardless it is classic or learning-based though the latter can often be more interesting to the ML community. Specifically, we develop a practically feasible robustness metric for general CO solvers. A no-worse optimal cost guarantee is developed as such the optimal solutions are not required to achieve for solvers, and we tackle the non-differentiable challenge in input instance disturbance by resorting to black-box adversarial attack methods. Extensive experiments are conducted on 14 unique combinations of solvers and CO problems, and we demonstrate that the performance of state-of-the-art solvers like Gurobi can degenerate by over 20% under the given time limit bound on the hard instances discovered by our robustness metric, raising concerns about the robustness of combinatorial optimization solvers.","['Optimization', 'reinforcement learning', 'robustness', 'graph neural networks', 'combinatorial optimization']",[],"['Han Lu', 'Zenan Li', 'Runzhong Wang', 'Qibing Ren', 'Xijun Li', 'Mingxuan Yuan', 'Jia Zeng', 'Xiaokang Yang', 'Junchi Yan']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Massachusetts Institute of Technology', 'Shanghai Jiaotong University', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', ""Huawei Noah's Ark Lab"", 'Shanghai Jiao Tong University, China', 'Shanghai Jiao Tong University']",[]
https://iclr.cc/virtual/2023/poster/11767,Security,Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation,"Adversarial patch attacks are an emerging security threat for real world deep learning applications. We present Demasked Smoothing, the first approach (up to our knowledge) to certify the robustness of semantic segmentation models against this threat model. Previous work on certifiably defending against patch attacks has mostly focused on image classification task and often required changes in the model architecture and additional training which is undesirable and computationally expensive. In Demasked Smoothing, any segmentation model can be applied without particular training, fine-tuning, or restriction of the architecture. Using different masking strategies, Demasked Smoothing can be applied both for certified detection and certified recovery. In extensive experiments we show that Demasked Smoothing can on average certify 63% of the pixel predictions for a 1% patch in the detection task and 46% against a 0.5% patch for the recovery task on the ADE20K dataset.","['Social Aspects of Machine Learning', 'adversarial patch attacks', 'certified defences', 'adversarial robustness']",[],"['Maksym Yatsura', 'Kaspar Sakmann', 'N. Grace Hua', 'Matthias Hein', 'Jan Hendrik Metzen']","['Robert Bosch GmbH, Bosch', 'Robert Bosch GmbH, Bosch', 'Robert Bosch GmbH, Bosch', 'University of Tübingen', 'Bosch Center Artificial Intelligence']",[]
https://iclr.cc/virtual/2023/poster/11886,Security,Revisiting Graph Adversarial Attack and Defense From a Data Distribution Perspective,"Recent studies have shown that structural perturbations are significantly effective in degrading the accuracy of Graph Neural Networks (GNNs) in the semi-supervised node classification (SSNC) task. However, why the gradient-based methods are so destructive is rarely explored. In this work, we discover an interesting phenomenon: the adversarial edges are not uniformly distributed on the graph. Nearly all perturbations are generated around the training nodes in poisoning attack. Combined with this phenomenon, we provide an explanation for the effectiveness of the gradient-based attack method from a data distribution perspective and revisit both poisoning attack and evasion attack in SSNC. From this new perspective, we empirically and theoretically discuss some other attack tendencies. Based on the analysis, we provide nine practical tips on both attack and defense and meanwhile leverage them to improve existing attack and defense methods. Moreover, we design a fast attack method and a self-training defense method, which outperform the state-of-the-art methods and can effectively scale to large graphs like ogbn-arxiv. We conduct extensive experiments on four benchmark datasets to verify our claims.","['Social Aspects of Machine Learning', 'Data Distribution', 'robustness', 'Graph Adversarial Attack']",[],"['Kuan Li', 'Yang Liu', 'Xiang Ao', 'Qing He']","['Hong Kong University of Science and Technology', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, CAS']",[]
https://iclr.cc/virtual/2023/poster/11836,Security,SYNC: SAFETY-AWARE NEURAL CONTROL FOR STABILIZING STOCHASTIC DELAY-DIFFERENTIAL EQUATIONS,"Stabilization of the systems described by \textit{stochastic delay}-differential equations (SDDEs) under preset conditions is a challenging task in the control community. Here, to achieve this task, we leverage neural networks to learn control policies using the information of the controlled systems in some prescribed regions.  Specifically, two learned control policies, i.e., the neural deterministic controller (NDC) and the neural stochastic controller (NSC), work effectively in the learning procedures that rely on, respectively, the well-known LaSalle-type theorem and the newly-established theorem for guaranteeing the stochastic stability in SDDEs. We theoretically investigate the performance of the proposed controllers in terms of convergence time and energy cost.  More practically and significantly, we improve our learned control policies through considering the situation where the controlled trajectories only evolve in some specific safety set. {\color{black}  The practical validity of such control policies restricted in safety set is attributed to the theory that we further develop for safety and stability guarantees in SDDEs using the stochastic control barrier function and the spatial discretization}. We call this control as SYNC (\textbf{S}afet\textbf{Y}-aware \textbf{N}eural \textbf{C}ontrol).   The efficacy of all the articulated control policies, including the SYNC, is demonstrated systematically by using representative control problems.","['Theory', 'Stochastic delay-differential equations', 'safety guarantee', 'stochastic stabilization', 'neural networks']",[],"['Jingdong Zhang', 'Qunxi Zhu', 'Wei Yang', 'Wei Lin']","['Fudan University', 'Fudan University', 'Fudan University', 'Fudan University']",[]
https://iclr.cc/virtual/2023/poster/11957,Security,Robust Fair Clustering: A Novel Fairness Attack and Defense Framework,"Clustering algorithms are widely used in many societal resource allocation applications, such as loan approvals and candidate recruitment, among others, and hence, biased or unfair model outputs can adversely impact individuals that rely on these applications. To this end, many $\textit{fair}$ clustering approaches have been recently proposed to counteract this issue. Due to the potential for significant harm, it is essential to ensure that fair clustering algorithms provide consistently fair outputs even under adversarial influence. However, fair clustering algorithms have not been studied from an adversarial attack perspective. In contrast to previous research, we seek to bridge this gap and conduct a robustness analysis against fair clustering by proposing a novel $\textit{black-box fairness attack}$. Through comprehensive experiments, we find that state-of-the-art models are highly susceptible to our attack as it can reduce their fairness performance significantly. Finally, we propose Consensus Fair Clustering (CFC), the first $\textit{robust fair clustering}$ approach that transforms consensus clustering into a fair graph partitioning problem, and iteratively learns to generate fair cluster outputs. Experimentally, we observe that CFC is highly robust to the proposed attack and is thus a truly robust fair clustering alternative.","['Unsupervised and Self-supervised learning', 'consensus clustering', 'Fairness Attack', 'Data Clustering', 'Fairness Defense']",[],"['Anshuman Chhabra', 'Peizhao Li', 'Prasant Mohapatra', 'Hongfu Liu']","['University of California, Davis', 'Brandeis University', 'University of South Florida', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/11925,Security,On the Robustness of Safe Reinforcement Learning under Observational Perturbations,"Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting.  We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward.  One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \url{https://github.com/liuzuxin/safe-rl-robustness}","['Reinforcement Learning', 'deep reinforcement learning', 'state robust reinforcement learning', 'safe reinforcement learning']",[],"['Zuxin Liu', 'Zijian Guo', 'Zhepeng Cen', 'Huan Zhang', 'Jie Tan', 'Bo Li', 'Ding Zhao']","['Carnegie Mellon University', 'Boston University, Boston University', 'CMU, Carnegie Mellon University', 'University of Illinois at Urbana-Champaign', 'Google', 'University of Illinois, Urbana Champaign', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2023/poster/11520,Security,A Unified Algebraic Perspective on Lipschitz Neural Networks,"Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical solutions of a common semidefinite programming (SDP) condition.  We also prove that AOL biases the scaled weight to the ones which are close to the set of orthogonal matrices in a certain mathematical manner. Moreover, our algebraic condition, combined with the Gershgorin circle theorem, readily leads to new and diverse parameterizations for 1-Lipschitz network layers. Our approach, called SDP-based Lipschitz Layers (SLL), allows us to design non-trivial yet efficient generalization of convex potential layers.  Finally, the comprehensive set of experiments on image classification shows that SLLs outperform previous approaches on certified robust accuracy. Code is available at https://github.com/araujoalexandre/Lipschitz-SLL-Networks.","['Deep Learning and representational learning', 'Lipschitz neural networks', 'deep learning', 'robustness']",[],"['Alexandre Araujo', 'Aaron J Havens', 'Blaise Delattre', 'Alexandre Allauzen', 'Bin Hu']","['New York University', 'University of Illinois, Urbana Champaign', ', Université Paris-Dauphine (Paris IX)', 'Ecole supérieure de physique et chimie', 'University of Illinois, Urbana Champaign']",[]
https://iclr.cc/virtual/2023/poster/10789,Security,TextShield: Beyond Successfully Detecting Adversarial Sentences in text classification,"Adversarial attack serves as a major challenge for neural network models in NLP, which precludes the model's deployment in safety-critical applications. A recent line of work, detection-based defense, aims to distinguish adversarial sentences from benign ones. However, {the core limitation of previous detection methods is being incapable of giving correct predictions on adversarial sentences unlike defense methods from other paradigms.} To solve this issue, this paper proposes TextShield: (1) we discover a link between text attack and saliency information, and then we propose a saliency-based detector, which can effectively detect whether an input sentence is adversarial or not. (2) We design a saliency-based corrector, which converts the detected adversary sentences to benign ones. By combining the saliency-based detector and corrector, TextShield extends the detection-only paradigm to a detection-correction paradigm, thus filling the gap in the existing detection-based defense. Comprehensive experiments show that (a) TextShield consistently achieves higher or comparable performance than state-of-the-art defense methods across various attacks on different benchmarks. (b) our saliency-based detector outperforms existing detectors for detecting adversarial sentences.","['Applications', 'adversarial defense', 'adversarial attack', 'natural language processing', 'text classification']",[],"['Lingfeng Shen', 'Ze Zhang', 'Haiyun Jiang', 'Ying Chen']","['Johns Hopkins University', 'Fudan University', 'Tencent AI Lab', 'China Agricultural University']",[]
https://iclr.cc/virtual/2023/poster/11971,Security,Combating Exacerbated Heterogeneity for Robust Models in Federated Learning,"Privacy and security concerns in real-world applications have led to the development of adversarially robust federated models. However, the straightforward combination between adversarial training and federated learning in one framework can lead to the undesired robustness deterioration. We discover that the attribution behind this phenomenon is that the generated adversarial data could exacerbate the data heterogeneity among local clients, making the wrapped federated learning perform poorly. To deal with this problem, we propose a novel framework called Slack Federated Adversarial Training (SFAT), assigning the client-wise slack during aggregation to combat the intensified heterogeneity. Theoretically, we analyze the convergence of the proposed method to properly relax the objective when combining federated learning and adversarial training. Experimentally, we verify the rationality and effectiveness of SFAT on various benchmarked and real-world datasets with different adversarial training and federated optimization methods. The code is publicly available at: https://github.com/ZFancy/SFAT.",['Deep Learning and representational learning'],[],"['Jianing Zhu', 'Jiangchao Yao', 'Tongliang Liu', 'Quanming Yao', 'Jianliang Xu', 'Bo Han']","['Hong Kong Baptist University', 'Shanghai Jiaotong University', 'University of Sydney', 'Department of Electronic Engineering, Tsinghua University', 'Hong Kong Baptist University', 'HKBU']",[]
https://iclr.cc/virtual/2023/poster/11621,Security,Language Modelling with Pixels,"Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.","['Applications', 'masked autoencoder', 'representation learning', 'transformers', 'nlp', 'language model']",[],"['Phillip Rust', 'Jonas F. Lotz', 'Emanuele Bugliarello', 'Elizabeth Salesky', 'Miryam de Lhoneux', 'Desmond Elliott']","['University of Copenhagen', 'University of Copenhagen', 'Google', 'Johns Hopkins University', 'KU Leuven', '']",[]
https://iclr.cc/virtual/2023/poster/11601,Security,Neural Architecture Design and Robustness: A Dataset,"Deep learning models have proven to be successful in a wide range of machine learning tasks. Yet, they are often highly sensitive to perturbations on the input data which can lead to incorrect decisions with high confidence, hampering their deployment for practical use-cases. Thus, finding architectures that are (more) robust against perturbations has received much attention in recent years. Just like the search for well-performing architectures in terms of clean accuracy, this usually involves a tedious trial-and-error process with one additional challenge: the evaluation of a network's robustness is significantly more expensive than its evaluation for clean accuracy. Thus, the aim of this paper is to facilitate better streamlined research on architectural design choices with respect to their impact on robustness as well as, for example, the evaluation of surrogate measures for robustness. We therefore borrow one of the most commonly considered search spaces for neural architecture search for image classification, NAS-Bench-201, which contains a manageable size of 6466 non-isomorphic network designs. We evaluate all these networks on a range of common adversarial attacks and corruption types and introduce a database on neural architecture design and robustness evaluations. We further present three exemplary use cases of this dataset, in which we (i) benchmark robustness measurements based on Jacobian and Hessian matrices for their robustness predictability, (ii) perform neural architecture search on robust accuracies, and (iii) provide an initial analysis of how architectural design choices affect robustness. We find that carefully crafting the topology of a network can have substantial impact on its robustness, where networks with the same parameter count range in mean adversarial robust accuracy from 20%-41%. Code and data is available at http://robustness.vision/.","['Infrastructure', 'architecture design', 'robustness', 'dataset']",[],"['Steffen Jung', 'Jovita Lukasik', 'Margret Keuper']","['Saarland Informatics Campus, Max-Planck Institute', 'Universität Siegen', 'Universität Siegen']",[]
https://iclr.cc/virtual/2023/poster/11472,Security,Measuring Forgetting of Memorized Training Examples,"Machine learning models exhibit two seemingly contradictory phenomena: training data memorization and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena.We propose a technique to measure to what extent models ``forget'' the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently.We show that, while non-convexity can prevent forgetting from happening in the worst-case, standard image,speech, and language models empirically do forget examples over time.We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget.Our results suggest that examples seen early when training with extremely large datasets---for instance those examples used to pre-train a model---may observe privacy benefits at the expense of examples seen later.","['Social Aspects of Machine Learning', 'Memorization', 'canary extraction', 'convexity', 'membership inference', 'nondeterminism', 'forgetting']",[],"['Matthew Jagielski', 'Om Thakkar', 'Florian Tramèr', 'Daphne Ippolito', 'Katherine Lee', 'Nicholas Carlini', 'Eric Wallace', 'Shuang Song', 'Abhradeep Guha Thakurta', 'Nicolas Papernot', 'Chiyuan Zhang']","['Google', 'Google', 'ETHZ - ETH Zurich', 'School of Engineering and Applied Science, University of Pennsylvania', 'Cornell University', 'Google', 'University of California Berkeley', 'Google', 'Google', 'University of Toronto', 'Google']",[]
https://iclr.cc/virtual/2023/poster/10726,Security,LiftedCL: Lifting Contrastive Learning for Human-Centric Perception,"Human-centric perception targets for understanding human body pose, shape and segmentation. Pre-training the model on large-scale datasets and fine-tuning it on specific tasks has become a well-established paradigm in human-centric perception. Recently, self-supervised learning methods have re-investigated contrastive learning to achieve superior performance on various downstream tasks. When handling human-centric perception, there still remains untapped potential since 3D human structure information is neglected during the task-agnostic pre-training. In this paper, we propose the Lifting Contrastive Learning (LiftedCL) to obtain 3D-aware human-centric representations which absorb 3D human structure information. In particular, to induce the learning process, a set of 3D skeletons is randomly sampled by resorting to 3D human kinematic prior. With this set of generic 3D samples, 3D human structure information can be learned into 3D-aware representations through adversarial learning. Empirical results demonstrate that LiftedCL outperforms state-of-the-art self-supervised methods on four human-centric downstream tasks, including 2D and 3D human pose estimation (0.4% mAP and 1.8 mm MPJPE improvement on COCO 2D pose estimation and Human3.6M 3D pose estimation), human shape recovery and human parsing.","['Unsupervised and Self-supervised learning', 'contrastive learning', 'human-centric perception']",[],"['Ziwei Chen', 'Qiang Li', 'Xiaofeng Wang', 'Wankou Yang']","['Southeast University', 'SenseTime Research', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Southeast University']",[]
https://iclr.cc/virtual/2023/poster/10796,Security,"Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization","Graph Neural Networks (GNNs) have achieved state-of-the-art results on a variety of graph learning tasks, however, it has been demonstrated that they are vulnerable to adversarial attacks, raising serious security concerns. A lot of studies have been developed to train GNNs in a noisy environment and increase their robustness against adversarial attacks. However, existing methods have not uncovered a principled difficulty: the convoluted mixture distribution between clean and attacked data samples, which leads to sub-optimal model design and limits their frameworks’ robustness. In this work, we first begin by identifying the root cause of mixture distribution, then, for tackling it, we propose a novel method GAME - Graph Adversarial Mixture of Experts to enlarge the model capacity and enrich the representation diversity of adversarial samples, from three perspectives of model, training, and optimization. Specifically, we first propose a plug-and- play GAME layer that can be easily incorporated into any GNNs and enhance their adversarial learning capabilities. Second, we design a decoupling-based graph adversarial training in which the component of the model used to generate adversarial graphs is separated from the component used to update weights. Third, we introduce a graph diversity regularization that enables the model to learn diverse representation and further improves model performance. Extensive experiments demonstrate the effectiveness and advantages of GAME over the state-of-the-art adversarial training methods across various datasets given different attacks.","['Deep Learning and representational learning', 'mixture of experts', 'Graph adversarial learning', 'graph neural networks']",[],"['Chunhui Zhang', 'Yijun Tian', 'Mingxuan Ju', 'Zheyuan Liu', 'Yanfang Ye', 'Nitesh V Chawla', 'Chuxu Zhang']","['Dartmouth College', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/11708,Security,Adversarial Attacks on Adversarial Bandits,"We study a security threat to adversarial multi-armed bandit, in which an attacker perturbs the loss or reward signal to control the behavior of the victim bandit player. We show that the attacker is able to mislead any no-regret adversarial bandit algorithm into selecting a suboptimal target action in every but sublinear (T−o(T )) number of rounds, while incurring only sublinear (o(T)) cumulative attack cost. This result implies critical security concern in real-world bandit-based systems, e.g., in online recommendation, an attacker might be able to hijack the recommender system and promote a desired product. Our proposed attack algorithms require knowledge of only the regret rate, thus are agnostic to the concrete bandit algorithm employed by the victim player. We also derived a theoretical lower bound on the cumulative attack cost that any victim-agnostic attack algorithm must incur. The lower bound matches the upper bound achieved by our attack, which shows that our attack is asymptotically optimal.","['Social Aspects of Machine Learning', 'sublinear cumulative attack cost', 'target action', 'adversarial bandits', 'adversarial attacks']",[],"['Yuzhe Ma', 'Zhijin Zhou']","['Microsoft', 'University of Washington']",[]
https://iclr.cc/virtual/2023/poster/11540,Security,Hard-Meta-Dataset++: Towards Understanding Few-Shot Performance on Difficult Tasks,"Few-shot classification is the ability to adapt to any new classification task from only a few training examples. The performance of current top-performing few-shot classifiers varies widely across different tasks where they often fail on a subset of `difficult' tasks.This phenomenon has real-world consequences for deployed few-shot systems where safety and reliability are paramount, yet little has been done to understand these failure cases. In this paper, we study these difficult tasks to gain a more nuanced understanding of the limitations of current methods. To this end, we develop a general and computationally efficient algorithm called FastDiffSel to extract difficult tasks from any large-scale vision dataset. Notably, our algorithm can extract tasks at least 20x faster than existing methods enabling its use on large-scale datasets. We use FastDiffSel to extract difficult tasks from Meta-Datasset, a widely-used few-shot classification benchmark, and other challenging large-scale vision datasets including ORBIT, CURE-OR and ObjectNet. These tasks are curated into Hard-MD++, a new few-shot testing benchmark to promote the development of methods that are robust to even the most difficult tasks. We use Hard-MD++ to stress-test an extensive suite of few-shot classification methods and show that state-of-the-art approaches fail catastrophically on difficult tasks. We believe that our extraction algorithm FastDiffSel and Hard-MD++ will aid researchers in further understanding failure modes of few-shot classification models.","['Deep Learning and representational learning', 'few-shot learning', 'benchmarks', 'Meta-Dataset', 'evaluation']",[],"['Samyadeep Basu', 'Megan Stanley', 'John F Bronskill', 'Soheil Feizi', 'Daniela Massiceti']","['University of Maryland, College Park', 'Microsoft Research Cambridge', 'University of Cambridge', 'University of Maryland, College Park', 'Research, Microsoft']",[]
https://iclr.cc/virtual/2023/poster/11550,Security,Prompting GPT-3 To Be Reliable,"Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3’s reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM’s factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.","['Social Aspects of Machine Learning', 'prompting', 'GPT-3', 'biases', 'reliability', 'calibration', 'large language models', 'knowledge updating', 'robustness']",[],"['Chenglei Si', 'Zhe Gan', 'Zhengyuan Yang', 'Shuohang Wang', 'Jianfeng Wang', 'Jordan Lee Boyd-Graber', 'Lijuan Wang']","['Stanford University', 'Apple', 'Microsoft', 'Microsoft', 'Microsoft', 'University of Maryland, College Park', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/10735,Security,Guarded Policy Optimization with Imperfect Online Demonstrations,"The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.","['Reinforcement Learning', 'reinforcement learning', 'imperfect demonstrations', 'metadrive simulator', 'guarded policy optimization', 'shared control']",[],"['Zhenghai Xue', 'Zhenghao Mark Peng', 'Zhihan Liu', 'Bolei Zhou']","['Nanyang Technological University', 'University of California, Los Angeles', 'Northwestern University', 'University of California, Los Angeles']",[]
https://iclr.cc/virtual/2023/poster/12180,Security,Extracting Robust Models with Uncertain Examples,"Model extraction attacks are proven to be a severe privacy threat to Machine Learning as a Service (MLaaS). A variety of techniques have been designed to steal a remote machine learning model with high accuracy and fidelity. However, how to extract a robust model with similar resilience against adversarial attacks is never investigated. This paper presents the first study toward this goal. We first analyze those existing extraction solutions either fail to maintain the model accuracy or model robustness or lead to the robust overfitting issue. Then we propose Boundary Entropy Searching Thief (BEST), a novel model extraction attack to achieve both accuracy and robustness extraction under restricted attack budgets. BEST generates a new kind of uncertain examples for querying and reconstructing the victim model. These samples have uniform confidence scores across different classes, which can perfectly balance the trade-off between model accuracy and robustness. Extensive experiments demonstrate that BEST outperforms existing attack methods over different datasets and model architectures under limited data. It can also effectively invalidate state-of-the-art extraction defenses.",['Deep Learning and representational learning'],[],"['Guanlin Li', 'Guowen Xu', 'Shangwei Guo', 'Han Qiu', 'Jiwei Li', 'Tianwei Zhang']","['Nanyang Technological University', 'City University of Hong Kong', 'Chongqing University', 'Tsinghua University', 'Stanford University', 'Nanyang Technological University']",[]
https://iclr.cc/virtual/2023/poster/11656,Security,Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only,"Multi-label models have been widely used in various applications including image annotation and object detection. The fly in the ointment is its inherent vulnerability to backdoor attacks due to the adoption of deep learning techniques. However, all existing backdoor attacks exclusively require to modify training inputs (e.g., images), which may be impractical in real-world applications. In this paper, we aim to break this wall and propose the first clean-image backdoor attack, which only poisons the training labels without touching the training samples. Our key insight is that in a multi-label learning task, the adversary can just manipulate the annotations of training samples consisting of a specific set of classes to activate the backdoor. We design a novel trigger exploration method to find convert and effective triggers to enhance the attack performance. We also propose three target label selection strategies to achieve different goals. Experimental results indicate that our clean-image backdoor can achieve a 98% attack success rate while preserving the model's functionality on the benign inputs. Besides, the proposed clean-image backdoor can evade existing state-of-the-art defenses.",['Applications'],[],"['Kangjie Chen', 'Xiaoxuan Lou', 'Guowen Xu', 'Jiwei Li', 'Tianwei Zhang']","['Nanyang Technological University', 'Nanyang Technological University', 'City University of Hong Kong', 'Stanford University', 'Nanyang Technological University']",[]
https://iclr.cc/virtual/2023/poster/11807,Security,Fooling SHAP with Stealthily Biased Sampling,"SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups while remaining undetected. More precisely, experiments performed on real-world datasets showed that our attack could yield up to a 90\% relative decrease in amplitude of the sensitive feature attribution. These results highlight the manipulability of SHAP explanations and encourage auditors to treat them with skepticism.","['Social Aspects of Machine Learning', 'SHAP', 'Stealthily Sampling', 'explainability', 'robustness']",[],"['Gabriel Laberge', 'Ulrich Aïvodji', 'Satoshi Hara', 'Mario Marchand', 'Foutse Khomh']","['École Polytechnique de Montréal', 'École de technologie supérieure, Université du Québec', 'Osaka University', 'Laval university', 'École Polytechnique de Montréal, Université de Montréal']",[]
https://iclr.cc/virtual/2023/poster/11191,Security,Fundamental Limits in Formal Verification of Message-Passing Neural Networks,"Output reachability and adversarial robustness are among the most relevant safety properties of neural networks. We show that in the context of Message Passing Neural Networks (MPNN), a common Graph Neural Network (GNN) model, formal verification is impossible. In particular, we show that output reachability of graph-classifier MPNN, working over graphs of unbounded size, non-trivial degree and sufficiently expressive node labels, cannot be verified formally: thereis no algorithm that answers correctly (with yes or no), given an MPNN, whether there exists some valid input to the MPNN such that the corresponding output satisfies a given specification. However, we also show that output reachability and adversarial robustness of node-classifier MPNN can be verified formally when a limit onthe degree of input graphs is given a priori. We discuss the implications of these results, for the purpose ofobtaining a complete picture of the principle possibility to formally verify GNN, depending on the expressiveness of the involved GNN models and input-output specifications.","['Social Aspects of Machine Learning', 'formal verification', 'graph neural networks']",[],"['Marco Sälzer', 'Martin Lange']","['Universität Kassel', 'Universität Kassel']",[]
https://iclr.cc/virtual/2023/poster/11640,Security,PowerQuant: Automorphism Search for Non-Uniform Quantization,"Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as computer vision. However, due to their high latency, the deployment of DNNs hinges on the development of compression techniques such as quantization which consists in lowering the number of bits used to encode the weights and activations. Growing concerns for privacy and security have motivated the development of data-free techniques, at the expanse of accuracy. In this paper, we identity the uniformity of the quantization operator as a limitation of existing approaches, and propose a data-free non-uniform method. More specifically, we argue that to be readily usable without dedicated hardware and implementation, non-uniform quantization shall not change the nature of the mathematical operations performed by the DNN. This leads to search among the continuous automorphisms of $(\mathbb{R}_+^*,\times)$, which boils down to the power functions defined by their exponent. To find this parameter, we propose to optimize the reconstruction error of each layer: in particular, we show that this procedure is locally convex and admits a unique solution. At inference time, we show that our approach, dubbed PowerQuant, only require simple modifications in the quantized DNN activation functions. As such, with only negligible overhead, it significantly outperforms existing methods in a variety of configurations.","['Deep Learning and representational learning', 'deep learning', 'data-free', 'compression', 'quantization', 'acceleration']",[],"['Edouard YVINEC', 'Arnaud Dapogny', 'Matthieu Cord', 'Kevin Bailly']","['Computer Science Lab  - Pierre and Marie Curie University, Paris, France', 'LIP6', 'Sorbonne Université', 'Datakalab']",[]
https://iclr.cc/virtual/2023/poster/10706,Security,Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples,"The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https://github.com/qizhangli/MoreBayesian-attack.","['Deep Learning and representational learning', 'adversarial examples', 'black-box attacks', 'Adversarial Transferability']",[],"['Qizhang Li', 'Yiwen Guo', 'Wangmeng Zuo', 'Hao Chen']","['Harbin Institute of Technology', 'ByteDance', 'Harbin Institute of Technology', 'University of California, Davis']",[]
https://iclr.cc/virtual/2023/poster/11173,Security,Why adversarial training can hurt robust accuracy,"Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite can be true for a natural class of perceptible perturbations --- even though adversarial training helps when enough data is  available, it may in fact hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Using intuitive insights from the proof, we could surprisingly find perturbations on standard image datasets for which this behavior persists. Specifically, it occurs for perceptible attacks that effectively reduce class information such as object occlusions or corruptions.","['Theory', 'adversarial training', 'Robust generalisation', 'learning theory']",[],"['Jacob Clarysse', 'Julia Hörrmann', 'Fanny Yang']","['Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/10906,Security,Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation,"As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase ""my credit card number is ..."". In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.","['Social Aspects of Machine Learning', 'Privacy attack', 'privacy', 'security', 'federated learning']",[],"['Hong-Min Chu', 'Jonas Geiping', 'Liam H Fowl', 'Micah Goldblum', 'Tom Goldstein']","['Department of Computer Science, University of Maryland, College Park', 'ELLIS Institute Tübingen', 'Google', 'New York University', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2023/poster/12092,Security,Towards Addressing Label Skews in One-Shot Federated Learning,"Federated learning (FL) has been a popular research area, where multiple clients collaboratively train a model without sharing their local raw data. Among existing FL solutions, one-shot FL is a promising and challenging direction, where the clients conduct FL training with a single communication round. However, while label skew is a common real-world scenario where some clients may have few or no data of some classes, existing one-shot FL approaches that conduct voting on the local models are not able to produce effective global models. Due to the limited number of classes in each party, the local models misclassify the data from unseen classes into seen classes, which leads to very ineffective global models from voting. To address the label skew issue in one-shot FL, we propose a novel approach named FedOV which generates diverse outliers and introduces them as an additional unknown class in local training to improve the voting performance. Specifically, based on open-set recognition, we propose novel outlier generation approaches by corrupting the original features and further develop adversarial learning to enhance the outliers. Our extensive experiments show that FedOV can significantly improve the test accuracy compared to state-of-the-art approaches in various label skew settings.","['Deep Learning and representational learning', 'federated learning']",[],"['Yiqun Diao', 'Qinbin Li', 'Bingsheng He']","['National University of Singapore', 'University of California, Berkeley', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/10755,Security,Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation,"We address the problem of safe reinforcement learning from pixel observations. Inherent challenges in such settings are (1) a trade-off between reward optimization and adhering to safety constraints, (2) partial observability, and (3) high-dimensional observations. We formalize the problem in a constrained, partially observable Markov decision process framework, where an agent obtains distinct reward and safety signals. To address the curse of dimensionality, we employ a novel safety critic using the stochastic latent actor-critic (SLAC) approach. The latent variable model predicts rewards and safety violations, and we use the safety critic to train safe policies. Using well-known benchmark environments, we demonstrate competitive performance over existing approaches regarding computational requirements, final reward return, and satisfying the safety constraints.","['Reinforcement Learning', 'reinforcement learning', 'POMDP', 'MDP', 'safe reinforcement learning', 'partially observable Markov decision process', 'constrained markov decision process', 'safety']",[],"['Yannick Hogewind', 'Nils Jansen']","['Radboud University', 'Radboud University Nijmegen']",[]
https://iclr.cc/virtual/2023/poster/10747,Security,Efficient Certified Training and Robustness Verification of Neural ODEs,"Neural Ordinary Differential Equations (NODEs) are a novel neural architecture, built around initial value problems with learned dynamics which are solved during inference. Thought to be inherently more robust against adversarial perturbations, they were recently shown to be vulnerable to strong adversarial attacks, highlighting the need for formal guarantees.  However, despite significant progress in robustness verification for standard feed-forward architectures, the verification of high dimensional NODEs remains an open problem. In this work we address this challenge and propose GAINS, an analysis framework for NODEs combining three key ideas: (i) a novel class of ODE solvers, based on variable but discrete time steps, (ii) an efficient graph representation of solver trajectories, and (iii) a novel abstraction algorithm operating on this graph representation. Together, these advances enable the efficient analysis and certified training of high-dimensional NODEs, by reducing the runtime from an intractable $\mathcal{O}(\exp(d)+\exp(T))$ to $\mathcal{O}(d+T^2\log^2T)$ in the dimensionality $d$ and integration time $T$.  In an extensive evaluation on computer vision (MNIST and Fashion-MNIST) and time-series forecasting (Physio-Net) problems, we demonstrate the effectiveness of both our certified training and verification methods.","['Social Aspects of Machine Learning', 'certified robustness', 'Neural ODEs', 'Certified Training', 'Robustness Verification', 'adversarial robustness']",[],"['Mustafa Zeqiri', 'Mark Niklas Mueller', 'Marc Fischer', 'Martin Vechev']","['ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11731,Security,SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency,"Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries embed a hidden backdoor trigger during the training process for malicious prediction manipulation. These attacks pose great threats to the applications of DNNs under the real-world machine learning as a service (MLaaS) setting, where the deployed model is fully black-box while the users can only query and obtain its predictions. Currently, there are many existing defenses to reduce backdoor threats. However, almost all of them cannot be adopted in MLaaS scenarios since they require getting access to or even modifying the suspicious models. In this paper, we propose a simple yet effective black-box input-level backdoor detection, called SCALE-UP, which requires only the predicted labels to alleviate this problem. Specifically, we identify and filter malicious testing samples by analyzing their prediction consistency during the pixel-wise amplification process. Our defense is motivated by an intriguing observation (dubbed \emph{scaled prediction consistency}) that the predictions of poisoned samples are significantly more consistent compared to those of benign ones when amplifying all pixel values. Besides, we also provide theoretical foundations to explain this phenomenon. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our defense and its resistance to potential adaptive attacks. Our codes are available at \url{https://github.com/JunfengGo/SCALE-UP}.","['Deep Learning and representational learning', 'deep learning', 'AI security', 'Backdoor Defense', 'Backdoor Detection', 'Backdoor Learning']",[],"['Junfeng Guo', 'Yiming Li', 'Hanqing Guo', 'Lichao Sun', 'Cong Liu']","['University of Maryland Institute for Advanced Computer Studies, University of Maryland, College Park', 'Zhejiang University', 'Michigan State University', 'Lehigh University', 'University of California, Riverside']",[]
https://iclr.cc/virtual/2023/poster/11481,Security,Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation,"Vessel segmentation in medical images is one of the important tasks in the diagnosis of vascular diseases and therapy planning. Although learning-based segmentation approaches have been extensively studied, a large amount of ground-truth labels are required in supervised methods and confusing background structures make neural networks hard to segment vessels in an unsupervised manner. To address this, here we introduce a novel diffusion adversarial representation learning (DARL) model that leverages a denoising diffusion probabilistic model with adversarial learning, and apply it to vessel segmentation. In particular, for self-supervised vessel segmentation, DARL learns the background signal using a diffusion module, which lets a generation module effectively provide vessel representations. Also, by adversarial learning based on the proposed switchable spatially-adaptive denormalization, our model estimates synthetic fake vessel images as well as vessel segmentation masks, which further makes the model capture vessel-relevant semantic information. Once the proposed model is trained, the model generates segmentation masks in a single step and can be applied to general vascular structure segmentation of coronary angiography and retinal images. Experimental results on various datasets show that our method significantly outperforms existing unsupervised and self-supervised vessel segmentation methods.","['Unsupervised and Self-supervised learning', 'self-supervised learning', 'adversarial learning', 'diffusion model', 'Vessel segmentation']",[],"['Boah Kim', 'Yujin Oh', 'Jong Chul Ye']","['National Institutes of Health', 'KAIST', 'Korea Advanced Institute of Science and Technology']",[]
https://iclr.cc/virtual/2023/poster/10788,Security,Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms,"This work studies the threats of adversarial attack on multivariate probabilistic forecasting models and viable defense mechanisms. Our studies discover a new attack pattern that negatively impact the forecasting of a target time series via making strategic, sparse (imperceptible) modifications to the past observations of a small number of other time series. To mitigate the impact of such attack, we have developed two defense strategies. First, we extend a previously developed randomized smoothing technique in classification to multivariate forecasting scenarios. Second, we develop an adversarial training algorithm that learns to create adversarial examples and at the same time optimizes the forecasting model to improve its robustness against such adversarial simulation. Extensive experiments on real-world datasets confirm that our attack schemes are powerful and our defense algorithms are more effective compared with baseline defense mechanisms.","['Probabilistic Methods', 'Multivariate Timeseries Forecasting']",[],"['Linbo Liu', 'Youngsuk Park', 'Trong Nghia Hoang', 'Hilaf Hasson', 'Luke Huan']","['AWS AI Labs ', 'Amazon, AWS AI Labs', 'Washington State University', 'Amazon', 'Department of Computer Science, University of Massachusetts at Amherst']",[]
https://iclr.cc/virtual/2023/poster/10762,Security,FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning,"Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on nine competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https://github.com/KaiyuanZh/FLIP.","['General Machine Learning', 'backdoor mitigation', 'federated learning']",[],"['Kaiyuan Zhang', 'Guanhong Tao', 'Qiuling Xu', 'Siyuan Cheng', 'Shengwei An', 'Yingqi Liu', 'Shiwei Feng', 'Guangyu Shen', 'Pin-Yu Chen', 'Xiangyu Zhang']","['Purdue University', 'Purdue University', 'Netflix', 'Purdue University', 'Purdue University', 'Microsoft', 'Purdue University', 'Purdue University', 'International Business Machines', ', Purdue University']",[]
https://iclr.cc/virtual/2023/poster/11830,Security,An Exact Poly-Time Membership-Queries Algorithm for Extracting a Three-Layer ReLU Network,"We consider the natural problem of learning a ReLU network from queries, which was recently remotivated by model extraction attacks. In this work, we present a polynomial-time algorithm that can learn a depth-two ReLU network from queries under mild general position assumptions. We also present a polynomial-time algorithm that, under mild general position assumptions, can learn a rich class of depth-three ReLU networks from queries. For instance, it can learn most networks where the number of first layer neurons is smaller than the dimension and the number of second layer neurons.These two results substantially improve state-of-the-art: Until our work, polynomial-time algorithms were only shown to learn from queries depth-two networks under the assumption that either the underlying distribution is Gaussian (Chen et al. (2021)) or that the weights matrix rows are linearly independent (Milli et al. (2019)). For depth three or more, there were no known poly-time results.","['Social Aspects of Machine Learning', 'Learning With Queries', 'ReLU networks', 'model extraction']",[],"['Amit Daniely', 'Elad Granot']","['Hebrew University of Jerusalem', 'Hebrew University of Jerusalem']",[]
https://iclr.cc/virtual/2023/poster/10756,Security,TrojText: Test-time Invisible Textual Trojan Insertion,"In Natural Language Processing (NLP), intelligent neuron models can be susceptible to textual Trojan attacks. Such attacks occur when Trojan models behave normally for standard inputs but generate malicious output for inputs that contain a specific trigger. Syntactic-structure triggers, which are invisible, are becoming more popular for Trojan attacks because they are difficult to detect and defend against. However, these types of attacks require a large corpus of training data to generate poisoned samples with the necessary syntactic structures for Trojan insertion. Obtaining such data can be difficult for attackers, and the process of generating syntactic poisoned triggers and inserting Trojans can be time-consuming. This paper proposes a solution called TrojText, which aims to determine whether invisible textual Trojan attacks can be performed more efficiently and cost-effectively without training data. The proposed approach, called the Representation-Logit Trojan Insertion (RLI) algorithm, uses smaller sampled test data instead of large training data to achieve the desired attack. The paper also introduces two additional techniques, namely the accumulated gradient ranking (AGR) and Trojan Weights Pruning (TWP), to reduce the number of tuned parameters and the attack overhead. The TrojText approach was evaluated on three datasets (AG’s News, SST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The experiments demonstrated that the TrojText approach achieved a 98.35% classification accuracy for test sentences in the target class on the BERT model for the AG’s News dataset. The source code for TrojText is available at https://github.com/UCF-ML-Research/TrojText.","['Social Aspects of Machine Learning', 'Backdoor', 'Invisible', 'Syntactic', 'defense', 'attack', 'Trojan', 'Trigger', 'Textual', 'Test-time']",[],"['Qian Lou', 'Bo Feng']","['University of Central Florida', 'Indiana University']",[]
https://iclr.cc/virtual/2023/poster/10839,Security,Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models,"Privacy is a central tenet of Federated learning (FL), in which a central server trains models without centralizing user data. However, gradient updates used in FL can leak user information.  While the most industrial uses of FL are for text applications (e.g. keystroke prediction), the majority of attacks on user privacy in FL have focused on simple image classifiers and threat models that assume honest execution of the FL protocol from the server. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. We argue that the threat model of malicious server states is highly relevant from a user-centric perspective, and show that in this scenario, text applications using transformer models are much more vulnerable than previously thought.","['Deep Learning and representational learning', 'Gradient Inversion', 'attack', 'transformers', 'privacy', 'federated learning']",[],"['Liam H Fowl', 'Jonas Geiping', 'Steven Reich', 'Yuxin Wen', 'Micah Goldblum', 'Tom Goldstein']","['Google', 'ELLIS Institute Tübingen', 'University of Maryland, College Park', 'University of Maryland, College Park', 'New York University', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2023/poster/11486,Security,Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks,"We propose a novel clustering mechanism based on an incompatibility property between subsets of data that emerges during model training. This mechanism partitions the dataset into subsets that generalize only to themselves, i.e., training on one subset does not improve performance on the other subsets. Leveraging the interaction between the dataset and the training process, our clustering mechanism partitions datasets into clusters that are defined by—and therefore meaningful to—the objective of the training process.We apply our clustering mechanism to defend against data poisoning attacks, in which the attacker injects malicious poisoned data into the training dataset to affect the trained model's output. Our evaluation focuses on backdoor attacks against deep neural networks trained to perform image classification using the GTSRB and CIFAR-10 datasets. Our results show that (1) these attacks produce poisoned datasets in which the poisoned and clean data are incompatible and (2) our technique successfully identifies (and removes) the poisoned data. In an end-to-end evaluation, our defense reduces the attack success rate to below 1% on 134 out of 165 scenarios, with only a 2% drop in clean accuracy on CIFAR-10 and a negligible drop in clean accuracy on GTSRB.","['Social Aspects of Machine Learning', 'defense', 'Data Poisoning']",[],"['Charles Jin', 'Melinda Sun', 'Martin Rinard']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11277,Security,Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning,"Indiscriminate data poisoning attacks are quite effective against supervised learning. However, not much is known about their impact on unsupervised contrastive learning (CL). This paper is the first to consider indiscriminate poisoning attacks of contrastive learning. We propose Contrastive Poisoning (CP), the first effective such attack on CL. We empirically show that Contrastive Poisoning, not only drastically reduces the performance of CL algorithms, but also attacks supervised learning models, making it the most generalizable indiscriminate poisoning attack. We also show that CL algorithms with a momentum encoder are more robust to indiscriminate poisoning, and propose a new countermeasure based on matrix completion. Code is available at: https://github.com/kaiwenzha/contrastive-poisoning.","['General Machine Learning', 'contrastive learning', 'Data Poisoning']",[],"['Hao He', 'Kaiwen Zha', 'Dina Katabi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/10844,Security,Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-Free RL,"Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Both algorithms leverage the concavity and continuity of the newly introduced truncated value functions, and are guaranteed to achieve zero constraint violation during exploration with high probability. Furthermore, both algorithms can provably find a near-optimal policy subject to any constraint in the planning phase. Remarkably, the sample complexities under both algorithms match or even outperform the state of the art in their constraint-free counterparts up to some constant factors, proving that safety constraint hardly increases the sample complexity for RF-RL.","['Theory', 'Safety constraint', 'Sample complexity', 'pure exploration', 'Reward-free RL']",[],"['Ruiquan Huang', 'Jing Yang', 'Yingbin Liang']","['Pennsylvania State University', 'Pennsylvania State University', 'The Ohio State University']",[]
https://iclr.cc/virtual/2023/poster/10867,Security,Dual Student Networks for Data-Free Model Stealing,"Data-free model stealing aims to replicate a target model without direct access to either the training data or the target model. To accomplish this, existing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. In other words, our method alters the standard data-free model stealing paradigm by substituting the target model with a separate student model, thereby creating a lower bound which can be directly optimized without additional target model queries or separate synthetic datasets. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.",['General Machine Learning'],[],"['Ajmal Saeed Mian', 'Mubarak Shah']","['University of Western Australia', 'University of Central Florida']",[]
https://iclr.cc/virtual/2023/poster/11962,Security,Data-Free One-Shot Federated Learning Under Very High Statistical Heterogeneity,"Federated learning (FL) is an emerging distributed learning framework that collaboratively trains a shared model without transferring the local clients' data to a centralized server. Motivated by concerns stemming from extended communication and potential attacks, one-shot FL limits communication to a single round while attempting to retain performance. However, one-shot FL methods often degrade under high statistical heterogeneity, fail to promote pipeline security, or require an auxiliary public dataset. To address these limitations, we propose two novel data-free one-shot FL methods: FedCVAE-Ens and its extension FedCVAE-KD. Both approaches reframe the local learning task using a conditional variational autoencoder (CVAE) to address high statistical heterogeneity. Furthermore, FedCVAE-KD leverages knowledge distillation to compress the ensemble of client decoders into a single decoder. We propose a method that shifts the center of the CVAE prior distribution and experimentally demonstrate that this promotes security, and show how either method can incorporate heterogeneous local models. We confirm the efficacy of the proposed methods over baselines under high statistical heterogeneity using multiple benchmark datasets. In particular, at the highest levels of statistical heterogeneity, both FedCVAE-Ens and FedCVAE-KD typically more than double the accuracy of the baselines.","['Social Aspects of Machine Learning', 'Statistical Heterogeneity', 'One-Shot Federated Learning', 'variational autoencoder', 'Model Heterogeneity']",[],"['Clare Elizabeth Heinbaugh', 'Emilio Luz-Ricca', 'Huajie Shao']","['College of William and Mary', 'University of Cambridge', 'College of William and Mary']",[]
https://iclr.cc/virtual/2023/poster/11247,Security,Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees,"Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the distilled policy, for which the formal guarantees apply. Our approach yields bisimulation guarantees while learning the distilled policy, allowing concrete optimization of the abstraction and representation model quality. Our experiments show that, besides distilling policies up to 10 times faster, the latent model quality is indeed better in general. Moreover, we present experiments from a simple time-to-failure verification algorithm on the latent space. The fact that our approach enables such simple verification techniques highlights its applicability.","['Reinforcement Learning', 'reinforcement learning', 'formal verification', 'representation learning']",[],"['Florent Delgrange', 'Ann Nowe', 'Guillermo Perez']","['Vrije Universiteit Brussel', 'Vrije Universiteit Brussel', 'University of Antwerp']",[]