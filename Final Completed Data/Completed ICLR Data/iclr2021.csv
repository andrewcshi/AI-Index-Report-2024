link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://iclr.cc/virtual/2021/poster/3366,Transparency & Explainability,A Geometric Analysis of Deep Generative Image Models and Its Applications,"Generative adversarial networks (GANs) have emerged as a powerful unsupervised method to model the statistical patterns of real-world data sets, such as natural images. These networks are trained to map random inputs in their latent space to new samples representative of the learned data. However, the structure of the latent space is hard to intuit due to its high dimensionality and the non-linearity of the generator, which limits the usefulness of the models. Understanding the latent space requires a way to identify input codes for existing real-world images (inversion), and a way to identify directions with known image transformations (interpretability). Here, we use a geometric framework to address both issues simultaneously. We develop an architecture-agnostic method to compute the Riemannian metric of the image manifold created by GANs. The eigen-decomposition of the metric isolates axes that account for different levels of image variability. An empirical analysis of several pretrained GANs shows that image variation around each position is concentrated along surprisingly few major axes (the space is highly anisotropic) and the directions that create this large variation are similar at different positions in the space (the space is homogeneous). We show that many of the top eigenvectors correspond to interpretable transforms in the image space, with a substantial part of eigenspace corresponding to minor transforms which could be compressed out. This geometric understanding unifies key previous results related to GAN interpretability. We show that the use of this metric allows for more efficient optimization in the latent space (e.g. GAN inversion) and facilitates unsupervised discovery of interpretable axes. Our results illustrate that defining the geometry of the GAN image manifold can serve as a general framework for understanding GANs.","['feature visualization', 'Model Inversion', 'optimization', 'Differential Geometry', 'deep generative model', 'gan', 'interpretability']",[],"['Binxu Wang', 'Carlos R Ponce']","['Harvard University', 'Harvard University']",[]
https://iclr.cc/virtual/2021/poster/3361,Transparency & Explainability,Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels,"Current neural architecture search (NAS) strategies focus only on finding a single, good, architecture. They offer little insight into why a specific network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation (BO) approach for NAS that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method not only optimises the architecture in a highly data-efficient manner, but also affords interpretability by discovering useful network features and their corresponding impact on the network performance. Moreover, our method is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to BO. We demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We finally show that our method outperforms existing NAS approaches to achieve the state of the art on both closed- and open-domain search spaces.",[],[],"['Binxin Ru', 'Xingchen Wan', 'Xiaowen Dong', 'Michael A Osborne']","['University of Oxford', 'Google', 'University of Oxford', 'University of Oxford']",[]
https://iclr.cc/virtual/2021/poster/3350,Transparency & Explainability,Representation Learning via Invariant Causal Mechanisms,"Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework.  We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method,  and provide an alternative theoretical explanation for the  success  of  these  methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games.","['causality', 'Contrastive Methods', 'self-supervised learning', 'representation learning']",[],"['Jovana Mitrovic', 'Brian McWilliams', 'Jacob C Walker', 'Lars Holger Buesing', 'Charles Blundell']","['DeepMind', 'Google', 'Carnegie Mellon University', 'Deepmind', 'DeepMind']",[]
https://iclr.cc/virtual/2021/poster/3269,Transparency & Explainability,Evaluating the Disentanglement of Deep Generative Models through Manifold Topology,"Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make our code publicly available at https://github.com/stanfordmlgroup/disentanglement.","['evaluation', 'disentanglement', 'generative models']",[],"['Sharon Zhou', 'Eric Zelikman', 'Andrew Y. Ng', 'Stefano Ermon']","['Stanford University', 'Stanford University', 'Computer Science Department', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/3197,Transparency & Explainability,Representation learning for improved interpretability and classification accuracy of clinical factors from EEG,"Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using $\beta$-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.","['beta-vae', 'depression', 'electroencephalography', 'ERP', 'EEG', 'representation learning', 'disentanglement']",[],"['Garrett Honke', 'Irina Higgins']","['State University of New York, Binghamton', 'DeepMind']",[]
https://iclr.cc/virtual/2021/poster/3189,Transparency & Explainability,Trajectory Prediction using Equivariant Continuous Convolution,"Trajectory prediction is a critical part of many AI applications, for example, the safe operation of autonomous vehicles. However, current methods are prone to making inconsistent and physically unrealistic predictions. We leverage insights from  fluid dynamics to overcome this limitation by considering internal symmetry in real-world trajectories. We propose a novel model, Equivariant Continous COnvolution (ECCO) for improved trajectory prediction.  ECCO uses rotationally-equivariant continuous convolutions to embed the symmetries of the system. On both vehicle and pedestrian trajectory datasets, ECCO attains competitive accuracy  with significantly fewer parameters. It is also more sample efficient, generalizing automatically from few data points in any orientation.  Lastly, ECCO improves generalization with equivariance, resulting in more physically consistent predictions.   Our method provides a fresh perspective towards increasing trust and transparency in deep learning models. Our code and data can be found at https://github.com/Rose-STL-Lab/ECCO.","['argoverse', 'continuous convolution', 'trajectory prediction', 'equivariant', 'symmetry']",[],"['Robin Walters', 'Jinxi Li', 'Rose Yu']","['Northeastern University ', 'Northeastern University', 'University of California, San Diego']",[]
https://iclr.cc/virtual/2021/poster/3173,Transparency & Explainability,The role of Disentanglement in Generalisation,"Combinatorial generalisation — the ability to understand and produce novel combinations of familiar elements — is a core capacity of human intelligence that current AI systems struggle with. Recently, it has been suggested that learning disentangled representations may help address this problem. It is claimed that such representations should be able to capture the compositional structure of the world which can then be combined to support combinatorial generalisation. In this study, we systematically tested how the degree of disentanglement affects various forms of generalisation, including two forms of combinatorial generalisation that varied in difficulty. We trained three classes of variational autoencoders (VAEs) on two datasets on an unsupervised task by excluding combinations of generative factors during training. At test time we ask the models to reconstruct the missing combinations in order to measure generalisation performance. Irrespective of the degree of disentanglement, we found that the models supported only weak combinatorial generalisation. We obtained the same outcome when we directly input perfectly disentangled representations as the latents, and when we tested a model on a more complex task that explicitly required independent generative factors to be controlled. While learning disentangled representations does improve interpretability and sample efficiency in some downstream tasks, our results suggest that they are not sufficient for supporting more difficult forms of generalisation.","['generalisation', 'compositional generalization', 'variational autoencoders', 'compositionality', 'disentanglement', 'generative models']",[],"['Milton L. Montero', 'Casimir JH Ludwig', 'Rui Ponte Costa', 'Gaurav Malhotra', 'Jeffrey Bowers']","['IT University of Copenhagen', 'University of Bristol', 'University of Bristol', 'University of Bristol', 'University of Bristol']",[]
https://iclr.cc/virtual/2021/poster/3126,Transparency & Explainability,Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting,"Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters.","['hybrid systems', 'spatio-temporal forecasting', 'physics', 'differential equations', 'deep learning']",[],"['Yuan Yin', 'Vincent LE GUEN', 'Jérémie DONA', 'Ibrahim Ayed', 'Nicolas THOME', 'patrick gallinari']","['Sorbonne Université, CNRS, ISIR', 'Conservatoire National des Arts et Métiers', 'InstaDeep', 'LIP6', 'Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)', 'Criteo AI Lab']",[]
https://iclr.cc/virtual/2021/poster/3107,Transparency & Explainability,Prototypical Representation Learning for Relation Extraction,"Recognizing relations between entities is a pivotal task of relational learning.Learning relation representations from distantly-labeled datasets is difficult because of the abundant label noise and complicated expressions in human language.This paper aims to learn predictive, interpretable, and robust relation representations from distantly-labeled data that are effective in different settings, including supervised, distantly supervised, and few-shot learning. Instead of solely relying on the supervision from noisy labels, we propose to learn prototypes for each relation from contextual information to best explore the intrinsic semantics of relations. Prototypes are representations in the feature space abstracting the essential semantics of relations between entities in sentences. We learn prototypes based on objectives with clear geometric interpretation, where the prototypes are unit vectors uniformly dispersed in a unit ball, and statement embeddings are centered at the end of their corresponding prototype vectors on the surface of the ball. This approach allows us to learn meaningful, interpretable prototypes for the final classification. Results on several relation learning tasks show that our model significantly outperforms the previous state-of-the-art models. We further demonstrate the robustness of the encoder and the interpretability of prototypes with extensive experiments.","['Relation Extraction', 'nlp', 'representation learning']",[],"['Ning Ding', 'Xiaobin Wang', 'Yao Fu', 'Guangwei Xu', 'Rui Wang', 'Pengjun Xie', 'Ying Shen', 'Fei Huang', 'Hai-Tao Zheng']","['Tsinghua University, Tsinghua University', 'Soochow University, China', 'University of Edinburgh', 'Alibaba Group', 'nyonic', 'Alibaba Group', 'SUN YAT-SEN UNIVERSITY', 'Alibaba Group', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2021/poster/3095,Transparency & Explainability,Multi-timescale Representation Learning in LSTM Language Models,"Language models must capture statistical dependencies between words at timescales ranging from very short to very long. Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory (LSTM) language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency (rare) words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models.","['timescales', 'language model', 'lstm']",[],"['Vy A. Vo', 'Javier S. Turek', 'Alexander Huth']","['Intel', 'Intel', 'The University of Texas at Austin']",[]
https://iclr.cc/virtual/2021/poster/3057,Transparency & Explainability,BERTology Meets Biology: Interpreting Attention in Protein Language Models,"Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. In this work, we demonstrate a set of methods for analyzing protein Transformer models through the lens of attention. We show that attention: (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer architectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We also present a three-dimensional visualization of the interaction between attention and protein structure. Code for visualization and analysis is available at https://github.com/salesforce/provis.","['Computational Biology', 'visualization', 'transformers', 'black box', 'attention', 'representation learning', 'natural language processing', 'interpretability']",[],"['Jesse Vig', 'Ali Madani', 'Lav R. Varshney', 'Caiming Xiong', 'richard socher', 'Nazneen Rajani']","['Salesforce Research', 'Profluent Bio', 'University of Illinois at Urbana-Champaign', 'Salesforce Research', 'SalesForce.com', 'Salesforce']",[]
https://iclr.cc/virtual/2021/poster/2950,Transparency & Explainability,Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting,"The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the \textit{evidence} for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has ``the right reasons'' for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization-based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at \url{https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons}.","['XAI', 'lifelong learning', 'catastrophic forgetting', 'continual learning', 'explainability']",[],"['Sayna Ebrahimi', 'Suzanne Petryk', 'William Gan', 'Joseph E. Gonzalez', 'Marcus Rohrbach', 'Trevor Darrell']","['Google', 'University of California Berkeley', 'University of California Berkeley', 'University of California - Berkeley', 'Technische Universität Darmstadt', 'Electrical Engineering & Computer Science Department']",[]
https://iclr.cc/virtual/2021/poster/2943,Transparency & Explainability,Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability,"Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common assumption is that these input-gradients contain information regarding $p_{\theta} ( y\mid \mathbf{x} )$, the model's discriminative capabilities, thus justifying their use for interpretability. However, in this work, we show that these input-gradients can be arbitrarily manipulated as a consequence of the shift-invariance of softmax without changing the discriminative function. This leaves an open question: given that input-gradients can be arbitrary, why are they highly structured and explanatory in standard models? In this work, we re-interpret the logits of standard softmax-based classifiers as unnormalized log-densities of the data distribution and show that input-gradients can be viewed as gradients of a class-conditional generative model $p_{\theta}(\mathbf{x} \mid y)$ implicit in the discriminative model. This leads us to hypothesize that the highly structured and explanatory nature of input-gradients may be due to the alignment of this class-conditional model $p_{\theta}(\mathbf{x} \mid y)$ with that of the ground truth data distribution $p_{\text{data}} (\mathbf{x} \mid y)$. We test this hypothesis by studying the effect of density alignment on gradient explanations. To achieve this density alignment, we use an algorithm called score-matching, and propose novel approximations to this algorithm to enable training large-scale models. Our experiments show that improving the alignment of the implicit density model with the data distribution enhances gradient structure and explanatory power while reducing this alignment has the opposite effect. This also leads us to conjecture that unintended density alignment in standard neural network training may explain the highly structured nature of input-gradients observed in practice. Overall, our finding that input-gradients capture information regarding an implicit generative model implies that we need to re-think their use for interpreting discriminative models.","['score-matching', 'saliency maps', 'interpretability']",[],"['Suraj Srinivas', 'François Fleuret']","['School of Engineering and Applied Sciences, Harvard University', 'University of Geneva']",[]
https://iclr.cc/virtual/2021/poster/2906,Transparency & Explainability,On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning,"Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\textit{meta-initialization}$ of model parameters (that we call $\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\textit{how}$ robust regularization can $\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.",[],[],"['Ren Wang', 'Kaidi Xu', 'Sijia Liu', 'Pin-Yu Chen', 'Tsui-Wei Weng', 'Chuang Gan', 'Meng Wang']","['Illinois Institute of Technology', 'Drexel University', 'Michigan State University', 'International Business Machines', 'University of California, San Diego', 'MIT-IBM Watson AI Lab', 'Rensselaer Polytechnic Institute']",[]
https://iclr.cc/virtual/2021/poster/2856,Transparency & Explainability,Shapley explainability on the data manifold,"Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model’s predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model’s features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While “off-manifold” Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.",[],[],"['Christopher Frye', 'Damien De Mijolla', 'Tom Begley', 'Laurence Cowton', 'Megan Stanley', 'Ilya Feige']","['Faculty', 'Faculty AI', 'Faculty', 'University of Cambridge', 'Microsoft Research Cambridge', 'University College London']",[]
https://iclr.cc/virtual/2021/poster/2854,Transparency & Explainability,"Learning ""What-if"" Explanations for Sequential Decision-Making","Building interpretable parameterizations of real-world decision-making on the basis of demonstrated behavior--i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function--is essential for introspecting and auditing policies in different institutions. In this paper, we propose learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to ``""what if'' outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these cost-benefit tradeoffs associated with the expert's actions, we integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making---where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting, and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states. Through illustrative experiments in both real and simulated medical environments, we highlight the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of behavior.","['explaining decision-making', 'counterfactuals', 'preference learning']",[],"['Ioana Bica', 'Daniel Jarrett', 'Alihan Hüyük', 'Mihaela van der Schaar']","['DeepMind', 'DeepMind', 'University of Cambridge', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2780,Transparency & Explainability,Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning,"Understanding human behavior from observed data is critical for transparency and accountability in decision-making. Consider real-world settings such as healthcare, in which modeling a decision-maker’s policy is challenging—with no access to underlying states, no knowledge of environment dynamics, and no allowance for live experimentation. We desire learning a data-driven representation of decision- making behavior that (1) inheres transparency by design, (2) accommodates partial observability, and (3) operates completely offline. To satisfy these key criteria, we propose a novel model-based Bayesian method for interpretable policy learning (“Interpole”) that jointly estimates an agent’s (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping. Through experiments on both simulated and real-world data for the problem of Alzheimer’s disease diagnosis, we illustrate the potential of our approach as an investigative device for auditing, quantifying, and understanding human decision-making behavior.","['understanding decision-making', 'interpretable policy learning']",[],"['Alihan Hüyük', 'Daniel Jarrett', 'Cem Tekin', 'Mihaela van der Schaar']","['University of Cambridge', 'DeepMind', 'Bilkent University', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2741,Transparency & Explainability,Getting a CLUE: A  Method for Explaining Uncertainty Estimates,"Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.","['explainability', 'uncertainty', 'interpretability']",[],"['Javier Antoran', 'Umang Bhatt', 'Tameem Adel', 'Adrian Weller', 'José Miguel Hernández-Lobato']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'Alan Turing Institute', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2719,Transparency & Explainability,"Physics-aware, probabilistic model order reduction with guaranteed stability","Given (small amounts of) time-series' data from  a high-dimensional, fine-grained, multiscale dynamical system, we propose a generative framework for learning an effective, lower-dimensional, coarse-grained dynamical model that is predictive of the fine-grained system's long-term evolution but also of its behavior under different initial conditions. We target fine-grained models as they arise in physical applications (e.g. molecular dynamics, agent-based models), the dynamics  of which are strongly non-stationary but their transition to equilibrium is governed by unknown slow processes which are largely inaccessible by brute-force simulations. Approaches based on domain knowledge heavily rely on physical insight in identifying temporally slow features and fail to enforce the long-term stability of the learned dynamics. On the other hand, purely statistical frameworks lack interpretability and rely on large amounts of expensive simulation data (long and multiple trajectories) as they cannot infuse domain knowledge. The generative framework proposed achieves  the aforementioned desiderata by  employing a flexible prior on the complex plane for the latent, slow processes, and  an intermediate layer of physics-motivated latent variables that reduces reliance on data and imbues inductive bias. In contrast to existing schemes, it does not require  the a priori definition of projection operators from the fine-grained description and addresses simultaneously the tasks of dimensionality reduction and model estimation. We demonstrate its efficacy and accuracy in multiscale physical systems of particle dynamics where probabilistic, long-term predictions of phenomena not contained in the training data are produced.","['long-term stability', 'slowness', 'model order reduction', 'probabilistic generative models', 'state-space models', 'inductive bias']",[],"['Sebastian Kaltenbach', 'Phaedon Stelios Koutsourelakis']","['ETHZ - ETH Zurich', 'Technische Universität München']",[]
https://iclr.cc/virtual/2021/poster/2667,Transparency & Explainability,Monotonic Kronecker-Factored Lattice,"It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.","['evaluation', 'Matrix and Tensor Factorization', 'algorithms', 'theory', 'efficiency', 'fairness', 'regularization', 'regression', 'classification', 'machine learning']",[],"['William Taylor Bakst', 'Nobuyuki Morioka', 'Erez Louidor']","['Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/3262,Transparency & Explainability,Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization,"Dimensionality reduction methods for count data are critical to a wide range of applications in medical informatics and other fields where model interpretability is paramount. For such data, hierarchical Poisson matrix factorization (HPF) and other sparse probabilistic non-negative matrix factorization (NMF) methods are considered to be interpretable generative models. They consist of sparse transformations for decoding their learned representations into predictions. However, sparsity in representation decoding does not necessarily imply sparsity in the encoding of representations from the original data features.  HPF is often incorrectly interpreted in the literature as if it possesses encoder sparsity. The distinction between decoder sparsity and encoder sparsity is subtle but important. Due to the lack of encoder sparsity, HPF does not possess the column-clustering property of classical NMF -- the factor loading matrix does not sufficiently define how each factor is formed from the original features. We address this deficiency by self-consistently enforcing encoder sparsity, using a generalized additive model  (GAM), thereby allowing one to relate each representation coordinate to a subset of the original data features. In doing so, the method also gains the ability to perform feature selection. We demonstrate our method on simulated data and give an example of how encoder sparsity is of practical use in a concrete application of representing inpatient comorbidities in Medicare patients.","['factor analysis', 'probabilistic matrix factorization', 'poisson matrix factorization', 'bayesian', 'sparse coding', 'generalized additive model', 'interpretability']",[],"['Joshua C Chang', 'Patrick Allen Fletcher', 'Bart Desmet', 'Ayah Zirikly']","['National Institutes of Health', 'NIH', 'Universiteit Gent', 'Johns Hopkins University']",[]
https://iclr.cc/virtual/2021/poster/3310,Transparency & Explainability,NBDT: Neural-Backed Decision Tree,"Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.","['explainability', 'computer vision', 'interpretability']",[],"['Alvin Wan', 'Lisa Dunlap', 'Daniel Ho', 'Jihan Yin', 'Scott Lee', 'Suzanne Petryk', 'Sarah Adel Bargal', 'Joseph E. Gonzalez']","['Apple', 'University of California, Berkeley', 'University of California Berkeley', 'University of California Berkeley', 'University of California Berkeley', 'University of California Berkeley', 'Georgetown University', 'University of California - Berkeley']",[]
https://iclr.cc/virtual/2021/poster/2605,Transparency & Explainability,Interpretable Models for Granger Causality Using Self-explaining Neural Networks,"Exploratory analysis of time series data can yield a better understanding of complex dynamical systems. Granger causality is a practical framework for analysing interactions in sequential data, applied in a wide range of domains. In this paper, we propose a novel framework for inferring multivariate Granger causality under nonlinear dynamics based on an extension of self-explaining neural networks. This framework is more interpretable than other neural-network-based techniques for inferring Granger causality, since in addition to relational inference, it also allows detecting signs of Granger-causal effects and inspecting their variability over time. In comprehensive experiments on simulated data, we show that our framework performs on par with several powerful baseline methods at inferring Granger causality and that it achieves better performance at inferring interaction signs. The results suggest that our framework is a viable and more interpretable alternative to sparse-input neural networks for inferring Granger causality.","['Granger causality', 'time series', 'inference', 'neural networks', 'interpretability']",[],"['Ričards Marcinkevičs', 'Julia E Vogt']","['Department of Computer Science, Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/3200,Transparency & Explainability,Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks,"Neural networks (NNs) whose subnetworks implement reusable functions are expected to offer numerous advantages, including compositionality through efficient recombination of functional building blocks, interpretability, preventing catastrophic interference, etc. Understanding if and how NNs are modular could provide insights into how to improve them. Current inspection methods, however, fail to link modules to their functionality. In this paper, we present a novel method based on learning binary weight masks to identify individual weights and subnets responsible for specific functions. Using this powerful tool, we contribute an extensive study of emerging modularity in NNs that covers several standard architectures and datasets. We demonstrate how common NNs fail to reuse submodules and offer new insights into the related issue of systematic generalization on language tasks.","['systematic generalization', 'modularity', 'compositionality']",[],"['Róbert Csordás', 'Sjoerd van Steenkiste', 'Jürgen Schmidhuber']","['IDSIA', 'Google', 'King Abdullah University of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/2706,Transparency & Explainability,A Learning Theoretic Perspective on Local Explainability,"In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of performance generalization and bound the test-time predictive accuracy of a model using a notion of how locally explainable it is.  Second, we explore the novel problem of explanation generalization which is an important concern for a growing class of finite sample-based local approximation explanations. Finally, we validate our theoretical results empirically and show that they reflect what can be seen in practice.","['Local Explanations', 'learning theory', 'generalization', 'interpretability']",[],"['Jeffrey Li', 'Vaishnavh Nagarajan', 'Gregory Plumb', 'Ameet Talwalkar']","['Department of Computer Science, University of Washington', 'Google', 'Carnegie Mellon University', 'University of California-Los Angeles']",[]
https://iclr.cc/virtual/2021/poster/3257,Transparency & Explainability,Disentangled Recurrent Wasserstein Autoencoder ,"Learning disentangled representations leads to interpretable models and facilitates data generation with style transfer, which has been extensively studied on static data such as images in an unsupervised learning framework. However, only a few works have explored unsupervised disentangled sequential representation learning due to challenges of generating sequential data. In this paper, we propose recurrent Wasserstein Autoencoder (R-WAE), a new framework for generative modeling of sequential data. R-WAE disentangles the representation of an input sequence into static and dynamic factors (i.e., time-invariant and time-varying parts). Our theoretical analysis shows that, R-WAE minimizes an upper bound of a penalized form of the Wasserstein distance between model distribution and sequential data distribution, and simultaneously maximizes the mutual information between input data and different disentangled latent factors, respectively. This is superior to (recurrent) VAE which does not explicitly enforce mutual information maximization between input data and disentangled latent representations. When the number of actions in sequential data is available as weak supervision information, R-WAE is extended to learn a categorical latent representation of actions to improve its disentanglement. Experiments on a variety of datasets show that our models outperform other baselines with the same settings in terms of disentanglement and unconditional video generation both quantitatively and qualitatively.","['Recurrent Generative Model', 'Sequential  Representation Learning', 'disentanglement']",[],"['Jun Han', 'Martin Renqiang Min', 'Ligong Han', 'Li Erran Li', 'Xuan Zhang']","['PCG, Tencent', 'NEC Laboratories America', 'Rutgers University', 'Amazon', 'Texas A&M']",[]
https://iclr.cc/virtual/2021/poster/3153,Transparency & Explainability,Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization,"Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans to predict CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. (2017) with a simple baseline visualization, namely exemplary natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiment is designed to maximize participants' performance, and is the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations ($82\pm4\%$ accuracy; chance would be $50\%$). However, natural images --- originally intended to be a baseline --- outperform these synthetic images by a wide margin ($92\pm2\%$). Additionally, participants are faster and more confident for natural images, whereas subjective impressions about the interpretability of the feature visualizations by Olah et al. (2017) are mixed. The higher informativeness of natural images holds across most layers, for both expert and lay participants as well as for hand- and randomly-picked feature visualizations. Even if only a single reference image is given, synthetic images provide less information than natural images ($65\pm5\%$ vs. $73\pm4\%$). In summary, synthetic images from a popular feature visualization method are significantly less informative for assessing CNN activations than natural images. We argue that visualization methods should improve over this simple baseline.","['explanation method', 'understanding CNNs', 'human psychophysics', 'activation maximization', 'feature visualization', 'evaluation of interpretability']",[],"['Judy Borowski', 'Roland S. Zimmermann', 'Robert Geirhos', 'Thomas S. A. Wallis', 'Matthias Bethge', 'Wieland Brendel']","['University of Tuebingen', 'Eberhard-Karls-Universität Tübingen', 'Google DeepMind', 'TU Darmstadt', 'University of Tuebingen', 'ELLIS Institute Tübingen']",[]
https://iclr.cc/virtual/2021/poster/2637,Transparency & Explainability,Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders,"Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.","['Noise model', 'Unsupervised denoising', 'Diversity denoising', 'variational autoencoders']",[],"['Mangal Prakash', 'Alexander Krull', 'Florian Jug']","['Johnson & Johnson', 'Birmingham University', 'Fondation Human Technopole']",[]
https://iclr.cc/virtual/2021/poster/2841,Transparency & Explainability,Influence Functions in Deep Learning Are Fragile,"Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A commonly-used (first-order) influence function can be implemented efficiently as a post-hoc method requiring access only to the gradients and Hessian of the model. For linear models, influence functions are well-defined due to the convexity of the underlying loss function and are generally accurate even across difficult settings where model changes are fairly large such as estimating group influences. Influence functions, however, are not well-understood in the context of deep learning with non-convex loss functions.  In this paper, we provide a comprehensive and large-scale empirical study of successes and failures of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments, we show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of influence functions. In particular, we find that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy of influence estimates can vary significantly depending on the examined test points. These results suggest that in general influence functions in deep learning are fragile and call for developing improved influence estimation methods to mitigate these issues in non-convex setups.","['influence functions', 'interpretability']",[],"['Samyadeep Basu', 'Phil Pope', 'Soheil Feizi']","['University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/2987,Transparency & Explainability,Task-Agnostic Morphology Evolution,"Deep reinforcement learning primarily focuses on learning behavior, usually overlooking the fact that an agent's function is largely determined by form. So, how should one go about finding a morphology fit for solving tasks in a given environment? Current approaches that co-adapt morphology and behavior use a specific task's reward as a signal for morphology optimization. However, this often requires expensive policy optimization and results in task-dependent morphologies that are not built to generalize. In this work, we propose a new approach, Task-Agnostic Morphology Evolution (TAME), to alleviate both of these issues. Without any task or reward specification, TAME evolves morphologies by only applying randomly sampled action primitives on a population of agents. This is accomplished using an information-theoretic objective that efficiently ranks agents by their ability to reach diverse states in the environment and the causality of their actions. Finally, we empirically demonstrate that across 2D, 3D, and manipulation environments TAME can evolve morphologies that match the multi-task performance of those learned with task supervised algorithms. Our code and videos can be found at https://sites.google.com/view/task-agnostic-evolution .","['evolution', 'morphology', 'empowerment', 'information theory', 'unsupervised']",[],"['Joey Hejna', 'Pieter Abbeel', 'Lerrel Pinto']","['Stanford University', 'Covariant', 'New York University']",[]
https://iclr.cc/virtual/2021/poster/2975,Fairness & Bias,Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification,"Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.","['limited data', 'influence functions', 'transfer learning', 'adversarial training']",[],"['Francisco Utrera', 'N. Benjamin Erichson', 'Rajiv Khanna', 'Michael W. Mahoney']","['University of Pittsburgh', 'Lawrence Berkeley National Lab', 'Purdue University', 'University of California Berkeley']",[]
https://iclr.cc/virtual/2021/poster/2679,Fairness & Bias,Tilted Empirical Risk Minimization,"Empirical risk minimization (ERM) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly. While many methods aim to address these problems individually, in this work, we explore them through a unified framework---tilted empirical risk minimization (TERM). In particular, we show that it is possible to flexibly tune the impact of individual losses through a straightforward extension to ERM using a hyperparameter called the tilt. We provide several interpretations of the resulting framework: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. We develop batch and stochastic first-order optimization methods for solving TERM, and show that the problem can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. TERM is not only competitive with existing solutions tailored to these individual problems, but can also enable entirely new applications, such as simultaneously addressing outliers and promoting fairness.","['fairness', 'label noise robustness', 'models of learning and generalization', 'exponential tilting']",[],"['Tian Li', 'Ahmad Beirami', 'Maziar Sanjabi', 'Virginia Smith']","['University of Chicago', 'Google Research', 'Meta', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2021/poster/2968,Fairness & Bias,Greedy-GQ with Variance Reduction: Finite-time Analysis and Improved Complexity,"Greedy-GQ is a value-based reinforcement learning (RL) algorithm for optimal control. Recently, the finite-time analysis of Greedy-GQ has been developed under linear function approximation and Markovian sampling, and the algorithm is shown to achieve an $\epsilon$-stationary point with a sample complexity in the order of $\mathcal{O}(\epsilon^{-3})$. Such a high sample complexity is due to the large variance induced by the Markovian samples. In this paper, we propose a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm for off-policy optimal control. In particular, the algorithm applies the SVRG-based variance reduction scheme to reduce the stochastic variance of the two time-scale updates. We study the finite-time convergence of VR-Greedy-GQ under linear function approximation and Markovian sampling and show that the algorithm achieves a much smaller bias and variance error than the original Greedy-GQ. In particular, we prove that VR-Greedy-GQ achieves an improved sample complexity that is in the order of $\mathcal{O}(\epsilon^{-2})$. We further compare the performance of VR-Greedy-GQ with that of Greedy-GQ in various RL experiments to corroborate our theoretical findings.","['optimization', 'reinforcement learning', 'machine learning']",[],"['Shaocong Ma', 'Ziyi Chen', 'Yi Zhou']","['University of Utah', 'University of Maryland, College Park', 'University of Utah']",[]
https://iclr.cc/virtual/2021/poster/2966,Fairness & Bias,Statistical inference for individual fairness,"As we rely on machine learning (ML) models to make more consequential decisions, the issue of ML models perpetuating unwanted social biases has come to the fore of the public's and the research community's attention. In this paper, we focus on the problem of detecting violations of individual fairness in ML models. We formalize the problem as measuring the susceptibility of ML models against a form of adversarial attack and develop a suite of inference tools for the adversarial loss. The tools allow practitioners to assess the individual fairness of ML models in a statistically-principled way: form confidence intervals for the adversarial loss and test hypotheses of model fairness with (asymptotic) non-coverage/Type I error rate control. We demonstrate the utility of our tools in a real-world case study.",[],[],"['Subha Maity', 'Songkai Xue', 'Mikhail Yurochkin', 'Yuekai Sun']","['University of Michigan, Ann Arbor', 'University of Michigan', 'International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/2647,Fairness & Bias,Early Stopping in Deep Networks: Double Descent and How to Eliminate it,"Over-parameterized models, such as large deep networks, often exhibit a double descent phenomenon, whereas a function of model size, error first decreases, increases, and decreases at last. This intriguing double descent behavior also occurs as a function of training epochs and has been conjectured to arise because training epochs control the model complexity. In this paper, we show that such epoch-wise double descent occurs for a different reason: It is caused by a superposition of two or more bias-variance tradeoffs that arise because different parts of the network are learned at different epochs, and mitigating this by proper scaling of stepsizes can significantly improve the early stopping performance. We show this analytically for i) linear regression, where differently scaled features give rise to a superposition of bias-variance tradeoffs, and for ii) a wide two-layer neural network, where the first and second layers govern bias-variance tradeoffs. Inspired by this theory, we study two standard convolutional networks empirically and show that eliminating epoch-wise double descent through adjusting stepsizes of different layers improves the early stopping performance.","['early stopping', 'double descent']",[],"['Reinhard Heckel', 'Fatih Furkan Yilmaz']","['Technical University Munich', 'Rice University']",[]
https://iclr.cc/virtual/2021/poster/3354,Fairness & Bias,CO2: Consistent Contrast for Unsupervised Visual Representation Learning,"Contrastive learning has recently been a core for unsupervised visual representation learning. Without human annotation, the common practice is to perform an instance discrimination task: Given a query image crop, label crops from the same image as positives, and crops from other randomly sampled images as negatives. An important limitation of this label assignment is that it can not reflect the heterogeneous similarity of the query crop to crops from other images, but regarding them as equally negative. To address this issue, inspired by consistency regularization in semi-supervised learning, we propose Consistent Contrast (CO2), which introduces a consistency term into unsupervised contrastive learning framework. The consistency term takes the similarity of the query crop to crops from other images as unlabeled, and the corresponding similarity of a positive crop as a pseudo label. It then encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC. This shows that CO2 learns better visual representations for downstream tasks.","['contrastive learning', 'consistency regularization', 'unsupervised representation learning']",[],"['Chen Wei', 'Huiyu Wang', 'Wei Shen', 'Alan Yuille']","['Johns Hopkins University', 'Facebook', 'Shanghai Jiao Tong University', 'Johns Hopkins University']",[]
https://iclr.cc/virtual/2021/poster/3333,Fairness & Bias,Spatially Structured Recurrent Modules,"Capturing the structure of a data-generating process by means of appropriate inductive biases can help in learning models that generalise well and are robust to changes in the input distribution. While methods that harness spatial and temporal structures find broad application, recent work has demonstrated the potential of models that leverage sparse and modular structure using an ensemble of sparingly interacting modules. In this work, we take a step towards dynamic models that are capable of simultaneously exploiting both modular and spatiotemporal structures. To this end, we model the dynamical system as a collection of autonomous but sparsely interacting sub-systems that interact according to a learned topology which is informed by the spatial structure of the underlying system. This gives rise to a class of models that are well suited for capturing the dynamics of systems that only offer local views into their state, along with corresponding spatial locations of those views. On the tasks of video prediction from cropped frames and multi-agent world modelling from partial observations in the challenging Starcraft2 domain, we find our models to be more robust to the number of available views and better capable of generalisation to novel tasks without additional training than strong baselines that perform equally well or better on the training distribution.","['partially observed environments', 'modular architectures', 'spatio-temporal modelling', 'recurrent neural networks']",[],"['Nasim Rahaman', 'Anirudh Goyal', 'Muhammad Waleed Gondal', 'Manuel Wuthrich', 'Stefan Bauer', 'Yash Sharma', 'Yoshua Bengio']","['Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Google DeepMind', 'Amazon Development Center Germany', 'Max Planck Institute for Intelligent Systems', 'Technische Universität München', 'Centre for Integrative Neuroscience, AG Bethge', 'University of Montreal']",[]
https://iclr.cc/virtual/2021/poster/3307,Fairness & Bias,GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing,"We present GraPPa, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). We pre-train our model on the synthetic data to inject important structural properties commonly found in semantic parsing into the pre-training language model. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) on several existing table-related datasets to regularize our pre-training process.  Our proposed pre-training strategy is much data-efficient. When incorporated with strong base semantic parsers, GraPPa achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parsing tasks.","['semantic parsing', 'text-to-sql', 'pre-training', 'nlp']",[],"['Tao Yu', 'Chien-Sheng Wu', 'Xi Victoria Lin', 'Bailin Wang', 'Yi Chern Tan', 'Xinyi Yang', 'Dragomir Radev', 'richard socher', 'Caiming Xiong']","['The University of Hong Kong', 'Salesforce AI', 'Facebook', 'Massachusetts Institute of Technology', 'Yale University', 'Salesforce Research', 'Yale University', 'SalesForce.com', 'Salesforce Research']",[]
https://iclr.cc/virtual/2021/poster/3303,Fairness & Bias,Disentangling 3D Prototypical Networks for Few-Shot Concept Learning,"We present neural architectures that disentangle RGB-D images into objects’ shapes and styles and a map of the background scene, and explore their applications for few-shot 3D object detection and few-shot concept classification. Our networks incorporate architectural biases that reflect the image formation process, 3D  geometry of the world scene, and shape-style interplay. They are trained end-to-end self-supervised by predicting views in static scenes, alongside a small number of 3D object boxes. Objects and scenes are represented in terms of 3D feature grids in the bottleneck of the network. We show the proposed 3D neural representations are compositional: they can generate novel 3D scene feature maps by mixing object shapes and styles, resizing and adding the resulting object 3D feature maps over background scene feature maps. We show object detectors trained on hallucinated 3D neural scenes generalize better to novel environments. We show classifiers for object categories, color, materials, and spatial relationships trained over the  disentangled 3D feature sub-spaces generalize better with dramatically fewer exemplars over the current state-of-the-art, and enable a visual question answering system that uses them as its modules to generalize one-shot to novel objects in the scene.","['vqa', '3D vision', 'Few Shot Learning', 'disentanglement']",[],"['Mihir Prabhudesai', 'Shamit Lal', 'Darshan Patil', 'Hsiao-Yu Tung', 'Adam W Harley', 'Katerina Fragkiadaki']","['School of Computer Science, Carnegie Mellon University', 'Amazon', 'Université de Montréal', 'Massachusetts Institute of Technology', 'Ryerson University', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2021/poster/3302,Fairness & Bias,Iterated learning for emergent systematicity in VQA,"Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.","['shapes', 'neural module network', 'cultural transmission', 'iterated learning', 'systematic generalization', 'visual question answering', 'clevr', 'vqa', 'compositionality']",[],"['Ankit Vani', 'Max Schwarzer', 'Yuchen Lu', 'Eeshan Gunesh Dhekane', 'Aaron Courville']","['University of Montreal', 'University of Montreal', 'University of Montreal', 'Apple', 'University of Montreal']",[]
https://iclr.cc/virtual/2021/poster/3299,Fairness & Bias,Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?,"Convolutional neural networks often dominate fully-connected counterparts in generalization performance, especially on image classification tasks. This is often explained in terms of \textquotedblleft better inductive bias.\textquotedblright\  However, this has not been made mathematically rigorous, and the hurdle is that the sufficiently wide fully-connected net can always simulate the convolutional net. Thus the training algorithm plays a role. The current work describes a natural task on which a provable sample complexity gap can be shown, for standard training algorithms. We construct a single natural distribution on $\mathbb{R}^d\times\{\pm 1\}$ on which any orthogonal-invariant algorithm (i.e. fully-connected networks trained with most gradient-based methods from gaussian initialization) requires $\Omega(d^2)$ samples to generalize while $O(1)$ samples suffice for convolutional architectures. Furthermore, we demonstrate a single target function, learning which on all possible distributions leads to an $O(1)$ vs $\Omega(d^2/\varepsilon)$ gap. The proof relies on the fact that SGD on fully-connected network is orthogonal equivariant. Similar results are achieved for $\ell_2$ regression and adaptive training algorithms, e.g. Adam and AdaGrad, which are only permutation equivariant.","['fully-connected', 'sample complexity separation', 'convolutional neural networks', 'equivariance']",[],"['Zhiyuan Li', 'Yi Zhang', 'Sanjeev Arora']","['Toyota Technological Institute at Chicago', 'Microsoft', 'Princeton University']",[]
https://iclr.cc/virtual/2021/poster/3285,Fairness & Bias,Efficient Empowerment Estimation for Unsupervised Stabilization,"Intrinsically motivated artificial agents learn advantageous behavior without externally-provided rewards. Previously, it was shown that maximizing mutual information between agent actuators and future states, known as the empowerment principle, enables unsupervised stabilization of dynamical systems at upright positions, which is a prototypical intrinsically motivated behavior for upright standing and walking. This follows from the coincidence between the objective of stabilization and the objective of empowerment. Unfortunately, sample-based estimation of this kind of mutual information is challenging. Recently, various variational lower bounds (VLBs) on empowerment have been proposed as solutions; however, they are often biased, unstable in training, and have high sample complexity. In this work, we propose an alternative solution based on a trainable representation of a dynamical system as a Gaussian channel, which allows us to efficiently calculate an unbiased estimator of empowerment by convex optimization. We demonstrate our solution for sample-based unsupervised stabilization on different dynamical control systems and show the advantages of our method by comparing it to the existing VLB approaches. Specifically, we show that our method has a lower sample complexity, is more stable in training, possesses the essential properties of the empowerment function, and allows estimation of empowerment from images. Consequently, our method opens a path to wider and easier adoption of empowerment for various applications.","['empowerment', 'representation of dynamical systems', 'unsupervised stabilization', 'intrinsic motivation', 'neural networks']",[],"['Ruihan Zhao', 'Kevin Lu', 'Pieter Abbeel', 'Stas Tiomkin']","['University of Texas at Austin', 'Facebook AI Research', 'Covariant', 'San Jose State University']",[]
https://iclr.cc/virtual/2021/poster/3271,Fairness & Bias,Decoupling Global and Local Representations via Invertible Generative Flows,"In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder. Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at \url{https://github.com/XuezheMax/wolf}.","['image generation', 'Generative Flow', 'Normalizing Flow', 'representation learning', 'generative models']",[],"['Xuezhe Ma', 'Xiang Kong', 'Shanghang Zhang', 'Eduard Hovy']","['USC/ISI', 'Carnegie Mellon University', 'Peking University', 'University of Melbourne']",[]
https://iclr.cc/virtual/2021/poster/3260,Fairness & Bias,Hopper: Multi-hop Transformer for Spatiotemporal Reasoning,"This paper considers the problem of spatiotemporal object-centric reasoning in videos. Central to our approach is the notion of object permanence, i.e., the ability to reason about the location of objects as they move through the video while being occluded, contained or carried by other objects. Existing deep learning based approaches often suffer from spatiotemporal biases when applied to video reasoning problems. We propose Hopper, which uses a Multi-hop Transformer for reasoning object permanence in videos. Given a video and a localization query, Hopper reasons over image and object tracks to automatically hop over critical frames in an iterative fashion to predict the final position of the object of interest. We demonstrate the effectiveness of using a contrastive loss to reduce spatiotemporal biases. We evaluate over CATER dataset and find that Hopper achieves 73.2% Top-1 accuracy using just 1 FPS by hopping through just a few critical frames. We also demonstrate Hopper can perform long-term reasoning by building a CATER-h dataset that requires multi-step reasoning to localize objects of interest correctly.","['transformer', 'Video Recognition', 'Spatiotemporal Understanding', 'Object Permanence', 'Multi-hop Reasoning']",[],"['Honglu Zhou', 'Asim Kadav', 'Farley Lai', 'Alexandru Niculescu-Mizil', 'Martin Renqiang Min', 'Mubbasir Kapadia']","['NEC-Labs', 'freenome', 'Qualcomm Technologies, Inc.', 'NEC-Labs', 'NEC Laboratories America', 'Rutgers University ']",[]
https://iclr.cc/virtual/2021/poster/3258,Fairness & Bias,Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients,"A data set sampled from a certain population is biased if the subgroups of the population are sampled at proportions that are significantly different from their underlying proportions. Training machine learning models on biased data sets requires correction techniques to compensate for the bias. We consider two commonly-used techniques, resampling and reweighting, that rebalance the proportions of the subgroups to maintain the desired objective function. Though statistically equivalent, it has been observed that resampling outperforms reweighting when combined with stochastic gradient algorithms. By analyzing illustrative examples, we explain the reason behind this phenomenon using tools from dynamical stability and stochastic asymptotics. We also present experiments from regression, classification, and off-policy prediction to demonstrate that this is a general phenomenon. We argue that it is imperative to consider the objective function design and the optimization algorithm together while addressing the sampling bias.","['stochastic asymptotics', 'resampling', 'reweighting', 'biased sampling', 'stability']",[],"['Jing An', 'Lexing Ying']","['Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/3245,Fairness & Bias,Conditional Negative Sampling for Contrastive Learning of Visual Representations,"Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a ""ring"" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.","['MoCo', 'hard negative mining', 'lower bound', 'contrastive learning', 'detection', 'segmentation', 'mutual information']",[],"['Mike Wu', 'Chengxu Zhuang', 'Noah Goodman']","['Stanford University', 'Massachusetts Institute of Technology', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/3235,Fairness & Bias,Does enhanced shape bias improve neural network robustness to common corruptions?,"Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation  accounts for the improved corruption robustness and increased shape bias is only a byproduct.","['corruptions', 'shape bias', 'neural network robustness', 'distribution shift']",[],"['Chaithanya Kumar Mummadi', 'Ranjitha Subramaniam', 'Robin Hutmacher', 'Julien Vitay', 'Volker Fischer', 'Jan Hendrik Metzen']","['Bosch Center for Artificial Intelligence', 'Chemnitz University of Technology', 'Robert Bosch GmbH, Bosch', 'Chemnitz University of Technology', 'Bosch Center for Artificial Intelligence', 'Bosch Center Artificial Intelligence']",[]
https://iclr.cc/virtual/2021/poster/3213,Fairness & Bias,Random Feature Attention,"Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA’s efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.","['attention', 'transformers', 'language modeling', 'machine translation']",[],"['Hao Peng', 'Nikolaos Pappas', 'Dani Yogatama', 'Roy Schwartz', 'Noah A. Smith', 'Lingpeng Kong']","['Department of Computer Science,  University of Illinois Urbana-Champaign', 'AWS AI Labs', 'Google DeepMind', 'Hebrew University, Hebrew University of Jerusalem', 'University of Washington', 'Department of Computer Science, The University of Hong Kong']",[]
https://iclr.cc/virtual/2021/poster/3203,Fairness & Bias,Practical Real Time Recurrent Learning with a Sparse Approximation,"Recurrent neural networks are usually trained with backpropagation through time, which requires storing a complete history of network states, and prohibits updating the weights ""online"" (after every timestep). Real Time Recurrent Learning (RTRL) eliminates the need for history storage and allows for online weight updates, but does so at the expense of computational costs that are quartic in the state size. This renders RTRL training intractable for all but the smallest networks, even ones that are made highly sparse. We introduce the Sparse n-step Approximation (SnAp) to the RTRL influence matrix. SnAp only tracks the influence of a parameter on hidden units that are reached by the computation graph within $n$ timesteps of the recurrent core. SnAp with $n=1$ is no more expensive than backpropagation but allows training on arbitrarily long sequences. We find that it substantially outperforms other RTRL approximations with comparable costs such as Unbiased Online Recurrent Optimization. For highly sparse networks, SnAp with $n=2$ remains tractable and can outperform backpropagation through time in terms of learning speed when updates are done online.","['bptt', 'rtrl', 'real time recurrent learning', 'forward mode', 'biologically plausible', 'backpropagation', 'recurrent neural networks']",[],"['Jacob Menick', 'Erich Elsen', 'Utku Evci', 'Simon Osindero', 'Karen Simonyan']","['University College London', 'Royal Caliber', 'Google', 'Google', 'Inflection AI']",[]
https://iclr.cc/virtual/2021/poster/3181,Fairness & Bias,Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization,"Flow-based models are powerful tools for designing probabilistic models with tractable density. This paper introduces Convex Potential Flows (CP-Flow), a natural and efficient parameterization of invertible models inspired by the optimal transport (OT) theory. CP-Flows are the gradient map of a strongly convex neural potential function. The convexity implies invertibility and allows us to resort to convex optimization to solve the convex conjugate for efficient inversion. To enable maximum likelihood training, we derive a new gradient estimator of the log-determinant of the Jacobian, which involves solving an inverse-Hessian vector product using the conjugate gradient method. The gradient estimator has constant-memory cost, and can be made effectively unbiased by reducing the error tolerance level of the convex optimization routine. Theoretically, we prove that CP-Flows are universal density approximators and are optimal in the OT sense. Our empirical results show that CP-Flow performs competitively on standard benchmarks of density estimation and variational inference.","['universal approximation', 'Normalizing flows', 'invertible neural networks', 'convex optimization', 'optimal transport', 'variational inference', 'generative models']",[],"['Chin-Wei Huang', 'Ricky T. Q. Chen', 'Christos Tsirigotis', 'Aaron Courville']","['Microsoft', 'FAIR Labs, Meta AI', 'ServiceNow Research', 'University of Montreal']",[]
https://iclr.cc/virtual/2021/poster/3180,Fairness & Bias,Shape or Texture: Understanding Discriminative Features in CNNs,"Contrasting the previous evidence that neurons in the later layers of a Convolutional Neural Network (CNN) respond to complex object shapes, recent studies have shown that CNNs actually exhibit a 'texture bias': given an image with both texture and shape cues (e.g., a stylized image), a CNN is biased towards predicting the category corresponding to the texture. However, these previous studies conduct experiments on the final classification output of the network, and fail to robustly evaluate the bias contained (i) in the latent representations, and (ii) on a per-pixel level. In this paper, we design a series of experiments that overcome these issues. We do this with the goal of better understanding what type of shape information contained in the network is discriminative, where shape information is encoded, as well as when the network learns about object shape during training. We show that a network learns the majority of overall shape information at the first few epochs of training and that this information is largely encoded in the last few layers of a CNN. Finally, we show that the encoding of shape does not imply the encoding of localized per-pixel semantic information. The experimental results and findings provide a more accurate understanding of the behaviour of current CNNs, thus helping to inform future design choices.","['Shape Encoding', 'Texture Bias', 'shape bias', 'Texture', 'Shape', 'mutual information']",[],"['Md Amirul Islam', 'Matthew Kowal', 'Patrick Esser', 'Sen Jia', 'Björn Ommer', 'Konstantinos G. Derpanis', 'Neil Bruce']","['Ryerson University', 'York University', 'Heidelberg University', 'Toronto AI Lab, LG Electronics', 'Ludwig-Maximilians-Universität München', 'York University', 'University of Guelph']",[]
https://iclr.cc/virtual/2021/poster/3150,Fairness & Bias,Implicit Gradient Regularization,"Gradient descent can be surprisingly good at optimizing deep neural networks without overfitting and without explicit regularization. We find that the discrete steps of gradient descent implicitly regularize models by penalizing gradient descent trajectories that have large loss gradients. We call this Implicit Gradient Regularization (IGR) and we use backward error analysis to calculate the size of this regularization. We confirm empirically that implicit gradient regularization biases gradient descent toward flat minima, where test errors are small and solutions are robust to noisy parameter perturbations. Furthermore, we demonstrate that the implicit gradient regularization term can be used as an explicit regularizer, allowing us to control this gradient regularization directly. More broadly, our work indicates that backward error analysis is a useful theoretical approach to the perennial question of how learning rate, model size, and parameter regularization interact to determine the properties of overparameterized models optimized with gradient descent.","['theoretical issues in deep learning', 'deep learning theory', 'regularization', 'implicit regularization', 'theory', 'deep learning']",[],"['David GT Barrett', 'Benoit Dherin']","['Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/3149,Fairness & Bias,Variational Intrinsic Control Revisited,"In this paper, we revisit variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. We show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior, we propose two methods respectively based on the transitional probability model and Gaussian Mixture Model. We substantiate our claims through rigorous mathematical derivations and experimental analyses.","['information theory', 'Unsupervised reinforcement learning']",[],['Taehwan Kwon'],['Korea Advanced Institute of Science and Technology'],[]
https://iclr.cc/virtual/2021/poster/3138,Fairness & Bias,Learning to Sample with Local and Global Contexts  in Experience Replay Buffer,"Experience replay, which enables the agents to remember and reuse experience from the past, has played a significant role in the success of off-policy reinforcement learning (RL). To utilize the experience replay efficiently, the existing sampling methods allow selecting out more meaningful experiences by imposing priorities on them based on certain metrics (e.g. TD-error). However, they may result in sampling highly biased, redundant transitions since they compute the sampling rate for each transition independently, without consideration of its importance in relation to other transitions. In this paper, we aim to address the issue by proposing a new learning-based sampling method that can compute the relative importance of transition. To this end, we design a novel permutation-equivariant neural architecture that takes contexts from not only features of each transition (local) but also those of others (global) as inputs. We validate our framework, which we refer to as Neural Experience Replay Sampler (NERS), on multiple benchmark tasks for both continuous and discrete control tasks and show that it can significantly improve the performance of various off-policy RL methods. Further analysis confirms that the improvements of the sample efficiency indeed are due to sampling diverse and meaningful transitions by NERS that considers both local and global contexts.","['off-policy RL', 'experience replay buffer', 'reinforcement learning']",[],"['Youngmin Oh', 'Kimin Lee', 'Jinwoo Shin', 'Eunho Yang', 'Sung Ju Hwang']","['Pohang University of Science and Technology', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology', 'KAIST', 'Korea Advanced Institute of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/3119,Fairness & Bias,A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning,"Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed computing systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-$K$ or PowerSGD.  In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.","['distributed optimization', 'communication efficiency']",[],"['Samuel Horváth', 'Peter Richtárik']","['Mohamed bin Zayed University of Artificial Intelligence', 'King Abdullah University of Science and Technology (KAUST)']",[]
https://iclr.cc/virtual/2021/poster/3104,Fairness & Bias,When does preconditioning help or hurt generalization?,"While second order optimizers such as natural gradient descent (NGD) often speed up optimization, their effect on generalization has been called into question. This work presents a more nuanced view on how the \textit{implicit bias} of optimizers affects the comparison of generalization properties.  We provide an exact asymptotic bias-variance decomposition of the generalization error of preconditioned ridgeless regression in the overparameterized regime, and consider the inverse population Fisher information matrix (used in NGD) as a particular example. We determine the optimal preconditioner $\boldsymbol{P}$ for both the bias and variance, and find that the relative generalization performance of different optimizers depends on label noise and ``shape'' of the signal (true parameters): when the labels are noisy, the model is misspecified, or the signal is misaligned with the features, NGD can achieve lower risk; conversely, GD generalizes better under clean labels, a well-specified model, or aligned signal. Based on this analysis, we discuss several approaches to manage the bias-variance tradeoff, and the potential benefit of interpolating between first- and second-order updates. We then extend our analysis to regression in the reproducing kernel Hilbert space and demonstrate that preconditioning can lead to more efficient decrease in the population risk. Lastly, we empirically compare the generalization error of first- and second-order optimizers in neural network experiments, and observe robust trends matching our theoretical analysis.","['high-dimensional asymptotics', 'natural gradient descent', 'second-order optimization', 'generalization']",[],"['Jimmy Ba', 'Roger Baker Grosse', 'Xuechen Li', 'Atsushi Nitanda', 'Taiji Suzuki', 'Denny Wu', 'Ji Xu']","['Department of Computer Science, University of Toronto', 'Department of Computer Science, University of Toronto', 'Computer Science Department, Stanford University', 'A*STAR', 'The University of Tokyo', 'New York University', 'Columbia University']",[]
https://iclr.cc/virtual/2021/poster/3098,Fairness & Bias,Predicting Inductive Biases of Pre-Trained Models,"Most current NLP systems are based on a pre-train-then-fine-tune paradigm, in which a large neural network is first trained in a self-supervised way designed to encourage the network to extract broadly-useful linguistic features, and then fine-tuned for a specific task of interest. Recent work attempts to understand why this recipe works and explain when it fails. Currently, such analyses have produced two sets of apparently-contradictory results. Work that analyzes the representations that result from pre-training (via ""probing classifiers"") finds evidence that rich features of linguistic structure can be decoded with high accuracy, but work that analyzes model behavior after fine-tuning (via ""challenge sets"") indicates that decisions are often not based on such structure but rather on spurious heuristics specific to the training set. In this work, we test the hypothesis that the extent to which a feature influences a model's decisions can be predicted using a combination of two factors: The feature's ""extractability"" after pre-training (measured using information-theoretic probing techniques), and the ""evidence"" available during fine-tuning (defined as the feature's co-occurrence rate with the label). In experiments with both synthetic and natural language data, we find strong evidence (statistically significant correlations) supporting this hypothesis.","['challenge sets', 'probing', 'information-theoretical probing', 'natural language processing']",[],"['Charles Lovering', 'Rohan Jha', 'Tal Linzen', 'Ellie Pavlick']","['Brown University', 'Brown University', 'New York University', 'Brown University']",[]
https://iclr.cc/virtual/2021/poster/3095,Fairness & Bias,Multi-timescale Representation Learning in LSTM Language Models,"Language models must capture statistical dependencies between words at timescales ranging from very short to very long. Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory (LSTM) language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency (rare) words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models.","['timescales', 'language model', 'lstm']",[],"['Vy A. Vo', 'Javier S. Turek', 'Alexander Huth']","['Intel', 'Intel', 'The University of Texas at Austin']",[]
https://iclr.cc/virtual/2021/poster/3090,Fairness & Bias,Prototypical Contrastive Learning of Unsupervised Representations,"This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.","['contrastive learning', 'self-supervised learning', 'representation learning', 'unsupervised learning']",[],"['Junnan Li', 'Caiming Xiong', 'Steven Hoi']","['Salesforce Research', 'Salesforce Research', 'Salesforce Research Asia']",[]
https://iclr.cc/virtual/2021/poster/3089,Fairness & Bias,Learning from others' mistakes: Avoiding dataset biases without modeling them,"State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.","['product of experts', 'dataset bias', 'natural language processing']",[],"['Victor Sanh', 'Thomas Wolf', 'Yonatan Belinkov', 'Alexander M Rush']","['Hugging Face', 'Hugging Face', 'Technion, Technion', 'Cornell University']",[]
https://iclr.cc/virtual/2021/poster/3086,Fairness & Bias,Neural representation and generation for RNA secondary structures,"Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.","['RNA-protein interaction prediction', 'RNA structure embedding', 'RNA structure', 'drug discovery', 'machine learning', 'Deep generative modeling', 'graph neural network']",[],"['Zichao Yan', 'William L. Hamilton', 'Mathieu Blanchette']","['Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'McGill University', 'McGill University']",[]
https://iclr.cc/virtual/2021/poster/3083,Fairness & Bias,Evaluations and Methods for Explanation through Robustness Analysis,"Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to ""remove"" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.","['adversarial robustness', 'Explanations', 'interpretability']",[],"['Cheng-Yu Hsieh', 'Chih-Kuan Yeh', 'Xuanqing Liu', 'Pradeep Kumar Ravikumar', 'Seungyeon Kim', 'Sanjiv Kumar', 'Cho-Jui Hsieh']","['University of Washington', 'Google', 'University of California, Los Angeles', 'Carnegie Mellon University', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/3039,Fairness & Bias,Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time,"We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.","['polynomial time', 'convex duality', '$\\ell_1$ norm', 'group sparsity', 'convex optimization', 'non-convex optimization', 'deep learning']",[],"['Tolga Ergen', 'Mert Pilanci']","['LG AI Research', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/3037,Fairness & Bias,On Statistical Bias In Active Learning: How and When to Fix It,"Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneficial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation for the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models---like neural networks---with relatively modest dataset sizes.","['Risk Estimation', 'monte carlo', 'active learning']",[],"['Sebastian Farquhar', 'Yarin Gal', 'Tom Rainforth']","['DeepMind', 'University of Oxford', 'University of Oxford']",[]
https://iclr.cc/virtual/2021/poster/3031,Fairness & Bias,Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning,"The lottery ticket hypothesis states that a highly sparsified sub-network can be trained in isolation, given the appropriate weight initialization. This paper extends that hypothesis from one-shot task learning, and demonstrates for the first time that such extremely compact and independently trainable sub-networks can be also identified in the lifelong learning scenario, which we call lifelong tickets. We show that the resulting lifelong ticket can further be leveraged to improve the performance of learning over continual tasks. However, it is highly non-trivial to conduct network pruning in the lifelong setting. Two critical roadblocks arise: i) As many tasks now arrive sequentially, finding tickets in a greedy weight pruning fashion will inevitably suffer from the intrinsic bias, that the earlier emerging tasks impact more; ii) As lifelong learning is consistently challenged by catastrophic forgetting, the compact network capacity of tickets might amplify the risk of forgetting. In view of those, we introduce two pruning options, e.g., top-down and bottom-up, for finding lifelong tickets. Compared to the top-down pruning that extends vanilla (iterative) pruning over sequential tasks, we show that the bottom-up one, which can dynamically shrink and (re-)expand model capacity, effectively avoids the undesirable excessive pruning in the early stage. We additionally introduce lottery teaching that further overcomes forgetting via knowledge distillation aided by external unlabeled data. Unifying those ingredients, we demonstrate the existence of very competitive lifelong tickets, e.g., achieving 3-8% of the dense model size with even higher accuracy, compared to strong class-incremental learning baselines on CIFAR-10/CIFAR-100/Tiny-ImageNet datasets. Codes available at https://github.com/VITA-Group/Lifelong-Learning-LTH.","['winning tickets', 'lottery tickets', 'lifelong learning']",[],"['Tianlong Chen', 'Zhenyu Zhang', 'Sijia Liu', 'Shiyu Chang', 'Zhangyang Wang']","['Massachusetts Institute of Technology', 'University of Texas at Austin', 'Michigan State University', 'UC Santa Barbara', 'University of Texas at Austin']",[]
https://iclr.cc/virtual/2021/poster/3023,Fairness & Bias,Unsupervised Object Keypoint Learning using Local Spatial Predictability,"We propose PermaKey, a novel approach to representation learning based on object keypoints. It leverages the predictability of local image regions from spatial neighborhoods to identify salient regions that correspond to object parts, which are then converted to keypoints. Unlike prior approaches, it utilizes predictability to discover object keypoints, an intrinsic property of objects. This ensures that it does not overly bias keypoints to focus on characteristics that are not unique to objects, such as movement, shape, colour etc.  We demonstrate the efficacy of PermaKey on Atari where it learns keypoints corresponding to the most salient object parts and is robust to certain visual distractors. Further, on downstream RL tasks in the Atari domain we demonstrate how agents equipped with our keypoints outperform those using competing alternatives, even on challenging environments with moving backgrounds or distractor objects.","['visual saliency', 'object-keypoint representations', 'unsupervised representation learning']",[],"['Anand Gopalakrishnan', 'Sjoerd van Steenkiste', 'Jürgen Schmidhuber']","['Dalle Molle Institute for Artificial Intelligence Research', 'Google', 'King Abdullah University of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/3018,Fairness & Bias,Long-tailed Recognition by Routing Diverse Distribution-Aware Experts,"Natural data are often long-tail distributed over semantic classes. Existing recognition methods tend to focus on gaining performance on tail classes, often at the expense of losing performance on head classes and with increased classifier variance. The low tail performance manifests itself in large inter-class confusion and high classifier variance. We aim to reduce both the bias and the variance of a long-tailed classifier by RoutIng Diverse Experts (RIDE), consisting of three components: 1) a shared architecture for multiple classifiers (experts); 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; and 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts.  With on-par computational complexity, RIDE significantly outperforms the state-of-the-art methods by 5% to 7% on all the benchmarks including CIFAR100-LT, ImageNet-LT, and iNaturalist 2018. RIDE is also a universal framework that can be applied to different backbone networks and integrated into various long-tailed algorithms and training mechanisms for consistent performance gains. Our code is publicly available at https://github.com/frank-xwang/RIDE-LongTailRecognition.",[],[],"['Xudong Wang', 'Long Lian', 'Zhongqi Miao', 'Ziwei Liu', 'Stella X. Yu']","['Electrical Engineering & Computer Science Department, University of California Berkeley', 'University of California, Berkeley', 'Microsoft', 'Nanyang Technological University', 'University of Michigan - Ann Arbor']",[]
https://iclr.cc/virtual/2021/poster/3006,Fairness & Bias,What they do when in doubt: a study of inductive biases in seq2seq learners,"Sequence-to-sequence (seq2seq) learners are widely used, but we still have only limited knowledge about what inductive biases shape the way they generalize. We address that by investigating how popular seq2seq learners generalize in tasks that have high ambiguity in the training data. We use four new tasks  to study learners' preferences for memorization, arithmetic, hierarchical, and compositional reasoning. Further, we connect to Solomonoff's theory of induction and propose to use description length as a principled and sensitive measure of inductive biases. In our experimental study, we find that LSTM-based learners can learn to perform counting, addition, and multiplication by a constant from a single training example. Furthermore, Transformer and LSTM-based learners show a bias toward the hierarchical induction over the linear one, while CNN-based learners prefer the opposite. The latter also show a bias toward a compositional generalization over memorization. Finally, across all our experiments, description length proved to be a sensitive measure of inductive biases.","['sequence-to-sequence models', 'description length', 'inductive biases']",[],"['Eugene Kharitonov', 'Rahma Chaabouni']","['Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2960,Fairness & Bias,Aligning AI With Shared Human Values,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.","['alignment', 'human preferences', 'value learning']",[],"['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andrew Critch', 'Jerry Li', 'Dawn Song', 'Jacob Steinhardt']","['UC Berkeley', 'University of California Berkeley', 'Center for AI Safety ', 'University of California Berkeley', 'Microsoft', 'University of California Berkeley', 'University of California Berkeley']",[]
https://iclr.cc/virtual/2021/poster/2959,Fairness & Bias,Learning Manifold Patch-Based Representations of Man-Made Shapes,"Choosing the right representation for geometry is crucial for making 3D models compatible with existing applications. Focusing on piecewise-smooth man-made shapes, we propose a new representation that is usable in conventional CAD modeling pipelines and can also be learned by deep neural networks. We demonstrate its benefits by applying it to the task of sketch-based modeling. Given a raster image, our system infers a set of parametric surfaces that realize the input in 3D. To capture piecewise smooth geometry, we learn a special shape representation: a deformable parametric template composed of Coons patches. Naively training such a system, however, is hampered by non-manifold artifacts in the parametric shapes and by a lack of data. To address this, we introduce loss functions that bias the network to output non-self-intersecting shapes and implement them as part of a fully self-supervised system, automatically generating both shape templates and synthetic training data. We develop a testbed for sketch-based modeling, demonstrate shape interpolation, and provide comparison to related work.","['computer graphics', 'sketch-based modeling', 'CAD modeling', '3D shape representations', 'computer vision', 'deep learning']",[],"['Dmitriy Smirnov', 'Mikhail Bessmeltsev', 'Justin Solomon']","['Netflix Research', 'University of Montreal', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2951,Fairness & Bias,Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online,"Recent work has shown that sparse representations---where only a small percentage of units are active---can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In this work, we pursue a direction that achieves sparsity by design, rather than by learning. Specifically, we design an activation function that produces sparse representations deterministically by construction, and so is more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere,  and lost precision---reduced discrimination---due to coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We first show that FTA is robust under covariate shift in a synthetic online supervised learning problem, where we can vary the level of correlation and drift. Then we move to the deep reinforcement learning setting and investigate both value-based and policy gradient algorithms that use neural networks with FTAs, in classic discrete control and Mujoco continuous control environments. We show that algorithms equipped with FTAs are able to learn a stable policy faster without needing target networks on most domains.","['fuzzy tiling activation function', 'sparse representation', 'natural sparsity', 'reinforcement learning']",[],"['Yangchen Pan', 'Kirby Banman', 'Martha White']","['University of Oxford', 'University of Alberta', 'University of Alberta']",[]
https://iclr.cc/virtual/2021/poster/2942,Fairness & Bias,My Body is a Cage: the Role of Morphology in Graph-Based Incompatible Control,"Multitask Reinforcement Learning is a promising way to obtain models with better performance, generalisation, data efficiency, and robustness. Most existing work is limited to compatible settings, where the state and action space dimensions are the same across tasks. Graph Neural Networks (GNN) are one way to address incompatible environments, because they can process graphs of arbitrary size. They also allow practitioners to inject biases encoded in the structure of the input graph. Existing work in graph-based continuous control uses the physical morphology of the agent to construct the input graph, i.e., encoding limb features as node labels and using edges to connect the nodes if their corresponded limbs are physically connected. In this work, we present a series of ablations on existing methods that show that morphological information encoded in the graph does not improve their performance. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, we also propose Amorpheus, a transformer-based approach. Further results show that, while Amorpheus ignores the morphological information that GNNs encode, it nonetheless substantially outperforms GNN-based methods.","['Incompatible Environments', 'continuous control', 'Multitask Reinforcement Learning', 'deep reinforcement learning', 'graph neural networks']",[],"['Vitaly Kurin', 'Maximilian Igl', 'Tim Rocktäschel', 'Wendelin Boehmer', 'Shimon Whiteson']","['University of Oxford', 'University of Oxford', 'Department of Computer Science, University College London, University of London', 'Delft University of Technology', 'University of Oxford']",[]
https://iclr.cc/virtual/2021/poster/2937,Fairness & Bias,Semantic Re-tuning with Contrastive Tension,"Extracting semantically useful natural language sentence representations from pre-trained deep neural networks such as Transformers remains a challenge. We first demonstrate that pre-training objectives impose a significant task bias onto the final layers of models with a layer-wise survey of the Semantic Textual Similarity (STS) correlations for multiple common Transformer language models. We then propose a new self-supervised method called Contrastive Tension (CT) to counter such biases. CT frames the training objective as a noise-contrastive task between the final layer representations of two independent models, in turn making the final layer representations suitable for feature extraction. Results from multiple common unsupervised and supervised STS tasks indicate that CT outperforms previous State Of The Art (SOTA), and when combining CT with supervised data we improve upon previous SOTA results with large margins.","['Fine-tuning', 'pre-training', 'sentence representations', 'sentence embeddings', 'language modelling', 'Semantic Textual Similarity', 'transformers']",[],"['Fredrik Carlsson', 'Amaru Cuba Gyllensten', 'Evangelia Gogoulou', 'Erik Ylipää Hellqvist', 'Magnus Sahlgren']","['KTH Royal Institute of Technology, Stockholm, Sweden', 'KTH Royal Institute of Technology, Stockholm, Sweden', 'RISE Research Institutes of Sweden', 'Uppsala University', 'AI Sweden']",[]
https://iclr.cc/virtual/2021/poster/2935,Fairness & Bias,On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers,"A deep equilibrium model uses implicit layers, which are implicitly defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature.","['non-convex optimization', 'deep equilibrium models', 'Implicit Deep Learning', 'gradient descent', 'learning theory']",[],['Kenji Kawaguchi'],['National University of Singapore'],[]
https://iclr.cc/virtual/2021/poster/2897,Fairness & Bias,Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding,"Time series are often complex and rich in information but sparsely labeled and therefore challenging to model. In this paper, we propose a self-supervised framework for learning robust and generalizable representations for time series. Our approach, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal's generative process to define neighborhoods in time with stationary properties. Using a debiased contrastive objective, our framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distribution of non-neighboring signals. Our motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients' latent states in settings where labeling data is practically impossible. We compare our method to recently developed unsupervised representation learning approaches and demonstrate superior performance on clustering and classification tasks for multiple datasets.",[],[],"['Sana Tonekaboni', 'Danny Eytan', 'Anna Goldenberg']","['Department of Computer Science, University of Toronto', 'Technion, Technion', 'University of Toronto']",[]
https://iclr.cc/virtual/2021/poster/2892,Fairness & Bias,Counterfactual Generative Networks,"Neural networks are prone to learning shortcuts -- they often model simple correlations, ignoring more complex ones that potentially generalize better. Prior works on image classification show that instead of learning a connection to object shape, deep classifiers tend to exploit spurious correlations with low-level texture or the background for solving the classification task. In this work, we take a step towards more robust and interpretable classifiers that explicitly expose the task's causal structure. Building on current advances in deep generative modeling, we propose to decompose the image generation process into independent causal mechanisms that we train without direct supervision. By exploiting appropriate inductive biases, these mechanisms disentangle object shape, object texture, and background; hence, they allow for generating counterfactual images. We demonstrate the ability of our model to generate such images on MNIST and ImageNet. Further, we show that the counterfactual images can improve out-of-distribution robustness with a marginal drop in performance on the original classification task, despite being synthetic. Lastly, our generative model can be trained efficiently on a single GPU, exploiting common pre-trained models as inductive biases.","['counterfactuals', 'image classification', 'data augmentation', 'causality', 'generative models', 'robustness']",[],"['Axel Sauer', 'Andreas Geiger']","['Stability AI', 'University of Tuebingen']",[]
https://iclr.cc/virtual/2021/poster/2889,Fairness & Bias,The inductive bias of ReLU networks on orthogonally separable data,"We study the inductive bias of two-layer ReLU networks trained by gradient flow. We identify a class of easy-to-learn (`orthogonally separable') datasets, and characterise the solution that ReLU networks trained on such datasets converge to. Irrespective of network width, the solution turns out to be a combination of two max-margin classifiers: one corresponding to the positive data subset and one corresponding to the negative data subset. The proof is based on the recently introduced concept of extremal sectors, for which we prove a number of properties in the context of orthogonal separability. In particular, we prove stationarity of activation patterns from some time $T$ onwards, which enables a reduction of the ReLU network to an ensemble of linear subnetworks.","['extremal sector', 'max-margin', 'ReLU networks', 'implicit bias', 'inductive bias', 'gradient descent']",[],"['Mary Phuong', 'Christoph H. Lampert']","['IST Austria', 'Institute of Science and Technology Austria']",[]
https://iclr.cc/virtual/2021/poster/2872,Fairness & Bias,Blending MPC & Value Function Approximation for Efficient Reinforcement Learning,"Model-Predictive Control (MPC) is a powerful tool for controlling complex, real-world systems that uses a model to make predictions about future behavior. For each state encountered, MPC solves an online optimization problem to choose a control action that will minimize future cost. This is a surprisingly effective strategy, but real-time performance requirements warrant the use of simple models. If the model is not sufficiently accurate, then the resulting controller can be biased, limiting performance. We present a framework for improving on MPC with model-free reinforcement learning (RL). The key insight is to view MPC as constructing a series of local Q-function approximations. We show that by using a parameter $\lambda$, similar to the trace decay parameter in TD($\lambda$), we can systematically trade-off learned value estimates against the local Q-function approximations. We present a theoretical analysis that shows how error from inaccurate models in MPC and value function estimation in RL can be balanced. We further propose an algorithm that changes $\lambda$ over time to reduce the dependence on MPC as our estimates of the value function improve, and test the efficacy our approach on challenging high-dimensional manipulation tasks with biased models in simulation. We demonstrate that our approach can obtain performance comparable with MPC with access to true dynamics even under severe model bias and is more sample efficient as compared to model-free RL.","['model-predictive control', 'reinforcement learning']",[],"['Sanjiban Choudhury', 'Byron Boots']","['Department of Computer Science, University of Washington', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2870,Fairness & Bias,Shape-Texture Debiased Neural Network Training,"Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously.Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.","['debiased training', 'data augmentation', 'representation learning']",[],"['Yingwei Li', 'Qihang Yu', 'Mingxing Tan', 'Jieru Mei', 'Peng Tang', 'Wei Shen', 'Alan Yuille', 'Cihang Xie']","['Waymo LLC', 'ByteDance', 'Google', 'Johns Hopkins University', 'Amazon', 'Shanghai Jiao Tong University', 'Johns Hopkins University', 'University of California, Santa Cruz']",[]
https://iclr.cc/virtual/2021/poster/2852,Fairness & Bias,Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking,"Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.","['semantic role labeling', 'sparse stochastic gates', 'graph neural networks', 'interpretability', 'question answering']",[],"['Michael Sejr Schlichtkrull', 'Nicola De Cao', 'Ivan Titov']","['University of Cambridge', 'Google', 'University of Edinburgh']",[]
https://iclr.cc/virtual/2021/poster/2842,Fairness & Bias,Separation and Concentration in Deep Networks,"Numerical experiments demonstrate that deep neural network classifiers progressively separate class distributions around their mean, achieving linear separability on the training set, and increasing the Fisher discriminant ratio. We explain this mechanism with two types of operators. We prove that a rectifier without biases applied to sign-invariant tight frames can separate class means and increase Fisher ratios. On the opposite, a soft-thresholding on tight frames can reduce within-class variabilities while preserving class means. Variance reduction bounds are proved for Gaussian mixture models. For image classification, we show that separation of class means can be achieved with rectified wavelet tight frames that are not learned. It defines a scattering transform. Learning  $1 \times 1$ convolutional tight frames along scattering channels and applying a soft-thresholding reduces within-class variabilities. The resulting scattering network reaches the classification accuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no learned biases.","['concentration', 'mean separation', 'neural collapse', 'fisher ratio', 'variance reduction', 'image classification', 'deep learning']",[],"['John Zarka', 'Florentin Guth', 'Stéphane Mallat']","['Ecole Normale Superieure', 'New York University', 'College de France']",[]
https://iclr.cc/virtual/2021/poster/2828,Fairness & Bias,What Makes Instance Discrimination Good for Transfer Learning?,"Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.","['transfer learning', 'unsupervised learning', 'self-supervised learning']",[],"['Nanxuan Zhao', 'Zhirong Wu', 'Rynson W. H. Lau', 'Stephen Lin']","['Adobe Research', 'Microsoft Research', 'City University of Hong Kong', 'Microsoft Research']",[]
https://iclr.cc/virtual/2021/poster/2826,Fairness & Bias,A unifying view on implicit bias in training linear neural networks,"We study the implicit bias of gradient flow (i.e., gradient descent with infinitesimal step size) on linear neural network training. We propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. With this formulation, we can characterize the convergence direction of the network parameters as singular vectors of a tensor defined by the network. For $L$-layer linear tensor networks that are orthogonally decomposable, we show that gradient flow on separable classification finds a stationary point of the $\ell_{2/L}$ max-margin problem in a ""transformed"" input space defined by the network. For underdetermined regression, we prove that gradient flow finds a global minimum which minimizes a norm-like function that interpolates between weighted $\ell_1$ and $\ell_2$ norms in the transformed input space. Our theorems subsume existing results in the literature while removing standard convergence assumptions. We also provide experiments that corroborate our analysis.","['gradient flow', 'implicit bias', 'convergence', 'implicit regularization', 'gradient descent']",[],"['Chulhee Yun', 'Shankar Krishnan', 'Hossein Mobahi']","['Korea Advanced Institute of Science & Technology', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2821,Fairness & Bias,Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with Moderate Learning Rate,"Understanding the algorithmic bias of stochastic gradient descent (SGD) is one of the key challenges in modern machine learning and deep learning theory. Most of the existing works, however, focus on very small or even infinitesimal learning rate regime, and fail to cover practical scenarios where the learning rate is moderate and annealing. In this paper, we make an initial attempt to characterize the particular regularization effect of SGD in the moderate learning rate regime by studying its behavior for optimizing an overparameterized linear regression problem. In this case, SGD and GD are known to converge to the unique minimum-norm solution; however, with the moderate and annealing learning rate, we show that they exhibit different directional bias: SGD converges along the large eigenvalue directions of the data matrix, while GD goes after the small eigenvalue directions. Furthermore, we show that such directional bias does matter when early stopping is adopted, where the SGD output is nearly optimal but the GD output is suboptimal. Finally, our theory explains several folk arts in practice used for SGD hyperparameter tuning, such as (1) linearly scaling the initial learning rate with batch size; and (2) overrunning SGD with high learning rate even when the loss stops decreasing.","['implicit bias', 'sgd', 'regularization']",[],"['Jingfeng Wu', 'Difan Zou', 'Vladimir Braverman', 'Quanquan Gu']","['University of California, Berkeley', 'University of Hong Kong', 'Rice University', 'University of California, Los Angeles']",[]
https://iclr.cc/virtual/2021/poster/2816,Fairness & Bias,Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective,"Neural Architecture Search (NAS) has been explosively studied to automate the discovery of top-performer neural networks. Current works require heavy training of supernet or intensive architecture evaluations, thus suffering from heavy resource consumption and often incurring search bias due to truncated training or approximations. Can we select the best neural architectures without involving any training and eliminate a drastic portion of the search cost? We provide an affirmative answer, by proposing a novel framework called \textit{training-free neural architecture search} ($\textbf{TE-NAS}$). TE-NAS ranks architectures by analyzing the spectrum of the neural tangent kernel (NTK), and the number of linear regions in the input space. Both are motivated by recent theory advances in deep networks, and can be computed without any training. We show that: (1) these two measurements imply the $\textit{trainability}$ and $\textit{expressivity}$ of a neural network; and (2) they strongly correlate with the network's actual test accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more flexible and superior trade-off between the trainability and expressivity during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes high-quality search but only costs $\textbf{0.5}$ and $\textbf{4}$ GPU hours with one 1080Ti on CIFAR-10 and ImageNet, respectively. We hope our work to inspire more attempts in bridging between the theoretic findings of deep networks and practical impacts in real NAS applications.","['number of linear regions', 'neural tangent kernel', 'neural architecture search']",[],"['Wuyang Chen', 'Xinyu Gong', 'Zhangyang Wang']","['University of Texas, Austin', 'University of Texas, Austin', 'University of Texas at Austin']",[]
https://iclr.cc/virtual/2021/poster/2807,Fairness & Bias,Rethinking Soft Labels for Knowledge Distillation: A Bias–Variance Tradeoff Perspective,"Knowledge distillation is an effective approach to leverage a well-trained network or an ensemble of them, named as the teacher, to guide the training of a student network.  The outputs from the teacher network are used as soft labels for supervising the training of a new network.  Recent studies (M ̈uller et al., 2019; Yuan et al., 2020) revealed an intriguing property of the soft labels that making labels soft serves as a good regularization to the student network.   From the perspective of statistical learning,  regularization aims to reduce the variance,  however how bias and variance change is not clear for training with soft labels.   In this paper, we investigate the bias-variance tradeoff brought by distillation with soft labels.   Specifically,  we observe that during training the bias-variance tradeoff varies sample-wisely. Further, under the same distillation temperature setting, we observe that the distillation performance is negatively associated with the number of some specific samples, which are named as regularization samples since these samples lead to bias increasing and variance decreasing.  Nevertheless, we empirically find that completely filtering out regularization samples also deteriorates distillation performance.  Our discoveries inspired us to propose the novel weighted soft labels to help the network adaptively handle the sample-wise bias-variance tradeoff.  Experiments on standard evaluation benchmarks validate the effectiveness of our method. Our code is available in the supplementary.","['teacher-student model', 'soft labels', 'knowledge distillation']",[],"['Helong Zhou', 'Jiajie Chen', 'Ye Zhou', 'Guoli Wang', 'Junsong Yuan', 'Qian Zhang']","['Horizon Robotics', 'Horizon Robotics', 'Horizon Robotics', 'Tsinghua University, Tsinghua University', 'State University of New York at Buffalo', 'Horizon Robotics']",[]
https://iclr.cc/virtual/2021/poster/2795,Fairness & Bias,A Block Minifloat Representation for Training Deep Neural Networks,"Training Deep Neural Networks (DNN) with high efficiency can be difficult to achieve with native floating-point representations and commercially available hardware. Specialized arithmetic with custom acceleration offers perhaps the most promising alternative. Ongoing research is trending towards narrow floating-point representations, called minifloats, that pack more operations for a given silicon area and consume less power. In this paper, we introduce Block Minifloat (BM), a new spectrum of minifloat formats capable of training DNNs end-to-end with only 4-8 bit weight, activation and gradient tensors. While standard floating-point representations have two degrees of freedom, via the exponent and mantissa, BM exposes the exponent bias as an additional field for optimization. Crucially, this enables training with fewer exponent bits, yielding dense integer-like hardware for fused multiply-add (FMA) operations. For ResNet trained on ImageNet, 6-bit BM achieves almost no degradation in floating-point accuracy with FMA units that are $4.1\times(23.9\times)$ smaller and consume $2.3\times(16.1\times)$ less energy than FP8 (FP32). Furthermore, our 8-bit BM format matches floating-point accuracy while delivering a higher computational density and faster expected training times.",[],[],"['Sean Fox', 'Seyedramin Rasoulinezhad', 'Julian Faraone']","['University of Sydney', 'University of Sydney', 'University of Sydney']",[]
https://iclr.cc/virtual/2021/poster/2780,Fairness & Bias,Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning,"Understanding human behavior from observed data is critical for transparency and accountability in decision-making. Consider real-world settings such as healthcare, in which modeling a decision-maker’s policy is challenging—with no access to underlying states, no knowledge of environment dynamics, and no allowance for live experimentation. We desire learning a data-driven representation of decision- making behavior that (1) inheres transparency by design, (2) accommodates partial observability, and (3) operates completely offline. To satisfy these key criteria, we propose a novel model-based Bayesian method for interpretable policy learning (“Interpole”) that jointly estimates an agent’s (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping. Through experiments on both simulated and real-world data for the problem of Alzheimer’s disease diagnosis, we illustrate the potential of our approach as an investigative device for auditing, quantifying, and understanding human decision-making behavior.","['understanding decision-making', 'interpretable policy learning']",[],"['Alihan Hüyük', 'Daniel Jarrett', 'Cem Tekin', 'Mihaela van der Schaar']","['University of Cambridge', 'DeepMind', 'Bilkent University', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2777,Fairness & Bias,SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness,"In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.","['Algorithmic fairness', 'invariance']",[],"['Mikhail Yurochkin', 'Yuekai Sun']","['International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/2772,Fairness & Bias,Training with Quantization Noise for Extreme Model Compression,"We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work with extreme compression methods where the approximations introduced by STE are severe. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5% accuracy on MNLI by compressing RoBERTa to 14 MB and 80.0% top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3 MB.","['Product Quantization', 'efficiency', 'compression']",[],"['Pierre Stock', 'Angela Fan', 'Benjamin Graham', 'Edouard Grave', 'Rémi Gribonval', 'Herve Jegou', 'Armand Joulin']","['Facebook', 'Facebook', 'Meta', 'Facebook', 'INRIA', 'Meta', 'Facebook']",[]
https://iclr.cc/virtual/2021/poster/2763,Fairness & Bias,When Optimizing  $f$-Divergence is Robust with Label Noise,"We show when maximizing a properly defined $f$-divergence measure with respect to a classifier's predictions and the supervised labels is robust with label noise. Leveraging its variational form, we derive a nice decoupling property for a family of $f$-divergence measures when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. The above derivation helps us analyze the robustness of different $f$-divergence functions. With established robustness, this family of $f$-divergence functions arises as useful metrics for the problem of learning with noisy labels, which do not require the specification of the labels' noise rate. When they are possibly not robust, we propose fixes to make them so. In addition to the analytical results, we present thorough experimental evidence. Our code is available at https://github.com/UCSC-REAL/Robust-f-divergence-measures.","['Learning with noisy labels', '$f-$divergence', 'robustness']",[],"['Jiaheng Wei', 'Yang Liu']","['University of California, Santa Cruz', 'University of California, Santa Cruz']",[]
https://iclr.cc/virtual/2021/poster/2762,Fairness & Bias,Contrastive  Learning  with Adversarial Perturbations for Conditional Text Generation,"Recently, sequence-to-sequence (seq2seq) models with the Transformer architecture have achieved remarkable performance on various conditional text generation tasks, such as machine translation. However, most of them are trained with teacher forcing with the ground truth label given at each time step, without being exposed to incorrectly generated tokens during training, which hurts its generalization to unseen inputs, that is known as the ""exposure bias"" problem. In this work, we propose to solve the conditional text generation problem by contrasting positive pairs with negative pairs, such that the model is exposed to various valid or incorrect perturbations of the inputs, for improved generalization. However, training the model with naïve contrastive learning framework using random non-target sequences as negative examples is suboptimal, since they are easily distinguishable from the correct output, especially so with models pretrained with large text corpora. Also, generating positive examples requires domain-specific augmentation heuristics which may not generalize over diverse domains. To tackle this problem, we propose a principled method to generate positive and negative samples for contrastive learning of seq2seq models. Specifically, we generate negative examples by adding small perturbations to the input sequence to minimize its conditional likelihood, and positive examples by adding  large perturbations while enforcing it to have a high conditional likelihood. Such `""hard'' positive and negative pairs generated using our method guides the model to better distinguish correct outputs from incorrect ones. We empirically show that our proposed method significantly improves the generalization of the seq2seq on three text generation tasks --- machine translation, text summarization, and question generation.","['conditional text generation', 'contrastive learning']",[],"['Seanie Lee', 'Dong Bok Lee', 'Sung Ju Hwang']","['Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/2749,Fairness & Bias,Individually Fair Gradient Boosting,"We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which arise often in applications where algorithmic fairness is a concern. At a high level, our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.","['non-smooth models', 'boosting', 'Algorithmic fairness']",[],"['Alexander Vargo', 'Fan Zhang', 'Mikhail Yurochkin', 'Yuekai Sun']","['University of Michigan', 'ShanghaiTech University', 'International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/2746,Fairness & Bias,Contemplating Real-World Object Classification,"Deep object recognition models have been very successful over benchmark datasets such as ImageNet. How accurate and robust are they to distribution shifts arising from natural and synthetic variations in datasets? Prior research on this problem has primarily focused on ImageNet variations (e.g., ImageNetV2, ImageNet-A). To avoid potential inherited biases in these studies, we take a different approach. Specifically, we reanalyze the ObjectNet dataset recently proposed by Barbu et al. containing objects in daily life situations. They showed a dramatic performance drop of the state of the art object recognition models on this dataset. Due to the importance and implications of their results regarding the generalization ability of deep models, we take a second look at their analysis. We find that applying deep models to the isolated objects, rather than the entire scene as is done in the original paper, results in around 20-30% performance improvement. Relative to the numbers reported in Barbu et al., around 10-15% of the performance loss is recovered, without any test time data augmentation. Despite this gain, however, we conclude that deep models still suffer drastically on the ObjectNet dataset. We also investigate the robustness of models against synthetic image perturbations such as geometric transformations (e.g., scale, rotation, translation), natural image distortions (e.g., impulse noise, blur) as well as adversarial attacks (e.g., FGSM and PGD-5). Our results indicate that limiting the object area as much as possible (i.e., from the entire image to the bounding box to the segmentation mask) leads to consistent improvement in accuracy and robustness. Finally, through a qualitative analysis of ObjectNet data, we find that i) a large number of images in this dataset are hard to recognize even for humans, and ii) easy (hard) samples for models match with easy (hard) samples for humans. Overall, our analysis shows that ObjecNet is still a challenging test platform that can be used to measure the generalization ability of models. The code and data are available in [masked due to blind review].","['robustness', 'ObjectNet', 'object recognition', 'deep learning']",[],['ali borji'],['PrimerAI'],[]
https://iclr.cc/virtual/2021/poster/2744,Fairness & Bias,When Do Curricula Work?,"Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the implicit curricula resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of explicit curricula, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum or random ordering can indeed improve the performance either with limited training time budget or in the existence of noisy data.","['Empirical Investigation', 'Understanding Deep Learning', 'curriculum learning']",[],"['Xiaoxia Wu', 'Ethan Dyer', 'Behnam Neyshabur']","['Microsoft', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2732,Fairness & Bias,Debiasing Concept-based Explanations with Causal Analysis,"Studying the concept-based explanation techniques, we provided evidences for potential existence of spurious association between the features and concepts due to  unobserved latent variables or noise. We proposed a new causal prior graph that models the impact of the noise and latent confounding fron the estimated concepts. We showed that using the labels as instruments, we can remove the impact of the context from the explanations. Our experiments showed that our debiasing technique not only improves the quality of the explanations, but also improve the accuracy of predicting labels through the concepts. As future work, we will investigate other two-stage-regression techniques to find the most accurate debiasing method.","['Concept-based Explanation', 'interpretability']",[],['Mohammad Taha Bahadori'],['Amazon'],[]
https://iclr.cc/virtual/2021/poster/2731,Fairness & Bias,Learning to Represent Action Values as a Hypergraph on the Action Vertices,"Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework---a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks.","['learning action representations', 'multi-dimensional discrete action spaces', 'structural inductive bias', 'structural credit assignment', 'reinforcement learning']",[],"['Arash Tavakoli', 'Mehdi Fatemi', 'Petar Kormushev']","['Shift Lab', 'Microsoft', 'Imperial College London, Imperial College London']",[]
https://iclr.cc/virtual/2021/poster/2726,Fairness & Bias,Rethinking Attention with Performers,"We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low  estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.","['uniprot', 'trembl', 'protein', 'linformer', 'reformer', 'generalized', 'kernel', 'FAVOR', 'features', 'random', 'orthogonal', 'unidirectional', 'bidirectional', 'BERT', 'performer', 'Approximation', 'linear', 'softmax', 'sparsity', 'transformer', 'attention']",[],"['Krzysztof Marcin Choromanski', 'Valerii Likhosherstov', 'David Dohan', 'Xingyou Song', 'Andreea Gane', 'Tamas Sarlos', 'Peter Hawkins', 'Jared Quincy Davis', 'Afroz Mohiuddin', 'Lukasz Kaiser', 'David Belanger', 'Lucy J Colwell', 'Adrian Weller']","['Google Brain Robotics & Columbia University', 'Waymo', 'OpenAI', 'Google DeepMind', 'Google', 'Google Research', 'Google', 'Google', 'Google', 'OpenAI', 'Google Brain', 'Google DeepMind', 'Alan Turing Institute']",[]
https://iclr.cc/virtual/2021/poster/2724,Fairness & Bias,Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator,"Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.","['rao-blackwell', 'rao', 'straightthrough', 'straight-through', 'gumbel-softmax', 'softmax', 'gumbel']",[],"['Max B. Paulus', 'Chris J. Maddison', 'Andreas Krause']","['Swiss Federal Institute of Technology', 'University of Toronto', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2720,Fairness & Bias,Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System,"Designing task-oriented dialogue systems is a challenging research topic, since it needs not only to generate utterances fulfilling user requests but also to guarantee the comprehensibility. Many previous works trained end-to-end (E2E) models with supervised learning (SL), however, the bias in annotated system utterances remains as a bottleneck. Reinforcement learning (RL) deals with the problem through using non-differentiable evaluation metrics (e.g., the success rate) as rewards. Nonetheless, existing works with RL showed that the comprehensibility of generated system utterances could be corrupted when improving the performance on fulfilling user requests. In our work, we (1) propose modelling the hierarchical structure between dialogue policy and natural language generator (NLG) with the option framework, called HDNO, where the latent dialogue act is applied to avoid designing specific dialogue act representations; (2) train HDNO via hierarchical reinforcement learning (HRL), as well as suggest the asynchronous updates between dialogue policy and NLG during training to theoretically guarantee their convergence to a local maximizer; and (3) propose using a discriminator modelled with language models as an additional reward to further improve the comprehensibility. We test HDNO on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in comparison with word-level E2E model trained with RL, LaRL and HDSA, showing improvements on the performance evaluated by automatic evaluation metrics and human evaluation. Finally, we demonstrate the semantic meanings of latent dialogue acts to show the explanability for HDNO.","['Task-oriented Dialogue System', 'policy optimization', 'hierarchical reinforcement learning', 'natural language processing']",[],"['Jianhong Wang', 'Yuan Zhang', 'Tae-Kyun Kim']","['University of Manchester', 'Albert-Ludwigs-Universität Freiburg', 'Imperial College London']",[]
https://iclr.cc/virtual/2021/poster/2719,Fairness & Bias,"Physics-aware, probabilistic model order reduction with guaranteed stability","Given (small amounts of) time-series' data from  a high-dimensional, fine-grained, multiscale dynamical system, we propose a generative framework for learning an effective, lower-dimensional, coarse-grained dynamical model that is predictive of the fine-grained system's long-term evolution but also of its behavior under different initial conditions. We target fine-grained models as they arise in physical applications (e.g. molecular dynamics, agent-based models), the dynamics  of which are strongly non-stationary but their transition to equilibrium is governed by unknown slow processes which are largely inaccessible by brute-force simulations. Approaches based on domain knowledge heavily rely on physical insight in identifying temporally slow features and fail to enforce the long-term stability of the learned dynamics. On the other hand, purely statistical frameworks lack interpretability and rely on large amounts of expensive simulation data (long and multiple trajectories) as they cannot infuse domain knowledge. The generative framework proposed achieves  the aforementioned desiderata by  employing a flexible prior on the complex plane for the latent, slow processes, and  an intermediate layer of physics-motivated latent variables that reduces reliance on data and imbues inductive bias. In contrast to existing schemes, it does not require  the a priori definition of projection operators from the fine-grained description and addresses simultaneously the tasks of dimensionality reduction and model estimation. We demonstrate its efficacy and accuracy in multiscale physical systems of particle dynamics where probabilistic, long-term predictions of phenomena not contained in the training data are produced.","['long-term stability', 'slowness', 'model order reduction', 'probabilistic generative models', 'state-space models', 'inductive bias']",[],"['Sebastian Kaltenbach', 'Phaedon Stelios Koutsourelakis']","['ETHZ - ETH Zurich', 'Technische Universität München']",[]
https://iclr.cc/virtual/2021/poster/2938,Fairness & Bias,Property Controllable Variational Autoencoder via Invertible Mutual Dependence,"Deep generative models have made important progress towards modeling complex, high dimensional data via learning latent representations. Their usefulness is nevertheless often limited by a lack of control over the generative process or a poor understanding of the latent representation. To overcome these issues, attention is now focused on discovering latent variables correlated to the data properties and ways to manipulate these properties. This paper presents the new Property controllable VAE (PCVAE), where a new Bayesian model is proposed to inductively bias the latent representation using explicit data properties via novel group-wise and property-wise disentanglement. Each data property corresponds seamlessly to a latent variable, by innovatively enforcing invertible mutual dependence between them. This allows us to move along the learned latent dimensions to control specific properties of the generated data with great precision. Quantitative and qualitative evaluations confirm that the PCVAE outperforms the existing models by up to 28% in capturing and 65% in manipulating the desired properties.","['disentangled representation learning', 'interpretable latent representation', 'deep generative models']",[],"['Xiaojie Guo', 'Yuanqi Du', 'Liang Zhao']","['International Business Machines', 'Cornell University', 'Emory University']",[]
https://iclr.cc/virtual/2021/poster/2713,Fairness & Bias,Continuous Wasserstein-2 Barycenter Estimation without Minimax Optimization,"Wasserstein barycenters provide a geometric notion of the weighted average of probability measures based on optimal transport. In this paper, we present a scalable algorithm to compute Wasserstein-2 barycenters given sample access to the input measures, which are not restricted to being discrete. While past approaches rely on entropic or quadratic regularization, we employ input convex neural networks and cycle-consistency regularization to avoid introducing bias. As a result, our approach does not resort to minimax optimization. We provide theoretical analysis on error bounds as well as empirical evidence of the effectiveness of the proposed approach in low-dimensional qualitative scenarios and high-dimensional quantitative experiments.","['continuous case', 'input convex neural networks', 'cycle-consistency regularizer', 'wasserstein-2 barycenters', 'non-minimax optimization']",[],"['Alexander Korotin', 'Lingxiao Li', 'Justin Solomon', 'Evgeny Burnaev']","['Skolkovo Institute of Science and Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Skolkovo Institute of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/2683,Fairness & Bias,Overparameterisation and worst-case generalisation: friend or foe?,"Overparameterised neural networks have demonstrated the remarkable ability to perfectly fit training samples, while still generalising to unseen test samples. However, several recent works have revealed that such models' good average performance does not always translate to good worst-case performance: in particular, they may perform poorly on subgroups that are under-represented in the training set. In this paper, we show that in certain settings, overparameterised models' performance on under-represented subgroups may be improved via post-hoc processing. Specifically, such models' bias can be restricted to their classification layers, and manifest as structured prediction shifts for rare subgroups. We detail two post-hoc correction techniques to mitigate this bias, which operate purely on the outputs of standard model training. We empirically verify that with such post-hoc correction, overparameterisation can improve average and worst-case performance.","['worst-case generalisation', 'overparameterisation']",[],"['Aditya Krishna Menon', 'Ankit Singh Rawat', 'Sanjiv Kumar']","['Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2678,Fairness & Bias,Revisiting Few-sample BERT Fine-tuning,"This paper is a study of fine-tuning of BERT contextual representations, with focus on commonly observed instabilities in few-sample scenarios. We identify several factors that cause this instability: the common use of a non-standard optimization method with biased gradient estimation; the limited applicability of significant parts of the BERT network for down-stream tasks; and the prevalent practice of using a pre-determined, and small number of training iterations. We empirically test the impact of these factors, and identify alternative practices that resolve the commonly observed instability of the process. In light of these observations, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe the impact of these methods diminishes significantly with our modified process.","['Fine-tuning', 'optimization', 'BERT']",[],"['Tianyi Zhang', 'Felix Wu', 'Arzoo Katiyar', 'Kilian Q Weinberger']","['Stanford University', 'ASAPP Inc.', 'Pennsylvania State University', 'Cornell University']",[]
https://iclr.cc/virtual/2021/poster/2675,Fairness & Bias,Long-tail learning via logit adjustment,"Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels have only a few associated samples. This poses a challenge for generalisation on such labels, and also  makes naive learning biased towards dominant labels. In this paper,  we present a statistical framework that unifies and generalises several recent proposals to cope with these challenges. Our framework revisits the classic idea of logit adjustment based on the label frequencies, which encourages a large relative margin between logits of rare positive versus dominant negative labels. This yields two techniques  for long-tail learning, where such adjustment is either applied post-hoc to a trained model, or enforced in the loss during training. These techniques are statistically grounded, and practically effective on four real-world datasets with long-tailed label distributions.","['class imbalance', 'long-tail learning']",[],"['Aditya Krishna Menon', 'Sadeep Jayasumana', 'Ankit Singh Rawat', 'Himanshu Jain', 'Andreas Veit', 'Sanjiv Kumar']","['Google', 'Google', 'Google', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2669,Fairness & Bias,Understanding the role of importance weighting for deep learning,"The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.","['gradient descent', 'implicit bias', 'Importance Weighting', 'learning theory', 'deep learning']",[],"['Da Xu', 'Yuting Ye', 'Chuanwei Ruan']","['LinkedIn', 'University of California Berkeley', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/2664,Fairness & Bias,Wasserstein-2 Generative Networks,"We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.","['input-convex neural networks', 'cycle-consistency regularization', 'non-minimax optimization', 'optimal transport maps', 'wasserstein-2 distance']",[],"['Alexander Korotin', 'Vage Egiazarian', 'Arip Asadulaev', 'Aleksandr Safin', 'Evgeny Burnaev']","['Skolkovo Institute of Science and Technology', 'Higher School of Economics', 'ITMO University', 'Skolkovo Institute of Science and Technology', 'Skolkovo Institute of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/2652,Fairness & Bias,FairBatch: Batch Selection for Model Fairness,"Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit -- it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts.  Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes.","['batch selection', 'model fairness', 'bilevel optimization']",[],"['Yuji Roh', 'Kangwook Lee', 'Steven Euijong Whang', 'Changho Suh']","['Korea Advanced Institute of Science and Technology', 'University of Wisconsin, Madison', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/2635,Fairness & Bias,Large-width functional asymptotics for deep Gaussian neural networks,"In this paper, we consider fully connected feed-forward deep neural networks where weights and biases are independent and identically distributed according to Gaussian distributions. Extending previous results (Matthews et al., 2018a;b;Yang, 2019)  we adopt a function-space perspective, i.e. we look at neural networks as infinite-dimensional random elements on the input space $\mathbb{R}^I$. Under suitable assumptions on the activation function we show that: i) a network defines a continuous Gaussian process on the input space $\mathbb{R}^I$; ii) a network with re-scaled weights converges weakly to a continuous Gaussian process in the large-width limit; iii) the limiting Gaussian process has almost surely locally $\gamma$-Hölder continuous paths, for $0 < \gamma <1$. Our results contribute to recent theoretical studies on the interplay between infinitely wide deep neural networks and Gaussian processes by establishing weak convergence in function-space with respect to a stronger metric.","['stochastic process', 'gaussian process', 'infinitely wide neural network', 'deep learning theory']",[],"['Daniele Bracale', 'Stefano Favaro', 'Sandra Fortini', 'Stefano Peluchetti']","['University of Michigan', 'University of Torino', 'Bocconi University', 'Cogent Labs']",[]
https://iclr.cc/virtual/2021/poster/2634,Fairness & Bias,Deciphering and Optimizing Multi-Task Learning: a Random Matrix Approach,"This article provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large ($p$) and numerous ($n$) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge, as $n,p\to\infty$, to a deterministic limit involving simple (small-dimensional) statistics of the data. We prove (i) that the standard MTL LS-SVM algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-SVMs may outperform the MTL approach, even for quite resembling tasks): our analysis provides a simple method to correct these biases, and that we reveal (ii) the sufficient statistics at play in the method, which can be efficiently estimated, even for quite small datasets. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that our improved MTL LS-SVM method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi-task and transfer learning techniques.","['random matrix theory', 'Multi Task Learning', 'transfer learning']",[],"['Malik Tiomoko', 'Hafiz Tiomoko Ali', 'Romain Couillet']","['Huawei Technologies Ltd.', 'Samsung', 'University of Grenoble-Alpes']",[]
https://iclr.cc/virtual/2021/poster/2633,Fairness & Bias,Free Lunch for Few-shot Learning:  Distribution Calibration,"Learning from a limited number of samples is challenging since the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples. Then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on three datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation.","['distribution estimation', 'few-shot learning', 'image classification']",[],"['Shuo Yang', 'Lu Liu', 'Min Xu']","['University of Technology Sydney, Australia', 'Google', 'University of Technology Sydney']",[]
https://iclr.cc/virtual/2021/poster/2627,Fairness & Bias,Individually Fair Rankings,We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.,"['Learning to Rank', 'Algorithmic fairness', 'optimal transport']",[],"['Amanda Bower', 'Mikhail Yurochkin', 'Yuekai Sun']","['Twitter', 'International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/2612,Fairness & Bias,Fair Mixup: Fairness via Interpolation,"Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples  between the groups. We use mixup, a powerful data augmentation strategy  to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks.","['fairness', 'data augmentation']",[],"['Ching-Yao Chuang', 'Youssef Mroueh']","['Meta', 'IBM']",[]
https://iclr.cc/virtual/2021/poster/2580,Fairness & Bias,Empirical or Invariant Risk Minimization? A Sample Complexity Perspective,"Recently, invariant risk minimization (IRM) was proposed as a promising solution to address out-of-distribution (OOD) generalization. However, it is unclear when IRM should be preferred over the widely-employed empirical risk minimization (ERM) framework. In this work, we analyze both these frameworks from the perspective of sample complexity, thus taking a firm step towards answering this important question. We find that depending on the type of data generation mechanism, the two approaches might have very different finite sample and asymptotic behavior. For example, in the covariate shift setting we see that the two approaches not only arrive at the same asymptotic solution, but also have similar finite sample behavior with no clear winner. For other distribution shifts such as those involving confounders or anti-causal variables, however, the two approaches arrive at different asymptotic solutions where IRM is guaranteed to be close to the desired OOD solutions in the finite sample regime, while ERM is biased even asymptotically.  We further investigate how different factors --- the number of environments, complexity of the model, and IRM penalty weight ---  impact the sample complexity of IRM in relation to its distance from the OOD solutions.","['IRM', 'invariant risk minimization']",[],"['Kartik Ahuja', 'Amit Dhurandhar', 'Karthikeyan Shanmugam', 'Kush R. Varshney']","['FAIR (Meta)', 'International Business Machines', 'Google', 'International Business Machines']",[]
https://iclr.cc/virtual/2021/poster/2560,Fairness & Bias,Variational Information Bottleneck for Effective Low-Resource Fine-Tuning,"While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.","['variational information bottleneck', 'biases', 'robust', 'over-fitting', 'large-scale pre-trained language models', 'nlp', 'transfer learning']",[],"['Rabeeh Karimi mahabadi', 'Yonatan Belinkov', 'James Henderson']","['Swiss Federal Institute of Technology Lausanne', 'Technion, Technion', 'Idiap Research Institute']",[]
https://iclr.cc/virtual/2021/poster/2555,Fairness & Bias,FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders,"Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.","['Pretrained Text Encoders', 'contrastive learning', 'fairness', 'mutual information']",[],"['Pengyu Cheng', 'Weituo Hao', 'Siyang Yuan', 'Shijing Si', 'Lawrence Carin']","['Tencent', 'TikTok Inc.', 'Duke University', 'Shanghai International Studies University', 'Duke University']",[]
https://iclr.cc/virtual/2021/poster/2546,Fairness & Bias,Negative Data Augmentation ,"Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA) that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distribution,  and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable conditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved conditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image classification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks.","['anomaly detection', 'data augmentation', 'self-supervised learning', 'generative models']",[],"['Abhishek Sinha', 'Kumar Ayush', 'Jiaming Song', 'Burak Uzkent', 'Stefano Ermon']","['Waymo', 'Google', 'NVIDIA', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/2539,Fairness & Bias,Net-DNF: Effective Deep Modeling of Tabular Data,"A challenging open question in deep learning is how to handle tabular data. Unlike domains such as image and natural language processing, where deep architectures prevail, there is still no widely accepted neural architecture that dominates tabular data. As a step toward bridging this gap, we present Net-DNF a novel generic architecture whose inductive bias elicits models whose structure corresponds to logical Boolean formulas in disjunctive normal form (DNF) over affine soft-threshold decision terms. Net-DNFs also promote localized decisions that are taken over small subsets of the features. We present an extensive experiments showing that Net-DNFs significantly and consistently outperform fully connected networks over tabular data. With relatively few hyperparameters, Net-DNFs open the door to practical end-to-end handling of tabular data using neural networks. We present ablation studies, which justify the design choices of Net-DNF including the inductive bias elements, namely, Boolean formulation, locality, and feature selection.","['Predictive Modeling', 'tabular data', 'Architectures', 'neural networks']",[],"['Liran Katzir', 'Gal Elidan', 'Ran El-Yaniv']","['Technion, Technion', 'Google', 'Technion']",[]
https://iclr.cc/virtual/2021/poster/2949,Fairness & Bias,Contrastive Syn-to-Real Generalization,"Training on synthetic data can be beneficial for label or data-scarce scenarios. However, synthetically trained models often suffer from poor generalization in real domains due to domain gaps. In this work, we make a key observation that the diversity of the learned feature embeddings plays an important role in the generalization performance. To this end, we propose contrastive synthetic-to-real generalization (CSG), a novel framework that leverage the pre-trained ImageNet knowledge to prevent overfitting to the synthetic domain, while promoting the diversity of feature embeddings as an inductive bias to improve generalization. In addition, we enhance the proposed CSG framework with attentional pooling (A-pool) to let the model focus on semantically important regions and further improve its generalization. We demonstrate the effectiveness of CSG on various synthetic training tasks, exhibiting state-of-the-art performance on zero-shot domain generalization.","['synthetic-to-real generalization', 'domain generalization']",[],"['Wuyang Chen', 'Zhiding Yu', 'Shalini De Mello', 'Sifei Liu', 'Jose M. Alvarez', 'Zhangyang Wang', 'Anima Anandkumar']","['University of Texas, Austin', 'NVIDIA', 'NVIDIA', 'NVIDIA', 'NVIDIA', 'University of Texas at Austin', 'California Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/3295,Fairness & Bias,Mind the Pad -- CNNs Can Develop Blind Spots,"We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or weakened. The major source of this bias is the padding mechanism. Depending on several aspects of convolution arithmetic, this mechanism can apply the padding unevenly, leading to asymmetries in the learned weights. We demonstrate how such bias can be detrimental to certain tasks such as small object detection: the activation is suppressed if the stimulus lies in the impacted area, leading to blind spots and misdetection. We explore alternative padding methods and propose solutions for analyzing and mitigating spatial bias.","['visualization', 'debugging', 'exposition', 'padding', 'blind spots', 'spatial bias', 'cnn', 'foveation', 'convolution']",[],"['Bilal Alsallakh', 'Narine Kokhlikyan', 'Jun Yuan', 'Orion Reblitz-Richardson']","['Voxel AI', 'Facebook', 'New York University', 'Facebook AI']",[]
https://iclr.cc/virtual/2021/poster/2905,Fairness & Bias,Probing BERT in Hyperbolic Spaces,"Recently, a variety of probing tasks are proposed to discover linguistic properties learned in contextualized word embeddings. Many of these works implicitly assume these embeddings lay in certain metric spaces, typically the Euclidean space. This work considers a family of geometrically special spaces, the hyperbolic spaces, that exhibit better inductive biases for hierarchical structures and may better reveal linguistic hierarchies encoded in contextualized representations. We introduce a $\textit{Poincaré probe}$, a structural probe projecting these embeddings into a Poincaré subspace with explicitly defined hierarchies. We focus on two probing objectives: (a) dependency trees where the hierarchy is defined as head-dependent structures; (b) lexical sentiments where the hierarchy is defined as the polarity of words (positivity and negativity). We argue that a key desideratum of a probe is its sensitivity to the existence of linguistic structures. We apply our probes on BERT, a typical contextualized embedding model. In a syntactic subspace, our probe better recovers tree structures than Euclidean probes, revealing the possibility that the geometry of BERT syntax may not necessarily be Euclidean. In a sentiment subspace, we reveal two possible meta-embeddings for positive and negative sentiments and show how lexically-controlled contextualization would change the geometric localization of embeddings. We demonstrate the findings with our Poincaré probe via extensive experiments and visualization. Our results can be reproduced at https://github.com/FranxYao/PoincareProbe","['Sentiment', 'Syntax', 'Probe', 'BERT', 'hyperbolic']",[],"['Boli Chen', 'Yao Fu', 'Guangwei Xu', 'Chuanqi Tan', 'Mosha Chen', 'Liping Jing']","['Alibaba DAMO Academy', 'University of Edinburgh', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Beijing Jiaotong University']",[]
https://iclr.cc/virtual/2021/poster/3291,Fairness & Bias,Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning,"Matrix factorization is a simple and natural test-bed to investigate the implicit regularization of gradient descent. Gunasekar et al. (2017) conjectured that gradient flow with infinitesimal initialization converges to the solution that minimizes the nuclear norm, but a series of recent papers argued that the language of norm minimization is not sufficient to give a full characterization for the implicit regularization. In this work, we provide theoretical and empirical evidence that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to a simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under some reasonable assumptions. This generalizes the rank minimization view from previous works to a much broader setting and enables us to construct counter-examples to refute the conjecture from Gunasekar et al. (2017). We also extend the results to the case where depth >= 3, and we show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude so that this rank minimization is more likely to take effect for initialization with practical scale.","['implicit bias', 'matrix factorization', 'implicit regularization', 'gradient descent']",[],"['Zhiyuan Li', 'Yuping Luo', 'Kaifeng Lyu']","['Toyota Technological Institute at Chicago', 'Princeton University', 'Princeton University']",[]
https://iclr.cc/virtual/2021/poster/2684,Fairness & Bias,Randomized Automatic Differentiation,"The successes of deep learning, variational inference, and many other fields have been aided by specialized implementations of reverse-mode automatic differentiation (AD) to compute gradients of mega-dimensional objectives. The AD techniques underlying these tools were designed to compute exact gradients to numerical precision, but modern machine learning models are almost always trained with stochastic gradient descent. Why spend computation and memory on exact (minibatch) gradients only to use them for stochastic optimization? We develop a general framework and approach for randomized automatic differentiation (RAD), which can allow unbiased gradient estimates to be computed with reduced memory in return for variance. We examine limitations of the general approach, and argue that we must leverage problem specific structure to realize benefits. We develop RAD techniques for a variety of simple neural network architectures, and show that for a fixed memory budget, RAD converges in fewer iterations than using a small batch size for feedforward networks, and in a similar number for recurrent networks. We also show that RAD can be applied to scientific computing, and use it to develop a low-memory stochastic gradient method for optimizing the control parameters of a linear reaction-diffusion PDE representing a fission reactor.","['pdes', 'backprop', 'autodiff', 'automatic differentiation', 'stochastic optimization', 'deep learning']",[],"['Deniz Oktay', 'Nick McGreivy', 'Joshua Aduol', 'Alex Beatson', 'Ryan P Adams']","['NVIDIA', 'Princeton University', 'Princeton University', 'Princeton University', 'Harvard University']",[]
https://iclr.cc/virtual/2021/poster/2836,Fairness & Bias,Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation,"Clustering is one of the most fundamental tasks in machine learning. Recently, deep clustering has become a major trend in clustering techniques. Representation learning often plays an important role in the effectiveness of deep clustering, and thus can be a principal cause of performance degradation. In this paper, we propose a clustering-friendly representation learning method using instance discrimination and feature decorrelation. Our deep-learning-based representation learning method is motivated by the properties of classical spectral clustering. Instance discrimination learns similarities among data and feature decorrelation removes redundant correlation among features. We utilize an instance discrimination method in which learning individual instance classes leads to learning similarity among instances. Through detailed experiments and examination, we show that the approach can be adapted to learning a latent space for clustering. We design novel softmax-formulated decorrelation constraints for learning. In evaluations of image clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy of 81.5% and 95.4%, respectively. We also show that the softmax-formulated constraints are compatible with various neural networks.","['deep embedding', 'clustering', 'representation learning']",[],"['Yaling Tao', 'Kentaro Takagi', 'Kouta Nakata']","['university of tsukuba', 'Kyoto University', 'The University of Tokyo']",[]
https://iclr.cc/virtual/2021/poster/2974,Fairness & Bias,Learning Task Decomposition with Ordered Memory Policy Network,"Many complex real-world tasks are composed of several levels of subtasks. Humans leverage these hierarchical structures to accelerate the learning process and achieve better generalization. In this work, we study the inductive bias and propose Ordered Memory Policy Network (OMPN) to discover subtask hierarchy by learning from demonstration. The discovered subtask hierarchy could be used to perform task decomposition, recovering the subtask boundaries in an unstructured demonstration. Experiments on Craft and Dial demonstrate that our model can achieve higher task decomposition performance under both unsupervised and weakly supervised settings, comparing with strong baselines. OMPN can also be directly applied to partially observable environments and still achieve higher task decomposition performance. Our visualization further confirms that the subtask hierarchy can emerge in our model 1.","['Network Inductive Bias', 'Hierarchical Imitation Learning', 'Task Segmentation']",[],"['Yuchen Lu', 'Yikang Shen', 'Siyuan Zhou', 'Aaron Courville', 'Joshua B. Tenenbaum', 'Chuang Gan']","['University of Montreal', 'University of Montreal', 'Hong Kong University of Science and Technology', 'University of Montreal', 'Massachusetts Institute of Technology', 'MIT-IBM Watson AI Lab']",[]
https://iclr.cc/virtual/2021/poster/3241,Fairness & Bias,On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections,"Disparate impact has raised serious concerns in machine learning applications and its societal impacts. In response to the need of mitigating discrimination, fairness has been regarded as a crucial property in algorithmic design. In this work, we study the problem of disparate impact on graph-structured data. Specifically, we focus on dyadic fairness, which articulates a fairness concept that a predictive relationship between two instances should be independent of the sensitive attributes. Based on this, we theoretically relate the graph connections to dyadic fairness on link predictive scores in learning graph neural networks, and reveal that regulating weights on existing edges in a graph contributes to dyadic fairness conditionally. Subsequently, we propose our algorithm, \textbf{FairAdj}, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that our method delivers effective dyadic fairness in terms of various statistics, and at the same time enjoys a favorable fairness-utility tradeoff.","['graph-structured data', 'Algorithmic fairness']",[],"['Peizhao Li', 'Yifei Wang', 'Han Zhao', 'Pengyu Hong', 'Hongfu Liu']","['Brandeis University', 'Brandeis University', 'University of Illinois, Urbana Champaign', 'Brandeis University', 'Brandeis University']",[]
https://iclr.cc/virtual/2021/poster/2779,Fairness & Bias,IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression,"In this paper we analyse and improve integer discrete flows for lossless compression. Integer discrete flows are a recently proposed class of models that learn invertible transformations for integer-valued random variables. Their discrete nature makes them particularly suitable for lossless compression with entropy coding schemes. We start by investigating a recent theoretical claim that states that invertible flows for discrete random variables are less flexible than their continuous counterparts. We demonstrate with a proof that this claim does not hold for integer discrete flows due to the embedding of data with finite support into the countably infinite integer lattice. Furthermore, we zoom in on the effect of gradient bias due to the straight-through estimator in integer discrete flows, and demonstrate that its influence is highly dependent on architecture choices and less prominent than previously thought. Finally, we show how different architecture modifications improve the performance of this model class for lossless compression, and that they also enable more efficient compression: a model with half the number of flow layers performs on par with or better than the original integer discrete flow model.","['lossless source compression', 'Normalizing flows', 'generative modeling']",[],"['Rianne van den Berg', 'Alexey A. Gritsenko', 'Mostafa Dehghani', 'Casper Kaae Sønderby', 'Tim Salimans']","['Microsoft', 'Google', 'Google DeepMind', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2765,Fairness & Bias,Text Generation by Learning from Demonstrations,"Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation. This paradigm leads to (i) diverse but low-quality samples due to mismatched learning objective and evaluation metric (likelihood vs. quality) and (ii) exposure bias due to mismatched history distributions (gold vs. model-generated). To alleviate these problems, we frame text generation as an offline reinforcement learning (RL) problem with expert demonstrations (i.e., the reference), where the goal is to maximize quality given model-generated histories. We propose GOLD (generation by off-policy learning from demonstrations): an easy-to-optimize algorithm that learns from the demonstrations by importance weighting. Intuitively, GOLD upweights confident tokens and downweights unconfident ones in the reference during training, avoiding optimization issues faced by prior RL approaches that rely on online data collection. According to both automatic and human evaluation, models trained by GOLD outperform those trained by MLE and policy gradient on summarization, question generation, and machine translation. Further, our models are less sensitive to decoding algorithms and alleviate exposure bias.","['learning from demonstrations', 'text generation', 'nlp']",[],"['Richard Yuanzhe Pang', 'He He']","['New York University', 'New York University']",[]
https://iclr.cc/virtual/2021/poster/2757,Fairness & Bias,Unbiased Teacher for Semi-Supervised Object Detection,"Semi-supervised learning, i.e., training networks with both labeled and unlabeled data, has made significant progress recently. However, existing works have primarily focused on image classification tasks and neglected object detection which requires more annotation effort. In this work, we revisit the Semi-Supervised Object Detection (SS-OD) and identify the pseudo-labeling bias issue in SS-OD. To address this, we introduce Unbiased Teacher, a simple yet effective approach that jointly trains a student and a gradually progressing teacher in a mutually-beneficial manner. Together with a class-balance loss to downweight overly confident pseudo-labels, Unbiased Teacher consistently improved state-of-the-art methods by significant margins on COCO-standard, COCO-additional, and VOC datasets. Specifically, Unbiased Teacher achieves 6.8 absolute mAP improvements against state-of-the-art method when using 1% of labeled data on MS-COCO, achieves around 10 mAP improvements against the supervised baseline when using only 0.5, 1, 2% of labeled data on MS-COCO.",['object detection'],[],"['Yen-Cheng Liu', 'Chih-Yao Ma', 'Zijian He', 'Chia-Wen Kuo', 'Kan Chen', 'Peizhao Zhang', 'Bichen Wu', 'Zsolt Kira', 'Peter Vajda']","['Georgia Institute of Technology', 'Facebook', ', University of California, Los Angeles', 'Amazon', 'Waymo', 'Facebook', 'Facebook', 'Georgia Institute of Technology', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/2654,Fairness & Bias,Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction,"Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\""{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.","['Markov jump process', 'Dirichlet form', 'generalized Girsanov theorem', 'change of measure', 'stochastic gradient Langevin dynamics', 'parallel tempering', 'replica exchange', 'uncertainty quantification', 'variance reduction']",[],"['Wei Deng', 'Qi Feng', 'Georgios Karagiannis', 'Guang Lin', 'Faming Liang']","['Morgan Stanley', 'Florida State University', 'Durham University', 'Purdue University', 'Purdue University']",[]
https://iclr.cc/virtual/2021/poster/2564,Fairness & Bias,Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning,"Graph Representation Learning (GRL) methods have impacted fields from chemistry to social science. However, their algorithmic implementations are specialized to specific use-cases e.g. ""message passing"" methods are run differently from ""node embedding"" ones. Despite their apparent differences, all these methods utilize the graph structure,  and therefore, their learning can be approximated with stochastic graph traversals.  We propose Graph Traversal via Tensor Functionals (GTTF), a unifying meta-algorithm framework for easing the implementation of diverse graph algorithms and enabling transparent and efficient scaling to large graphs.  GTTF is founded upon a data structure (stored as a sparse tensor) and a stochastic graph traversal algorithm (described using tensor operations). The algorithm is a functional that accept two functions, and can be specialized to obtain a variety of GRL models and objectives, simply by changing those two functions. We show for a wide class of methods, our algorithm learns in an unbiased fashion and, in expectation, approximates the learning as if the specialized implementations were run directly. With these capabilities, we scale otherwise non-scalable methods to set state-of-the-art on large graph datasets while being more efficient than existing GRL libraries -- with only a handful of lines of code for each method specialization.","['node embeddings', 'message passing', 'Scale', 'algorithm', 'learning', 'graph']",[],"['Elan Sopher Markowitz', 'Mehrnoosh Mirtaheri', 'Sami Abu-El-Haija', 'Bryan Perozzi', 'Greg Ver Steeg', 'Aram Galstyan']","['University of Southern California', 'University of Southern California', 'Research, Google', 'Google', 'University of California, Riverside', 'Information Sciences Institute']",[]
https://iclr.cc/virtual/2021/poster/2730,Fairness & Bias,Categorical Normalizing Flows via Continuous Transformations,"Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate Categorical Normalizing Flows, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs. GraphCNF implements a three step approach modeling the nodes, edges, and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.","['graph generation', 'density estimation', 'Normalizing flows']",[],"['Phillip Lippe', 'Stratis Gavves']","['University of Amsterdam', 'University of Amsterdam']",[]
https://iclr.cc/virtual/2021/poster/3236,Fairness & Bias,Directed Acyclic Graph Neural Networks,"Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs---DAGs---and inject a stronger inductive bias---partial ordering---into the neural network design. We propose the directed acyclic graph neural network, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.","['inductive bias', 'DAG', 'Directed Acyclic Graphs', 'graph representation learning', 'graph neural networks']",[],"['Veronika Thost', 'Jie Chen']","['International Business Machines', 'International Business Machines']",[]
https://iclr.cc/virtual/2021/poster/3250,Privacy & Data Governance,Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning,"Model-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms ""vanilla"" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.","['Stepsize optimization', 'ensemble', 'Out-of-domain', 'adversarial training', 'uncertainty', 'meta-learning', 'few-shot learning']",[],"['Namyeong Kwon', 'Hwidong Na', 'Gabriel Huang', 'Simon Lacoste-Julien']","['Samsung Advanced Institute of Technology', 'Samsung', 'University of Montreal', 'University of Montreal']",[]
https://iclr.cc/virtual/2021/poster/2846,Privacy & Data Governance,FedBN: Federated Learning on Non-IID Features via Local Batch Normalization,"The emerging paradigm of federated learning (FL) strives to enable collaborative training of deep models on the network edge without centrally aggregating raw data and hence improving data privacy. In most cases, the assumption of independent and identically distributed samples across local clients does not hold for federated learning setups. Under this setting, neural network training performance may vary significantly according to the data distribution and even hurt training convergence. Most of the previous work has focused on a difference in the distribution of labels or client shifts. Unlike those settings, we address an important problem of FL, e.g., different scanners/sensors in medical imaging, different scenery distribution in autonomous driving (highway vs. city), where local clients store examples with different distributions compared to other clients, which we denote as feature shift non-iid. In this work, we propose an effective method that uses local batch normalization to alleviate the feature shift before averaging models. The resulting scheme, called FedBN, outperforms both classical FedAvg, as well as the state-of-the-art for non-iid data (FedProx) on our extensive experiments. These empirical results are supported by a convergence analysis that shows in a simplified setting that FedBN has a faster convergence rate than FedAvg. Code is available at https://github.com/med-air/FedBN.","['batch normalization', 'Non-IID', 'federated learning']",[],"['Xiaoxiao Li', 'Meirui Jiang', 'Xiaofei Zhang', 'Michael Kamp', 'Qi Dou']","['University of British Columbia', 'The Chinese University of Hong Kong', 'Zhongnan University of Economics and Law', 'Institute for AI in Medicine IKIM', 'The Chinese University of Hong Kong']",[]
https://iclr.cc/virtual/2021/poster/2834,Privacy & Data Governance,Do not Let Privacy Overbill Utility:  Gradient Embedding Perturbation for Private Learning,"The privacy leakage of the model about the training data can be bounded in the differential privacy mechanism. However, for meaningful privacy parameters, a differentially private model degrades the utility drastically when the model comprises a large number of trainable parameters.  In this paper, we propose an algorithm  \emph{Gradient Embedding Perturbation (GEP)} towards training differentially private deep models with decent accuracy. Specifically, in each gradient descent step, GEP first projects individual private gradient into a non-sensitive anchor subspace, producing a low-dimensional gradient embedding and a small-norm residual gradient. Then, GEP perturbs the low-dimensional embedding and the residual gradient separately according to the privacy budget. Such a decomposition permits a small perturbation variance, which greatly helps to break the dimensional barrier of private learning. With GEP, we achieve decent accuracy with low computational cost and modest privacy guarantee for deep models.  Especially, with privacy bound $\epsilon=8$, we achieve $74.9\%$ test accuracy on CIFAR10 and $95.1\%$ test accuracy on  SVHN, significantly improving over existing results.","['gradient redundancy', 'differentially private deep learning', 'privacy preserving machine learning']",[],"['Da Yu', 'Huishuai Zhang', 'Wei Chen', 'Tie-Yan Liu']","['SUN YAT-SEN UNIVERSITY', 'Microsoft Research Asia', ' Chinese Academy of Sciences', 'Microsoft']",[]
https://iclr.cc/virtual/2021/poster/2786,Privacy & Data Governance,CaPC Learning: Confidential and Private Collaborative Learning,"Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties.","['mpc', 'homomorphic encryption', 'confidentiality', 'security', 'machine learning', 'differential privacy', 'privacy', 'deep learning']",[],"['Christopher A. Choquette-Choo', 'Natalie Dullerud', 'Yunxiang Zhang', 'Somesh Jha', 'Nicolas Papernot', 'Xiao Wang']","['Google Research, Brain team', 'Stanford University', 'New York University', 'Department of Computer Science, University of Wisconsin, Madison', 'University of Toronto', 'Northwestern University']",[]
https://iclr.cc/virtual/2021/poster/2710,Privacy & Data Governance,Information Laundering for Model Privacy,"In this work, we propose information laundering, a novel framework for enhancing model privacy. Unlike data privacy that concerns the protection of raw data information, model privacy aims to protect an already-learned model that is to be deployed for public use. The private model can be obtained from general learning methods, and its deployment means that it will return a deterministic or random response for a given input query. An information-laundered model consists of probabilistic components that deliberately maneuver the intended input and output for queries of the model, so the model's adversarial acquisition is less likely. Under the proposed framework, we develop an information-theoretic principle to quantify the fundamental tradeoffs between model utility and privacy leakage and derive the optimal design.","['security', 'Privacy-utility tradeoff', 'Model privacy', 'adversarial attack', 'machine learning']",[],"['Xinran Wang', 'Yu Xiang', 'Jun Gao', 'Jie Ding']","['University of Minnesota - Twin Cities', 'University of Utah', 'Meta ', 'University of Minnesota, Minneapolis']",[]
https://iclr.cc/virtual/2021/poster/2729,Privacy & Data Governance,Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning,"Federated learning (FL) is a distributed machine learning architecture that leverages a large number of workers to jointly learn a model with decentralized data. FL has received increasing attention in recent years thanks to its data privacy protection, communication efficiency and a linear speedup for convergence in training (i.e., convergence performance increases linearly with respect to the number of workers). However, existing studies on linear speedup for convergence are only limited to the assumptions of i.i.d. datasets across workers and/or full worker participation, both of which rarely hold in practice. So far, it remains an open question whether or not the linear speedup for convergence is achievable under non-i.i.d. datasets with partial worker participation in FL.  In this paper, we show that the answer is affirmative. Specifically, we show that the federated averaging (FedAvg) algorithm (with two-sided learning rates) on non-i.i.d. datasets in non-convex settings achieves a convergence rate $\mathcal{O}(\frac{1}{\sqrt{mKT}} + \frac{1}{T})$ for full worker participation and a convergence rate $\mathcal{O}(\frac{1}{\sqrt{nKT}} + \frac{1}{T})$ for partial worker participation, where $K$ is the number of local steps, $T$ is the number of total communication rounds, $m$ is the total worker number and $n$ is the worker number in one communication round if for partial worker participation. Our results also reveal that the local steps in FL could help the convergence and show that the maximum number of local steps can be improved to $T/m$. We conduct extensive experiments on MNIST and CIFAR-10 to verify our theoretical results.",[],[],"['Haibo Yang', 'Minghong Fang', 'Jia Liu']","['Rochester Institute of Technology', 'Duke University', 'The Ohio State University']",[]
https://iclr.cc/virtual/2021/poster/2867,Security,Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks,"Adversarial poisoning attacks distort training data in order to corrupt the test-time behavior of a classifier. A provable defense provides a certificate for each test sample, which is a lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. We propose two novel provable defenses against poisoning attacks: (i) Deep Partition Aggregation (DPA), a certified defense against a general poisoning threat model, defined as the insertion or deletion of a bounded number of samples to the training set --- by implication, this threat model also includes arbitrary distortions to a bounded number of images and/or labels; and (ii) Semi-Supervised DPA (SS-DPA), a certified defense against label-flipping poisoning attacks. DPA is an ensemble method where base models are trained on partitions of the training set determined by a hash function. DPA is related to both subset aggregation, a well-studied ensemble method in classical machine learning, as well as to randomized smoothing, a popular provable defense against evasion (inference) attacks. Our defense against label-flipping poison attacks, SS-DPA, uses a semi-supervised learning algorithm as its base classifier model: each base classifier is trained using the entire unlabeled training set in addition to the labels for a partition. SS-DPA significantly outperforms the existing certified defense for label-flipping attacks (Rosenfeld et al., 2020) on both MNIST and CIFAR-10: provably tolerating, for at least half of test images, over 600 label flips (vs. < 200 label flips) on MNIST and over 300 label flips (vs. 175 label flips) on CIFAR-10. Against general poisoning attacks where no prior certified defenses exists, DPA can certify $\geq$ 50% of test images against over 500 poison image insertions on MNIST, and nine insertions on CIFAR-10. These results establish new state-of-the-art provable defenses against general and label-flipping poison attacks. Code is available at https://github.com/alevine0/DPA","['smoothing', 'poisoning', 'certificate', 'ensemble', 'bagging', 'robustness']",[],"['Alexander Levine', 'Soheil Feizi']","['University of Maryland, College Park', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/2561,Security,Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching,"Data Poisoning attacks modify training data to maliciously control a model trained on such data. In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a particularly malicious poisoning attack that is bothfrom scratch"" andclean label"", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.","['clean-label', 'from-scratch', 'Backdoor Attacks', 'gradient alignment', 'Large-scale', 'Data Poisoning', 'imagenet', 'security']",[],"['Jonas Geiping', 'Liam H Fowl', 'W Ronny Huang', 'Gavin Taylor', 'Michael Moeller', 'Tom Goldstein']","['ELLIS Institute Tübingen', 'Google', 'Google', 'US Naval Academy', 'University of Siegen', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/2969,Security,"On InstaHide, Phase Retrieval, and Sparse Matrix Factorization","In this work, we examine the security of InstaHide, a scheme recently proposed by \cite{hsla20} for preserving the security of private datasets in the context of distributed learning. To generate a synthetic training example to be shared among the distributed learners, InstaHide takes a convex combination of private feature vectors and randomly flips the sign of each entry of the resulting vector with probability 1/2. A salient question is whether this scheme is secure in any provable sense, perhaps under a plausible complexity-theoretic assumption.The answer to this turns out to be quite subtle and closely related to the average-case complexity of a multi-task, missing-data version of the classic problem of phase retrieval that is interesting in its own right. Motivated by this connection, under the standard distributional assumption that the public/private feature vectors are isotropic Gaussian, we design an algorithm that can actually recover a private vector using only the public vectors and a sequence of synthetic vectors generated by InstaHide.","['phase retrieval', 'InstaHide', 'distributed learning', 'matrix factorization']",[],"['Sitan Chen', 'Xiaoxiao Li', 'Zhao Song', 'Danyang Zhuo']","['University of California Berkeley', 'University of British Columbia', 'Adobe Research', 'Duke University']",[]
https://iclr.cc/virtual/2021/poster/2966,Security,Statistical inference for individual fairness,"As we rely on machine learning (ML) models to make more consequential decisions, the issue of ML models perpetuating unwanted social biases has come to the fore of the public's and the research community's attention. In this paper, we focus on the problem of detecting violations of individual fairness in ML models. We formalize the problem as measuring the susceptibility of ML models against a form of adversarial attack and develop a suite of inference tools for the adversarial loss. The tools allow practitioners to assess the individual fairness of ML models in a statistically-principled way: form confidence intervals for the adversarial loss and test hypotheses of model fairness with (asymptotic) non-coverage/Type I error rate control. We demonstrate the utility of our tools in a real-world case study.",[],[],"['Subha Maity', 'Songkai Xue', 'Mikhail Yurochkin', 'Yuekai Sun']","['University of Michigan, Ann Arbor', 'University of Michigan', 'International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/3369,Security,ARMOURED: Adversarially Robust MOdels using Unlabeled data by REgularizing Diversity,"Adversarial attacks pose a major challenge for modern deep neural networks. Recent advancements show that adversarially robust generalization requires a large amount of labeled data for training. If annotation becomes a burden, can unlabeled data help bridge the gap? In this paper, we propose ARMOURED, an adversarially robust training method based on semi-supervised learning that consists of two components. The first component applies multi-view learning to simultaneously optimize multiple independent networks and utilizes unlabeled data to enforce labeling consistency. The second component reduces adversarial transferability among the networks via diversity regularizers inspired by determinantal point processes and entropy maximization. Experimental results show that under small perturbation budgets, ARMOURED is robust against strong adaptive adversaries. Notably, ARMOURED does not rely on generating adversarial samples during training. When used in combination with adversarial training, ARMOURED yields competitive performance with the state-of-the-art adversarially-robust benchmarks on SVHN and outperforms them on CIFAR-10, while offering higher clean accuracy.","['Entropy Maximization', 'Diversity Regularization', 'Multi-View Learning', 'semi-supervised learning', 'adversarial robustness']",[],"['Kangkang Lu', 'Cuong Manh Nguyen', 'Xun Xu', 'Kiran Krishnamachari', 'Yu Jing Goh', 'Chuan-Sheng Foo']","['A*STAR', 'A*STAR', 'A*STAR', 'National University of Singapore', 'National University of Singapore', 'Centre for Frontier AI Research, A*STAR']",[]
https://iclr.cc/virtual/2021/poster/3351,Security,Fooling a Complete Neural Network Verifier,"The efficient and accurate characterization of the robustness of neural networks to input perturbation is an important open problem. Many approaches exist including heuristic and exact (or complete) methods. Complete methods are expensive but their mathematical formulation guarantees that they provide exact robustness metrics. However, this guarantee is valid only if we assume that the verified network applies arbitrary-precision arithmetic and the verifier is reliable. In practice, however, both the networks and the verifiers apply limited-precision floating point arithmetic. In this paper, we show that numerical roundoff errors can be exploited to craft adversarial networks, in which the actual robustness and the robustness computed by a state-of-the-art complete verifier radically differ. We also show that such adversarial networks can be used to insert a backdoor into any network in such a way that the backdoor is completely missed by the verifier. The attack is easy to detect in its naive form but, as we show, the adversarial network can be transformed to make its detection less trivial. We offer a simple defense against our particular attack based on adding a very small perturbation to the network weights. However, our conjecture is that other numerical attacks are possible, and exact verification has to take into account all the details of the computation executed by the verified networks, which makes the problem significantly harder.","['numerical errors', 'complete verifiers', 'adversarial examples']",[],"['Zombori Dániel', 'Tibor Csendes', 'Istvan Megyeri', 'Márk Jelasity']","['University of Szeged', 'University of Szeged', 'University of Szeged', 'University of Szeged']",[]
https://iclr.cc/virtual/2021/poster/3346,Security,Generating Adversarial Computer Programs using Optimized Obfuscations,"Machine learning (ML) models that learn and predict properties of computer programs are increasingly being adopted and deployed. These models have demonstrated success in applications such as auto-completing code, summarizing large programs, and detecting bugs and malware in programs. In this work, we investigate principled ways to adversarially perturb a computer program to fool such learned models, and thus determine their adversarial robustness. We use program obfuscations, which have conventionally been used to avoid attempts at reverse engineering programs, as adversarial perturbations. These perturbations modify programs in ways that do not alter their functionality but can be crafted to deceive an ML model when making a decision. We provide a general formulation for an adversarial program that allows applying multiple obfuscation transformations to a program in any language. We develop first-order optimization algorithms to  efficiently determine two key aspects -- which parts of the program to transform, and what transformations to use. We show that it is important to optimize both these aspects to generate the best adversarially perturbed program. Due to the discrete nature of this problem, we also propose using randomized smoothing to improve the attack loss landscape to ease optimization. We evaluate our work on Python and Java programs on the problem of program summarization. We show that our best attack proposal achieves a $52\%$ improvement over a state-of-the-art attack generation approach for programs trained on a \textsc{seq2seq} model. We further show that our formulation is better at training models that are robust to adversarial attacks.","['Models for code', 'Differentiable program generator', 'combinatorial optimization', 'Program obfuscation', 'Adversarial computer programs', 'Machine Learning (ML) for Programming Languages (PL)/Software Engineering (SE)']",[],"['Shashank Srikant', 'Sijia Liu', 'Tamara Mitrovska', 'Shiyu Chang', 'Quanfu Fan', 'Gaoyuan Zhang', ""Una-May O'Reilly""]","['Massachusetts Institute of Technology', 'Michigan State University', 'Massachusetts Institute of Technology', 'UC Santa Barbara', 'MIT-IBM Watson AI Lab', 'International Business Machines', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/3344,Security,Stabilized Medical Image Attacks,"Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.","['Biometrics', 'Healthcare']",[],"['Gege Qi', 'Lijun GONG', 'Yibing Song', 'Kai Ma', 'Yefeng Zheng']","['Peking University', 'Sensetime', 'Fudan University', 'Tencent', 'Tencent Jarvis Lab']",[]
https://iclr.cc/virtual/2021/poster/3312,Security,Byzantine-Resilient Non-Convex Stochastic Gradient Descent,"We study adversary-resilient stochastic distributed optimization, in which $m$ machines can independently compute stochastic gradients, and cooperate to jointly optimize over their local objective functions. However, an $\alpha$-fraction of the machines are Byzantine, in that they may behave in arbitrary, adversarial ways. We consider a variant of this procedure in the challenging non-convex case. Our main result is a new algorithm SafeguardSGD, which can provably escape saddle points and find approximate local minima of the non-convex objective. The algorithm is based on a new concentration filtering technique, and its sample and time complexity bounds match the best known theoretical bounds in the stochastic, distributed setting when no Byzantine machines are present. Our algorithm is very practical: it improves upon the performance of all prior methods when training deep neural networks, it is relatively lightweight, and it is the first method to withstand two recently-proposed Byzantine attacks.","['Byzantine resilience', 'robust deep learning', 'distributed deep learning', 'distributed machine learning', 'non-convex optimization']",[],"['Jerry Li', 'Dan Alistarh']","['Microsoft', 'Institute of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/3283,Security,"A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference","Recent increases in the computational demands of deep neural networks (DNNs), combined with the observation that most input samples require only simple models, have sparked interest in input-adaptive multi-exit architectures, such as MSDNets or Shallow-Deep Networks. These architectures enable faster inferences and could bring DNNs to low-power devices, e.g., in the Internet of Things (IoT). However, it is unknown if the computational savings provided by this approach are robust against adversarial pressure. In particular, an adversary may aim to slowdown adaptive DNNs by increasing their average inference time—a threat analogous to the denial-of-service attacks from the Internet. In this paper, we conduct a systematic evaluation of this threat by experimenting with three generic multi-exit DNNs (based on VGG16, MobileNet, and ResNet56) and a custom multi-exit architecture, on two popular image classification benchmarks (CIFAR-10 and Tiny ImageNet). To this end, we show that adversarial example-crafting techniques can be modified to cause slowdown, and we propose a metric for comparing their impact on different architectures. We show that a slowdown attack reduces the efficacy of multi-exit DNNs by 90–100%, and it amplifies the latency by 1.5–5× in a typical IoT deployment. We also show that it is possible to craft universal, reusable perturbations and that the attack can be effective in realistic black-box scenarios, where the attacker has limited knowledge about the victim. Finally, we show that adversarial training provides limited protection against slowdowns. These results suggest that further research is needed for defending multi-exit architectures against this emerging threat. Our code is available at https://github.com/sanghyun-hong/deepsloth.","['input-adaptive multi-exit neural networks', 'Slowdown attacks', 'efficient inference', 'adversarial examples']",[],"['Sanghyun Hong', 'Yigitcan Kaya', 'Ionut-Vlad Modoranu', 'Tudor Dumitras']","['Oregon State University', 'University of California, Santa Barbara', 'Institute of Science and Technology Austria', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/3279,Security,Provably robust classification of adversarial examples with detection,"Adversarial attacks against deep networks can be defended against either by building robust classifiers or, by creating classifiers that can \emph{detect} the presence of adversarial perturbations.  Although it may intuitively seem easier to simply detect attacks rather than build a robust classifier, this has not bourne out in practice even empirically, as most detection methods have subsequently been broken by adaptive attacks, thus necessitating \emph{verifiable} performance for detection mechanisms.  In this paper, we propose a new method for jointly training a provably robust classifier and detector.  Specifically, we show that by introducing an additional ""abstain/detection"" into a classifier, we can modify existing certified defense mechanisms to allow the classifier to either robustly classify \emph{or} detect adversarial attacks.  We extend the common interval bound propagation (IBP) method for certified robustness under $\ell_\infty$ perturbations to account for our new robust objective, and show that the method outperforms traditional IBP used in isolation, especially for large perturbation sizes.  Specifically, tests on MNIST and CIFAR-10 datasets exhibit promising results, for example with provable robust error less than $63.63\%$ and $67.92\%$, for $55.6\%$ and $66.37\%$ natural error, for $\epsilon=8/255$ and $16/255$ on the CIFAR-10 dataset, respectively.","['adversarial robustness', 'robust deep learning']",[],"['Fatemeh Sheikholeslami', 'Ali Lotfi', 'J Zico Kolter']","['Amazon', 'University of Texas, Austin', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2021/poster/3264,Security,PAC Confidence Predictions for Deep Neural Network Classifiers,"A key challenge for deploying deep neural networks (DNNs) in safety critical settings is the need to provide rigorous ways to quantify their uncertainty. In this paper, we propose a novel algorithm for constructing predicted classification confidences for DNNs that comes with provable correctness guarantees. Our approach uses Clopper-Pearson confidence intervals for the Binomial distribution in conjunction with the histogram binning approach to calibrated prediction. In addition, we demonstrate how our predicted confidences can be used to enable downstream guarantees in two settings: (i) fast DNN inference, where we demonstrate how to compose a fast but inaccurate DNN with an accurate but slow DNN in a rigorous way to improve performance without sacrificing accuracy, and (ii) safe planning, where we guarantee safety when using a DNN to predict whether a given action is safe based on visual observations. In our experiments, we demonstrate that our approach can be used to provide guarantees for state-of-the-art DNNs.","['safe planning', 'fast DNN inference', 'probably approximated correct guarantee', 'calibration', 'classification']",[],"['Sangdon Park', 'Shuo Li', 'Insup Lee', 'Osbert Bastani']","['POSTECH', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']",[]
https://iclr.cc/virtual/2021/poster/3228,Security,Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models,"The vulnerability of deep networks to adversarial attacks is a central problem for deep learning from the perspective of both cognition and security. The current most successful defense method is to train a classifier using adversarial images created during learning. Another defense approach involves transformation or purification of the original input to remove adversarial signals before the image is classified. We focus on defending naturally-trained classifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial purification. In contrast to adversarial training, our approach is intended to secure highly vulnerable pre-existing classifiers. To our knowledge, no prior defensive transformation is capable of securing naturally-trained classifiers, and our method is the first to validate a post-training defense approach that is distinct from current successful defenses which modify classifier training.The memoryless behavior of long-run MCMC sampling will eventually remove adversarial signals, while metastable behavior preserves consistent appearance of MCMC samples after many steps to allow accurate long-run prediction. Balancing these factors can lead to effective purification and robust classification. We evaluate adversarial defense with an EBM using the strongest known attacks against purification. Our contributions are 1) an improved method for training EBM's with realistic long-run MCMC samples for effective purification, 2) an Expectation-Over-Transformation (EOT) defense that resolves ambiguities for evaluating stochastic defenses and from which the EOT attack naturally follows, and 3) state-of-the-art adversarial defense for naturally-trained classifiers and competitive defense compared to adversarial training on CIFAR-10, SVHN, and CIFAR-100. Our code and pre-trained models are available at https://github.com/point0bar1/ebm-defense.","['Langevin sampling', 'Markov chain Monte Carlo', 'Energy-based model', 'adversarial defense', 'adversarial robustness', 'adversarial attack']",[],"['Mitch Hill', 'Jonathan Craig Mitchell', 'Song-Chun Zhu']","['InnoPeak Technology', 'University of California, Los Angeles', 'Beijing Institute for General Artificial Intelligence']",[]
https://iclr.cc/virtual/2021/poster/3194,Security,R-GAP: Recursive Gradient Attack on Privacy,"Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.","['collaborative learning', 'privacy leakage from gradients', 'federated learning']",[],"['Junyi Zhu', 'Matthew B. Blaschko']","['KU Leuven', 'KU Leuven']",[]
https://iclr.cc/virtual/2021/poster/3088,Security,LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition,"Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike.  These systems are typically built by scraping social media profiles for user images.  Adversarial perturbations have been proposed for bypassing facial recognition systems.  However, existing methods fail on full-scale systems and commercial APIs.  We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases.  Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%.","['facial recognition', 'adversarial attacks']",[],"['Valeriia Cherepanova', 'Micah Goldblum', 'John P Dickerson', 'Gavin Taylor', 'Tom Goldstein']","['University of Maryland, College Park', 'New York University', 'University of Maryland, College Park', 'US Naval Academy', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/3087,Security,WaNet - Imperceptible Warping-based Backdoor Attack,"With the thriving of deep learning and the widespread practice of using pre-trained networks, backdoor attacks have become an increasing security threat drawing many research interests in recent years. A third-party model can be poisoned in training to work well in normal conditions but behave maliciously when a trigger pattern appears. However, the existing backdoor attacks are all built on noise perturbation triggers, making them noticeable to humans. In this paper, we instead propose using warping-based triggers. The proposed backdoor outperforms the previous methods in a human inspection test by a wide margin, proving its stealthiness. To make such models undetectable by machine defenders, we propose a novel training mode, called the ``noise mode. The trained networks successfully attack and bypass the state-ofthe art defense methods on standard classification datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. Behavior analyses show that our backdoors are transparent to network inspection, further proving this novel attack mechanism's efficiency.","['wanet', 'image warping', 'backdoor attack']",[],['Anh Tuan Tran'],['VinAI Research'],[]
https://iclr.cc/virtual/2021/poster/3083,Security,Evaluations and Methods for Explanation through Robustness Analysis,"Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to ""remove"" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.","['adversarial robustness', 'Explanations', 'interpretability']",[],"['Cheng-Yu Hsieh', 'Chih-Kuan Yeh', 'Xuanqing Liu', 'Pradeep Kumar Ravikumar', 'Seungyeon Kim', 'Sanjiv Kumar', 'Cho-Jui Hsieh']","['University of Washington', 'Google', 'University of California, Los Angeles', 'Carnegie Mellon University', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/3066,Security,Benchmarks for Deep Off-Policy Evaluation,"Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.","['benchmarks', 'off-policy evaluation', 'reinforcement learning']",[],"['Justin Fu', 'Mohammad Norouzi', 'Ofir Nachum', 'George Tucker', 'ziyu wang', 'Alexander Novikov', 'Sherry Yang', 'Michael R. Zhang', 'Yutian Chen', 'Aviral Kumar', 'Cosmin Paduraru', 'Sergey Levine', 'Thomas Paine']","['University of California Berkeley', 'Google Brain', 'OpenAI', 'Google Brain', 'Google', 'Deep Mind', 'Google', 'University of Toronto', 'DeepMind', 'University of California Berkeley', 'DeepMind', 'Google', 'Google/DeepMind']",[]
https://iclr.cc/virtual/2021/poster/3051,Security,Average-case Acceleration for Bilinear Games and Normal Matrices,"Advances in generative modeling and adversarial learning have given rise to renewed interest in smooth games. However, the absence of symmetry in the matrix of second derivatives poses challenges that are not present in the classical minimization framework. While a rich theory of average-case analysis has been developed for minimization problems, little is known in the context of smooth games. In this work we take a first step towards closing this gap by developing average-case optimal first-order methods for a subset of smooth games. We make the following three main contributions. First, we show that for zero-sum bilinear games the average-case optimal method is the optimal method for the minimization of the Hamiltonian. Second, we provide an explicit expression for the optimal method corresponding to normal matrices, potentially non-symmetric. Finally, we specialize it to matrices with eigenvalues located in a disk and show a provable speed-up compared to worst-case optimal algorithms. We illustrate our findings through benchmarks with a varying degree of mismatch with our assumptions.","['Orthogonal Polynomials', 'Average-case Analysis', 'Bilinear games', 'acceleration', 'First-order Methods', 'Smooth games']",[],"['Carles Domingo-Enrich', 'Fabian Pedregosa', 'Damien Scieur']","['New York University', 'Google', 'Université Catholique de Louvain']",[]
https://iclr.cc/virtual/2021/poster/3030,Security,Robust Overfitting may be mitigated by properly learned smoothening,"A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\%\sim6.68\%$ and robust accuracy by $0.22\%\sim2 .03\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\ell_{\infty}$ and $\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.","['Robust Overfitting', 'adversarial training', 'adversarial robustness']",[],"['Tianlong Chen', 'Zhenyu Zhang', 'Sijia Liu', 'Shiyu Chang', 'Zhangyang Wang']","['Massachusetts Institute of Technology', 'University of Texas at Austin', 'Michigan State University', 'UC Santa Barbara', 'University of Texas at Austin']",[]
https://iclr.cc/virtual/2021/poster/3015,Security,Uncertainty Estimation in Autoregressive Structured Prediction,"Uncertainty estimation is important for ensuring safety and robustness of AI systems.  While most research in the area has focused on un-structured prediction tasks, limited work has investigated general uncertainty estimation approaches for structured prediction. Thus, this work aims to investigate uncertainty estimation for structured prediction tasks within a single unified and interpretable probabilistic ensemble-based framework.  We consider: uncertainty estimation for sequence data at the token-level and complete sequence-level; interpretations for, and applications of, various measures of uncertainty; and discuss both the theoretical and practical challenges associated with obtaining them. This work also provides baselines for token-level and sequence-level error detection, and sequence-level out-of-domain input detection on the WMT’14 English-French and WMT’17 English-German translation and LibriSpeech speech recognition datasets.","['speech recognition.', 'structures prediction', 'knowledge uncertainty', 'ensembles', 'autoregressive models', 'uncertainty estimation', 'information theory', 'machine translation']",[],"['Andrey Malinin', 'Mark Gales']","['Isomorphic Labs', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2978,Security,Calibration of Neural Networks using Splines,"Calibrating neural networks is of utmost importance when employing them in safety-critical applications where the downstream decision making depends on the predicted probabilities. Measuring calibration error amounts to comparing two empirical distributions. In this work, we introduce a binning-free calibration measure inspired by the classical Kolmogorov-Smirnov (KS) statistical test in which the main idea is to compare the respective cumulative probability distributions. From this, by approximating the empirical cumulative distribution using a differentiable function via splines, we obtain a recalibration function, which maps the network outputs to actual (calibrated) class assignment probabilities. The spline-fitting is performed using a held-out calibration set and the obtained recalibration function is evaluated on an unseen test set. We tested our method against existing calibration approaches on various image classification datasets and our spline-based recalibration approach consistently outperforms existing methods on KS error as well as other commonly used calibration measures. Code is available online at https://github.com/kartikgupta-at-anu/spline-calibration.","['calibration measure', 'neural network calibration', 'uncertainty']",[],"['Thalaiyasingam Ajanthan', 'Thomas Mensink', 'Cristian Sminchisescu', 'Richard Hartley']","['Amazon', 'Google Research', 'Lund University', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2955,Security,How Benign is Benign Overfitting ?,"We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting (Bartlett et al., 2020; Chatterji & Long, 2020).  However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don’t fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. We conjecture that in part sub-optimal representation learning is also responsible for adversarial vulnerability. By means of simple theoretical setups, we show how the choice of representation can drastically affect adversarial robustness.","['Memorization', 'benign overfitting', 'adversarial robustness', 'generalization']",[],"['Amartya Sanyal', 'Puneet K. Dokania', 'Varun Kanade', 'Philip Torr']","['Max-Planck Institute', 'University of Oxford', 'University of Oxford', 'University of Oxford']",[]
https://iclr.cc/virtual/2021/poster/2924,Security,Towards Robustness Against Natural Language Word Substitutions,"Robustness against word substitutions has a well-defined and widely acceptable form, i.e., using semantically similar words as substitutions, and thus it is considered as a fundamental stepping-stone towards broader robustness in natural language processing. Previous defense methods capture word substitutions in vector space by using either l_2-ball or hyper-rectangle, which results in perturbation sets that are not inclusive enough or unnecessarily large, and thus impedes mimicry of worst cases for robust training. In this paper, we introduce a novel Adversarial Sparse Convex Combination (ASCC) method. We model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution, thus aligning our modeling better with the discrete textual space. Based on  ASCC method, we further propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experiments show that ASCC-defense outperforms the current state-of-the-arts in terms of robustness on two prevailing NLP tasks, i.e., sentiment analysis and natural language inference, concerning several attacks across multiple model architectures. Besides, we also envision a new class of defense towards robustness in NLP, where our robustly trained word vectors can be plugged into a normally trained model and enforce its robustness without applying any other defense techniques.","['adversarial defense', 'natural language processing']",[],"['Xinshuai Dong', 'Anh Tuan Luu', 'Rongrong Ji', 'Hong Liu']","['Carnegie Mellon University', 'Nanyang Technological University', 'Xiamen University', 'Osaka University, Tokyo Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2906,Security,On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning,"Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\textit{meta-initialization}$ of model parameters (that we call $\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\textit{how}$ robust regularization can $\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.",[],[],"['Ren Wang', 'Kaidi Xu', 'Sijia Liu', 'Pin-Yu Chen', 'Tsui-Wei Weng', 'Chuang Gan', 'Meng Wang']","['Illinois Institute of Technology', 'Drexel University', 'Michigan State University', 'International Business Machines', 'University of California, San Diego', 'MIT-IBM Watson AI Lab', 'Rensselaer Polytechnic Institute']",[]
https://iclr.cc/virtual/2021/poster/2899,Security,Enforcing robust control guarantees within neural network policies,"When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often yield simple controllers that perform poorly in the average (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. In this paper, we propose a technique that combines the strengths of these two approaches: constructing a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Specifically, our approach entails integrating custom convex-optimization-based projection layers into a neural network-based policy. We demonstrate the power of this approach on several domains, improving in average-case performance over existing robust control methods and in worst-case stability over (non-robust) deep RL methods.","['differentiable optimization', 'robust control', 'reinforcement learning']",[],"['Priya L. Donti', 'Melrose Roderick', 'J Zico Kolter']","['Massachusetts Institute of Technology', 'Mila, University of Montreal', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2021/poster/2898,Security,Contrastive Divergence Learning is a Time Reversal Adversarial Game,"Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).","['noise contrastive estimation', 'contrastive divergence', 'energy based model', 'unsupervised learning', 'adversarial learning']",[],"['Omer Yair', 'Tomer Michaeli']","['Technion, Technion', 'Technion, Technion']",[]
https://iclr.cc/virtual/2021/poster/2896,Security,Domain-Robust Visual Imitation Learning with Mutual Information Constraints,"Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.","['Domain Adaption', 'Third-Person Imitation', 'Observational Imitation', 'imitation learning', 'mutual information', 'reinforcement learning', 'machine learning']",[],"['Edoardo Cetin', 'Oya Celiktutan']","[""King's College London"", ""King's College London, University of London""]",[]
https://iclr.cc/virtual/2021/poster/2886,Security,Private Image Reconstruction from System Side Channels Using Generative Models,"System side channels denote effects imposed on the underlying system and hardware when running a program, such as its accessed CPU cache lines. Side channel analysis (SCA) allows attackers to infer program secrets based on observed side channel signals. Given the ever-growing adoption of machine learning as a service (MLaaS), image analysis software on cloud platforms has been exploited by reconstructing private user images from system side channels. Nevertheless, to date, SCA is still highly challenging, requiring technical knowledge of victim software's internal operations. For existing SCA attacks, comprehending such internal operations requires heavyweight program analysis or manual efforts.This research proposes an attack framework to reconstruct private user images processed by media software via system side channels. The framework forms an effective workflow by incorporating convolutional networks, variational autoencoders, and generative adversarial networks. Our evaluation of two popular side channels shows that the reconstructed images consistently match user inputs, making privacy leakage attacks more practical. We also show surprising results that even one-bit data read/write pattern side channels, which are deemed minimally informative, can be used to reconstruct quality images using our framework.",['side channel analysis'],[],"['Yuanyuan Yuan', 'Shuai Wang', 'Junping Zhang']","['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology', 'Fudan University']",[]
https://iclr.cc/virtual/2021/poster/2870,Security,Shape-Texture Debiased Neural Network Training,"Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously.Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.","['debiased training', 'data augmentation', 'representation learning']",[],"['Yingwei Li', 'Qihang Yu', 'Mingxing Tan', 'Jieru Mei', 'Peng Tang', 'Wei Shen', 'Alan Yuille', 'Cihang Xie']","['Waymo LLC', 'ByteDance', 'Google', 'Johns Hopkins University', 'Amazon', 'Shanghai Jiao Tong University', 'Johns Hopkins University', 'University of California, Santa Cruz']",[]
https://iclr.cc/virtual/2021/poster/2860,Security,Perceptual Adversarial Robustness: Defense Against Unseen Threat Models,"A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to get around this issue by considering restrictive adversarial threat models such as those bounded by $L_2$ or $L_\infty$ distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models, i.e. they have poor generalization to unforeseen attacks. Moreover, even if a model is robust against the union of several restrictive threat models, it is still susceptible to other imperceptible adversarial examples that are not contained in any of the constituent threat models. To resolve these issues, we propose adversarial training against the set of all imperceptible adversarial examples. Since this set is intractable to compute without a human in the loop, we approximate it using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks: $L_2$, $L_\infty$, spatial, recoloring, and JPEG. We find that PAT achieves state-of-the-art robustness against the union of these five attacks—more than doubling the accuracy over the next best model—without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial defense with this property. Code and data are available at https://github.com/cassidylaidlaw/perceptual-advex",[],[],"['Cassidy Laidlaw', 'Sahil Singla', 'Soheil Feizi']","['University of California Berkeley', 'University of Maryland, College Park', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/2859,Security,Deep Neural Network Fingerprinting by Conferrable Adversarial Examples,"In Machine Learning as a Service, a provider trains a deep neural network and gives many users access. The hosted (source) model is susceptible to model stealing attacks, where an adversary derives a surrogate model from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model. We propose a fingerprinting method for deep neural network classifiers that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classification of such inputs. These inputs are a subclass of transferable adversarial examples which we call conferrable adversarial examples that exclusively transfer with a target label from a source model to its surrogates. We propose a new method to generate these conferrable adversarial examples. We present an extensive study on the irremovability of our fingerprint against fine-tuning, weight pruning, retraining, retraining with different architectures, three model extraction attacks from related work, transfer learning, adversarial training, and two new adaptive attacks. Our fingerprint is robust against distillation, related model extraction attacks, and even transfer learning when the attacker has no access to the model provider's dataset. Our fingerprint is the first method that reaches a ROC AUC of 1.0 in verifying surrogates, compared to a ROC AUC of 0.63 by previous fingerprints.","['Conferrability', 'Transferability', 'Fingerprinting', 'adversarial examples']",[],"['Nils Lukas', 'Yuxuan Zhang', 'Florian Kerschbaum']","['University of Waterloo', 'Princeton University', 'University of Waterloo']",[]
https://iclr.cc/virtual/2021/poster/2848,Security,Meta-Learning with Neural Tangent Kernels,"Model Agnostic Meta-Learning (MAML) has emerged as a standard framework for meta-learning, where a meta-model is learned with the ability of fast adapting to new tasks. However, as a double-looped optimization problem, MAML needs to differentiate through the whole inner-loop optimization path for every outer-loop training step, which may lead to both computational inefficiency and sub-optimal solutions. In this paper, we generalize MAML to allow meta-learning to be defined in function spaces, and propose the first meta-learning paradigm in the Reproducing Kernel Hilbert Space (RKHS) induced by the meta-model's Neural Tangent Kernel (NTK). Within this paradigm, we introduce two meta-learning algorithms in the RKHS, which no longer need a sub-optimal iterative inner-loop adaptation as in the MAML framework. We achieve this goal by 1) replacing the adaptation with a fast-adaptive regularizer in the RKHS; and 2) solving the adaptation analytically based on the NTK theory. Extensive experimental studies demonstrate advantages of our paradigm in both efficiency and quality of solutions compared to related meta-learning algorithms. Another interesting feature of our proposed methods is that they are demonstrated to be more robust to adversarial attacks and out-of-distribution adaptation than popular baselines, as demonstrated in our experiments.","['neural tangent kernel', 'meta-learning']",[],"['Yufan Zhou', 'Zhenyi Wang', 'Jiayi Xian', 'Changyou Chen', 'Jinhui Xu']","['Adobe ', 'University of Maryland, College Park', 'State University of New York, Buffalo', 'State University of New York, Buffalo', 'State University of New York, Buffalo']",[]
https://iclr.cc/virtual/2021/poster/2823,Security,Policy-Driven Attack: Learning to Query for Hard-label Black-box Adversarial Examples,"To craft black-box adversarial examples, adversaries need to query the victim model and take proper advantage of its feedback. Existing black-box attacks generally suffer from high query complexity, especially when only the top-1 decision (i.e., the hard-label prediction) of the victim model is available.  In this paper, we propose a novel hard-label black-box attack named Policy-Driven Attack, to reduce the query complexity. Our core idea is to learn promising search directions of the adversarial examples using a well-designed policy network in a novel reinforcement learning formulation, in which the queries become more sensible. Experimental results demonstrate that our method can significantly reduce the query complexity in comparison with existing state-of-the-art hard-label black-box attacks on various image classification benchmark datasets. Code and models for reproducing our results are available at https://github.com/ZiangYan/pda.pytorch","['hard-label attack', 'black-box attack', 'adversarial attack', 'reinforcement learning']",[],"['Ziang Yan', 'Yiwen Guo', 'Jian Liang', 'Changshui Zhang']","['Tsinghua University', 'ByteDance', 'Alibaba Group', 'Tsinghua University']",[]
https://iclr.cc/virtual/2021/poster/2788,Security,"Heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds","In the present work we study classifiers' decision boundaries via Brownian motion processes in ambient data space and associated probabilistic techniques. Intuitively, our ideas correspond to placing a heat source at the decision boundary and observing how effectively the sample points warm up. We are largely motivated by the search for a soft measure that sheds further light on the decision boundary's geometry. En route, we  bridge aspects of potential theory and geometric analysis (Maz'ya 2011, Grigor'Yan and Saloff-Coste 2002) with active fields of ML research such as adversarial examples and generalization bounds. First, we focus on the geometric behavior of decision boundaries in the light of adversarial attack/defense mechanisms. Experimentally, we observe a certain capacitory trend over different adversarial defense strategies: decision boundaries locally become flatter as measured by isoperimetric inequalities (Ford et al 2019); however, our more sensitive heat-diffusion metrics  extend this analysis and further reveal that some non-trivial geometry invisible to plain distance-based methods is still preserved. Intuitively, we provide evidence that the decision boundaries nevertheless retain many persistent ""wiggly and fuzzy"" regions on a finer scale. Second, we show how Brownian hitting probabilities translate to soft generalization bounds which are in turn connected to compression and noise stability (Arora et al 2018), and these bounds are significantly stronger if the decision boundary has controlled geometric features.","['adversarial attacks/defenses', 'curvature estimates', 'decision boundary geometry', 'Brownian motion', 'generalization bounds', 'deep learning theory']",[],"['Bogdan Georgiev', 'Lukas Franken', 'Mayukh Mukherjee']","['Fraunhofer IAIS', 'Fraunhofer Institute IAIS, Fraunhofer IAIS', 'Indian Institute of Technology Bombay']",[]
https://iclr.cc/virtual/2021/poster/2760,Security,Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks,"Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a training time attack that injects a trigger pattern into a small proportion of training data so as to control the model's prediction at the test time. Backdoor attacks are notably dangerous since they do not affect the model's performance on clean examples, yet can fool the model to make the incorrect prediction whenever the trigger pattern appears during testing. In this paper, we propose a novel defense framework Neural Attention Distillation (NAD) to erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to guide the finetuning of the backdoored student network on a small clean subset of data such that the intermediate-layer attention of the student network aligns with that of the teacher network. The teacher network can be obtained by an independent finetuning process on the same clean subset. We empirically show, against 6 state-of-the-art backdoor attacks,  NAD can effectively erase the backdoor triggers using only 5\% clean training data without causing obvious performance degradation on clean examples. Our code is available at https://github.com/bboylyg/NAD.","['Neural Attention Distillation', 'Backdoor Defense', 'deep neural networks']",[],"['Yige Li', 'Xixiang Lyu', 'Nodens Koren', 'Lingjuan Lyu', 'Bo Li', 'Xingjun Ma']","['Xidian University', ""Xi'an University of Electronic Science and Technology"", 'ETHZ - ETH Zurich', 'Sony Research', 'University of Illinois, Urbana Champaign', 'Fudan University']",[]
https://iclr.cc/virtual/2021/poster/2746,Security,Contemplating Real-World Object Classification,"Deep object recognition models have been very successful over benchmark datasets such as ImageNet. How accurate and robust are they to distribution shifts arising from natural and synthetic variations in datasets? Prior research on this problem has primarily focused on ImageNet variations (e.g., ImageNetV2, ImageNet-A). To avoid potential inherited biases in these studies, we take a different approach. Specifically, we reanalyze the ObjectNet dataset recently proposed by Barbu et al. containing objects in daily life situations. They showed a dramatic performance drop of the state of the art object recognition models on this dataset. Due to the importance and implications of their results regarding the generalization ability of deep models, we take a second look at their analysis. We find that applying deep models to the isolated objects, rather than the entire scene as is done in the original paper, results in around 20-30% performance improvement. Relative to the numbers reported in Barbu et al., around 10-15% of the performance loss is recovered, without any test time data augmentation. Despite this gain, however, we conclude that deep models still suffer drastically on the ObjectNet dataset. We also investigate the robustness of models against synthetic image perturbations such as geometric transformations (e.g., scale, rotation, translation), natural image distortions (e.g., impulse noise, blur) as well as adversarial attacks (e.g., FGSM and PGD-5). Our results indicate that limiting the object area as much as possible (i.e., from the entire image to the bounding box to the segmentation mask) leads to consistent improvement in accuracy and robustness. Finally, through a qualitative analysis of ObjectNet data, we find that i) a large number of images in this dataset are hard to recognize even for humans, and ii) easy (hard) samples for models match with easy (hard) samples for humans. Overall, our analysis shows that ObjecNet is still a challenging test platform that can be used to measure the generalization ability of models. The code and data are available in [masked due to blind review].","['robustness', 'ObjectNet', 'object recognition', 'deep learning']",[],['ali borji'],['PrimerAI'],[]
https://iclr.cc/virtual/2021/poster/2734,Security,How Does Mixup Help With Robustness and Generalization?,"Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However,  it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.","['MixUp', 'adversarial robustness', 'generalization']",[],"['Linjun Zhang', 'Zhun Deng', 'Kenji Kawaguchi', 'Amirata Ghorbani', 'James Zou']","['Rutgers University', 'Columbia University', 'National University of Singapore', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/2665,Security,Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics,"Poisoning attacks on Reinforcement Learning (RL) systems could take advantage of RL algorithm’s vulnerabilities and cause failure of the learning. However, prior works on poisoning RL usually either unrealistically assume the attacker knows the underlying Markov Decision Process (MDP), or directly apply the poisoning methods in supervised learning to RL. In this work, we build a generic poisoning framework for online RL via a comprehensive investigation of heterogeneous poisoning models in RL. Without any prior knowledge of the MDP, we propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. VA2C-P uses a novel metric, stability radius in RL, that measures the vulnerability of RL algorithms. Experiments on multiple deep RL agents and multiple environments show that our poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget.","['deep RL', 'vulnerability of RL', 'poisoning attack', 'policy gradient']",[],"['Yanchao Sun', 'Da Huo', 'Furong Huang']","['J.P. Morgan AI Research', 'Shanghai Jiao Tong University', 'Department of Computer Science, University of Maryland']",[]
https://iclr.cc/virtual/2021/poster/2636,Security,Distributed Momentum for Byzantine-resilient Stochastic Gradient Descent,"Byzantine-resilient Stochastic Gradient Descent (SGD) aims at shielding model training from Byzantine faults, be they ill-labeled training datapoints, exploited software/hardware vulnerabilities, or malicious worker nodes in a distributed setting. Two recent attacks have been challenging state-of-the-art defenses though, often successfully precluding the model from even fitting the training set. The main identified weakness in current defenses is their requirement of a sufficiently low variance-norm ratio for the stochastic gradients. We propose a practical method which, despite increasing the variance, reduces the variance-norm ratio, mitigating the identified weakness. We assess the effectiveness of our method over 736 different training configurations, comprising the 2 state-of-the-art attacks and 6 defenses. For confidence and reproducibility purposes, each configuration is run 5 times with specified seeds (1 to 5), totalling 3680 runs. In our experiments, when the attack is effective enough to decrease the highest observed top-1 cross-accuracy by at least 20% compared to the unattacked run, our technique systematically increases back the highest observed accuracy, and is able to recover at least 20% in more than 60% of the cases.","['momentum', 'Distributed ML', 'Byzantine SGD']",[],"['El-Mahdi El-Mhamdi', 'Rachid Guerraoui', 'Sébastien Rouault']","['Calicarpa', 'Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne']",[]
https://iclr.cc/virtual/2021/poster/2631,Security,Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits,"To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.","['weight attack', 'bit-flip', 'targeted attack']",[],"['Jiawang Bai', 'Baoyuan Wu', 'Yong Zhang', 'Yiming Li', 'Zhifeng Li', 'Shu-Tao Xia']","['Tsinghua University, Tsinghua University', 'The Chinese University of Hong Kong, Shenzhen', 'Tencent AI Lab', 'Zhejiang University', 'Tencent', 'Shenzhen International Graduate School, Tsinghua University']",[]
https://iclr.cc/virtual/2021/poster/2629,Security,Efficient Certified Defenses Against Patch Attacks on Image Classifiers,"Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.","['aversarial examples', 'adversarial patch', 'certified defense', 'robustness']",[],"['Jan Hendrik Metzen', 'Maksym Yatsura']","['Bosch Center Artificial Intelligence', 'Robert Bosch GmbH, Bosch']",[]
https://iclr.cc/virtual/2021/poster/2624,Security,Robust Reinforcement Learning on State Observations with Learned Optimal Adversary,"We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLArobustRL.","['adversarial defense', 'robustness', 'adversarial attacks', 'reinforcement learning']",[],"['Huan Zhang', 'Hongge Chen', 'Duane S Boning', 'Cho-Jui Hsieh']","['University of Illinois at Urbana-Champaign', 'Cruise AI Research', 'Massachusetts Institute of Technology', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2600,Security,Conservative Safety Critics for Exploration,"Safe exploration presents a major challenge in reinforcement learning (RL): when active data collection requires deploying partially trained policies, we must ensure that these policies avoid catastrophically unsafe regions, while still enabling trial and error learning. In this paper, we target the problem of safe exploration in RL, by learning a conservative safety estimate of environment states through a critic, and provably upper bound the likelihood of catastrophic failures at every training iteration. We theoretically characterize the tradeoff between safety and policy improvement, show that the safety constraints are satisfied with high probability during training, derive provable convergence guarantees for our approach which is no worse asymptotically then standard RL, and empirically demonstrate the efficacy of the proposed approach on a suite of challenging navigation, manipulation, and locomotion tasks. Our results demonstrate that the proposed approach can achieve competitive task performance, while incurring significantly lower catastrophic failure rates during training as compared to prior methods. Videos are at this URL https://sites.google.com/view/conservative-safety-critics/","['Safe exploration', 'reinforcement learning']",[],"['Homanga Bharadhwaj', 'Aviral Kumar', 'Nicholas Rhinehart', 'Sergey Levine', 'Florian Shkurti', 'Animesh Garg']","['Facebook', 'University of California Berkeley', 'Waymo Research', 'Google', 'Department of Computer Science, University of Toronto', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2594,Security,Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors,"Predictive uncertainty estimation is an essential next step for the reliable deployment of deep object detectors in safety-critical tasks. In this work, we focus on estimating predictive distributions for bounding box regression output with variance networks. We show that in the context of object detection, training variance networks with negative log likelihood (NLL) can lead to high entropy predictive distributions regardless of the correctness of the output mean. We propose to use the energy score as a non-local proper scoring rule and find that when used for training, the energy score leads to better calibrated and lower entropy predictive distributions than NLL. We also address the widespread use of non-proper scoring metrics for evaluating predictive distributions from deep object detectors by proposing an alternate evaluation approach founded on proper scoring rules. Using the proposed evaluation tools, we show that although variance networks can be used to produce high quality predictive distributions, ad-hoc approaches used by seminal object detectors for choosing regression targets during training do not provide wide enough data support for reliable variance learning. We hope that our work helps shift evaluation in probabilistic object detection to better align with predictive uncertainty evaluation in other machine learning domains. Code for all models, evaluation, and datasets is available at: https://github.com/asharakeh/probdet.git.","['Energy Score', 'Variance Networks', 'Proper Scoring Rules', 'Predictive Uncertainty Estimation', 'computer vision', 'object detection']",[],"['Ali Harakeh', 'Steven L. Waslander']","['Montreal Institute for Learning Algorithms', 'University of Toronto']",[]
https://iclr.cc/virtual/2021/poster/2585,Security,Improving VAEs' Robustness to Adversarial Attack,"Variational autoencoders (VAEs) have recently been shown to be vulnerable to adversarial attacks, wherein they are fooled into reconstructing a chosen target image. However, how to defend against such attacks remains an open problem. We make significant advances in addressing this issue by introducing methods for producing adversarially robust VAEs. Namely, we first demonstrate that methods proposed to obtain disentangled latent representations produce VAEs that are more robust to these attacks. However, this robustness comes at the cost of reducing the quality of the reconstructions. We ameliorate this by applying disentangling methods to hierarchical VAEs. The resulting models produce high--fidelity autoencoders that are also adversarially robust. We confirm their capabilities on several different datasets and with current state-of-the-art VAE adversarial attacks, and also show that they increase the robustness of downstream tasks to attack.","['adversarial attack', 'variational autoencoders', 'robustness', 'deep generative models']",[],"['Matthew Willetts', 'Alexander Camuto', 'Tom Rainforth', 'S Roberts', 'Christopher C. Holmes']","['University College London', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford']",[]
https://iclr.cc/virtual/2021/poster/3226,Security,Beyond Categorical Label Representations for Image Classification,"We find that the way we choose to represent data labels can have a profound effect on the quality of trained models. For example, training an image classifier to regress audio labels rather than traditional categorical probabilities produces a more reliable classification. This result is surprising, considering that audio labels are more complex than simpler numerical probabilities or text. We hypothesize that high dimensional, high entropy label representations are generally more useful because they provide a stronger error signal. We support this hypothesis with evidence from various label representations including constant matrices, spectrograms, shuffled spectrograms, Gaussian mixtures, and uniform random matrices of various dimensionalities. Our experiments reveal that high dimensional, high entropy labels achieve comparable accuracy to text (categorical) labels on standard image classification tasks, but features learned through our label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data. These results suggest that label representation may play a more important role than previously thought.","['image classification', 'Label Representation', 'representation learning']",[],"['Boyuan Chen', 'Sunand Raghupathi', 'Hod Lipson']","['Duke University', 'Columbia University', 'Columbia University']",[]
https://iclr.cc/virtual/2021/poster/3123,Security,InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective,"Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust ﬁne-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT.","['QA', 'NLI', 'BERT', 'adversarial robustness', 'information theory', 'adversarial training']",[],"['Boxin Wang', 'Shuohang Wang', 'Yu Cheng', 'Zhe Gan', 'Ruoxi Jia', 'Bo Li', 'Jingjing Liu']","['NVIDIA', 'Microsoft', 'Microsoft Research', 'Apple', 'Virginia Tech', 'University of Illinois, Urbana Champaign', 'Tsinghua University']",[]
https://iclr.cc/virtual/2021/poster/2745,Security,Dataset Inference: Ownership Resolution in Machine Learning,"With increasingly more data and computation involved in their training,  machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient:  the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce $\textit{dataset inference}$, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.","['MLaaS', 'model extraction', 'model ownership']",[],"['Pratyush Maini', 'Mohammad Yaghini', 'Nicolas Papernot']","['Carnegie Mellon University', 'University of Toronto, Vector Institute', 'University of Toronto']",[]
https://iclr.cc/virtual/2021/poster/2820,Security,Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates,"We study the multi-agent safe control problem where agents should avoid collisions to static obstacles and collisions with each other while reaching their goals. Our core idea is to learn the multi-agent control policy jointly with  learning the control barrier functions as safety certificates. We propose a new joint-learning framework that can be implemented in a decentralized fashion, which can adapt to an arbitrarily large number of agents. Building upon this framework, we further improve the scalability by  incorporating neural network architectures  that are invariant to the quantity and permutation of neighboring agents. In addition, we propose a new spontaneous policy refinement method to further enforce the certificate condition during testing. We provide extensive experiments to demonstrate that our method significantly outperforms other leading multi-agent control approaches in terms of maintaining safety and completing original tasks. Our approach also shows substantial generalization capability in that the control policy can be trained with 8 agents in one scenario, while being used on other scenarios with up to 1024 agents in complex multi-agent environments and dynamics. Videos and source code can be found at https://realm.mit.edu/blog/learning-safe-multi-agent-control-decentralized-neural-barrier-certificates.","['control barrier function', 'safe', 'multi-agent', 'reinforcement learning']",[],"['Zengyi Qin', 'Kaiqing Zhang', 'Jingkai Chen', 'Chuchu Fan']","['Massachusetts Institute of Technology', 'University of Maryland, College Park', 'Amazon Robotics', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2696,Security,Protecting DNNs from Theft using an Ensemble of Diverse Models,"Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.","['Model stealing', 'machine learning security']",[],"['Sanjay Kariyappa', 'Atul Prakash', 'Moinuddin K Qureshi']","['J.P. Morgan Chase', 'University of Michigan', 'Georgia Institute of Technology']",[]