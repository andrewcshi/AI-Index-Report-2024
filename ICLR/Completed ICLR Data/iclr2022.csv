link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://iclr.cc/virtual/2022/poster/6601,Transparency & Explainability,Huber Additive Models for Non-stationary Time Series Analysis,"Sparse additive models have shown promising ﬂexibility and interpretability in processing time series data. However, existing methods usually assume the time series data to be stationary and the innovation is sampled from a Gaussian distribution. Both assumptions are too stringent for heavy-tailed and non-stationary time series data that frequently arise in practice, such as ﬁnance and medical ﬁelds. To address these problems, we propose an adaptive sparse Huber additive model for robust forecasting in both non-Gaussian data and (non)stationary data. In theory, the generalization bounds of our estimator are established for both stationary and nonstationary time series data, which are independent of the widely used mixing conditions in learning theory of dependent observations. Moreover, the error bound for non-stationary time series contains a discrepancy measure for the shifts of the data distributions over time. Such a discrepancy measure can be estimated empirically and used as a penalty in our method. Experimental results on both synthetic and real-world benchmark datasets validate the effectiveness of the proposed method. The code is available at https://github.com/xianruizhong/SpHAM.",[],[],"['Yingjie Wang', 'Xianrui Zhong', 'Fengxiang He', 'Hong Chen', 'Dacheng Tao']","['China University of Petroleum', 'University of Illinois, Urbana Champaign', 'University of Edinburgh', 'Huazhong Agricultural University', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6461,Transparency & Explainability,Optimization inspired Multi-Branch Equilibrium Models,"Works have shown the strong connections between some implicit models and optimization problems. However, explorations on such relationships are limited. Most works pay attention to some common mathematical properties, such as sparsity. In this work, we propose a new type of implicit model inspired by the designing of the systems' hidden objective functions, called the Multi-branch Optimization induced Equilibrium networks~(MOptEqs). The model architecture is designed based on modelling the hidden objective function for the multi-resolution recognition task. Furthermore, we also propose a new training strategy inspired by our understandings of the hidden objective function. In this manner, the proposed model can better utilize the hierarchical patterns for recognition tasks and retain the abilities for interpreting the whole structure as trying to obtain the minima of the problem's goal. Comparing with the state-of-the-art models, our MOptEqs not only enjoys better explainability but are also superior to MDEQ with less parameter consumption and better performance on practical tasks. Furthermore, we also implement various experiments to demonstrate the effectiveness of our new methods and explore the applicability of the model's hidden objective function.",[],[],"['Mingjie Li', 'Yisen Wang', 'Xingyu Xie', 'Zhouchen Lin']","['Peking University', 'Peking University', 'National University of Singapore', 'Peking University']",[]
https://iclr.cc/virtual/2022/poster/6680,Transparency & Explainability,Granger causal inference on DAGs identifies genomic loci regulating transcription,"When a dynamical system can be modeled as a sequence of observations, Granger causality is a powerful approach for detecting predictive interactions between its variables. However, traditional Granger causal inference has limited utility in domains where the dynamics need to be represented as directed acyclic graphs (DAGs) rather than as a linear sequence, such as with cell differentiation trajectories. Here, we present GrID-Net, a framework based on graph neural networks with lagged message passing for Granger causal inference on DAG-structured systems. Our motivating application is the analysis of single-cell multimodal data to identify genomic loci that mediate the regulation of specific genes. To our knowledge, GrID-Net is the first single-cell analysis tool that accounts for the temporal lag between a genomic locus becoming accessible and its downstream effect on a target gene's expression. We applied GrID-Net on multimodal single-cell assays that profile chromatin accessibility (ATAC-seq) and gene expression (RNA-seq) in the same cell and show that it dramatically outperforms existing methods for inferring regulatory locus-gene links, achieving up to 71% greater agreement with independent population genetics-based estimates. By extending Granger causality to DAG-structured dynamical systems, our work unlocks new domains for causal analyses and, more specifically, opens a path towards elucidating gene regulatory interactions relevant to cellular differentiation and complex human diseases at unprecedented scale and resolution.","['Granger causality', 'Directed Acyclic Graphs', 'causal inference', 'graph neural networks']",[],"['Alexander P Wu', 'Rohit Singh', 'Bonnie Berger']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6645,Transparency & Explainability,Modeling Label Space Interactions in Multi-label Classification using Box Embeddings,"Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classification methods do not do so, resulting in label predictions that are inconsistent with the taxonomic constraints, thus failing to accurately represent the fundamentals of problem setting. In this work, we introduce the multi-label box model (MBM), a multi-label classification method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018).  Box embeddings can be understood as trainable Venn-diagrams based on hyper-rectangles.  Representing labels by boxes rather than vectors, MBM is able to capture taxonomic relations among labels.  Furthermore, since box embeddings allow these relations to be learned by stochastic gradient descent from data, and to be read as calibrated conditional probabilities, our model is endowed with a high degree of interpretability. This interpretability also facilitates the injection of partial information about label-label relationships into model training, to further improve its consistency. We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data. Through extensive empirical evaluations on both small and large-scale multi-label classification datasets, we show that BBM can significantly improve taxonomic consistency while preserving or surpassing the state-of-the-art predictive performance.","['embeddings', 'multi-label classification', 'representation learning']",[],"['Dhruvesh Patel', 'Pavitra Dangati', 'Jay-Yoon Lee', 'Michael Boratko', 'Andrew McCallum']","['College of Information and Computer Science, University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'Seoul National University', 'Google', 'Department of Computer Science, University of Massachusetts, Amherst']",[]
https://iclr.cc/virtual/2022/poster/6555,Transparency & Explainability,Discovering Invariant Rationales for Graph Neural Networks,"Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features --- rationale --- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.","['graph neural networks', 'interpretability']",[],"['Shirley Wu', 'Xiang Wang', 'An Zhang', 'Xiangnan He', 'Tat-Seng Chua']","['Computer Science Department, Stanford University', 'University of Science and Technology of China', 'National University of Singapore', 'University of Science and Technology of China', 'National University of Singapore']",[]
https://iclr.cc/virtual/2022/poster/6442,Transparency & Explainability,Meta Discovery: Learning to Discover Novel Classes given Very Limited Data,"In novel class discovery (NCD), we are given labeled data from seen classes and unlabeled data from unseen classes, and we train clustering models for the unseen classes. However, the implicit assumptions behind NCD are still unclear. In this paper, we demystify assumptions behind NCD and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, NCD is theoretically solvable under certain assumptions and can be naturally linked to meta-learning that has exactly the same assumption as NCD. Thus, we can empirically solve the NCD problem by meta-learning algorithms after slight modifications. This meta-learning-based methodology significantly reduces the amount of unlabeled data needed for training and makes it more practical, as demonstrated in experiments. The use of very limited data is also justified by the application scenario of NCD: since it is unnatural to label only seen-class data, NCD is sampling instead of labeling in causality. Therefore, unseen-class data should be collected on the way of collecting seen-class data, which is why they are novel and first need to be clustered.",[],[],"['Haoang Chi', 'Feng Liu', 'Wenjing Yang', 'Long Lan', 'Tongliang Liu', 'Bo Han', 'Gang Niu', 'Mingyuan Zhou', 'Masashi Sugiyama']","['National University of Defense Technology', 'University of Melbourne', 'National University of Defense Technology', 'National University of Defense Technology', 'University of Sydney', 'HKBU', 'RIKEN', 'The University of Texas at Austin', 'RIKEN']",[]
https://iclr.cc/virtual/2022/poster/6367,Transparency & Explainability,DISSECT: Disentangled Simultaneous Explanations via Concept Traversals,"Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore ""what-if"" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent ""notion"" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.","['explainability', 'generative adversarial network', 'variational autoencoder', 'interpretability']",[],"['Asma Ghandeharioun', 'Been Kim', 'Chun-Liang Li', 'Brendan Jou', 'Brian Eoff', 'Rosalind Picard']","['Google', 'Google DeepMind', 'Google', 'Google', 'Google', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6355,Transparency & Explainability,NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning,"Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability. Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable compared to other ensemble and deep learning models. We demonstrate that our models find interesting patterns in the data. Lastly, we show that we are able to improve model accuracy via self-supervised pre-training, an improvement that is not possible for non-differentiable GAMs.","['generalized additive model', 'interpretability']",[],"['Chun-Hao Chang', 'Rich Caruana', 'Anna Goldenberg']","['University of Toronto', 'School of Computer Science, Carnegie Mellon University', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6146,Transparency & Explainability,Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing,"Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i.e. understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters in representation space. This approach, which we call reverse linear probing, provides a single number sensitive to the semanticity of the representation. This measure is also able to detect when the representation contains combinations of concepts (e.g., ""red apple'') instead of just individual attributes (""red'' and ""apple'' independently). Finally, we propose to use supervised classifiers to automatically label large datasets in order to enrich the space of concepts used for probing. We use our method to evaluate a large number of self-supervised representations, ranking them by interpretability, highlight the differences that emerge compared to the standard evaluation with linear probes and discuss several qualitative insights. Code at: https://github.com/iro-cp/ssl-qrp.","['computer vision', 'representation learning', 'interpretability']",[],"['Iro Laina', 'Yuki M Asano', 'Andrea Vedaldi']","['University of Oxford', 'University of Amsterdam', 'University of Oxford']",[]
https://iclr.cc/virtual/2022/poster/5926,Transparency & Explainability,Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph,"This paper addresses the unsupervised learning of content-style decomposed representation. We first give a definition of style and then model the content-style representation as a token-level bipartite graph. An unsupervised framework, named Retriever, is proposed to learn such representations. First, a cross-attention module is employed to retrieve permutation invariant (P.I.) information, defined as style, from the input data. Second, a vector quantization (VQ) module is used, together with man-induced constraints, to produce interpretable content tokens. Last, an innovative link attention module serves as the decoder to reconstruct data from the decomposed content and style, with the help of the linking keys. Being modal-agnostic, the proposed Retriever is evaluated in both speech and image domains. The state-of-the-art zero-shot voice conversion performance confirms the disentangling ability of our framework. Top performance is also achieved in the part discovery task for images, verifying the interpretability of our representation. In addition, the vivid part-based style transfer quality demonstrates the potential of Retriever to support various fascinating generative tasks. Project page at https://ydcustc.github.io/retriever-demo/.","['transformer', 'style transfer', 'unsupervised learning']",[],"['Dacheng Yin', 'Xuanchi Ren', 'Chong Luo', 'Yuwang Wang', 'Zhiwei Xiong', 'Wenjun Zeng']","['University of Science and Technology of China', 'University of Toronto', 'Microsoft Research Asia', 'Tsinghua University, Tsinghua University', 'USTC', 'Eastern Institute for Advanced Study']",[]
https://iclr.cc/virtual/2022/poster/6330,Transparency & Explainability,Adversarial Robustness Through the Lens of Causality,"The adversarial vulnerability of deep neural networks has attracted signiﬁcant attention in machine learning. As causal reasoning has an instinct for modeling distribution change, it is essential to incorporate causality into analyzing this specific type of distribution change induced by adversarial attacks. However, causal formulations of the intuition of adversarial attacks and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of adversarial attacks. From the causal perspective, we study the distinction between the natural and adversarial distribution and conclude that the origin of adversarial vulnerability is the focus of models on spurious correlations. Inspired by the causal understanding, we propose the \emph{Causal}-inspired \emph{Adv}ersarial distribution alignment method, CausalAdv, to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method. Our work is the first attempt towards using causality to understand and mitigate the adversarial vulnerability.","['adversarial examples', 'causality']",[],"['Yonggang Zhang', 'Mingming Gong', 'Tongliang Liu', 'Gang Niu', 'Xinmei Tian', 'Bo Han', 'Kun Zhang']","['Hong Kong Baptist University', 'University of Melbourne', 'University of Sydney', 'RIKEN', 'University of Science and Technology of China', 'HKBU', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2022/poster/5962,Transparency & Explainability,Attention-based Interpretability with Concept Transformers,"Attention is a mechanism that has been instrumental in driving remarkable performance gains of deep neural network models in a host of visual, NLP and multimodal tasks.One additional notable aspect of attention is that it conveniently exposes the ``reasoning'' behind each particular output generated by the model.Specifically, attention scores over input regions or intermediate features have been interpreted as a measure of the contribution of the attended element to the model inference.While the debate in regard to the interpretability of attention is still not settled, researchers have pointed out the existence of architectures and scenarios that afford a meaningful interpretation of the attention mechanism.Here we propose the generalization of attention from low-level input features to high-level concepts as a mechanism to ensure the interpretability of attention scores within a given application domain.In particular, we design the ConceptTransformer, a deep learning module that exposes explanations of the output of a model in which it is embedded in terms of attention over user-defined high-level concepts.Such explanations are \emph{plausible} (i.e.\ convincing to the human user) and \emph{faithful} (i.e.\ truly reflective of the reasoning process of the model).Plausibility of such explanations is obtained by construction by training the attention heads to conform with known relations between inputs, concepts and outputs dictated by domain knowledge.Faithfulness is achieved by design by enforcing a linear relation between the transformer value vectors that represent the concepts and their contribution to the classification log-probabilities.We validate our ConceptTransformer module on established explainability benchmarks and show how it can be used to infuse domain knowledge into classifiers to improve accuracy, and conversely to extract concept-based explanations of classification outputs. Code to reproduce our results is available at: \url{https://github.com/ibm/concept_transformer}.","['transformer', 'attention', 'interpretability']",[],"['Mattia Rigotti', 'Christoph Miksovic', 'Ioana Giurgiu', 'Thomas Gschwind', 'Paolo Scotton']","['International Business Machines', 'International Business Machines', 'International Business Machines', 'IBM Research', 'International Business Machines']",[]
https://iclr.cc/virtual/2022/poster/6744,Transparency & Explainability,Programmatic Reinforcement Learning without Oracles,"Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable.","['reinforcement learning', 'program synthesis']",[],"['Wenjie Qiu', 'He Zhu']","['Rutgers University', 'Rutgers University']",[]
https://iclr.cc/virtual/2022/poster/7027,Transparency & Explainability,Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis,"Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model’s ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset (5,499 EEGs), we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types (e.g. 47 points increase in combined tonic seizure accuracy over baselines). Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.","['self-supervision', 'interpretability', 'electroencephalography', 'graph neural network', 'visualization', 'time series', 'neuroscience']",[],"['Siyi Tang', 'Jared Dunnmon', 'Khaled Kamal Saab', 'Xuan Zhang', 'Qianying Huang', 'Florian Dubost', 'Daniel Rubin', 'Christopher Lee-Messer']","['Artera Inc', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Liminal Sciences Inc', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2022/poster/6188,Transparency & Explainability,CLEVA-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability,"What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation.  The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and evaluation protocols being too narrow, resulting in several attempts at constructing guidelines in favor of specific desiderata or arguing against the validity of prevalent assumptions.  In this work, we depart from this mindset and argue that the goal of a precise formulation of desiderata is an ill-posed one, as diverse applications may always warrant distinct scenarios. Instead, we introduce the Continual Learning EValuation Assessment Compass: the CLEVA-Compass. The compass provides the visual means to both identify how approaches are practically reported and how works can simultaneously be contextualized in the broader literature landscape.  In addition to promoting compact specification in the spirit of recent replication trends, it thus provides an intuitive chart to understand the priorities of individual systems, where they resemble each other, and what elements are missing towards a fair comparison.","['lifelong learning', 'continual learning']",[],"['Martin Mundt', 'Steven Braun', 'Quentin Delfosse', 'Kristian Kersting']","['Technische Universität Darmstadt & hessian.AI', 'TU Darmstadt', 'CS Department, TU Darmstadt, TU Darmstadt', 'German Research Center for AI']",[]
https://iclr.cc/virtual/2022/poster/5977,Transparency & Explainability,Interpretable Unsupervised Diversity Denoising and Artefact Removal,"Image denoising and artefact removal are complex inverse problems admitting multiple valid solutions. Unsupervised diversity restoration, that is, obtaining a diverse set of possible restorations given a corrupted image, is important for ambiguity removal in many applications such as microscopy where paired data for supervised training are often unobtainable. In real world applications, imaging noise and artefacts are typically hard to model, leading to unsatisfactory performance of existing unsupervised approaches. This work presents an interpretable approach for unsupervised and diverse image restoration. To this end, we introduce a capable architecture called Hierarchical DivNoising (HDN) based on hierarchical Variational Autoencoder. We show that HDN learns an interpretable multi-scale representation of artefacts  and we leverage this interpretability to remove imaging artefacts commonly occurring in microscopy data. Our method achieves state-of-the-art results on twelve benchmark image denoising datasets while providing access to a whole distribution of sensibly restored solutions.Additionally, we demonstrate on three real microscopy datasets that HDN removes artefacts without supervision, being the first method capable of doing so while generating multiple plausible restorations all consistent with the given corrupted image.",['unsupervised image denoising'],[],"['Mangal Prakash', 'Mauricio Delbracio', 'Peyman Milanfar', 'Florian Jug']","['Johnson & Johnson', 'Google', 'Google Research', 'Fondation Human Technopole']",[]
https://iclr.cc/virtual/2022/poster/6983,Transparency & Explainability,"Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning","Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate ""fairness"" and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained opaque-box models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures.","['Shapley values', 'similarity learning', 'metric learning', 'information retrieval']",[],"['Mark Hamilton', 'Scott M Lundberg', 'Stephanie Fu', 'Lei Zhang', 'William T. Freeman']","['Massachusetts Institute of Technology', 'Microsoft', 'University of California, Berkeley', 'International Digital Economy Academy', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6404,Transparency & Explainability,You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction,"Predicting the future trajectory of a moving agent can be easy when the past trajectory continues smoothly but is challenging when complex interactions with other agents are involved. Recent deep learning approaches for trajectory prediction show promising performance and partially attribute this to successful reasoning about agent-agent interactions.  However, it remains unclear which features such black-box models actually learn to use for making predictions. This paper proposes a procedure that quantifies the contributions of different cues to model performance based on a variant of Shapley values. Applying this procedure to state-of-the-art trajectory prediction methods on standard benchmark datasets shows that they are, in fact, unable to reason about interactions. Instead, the past trajectory of the target is the only feature used for predicting its future. For a task with richer social interaction patterns, on the other hand, the tested models do pick up such interactions to a certain extent, as quantified by our feature attribution method. We discuss the limits of the proposed method and its links to causality.","['causality', 'trajectory prediction', 'Feature Attribution', 'Shapley values']",[],"['Osama Makansi', 'Julius von Kügelgen', 'Francesco Locatello', 'Peter Vincent Gehler', 'Dominik Janzing', 'Thomas Brox']","['Universität Freiburg', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Institute of Science and Technology', 'Zalando SE', 'Amazon', 'University of Freiburg']",[]
https://iclr.cc/virtual/2022/poster/6731,Transparency & Explainability,Explanations of Black-Box Models based on Directional Feature Interactions,"As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent.  Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature.  We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph.  Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features.  We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. We show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census, Divorce, Drug, and gene data.","['Shapley values', 'explainability', 'interpretability']",[],"['Aria Masoomi', 'Zhonghui Xu', 'Craig P Hersh', 'Edwin K. Silverman', 'Peter J. Castaldi', 'Stratis Ioannidis', 'Jennifer Dy']","['Northeastern University', 'Harvard University', ""Brigham and Women's Hospital"", ""Brigham and Women's Hospital"", 'Harvard University', 'Northeastern University', 'Northeastern University']",[]
https://iclr.cc/virtual/2022/poster/5988,Transparency & Explainability,Natural Language Descriptions of Deep Features,"Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.",[],[],"['Evan Hernandez', 'Sarah Schwettmann', 'David Bau', 'Teona Bagashvili', 'Antonio Torralba', 'Jacob Andreas']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Northeastern University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/7045,Transparency & Explainability,Fair Normalizing Flows,"Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets.",['fairness'],[],"['Mislav Balunovic', 'Anian Ruoss', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'DeepMind', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6741,Transparency & Explainability,Symbolic Learning to Optimize: Towards Interpretability and Scalability,"Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural networks create extra memory overhead for applying L2O models, and limits their applicability to optimizing larger tasks; (2) interpretability: it is unclear what each L2O model has learned in its black-box optimization rule, nor is it straightforward to compare different L2O models in an explainable way. To avoid both pitfalls, this paper proves the concept that we can ""kill two birds by one stone"", by introducing the powerful tool of symbolic regression to L2O. In this paper, we establish a holistic symbolic representation and analysis framework for L2O, which yields a series of insights for learnable optimizers. Leveraging our findings, we further propose a lightweight L2O model that can be meta-trained on large-scale problems and outperformed human-designed and tuned optimizers. Our work is set to supply a brand-new perspective to L2O research. Codes are available at: https://github.com/VITA-Group/Symbolic-Learning-To-Optimize.","['symbolic regression', 'learning to optimize', 'interpretability']",[],"['Wenqing Zheng', 'Tianlong Chen', 'Ting-Kuei Hu', 'Zhangyang Wang']","['University of Texas, Austin', 'Massachusetts Institute of Technology', 'Texas A&M', 'University of Texas at Austin']",[]
https://iclr.cc/virtual/2022/poster/6786,Transparency & Explainability,Model Agnostic Interpretability for Multiple Instance Learning,"In Multiple Instance Learning (MIL), models are trained using bags of instances, where only a single label is provided for each bag. A bag label is often only determined by a handful of key instances within a bag, making it difficult to interpret what information a classifier is using to make decisions. In this work, we establish the key requirements for interpreting MIL models. We then go on to develop several model-agnostic approaches that meet these requirements. Our methods are compared against existing inherently interpretable MIL models on several datasets, and achieve an increase in interpretability accuracy of up to 30%. We also examine the ability of the methods to identify interactions between instances and scale to larger datasets, improving their applicability to real-world problems.",['interpretability'],[],"['Joseph Early', 'Christine Evers', 'SArvapali Ramchurn']","['Helsing', 'University of Southampton', 'University of Southampton']",[]
https://iclr.cc/virtual/2022/poster/6556,Transparency & Explainability,Fast Generic Interaction Detection for Model Interpretability and Compression,"The ability of discovering feature interactions in a black-box model is vital to explainable deep learning. We propose a principled, global interaction detection method by casting our target as a multi-arm bandits problem and solving it swiftly with the UCB algorithm. This adaptive method is free of ad-hoc assumptions and among the cutting-edge methods with outstanding detection accuracy and stability. Based on the detection outcome, a lightweight and interpretable deep learning model (called ParaACE) is further built using the alternating conditional expectation (ACE) method. Our proposed ParaACE improves the prediction performance by 26 % and reduces the model size by 100+ times as compared to its Teacher model over various datasets. Furthermore, we show the great potential of our method for scientific discovery through interpreting various real datasets in the economics and smart medicine sectors. The code is available at https://github.com/zhangtj1996/ParaACE.",[],[],"['Tianjian Zhang', 'Feng Yin', 'Zhi-Quan Luo']","['The Chinese University of Hong Kong, Shenzhen', 'The Chinese University of Hong Kong', 'The Chinese University of Hong Kong, Shenzhen']",[]
https://iclr.cc/virtual/2022/poster/6978,Transparency & Explainability,A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease ,"We propose a novel end-to-end framework for whole-brain and whole-genome imaging-genetics. Our genetics network uses hierarchical graph convolution and pooling operations to embed subject-level data onto a low-dimensional latent space. The hierarchical network implicitly tracks the convergence of genetic risk across well-established biological pathways, while an attention mechanism automatically identifies the salient edges of this network at the subject level. In parallel, our imaging network projects multimodal data onto a set of latent embeddings. For interpretability, we implement a Bayesian feature selection strategy to extract the discriminative imaging biomarkers; these feature weights are optimized alongside the other model parameters. We couple the imaging and genetic embeddings with a predictor network, to ensure that the learned representations are linked to phenotype. We evaluate our framework on a schizophrenia dataset that includes two functional MRI paradigms and gene scores derived from Single Nucleotide Polymorphism data. Using repeated 10-fold cross-validation, we show that our imaging-genetics fusion achieves the better classification performance than state-of-the-art baselines. In an exploratory analysis, we further show that the biomarkers identified by our model are reproducible and closely associated with deficits in schizophrenia.",[],[],"['Sayan Ghosal', 'Qiang Chen', 'Giulio Pergola', 'Aaron L Goldman', 'William Ulrich', 'Daniel R Weinberger', 'Archana Venkataraman']","['Johns Hopkins University', 'Lieber Institute for Brain Development', 'University of Bari', 'Lieber Institute for Brain Development', 'Lieber Institute', 'Johns Hopkins University', 'Johns Hopkins University']",[]
https://iclr.cc/virtual/2022/poster/5894,Fairness & Bias,"Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset","A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model’s respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies.","['convolutional networks', 'invertible neural networks']",[],"['Leon Sixt', 'Martin Schuessler', 'Oana-Iuliana Popescu', 'Tim Landgraf']","['Freie Universität Berlin', 'TU Berlin', 'German Aerospace Center, Institute of Data Science', 'Freie Universität Berlin']",[]
https://iclr.cc/virtual/2022/poster/7212,Fairness & Bias,Rethinking Class-Prior Estimation for Positive-Unlabeled Learning,"Given only positive (P) and unlabeled (U) data, PU learning can train a binary classifier without any negative data. It has two building blocks: PU class-prior estimation (CPE) and PU classification; the latter has been well studied while the former has received less attention. Hitherto, the distributional-assumption-free CPE methods rely on a critical assumption that the support of the positive data distribution cannot be contained in the support of the negative data distribution. If this is violated, those CPE methods will systematically overestimate the class prior; it is even worse that we cannot verify the assumption based on the data. In this paper, we rethink CPE for PU learning—can we remove the assumption to make CPE always valid? We show an affirmative answer by proposing Regrouping CPE (ReCPE) that builds an auxiliary probability distribution such that the support of the positive data distribution is never contained in the support of the negative data distribution. ReCPE can work with any CPE method by treating it as the base method. Theoretically, ReCPE does not affect its base if the assumption already holds for the original probability distribution; otherwise, it reduces the positive bias of its base. Empirically, ReCPE improves all state-of-the-art CPE methods on various datasets, implying that the assumption has indeed been violated here.",[],[],"['Yu Yao', 'Tongliang Liu', 'Bo Han', 'Mingming Gong', 'Gang Niu', 'Masashi Sugiyama', 'Dacheng Tao']","['University of Sydney', 'University of Sydney', 'HKBU', 'University of Melbourne', 'RIKEN', 'RIKEN', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/7206,Fairness & Bias,Enabling Arbitrary Translation Objectives with Adaptive Tree Search,"We introduce an adaptive tree search algorithm, which is a deterministic variant of Monte Carlo tree search, that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, our algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in autoregressive models. Empirically, we show that our adaptive tree search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in models whose scores do not decompose additively with respect to the words in the output. We also characterise the correlation of several translation model objectives with respect to BLEU. We find that while some standard models are poorly calibrated and benefit from the beam search bias, other often more robust models (autoregressive models tuned to maximize expected automatic metric scores, the noisy channel model and a newly proposed objective) benefit from increasing amounts of search using our proposed decoder, whereas the beam search bias limits the improvements obtained from such objectives. Thus, we argue that as models improve, the improvements may be masked by over-reliance on beam search or reranking based methods.",['machine translation'],[],"['Wang Ling', 'Wojciech Jan Stokowiec', 'Domenic Donato', 'Chris Dyer', 'Lei Yu', 'Laurent Sartran', 'Austin Matthews']","['Carnegie Mellon University', 'Politechnika Warszawska', 'AssemblyAI', 'DeepMind', 'DeepMind', 'Google', 'Amazon']",[]
https://iclr.cc/virtual/2022/poster/7198,Fairness & Bias,Is Importance Weighting Incompatible with Interpolating Classifiers?,"Importance weighting is a classic technique to handle distribution shifts. However, prior work has presented strong empirical and theoretical evidence demonstrating that importance weights can have little to no effect on overparameterized neural networks. \emph{Is importance weighting truly incompatible with the training of overparameterized neural networks?} Our paper answers this in the negative. We show that importance weighting fails not because of the overparameterization, but instead, as a result of using exponentially-tailed losses like the logistic or cross-entropy loss. As a remedy, we show that polynomially-tailed losses restore the effects of importance reweighting in correcting distribution shift in overparameterized models. We characterize the behavior of gradient descent on importance weighted polynomially-tailed losses with overparameterized linear models, and theoretically demonstrate the advantage of using polynomially-tailed losses in a label shift setting. Surprisingly, our theory shows that using weights that are obtained by exponentiating the classical unbiased importance weights can improve performance. Finally, we demonstrate the practical value of our analysis with neural network experiments on a subpopulation shift and a label shift dataset. When reweighted, our loss function can outperform reweighted cross-entropy by as much as 9\% in test accuracy. Our loss function also gives test accuracies comparable to, or even exceeding, well-tuned state-of-the-art methods for correcting distribution shifts.","['interpolation', 'Importance Weighting', 'overparameterization', 'implicit bias']",[],"['Ke Alexander Wang', 'Niladri S. Chatterji', 'Saminul Haque', 'Tatsunori Hashimoto']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2022/poster/7195,Fairness & Bias,Path Integral Sampler: A Stochastic Control Approach For Sampling,"We present Path Integral Sampler~(PIS), a novel algorithm to draw samples from unnormalized probability density functions. The PIS is built on the Schr\""odinger bridge problem which aims to recover the most likely evolution of a diffusion process given its initial distribution and terminal distribution. The PIS draws samples from the initial distribution and then propagates the samples through the Schr\""odinger bridge to reach the terminal distribution. Applying the Girsanov theorem, with a simple prior diffusion, we formulate the PIS as a stochastic optimal control problem whose running cost is the control energy and terminal cost is chosen according to the target distribution. By modeling the control as a neural network, we establish a sampling algorithm that can be trained end-to-end. We provide theoretical justification of the sampling quality of PIS in terms of Wasserstein distance when sub-optimal control is used. Moreover, the path integrals theory is used to compute importance weights of the samples to compensate for the bias induced by the sub-optimality of the controller and the time-discretization. We experimentally demonstrate the advantages of PIS compared with other start-of-the-art sampling methods on a variety of tasks.","['mcmc', 'sampling', 'stochastic differential equation']",[],"['Qinsheng Zhang', 'Yongxin Chen']","['Georgia Institute of Technology', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/7194,Fairness & Bias,Feature Kernel Distillation,"Trained Neural Networks (NNs) can be viewed as data-dependent kernel machines, with predictions determined by the inner product of last-layer representations across inputs, referred to as the feature kernel. We explore the relevance of the feature kernel for Knowledge Distillation (KD), using a mechanistic understanding of an NN’s optimisation process. We extend the theoretical analysis of Allen-Zhu & Li (2020) to show that a trained NN’s feature kernel is highly dependent on its parameter initialisation, which biases different initialisations of the same architecture to learn different data attributes in a multi-view data setting. This enables us to prove that KD using only pairwise feature kernel comparisons can improve NN test accuracy in such settings, with both single & ensemble teacher models, whereas standard training without KD fails to generalise. We further use our theory to motivate practical considerations for improving student generalisation when using distillation with feature kernels, which allows us to propose a novel approach: Feature Kernel Distillation (FKD). Finally, we experimentally corroborate our theory in the image classification setting, showing that FKD is amenable to ensemble distillation, can transfer knowledge across datasets, and outperforms both vanilla KD & other feature kernel based KD baselines across a range of standard architectures & datasets.","['image classification', 'knowledge distillation']",[],"['Bobby He', 'Mete Ozay']","['Department of Computer Science, ETHZ - ETH Zurich', 'Samsung Research']",[]
https://iclr.cc/virtual/2022/poster/7129,Fairness & Bias,Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation,"Identifying the status of individual network units is critical for understanding the mechanism of convolutional neural networks (CNNs). However, it is still challenging to reliably give a general indication of unit status, especially for units in different network models. To this end, we propose a novel method for quantitatively clarifying the status of single unit in CNN using algebraic topological tools. Unit status is indicated via the calculation of a defined topological-based entropy, called feature entropy, which measures the degree of chaos of the global spatial pattern hidden in the unit for a category. In this way, feature entropy could provide an accurate indication of status for units in different networks with diverse situations like weight-rescaling operation. Further, we show that feature entropy decreases as the layer goes deeper and shares almost simultaneous trend with loss during training. We show that by investigating the feature entropy of units on only training data, it could give discrimination between networks with different generalization ability from the view of the effectiveness of feature representations.","['entropy', 'convolutional neural networks']",[],"['Yang Zhao', 'Hao Zhang']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2022/poster/7118,Fairness & Bias,Open-World Semi-Supervised Learning,"A fundamental limitation of applying semi-supervised learning in real-world settings is the assumption that unlabeled test data contains only classes previously encountered in the labeled training data. However, this assumption rarely holds for data in-the-wild, where instances belonging to novel classes may appear at testing time. Here, we introduce a novel open-world semi-supervised learning setting that formalizes the notion that novel classes may appear in the unlabeled test data. In this novel setting, the goal is to solve the class distribution mismatch problem between labeled and unlabeled data, where at the test time every input instance either needs to be classified into one of the existing classes or a new unseen class needs to be initialized and the instance assigned to it. To tackle this challenging problem, we propose ORCA, an end-to-end approach that assigns instances to previously seen classes or  forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. In this way, ORCA gradually increases the discriminability of the model during the training and reduces the gap between intra-class variance of seen with respect to novel classes. Extensive experiments on image classification datasets and a single-cell dataset demonstrate that ORCA consistently outperforms alternative baselines, achieving 25% improvement on seen and 96% improvement on novel classes of the ImageNet dataset.","['semi-supervised learning', 'clustering', 'deep learning']",[],"['Kaidi Cao', 'Maria Brbic', 'Jure Leskovec']","['Stanford University', 'EPFL - EPF Lausanne', 'Stanford University']",[]
https://iclr.cc/virtual/2022/poster/7096,Fairness & Bias,On the benefits of maximum likelihood estimation for Regression and Forecasting,"We advocate for a practical Maximum Likelihood Estimation (MLE) approach towards designing loss functions for regression and forecasting, as an alternative to the typical approach of direct empirical risk minimization on a specific target metric. The MLE approach is better suited to capture inductive biases such as prior domain knowledge in datasets, and can output post-hoc estimators at inference time that can optimize different types of target metrics. We present theoretical results to demonstrate that our approach is competitive with any estimator for the target metric under some general conditions. In two example practical settings, Poisson and Pareto regression, we show that our competitive results can be used to prove that the MLE approach has better excess risk bounds than directly minimizing the target metric. We also demonstrate empirically that our method instantiated with a well-designed general purpose mixture likelihood family can obtain superior performance for a variety of tasks across time-series forecasting and regression datasets with different data distributions.","['forecasting', 'regression']",[],"['Pranjal Awasthi', 'Abhimanyu Das', 'Rajat Sen', 'Ananda Theertha Suresh']","['Google', 'Research, Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2022/poster/7077,Fairness & Bias,On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning,"The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets affected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the exploration-exploitation problem with supervised agents trained to imitate an expert. We show that feed-forward networks trained with behavioural cloning compared to reinforcement learning can be pruned to higher levels of sparsity without performance degradation. This suggests that in order to solve the RL-specific distributional shift agents require more degrees of freedom. Using a set of carefully designed baseline conditions, we find that the majority of the lottery ticket effect in both learning paradigms can be attributed to the identified mask rather than the weight initialization. The input layer mask selectively prunes entire input dimensions that turn out to be irrelevant for the task at hand. At a moderate level of sparsity the mask identified by iterative magnitude pruning yields minimal task-relevant representations, i.e., an interpretable inductive bias. Finally, we propose a simple initialization rescaling which promotes the robust identification of sparse task representations in low-dimensional control tasks.","['reinforcement learning', 'sparsity', 'Lottery Ticket Hypothesis', 'pruning']",[],"['Robert Tjarko Lange', 'Henning Sprekeler']","['TU Berlin', 'Technische Universität Berlin']",[]
https://iclr.cc/virtual/2022/poster/7063,Fairness & Bias,Crystal Diffusion Variational Autoencoder for Periodic Material Generation,"Generating the periodic structure of stable materials is a long-standing challenge for the material design community. This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also requires the structure to follow the complex, yet specific bonding preferences between different atom types. Existing methods fail to incorporate these factors and often lack proper invariances. We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability. By learning from the data distribution of stable materials, the decoder generates materials in a diffusion process that moves atomic coordinates towards a lower energy state and updates atom types to satisfy bonding preferences between neighbors. Our model also explicitly encodes interactions across periodic boundaries and respects permutation, translation, rotation, and periodic invariances. We significantly outperform past methods in three tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and realistic materials, and 3) generating materials that optimize a specific property. We also provide several standard datasets and evaluation metrics for the broader machine learning community.","['score matching', 'generative', 'graph neural networks', '3d']",[],"['Tian Xie', 'Xiang Fu', 'Octavian-Eugen Ganea', 'Regina Barzilay', 'Tommi Jaakkola']","['Microsoft Research AI4Science', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/7019,Fairness & Bias,Implicit Bias of Adversarial Training for Deep Neural Networks,"We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear networks adversarially trained by gradient descent on a linearly separable dataset, we prove that the direction of the product of weight matrices converges to the direction of the max-margin solution of the original dataset. Furthermore, we generalize this result to the case of adversarial training for non-linear homogeneous deep neural networks without the linear separability of the dataset. We show that, when the neural network is adversarially trained with  $\ell_2$ or $\ell_{\infty}$ FGSM, FGM and PGD perturbations, the direction of the limit point of normalized parameters of the network along the trajectory of the gradient flow converges to a KKT point of a constrained optimization problem that aims to maximize the margin for adversarial examples. Our results theoretically justify the longstanding conjecture that adversarial training modifies the decision boundary by utilizing adversarial examples to improve robustness, and potentially provides insights for designing new robust training strategies.","['adversarial training', 'adversarial examples']",[],"['Bochen Lyu', 'Zhanxing Zhu']","['DataCanvas', 'Peking University']",[]
https://iclr.cc/virtual/2022/poster/6999,Fairness & Bias,The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design,"Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose ``kNN-Pretraining"": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities.This theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations.","['sentence embeddings', 'self-attention', 'language modeling', 'transformers']",[],"['Yoav Levine', 'Noam Wies', 'Daniel Jannai', 'Dan Navon', 'Yedid Hoshen', 'Amnon Shashua']","['AI21 Labs', 'Hebrew University of Jerusalem', 'Hebrew University of Jerusalem', 'Hebrew University of Jerusalem', 'Hebrew University of Jerusalem', 'Hebrew University, Hebrew University of Jerusalem']",[]
https://iclr.cc/virtual/2022/poster/6938,Fairness & Bias,PER-ETD: A Polynomially Efficient Emphatic Temporal Difference Learning Method,"Emphatic temporal difference (ETD) learning (Sutton et al., 2016) is a successful method to conduct the off-policy value function evaluation with function approximation. Although ETD has been shown to converge asymptotically to a desirable value function, it is well-known that ETD often encounters a large variance so that its sample complexity can increase exponentially fast with the number of iterations. In this work, we propose a new ETD method, called PER-ETD (i.e., PEriodically Restarted-ETD), which restarts and updates the follow-on trace only for a finite period for each iteration of the evaluation parameter. Further, PER-ETD features a design of the logarithmical increase of the restart period with the number of iterations, which guarantees the best trade-off between the variance and bias and keeps both vanishing sublinearly. We show that PER-ETD converges to the same desirable fixed point as ETD, but improves the exponential sample complexity of ETD to be polynomials. Our experiments validate the superior performance of PER-ETD and its advantage over ETD.","['reinforcement learning', 'off-policy evaluation']",[],"['Tengyu Xu', 'Yingbin Liang']","['Facebook', 'The Ohio State University']",[]
https://iclr.cc/virtual/2022/poster/6931,Fairness & Bias,Learning Hierarchical Structures with Differentiable Nondeterministic Stacks,"Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank.","['rnn', 'language modeling']",[],['David Chiang'],['University of Notre Dame'],[]
https://iclr.cc/virtual/2022/poster/6930,Fairness & Bias,Missingness Bias in Model Debugging,"Missingness, or the absence of features from an input, is a concept fundamental to many model debugging tools. However, in computer vision, pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels, which may in turn introduce bias into the debugging process. We study such biases and, in particular, show how transformer-based architectures can enable a more natural implementation of missingness, which side-steps these issues and improves the reliability of model debugging in practice.",[],[],"['Saachi Jain', 'Hadi Salman', 'Eric Wong', 'Pengchuan Zhang', 'Sai Vemprala', 'Aleksander Madry']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Pennsylvania', 'Meta AI', 'Microsoft', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/7123,Fairness & Bias,The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program,"We study non-convex subgradient flows for training two-layer ReLU neural networks from a convex geometry and duality perspective. We characterize the implicit bias of unregularized non-convex gradient flow as convex regularization of an equivalent convex model. We then show that the limit points of non-convex subgradient flows can be identified via primal-dual correspondence in this convex optimization problem.  Moreover, we derive a sufficient condition on the dual variables which ensures that the stationary points of the non-convex objective are the KKT points of the convex objective, thus proving convergence of non-convex gradient flows to the global optimum. For a class of regular training data distributions such as orthogonal separable data, we show that this sufficient condition holds. Therefore, non-convex gradient flows in fact converge to optimal solutions of a convex optimization problem. We present numerical results verifying the predictions of our theory for non-convex subgradient descent.","['convex duality', 'gradient flow', 'convex optimization']",[],"['Yifei Wang', 'Mert Pilanci']","['Stanford University', 'University of Michigan']",[]
https://iclr.cc/virtual/2022/poster/7099,Fairness & Bias,Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization,"Batch Normalization (BN) is a commonly used technique to accelerate and stabilize training of deep neural networks. Despite its empirical success, a full theoretical understanding of BN is yet to be developed. In this work, we analyze BN through the lens of convex optimization. We introduce an analytic framework based on convex duality to obtain exact convex representations of weight-decay regularized ReLU networks with BN, which can be trained in polynomial-time. Our analyses also show that optimal layer weights can be obtained as simple closed-form formulas in the high-dimensional and/or overparameterized regimes. Furthermore, we find that Gradient Descent provides an algorithmic bias effect on the standard non-convex BN network, and we design an approach to explicitly encode this implicit regularization into the convex objective. Experiments with CIFAR image classification highlight the effectiveness of this explicit regularization for mimicking and substantially improving the performance of standard BN networks.","['convex optimization', 'batch normalization', 'implicit regularization', 'ReLU networks', 'deep networks']",[],"['Tolga Ergen', 'Arda Sahiner', 'Batu Ozturkler', 'John M. Pauly', 'Morteza Mardani', 'Mert Pilanci']","['LG AI Research', 'Stanford University', 'Stanford University', 'Stanford University', 'NVIDIA', 'University of Michigan']",[]
https://iclr.cc/virtual/2022/poster/6885,Fairness & Bias,Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias,"Variational Autoencoders (VAEs) are one of the most commonly used generative models, particularly for image data. A prominent difficulty in training VAEs is data that is supported on a lower dimensional manifold. Recent work by Dai and Wipf (2020) proposes a two-stage training algorithm for VAEs, based on a conjecture that in standard VAE training the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold. They gave partial support for this conjecture by showing that some optima of the VAE loss do satisfy this property, but did not analyze the training dynamics.  In this paper, we show that for linear encoders/decoders, the conjecture is true—that is the VAE training does recover a generator with support equal to the ground truth manifold—and does so due to an implicit bias of gradient descent rather than merely the VAE loss itself. In the nonlinear case, we show that VAE training frequently learns a higher-dimensional manifold which is a superset of the ground truth manifold.","['stability', 'variational autoencoders']",[],"['Frederic Koehler', 'Viraj Mehta', 'Chenghui Zhou', 'Andrej Risteski']","['Stanford University', 'Carnegie Mellon University', 'CMU, Carnegie Mellon University', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2022/poster/6835,Fairness & Bias,Understanding the Variance Collapse of SVGD in High Dimensions,"Stein variational gradient descent (SVGD) is a deterministic inference algorithm that evolves a set of particles to fit a target distribution. Despite its computational efficiency, SVGD often underestimates the variance of the target distribution in high dimensions. In this work we attempt to explain the variance collapse in SVGD. On the qualitative side, we compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective; we observe that the variance collapse phenomenon relates to the bias from deterministic updates present in the ""driving force"" of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the quantitative side, we demonstrate that the variance collapse of SVGD can be accurately predicted in the proportional asymptotic limit, i.e., when the number of particles $n$ and dimensions $d$ diverge at the same rate. In particular, for learning high-dimensional isotropic Gaussians, we derive the exact equilibrium variance for both SVGD and MMD-descent under certain near-orthogonality assumption on the converged particles, and confirm that SVGD suffers from the ""curse of dimensionality"".",['approximate inference'],[],"['Jimmy Ba', 'Murat A Erdogdu', 'Marzyeh Ghassemi', 'Shengyang Sun', 'Taiji Suzuki', 'Denny Wu', 'Tianzong Zhang']","['Department of Computer Science, University of Toronto', 'University of Toronto', 'Massachusetts Institute of Technology', 'NVIDIA', 'The University of Tokyo', 'New York University', 'EPFL - EPF Lausanne']",[]
https://iclr.cc/virtual/2022/poster/6710,Fairness & Bias,The Spectral Bias of Polynomial Neural Networks,"Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\text{\it{spectral bias}}$ towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the $\Pi$-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. We verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials.","['neural tangent kernel', 'deep neural networks']",[],"['Moulik Choraria', 'Leello Tadesse Dadi', 'Grigorios Chrysos', 'Julien Mairal', 'Volkan Cevher']","['University of Illinois, Urbana-Champaign', 'Swiss Federal Institute of Technology Lausanne', 'University of Wisconsin - Madison', 'Inria', 'EPFL - EPF Lausanne']",[]
https://iclr.cc/virtual/2022/poster/6669,Fairness & Bias,Bi-linear Value Networks for Multi-goal Reinforcement Learning,"Universal value functions are a core component of off-policy multi-goal reinforcement learning. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks which lack inductive biases to produce complex interactions between the state s and the goal g. In this work, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, ϕ(s, g), captures the global relationship between the current state and the goal.We show that our bilinear decomposition scheme improves sample efficiency over the original monolithic value approximators, and transfer better to unseen goals. We demonstrate significant learning speed-up over a variety of tasks on a simulated robot arm, and the challenging task of dexterous manipulation with a Shadow hand.",['multi-goal reinforcement learning'],[],"['Zhang-Wei Hong', 'Ge Yang', 'Pulkit Agrawal']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6645,Fairness & Bias,Modeling Label Space Interactions in Multi-label Classification using Box Embeddings,"Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classification methods do not do so, resulting in label predictions that are inconsistent with the taxonomic constraints, thus failing to accurately represent the fundamentals of problem setting. In this work, we introduce the multi-label box model (MBM), a multi-label classification method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018).  Box embeddings can be understood as trainable Venn-diagrams based on hyper-rectangles.  Representing labels by boxes rather than vectors, MBM is able to capture taxonomic relations among labels.  Furthermore, since box embeddings allow these relations to be learned by stochastic gradient descent from data, and to be read as calibrated conditional probabilities, our model is endowed with a high degree of interpretability. This interpretability also facilitates the injection of partial information about label-label relationships into model training, to further improve its consistency. We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data. Through extensive empirical evaluations on both small and large-scale multi-label classification datasets, we show that BBM can significantly improve taxonomic consistency while preserving or surpassing the state-of-the-art predictive performance.","['embeddings', 'multi-label classification', 'representation learning']",[],"['Dhruvesh Patel', 'Pavitra Dangati', 'Jay-Yoon Lee', 'Michael Boratko', 'Andrew McCallum']","['College of Information and Computer Science, University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'Seoul National University', 'Google', 'Department of Computer Science, University of Massachusetts, Amherst']",[]
https://iclr.cc/virtual/2022/poster/6638,Fairness & Bias,Training invariances and the low-rank phenomenon: beyond linear networks,"The implicit bias induced by the training of neural networks has become a topic of rigorous study. In the limit of gradient flow and gradient descent with appropriate step size, it has been shown that when one trains a deep linear network with logistic or exponential loss on linearly separable data, the weights converge to rank-$1$ matrices. In this paper, we extend this theoretical result to the last few linear layers of the much wider class of nonlinear ReLU-activated feedforward networks containing fully-connected layers and skip connections.  Similar to the linear case, the proof relies on specific local training invariances, sometimes referred to as alignment, which we show to hold for submatrices where neurons are stably-activated in all training examples, and it reflects empirical results in the literature. We also show this is not true in general for the full matrix of ReLU fully-connected layers. Our proof relies on a specific decomposition of the network into a multilinear function and another ReLU network whose weights are constant under a certain parameter directional convergence.","['implicit regularization', 'deep learning', 'alignment']",[],"['Thien Le', 'Stefanie Jegelka']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/7119,Fairness & Bias,Representational Continuity for Unsupervised Continual Learning,"Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (Lump), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations.","['representational learning', 'deep learning', 'continual learning']",[],"['Divyam Madaan', 'Jaehong Yoon', 'Yuanchun Li', 'Yunxin Liu', 'Sung Ju Hwang']","['New York University', 'University of North Carolina at Chapel Hill', 'Institute for AI Industry Research, Tsinghua University', 'Institute for AI Industry Research, Tsinghua University', 'Korea Advanced Institute of Science and Technology']",[]
https://iclr.cc/virtual/2022/poster/6816,Fairness & Bias,Chunked Autoregressive GAN for Conditional Waveform Synthesis,"Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast inference speed suitable for real-time or interactive applications, and maintains or improves subjective quality.","['deep learning', 'generative models', 'speech synthesis', 'generative adversarial networks']",[],"['Max Morrison', 'Rithesh Kumar', 'Kundan Kumar', 'Prem Seetharaman', 'Aaron Courville', 'Yoshua Bengio']","['Northwestern University', 'Descript Inc.', 'University of Montreal', 'Descript', 'University of Montreal', 'University of Montreal']",[]
https://iclr.cc/virtual/2022/poster/6555,Fairness & Bias,Discovering Invariant Rationales for Graph Neural Networks,"Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features --- rationale --- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.","['graph neural networks', 'interpretability']",[],"['Shirley Wu', 'Xiang Wang', 'An Zhang', 'Xiangnan He', 'Tat-Seng Chua']","['Computer Science Department, Stanford University', 'University of Science and Technology of China', 'National University of Singapore', 'University of Science and Technology of China', 'National University of Singapore']",[]
https://iclr.cc/virtual/2022/poster/6367,Fairness & Bias,DISSECT: Disentangled Simultaneous Explanations via Concept Traversals,"Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore ""what-if"" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent ""notion"" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.","['explainability', 'generative adversarial network', 'variational autoencoder', 'interpretability']",[],"['Asma Ghandeharioun', 'Been Kim', 'Chun-Liang Li', 'Brendan Jou', 'Brian Eoff', 'Rosalind Picard']","['Google', 'Google DeepMind', 'Google', 'Google', 'Google', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6357,Fairness & Bias,When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,"Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and +11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at \url{https://github.com/google-research/vision_transformer}.",['optimization'],[],"['Xiangning Chen', 'Cho-Jui Hsieh', 'Boqing Gong']","['University of California, Los Angeles', 'Google', 'University of Central Florida']",[]
https://iclr.cc/virtual/2022/poster/6303,Fairness & Bias,Charformer: Fast Character Transformers via Gradient-based Subword Tokenization,"State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce Charformer, a deep Transformer model that integrates GBST and operates on the character level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that Charformer outperforms a series of competitive character-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, Charformer is fast, improving the speed of vanilla character-level Transformers by up to  while maintaining quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.","['nlp', 'language', 'transformers']",[],"['Yi Tay', 'Vinh Q. Tran', 'Sebastian Ruder', 'Jai Gupta', 'Hyung Won Chung', 'Dara Bahri', 'Zhen Qin', 'Simon Baumgartner', 'Cong Yu', 'Donald Metzler']","['Google', 'Google', 'Google', 'Google', 'Google Brain', 'Google Research', 'Google', 'Google', 'Google Research', 'Google']",[]
https://iclr.cc/virtual/2022/poster/6291,Fairness & Bias,The MultiBERTs: BERT Reproductions for Robustness Analysis,"Experiments with pre-trained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact tested in the experiment (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure which includes the architecture, training data, initialization scheme, and loss function. Recent work has shown that repeating the pre-training process can lead to substantially different performance, suggesting that an alternative strategy is needed to make principled statements about procedures. To enable researchers to draw more robust conclusions, we introduce MultiBERTs, a set of 25 BERT-Base checkpoints, trained with similar hyper-parameters as the original BERT model but differing in random weight initialization and shuffling of training data. We also define the Multi-Bootstrap, a non-parametric bootstrap method for statistical inference designed for settings where there are multiple pre-trained models and limited test data. To illustrate our approach, we present a case study of gender bias in coreference resolution, in which the Multi-Bootstrap lets us measure effects that may not be detected with a single checkpoint. The models and statistical library are available online, along with an additional set of 140 intermediate checkpoints captured during pre-training to facilitate research on learning dynamics.","['BERT', 'robustness', 'hypothesis testing']",[],"['Thibault Sellam', 'Steve Yadlowsky', 'Ian Tenney', 'Jason Wei', 'Naomi Saphra', ""Alexander Nicholas D'Amour"", 'Tal Linzen', 'Jasmijn Bastings', 'Iulia Raluca Turc', 'Jacob Eisenstein', 'Dipanjan Das', 'Ellie Pavlick']","['Google', 'Google Research, Brain Team', 'Google', 'OpenAI', 'Harvard University', 'Google', 'New York University', 'Google', 'University of Oxford', 'Georgia Institute of Technology', 'Google Deepmind', 'Brown University']",[]
https://iclr.cc/virtual/2022/poster/6287,Fairness & Bias,ViTGAN: Training GANs with Vision Transformers,"Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to faciliate convergence. Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNN- based GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom.",[],[],"['Kwonjoon Lee', 'Huiwen Chang', 'Lu Jiang', 'Han Zhang', 'Zhuowen Tu', 'Ce Liu']","['Honda Research Institute', 'OpenAI', 'Carnegie Mellon University', 'Rutgers University', 'University of California, San Diego', 'Microsoft']",[]
https://iclr.cc/virtual/2022/poster/6235,Fairness & Bias,IGLU: Efficient GCN Training via Lazy Updates,Training multi-layer Graph Convolution Networks (GCN) using standard SGD techniques scales poorly as each descent step ends up updating node embeddings for a large portion of the graph. Recent attempts to remedy this sub-sample the graph that reduces compute but introduce additional variance and may offer suboptimal performance. This paper develops the IGLU method that caches intermediate computations at various GCN layers thus enabling lazy updates that significantly reduce the compute cost of descent. IGLU introduces bounded bias into the gradients but nevertheless converges to a first-order saddle point under standard assumptions such as objective smoothness. Benchmark experiments show that IGLU offers up to 1.2% better accuracy despite requiring up to 88% less compute.,"['graph convolutional networks', 'optimization', 'graph neural networks']",[],"['Aditya Sinha', 'Prateek Jain', 'Purushottam Kar', 'SUNDARARAJAN SELLAMANICKAM']","['Department of Computer Science', 'Google', 'Indian Institute of Technology, Kanpur', 'Microsoft']",[]
https://iclr.cc/virtual/2022/poster/6177,Fairness & Bias,On Non-Random Missing Labels in Semi-Supervised Learning,"Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in which the label Missing Not At Random (MNAR) problem is more realistic and challenging, compared to the widely-adopted yet naive Missing Completely At Random assumption where both labeled and unlabeled data share the same class distribution. Different from existing SSL solutions that overlook the role of  ''class'' in causing the non-randomness, e.g., users are more likely to label popular classes, we explicitly incorporate ''class'' into SSL. Our method is three-fold: 1) We propose Class-Aware Propensity (CAP) that exploits the unlabeled data to train an improved classifier using the biased labeled data. 2) To encourage rare class training, whose model is low-recall but high-precision that discards too many pseudo-labeled data, we propose Class-Aware Imputation (CAI) that dynamically decreases (or increases) the pseudo-label assignment threshold for rare (or frequent) classes. 3) Overall, we integrate CAP and CAI into a Class-Aware Doubly Robust (CADR) estimator for training an unbiased SSL model. Under various MNAR settings and ablations, our method not only significantly outperforms existing baselines, but also surpasses other label bias removal SSL methods.","['image classification', 'semi-supervised learning']",[],"['Xinting Hu', 'Yulei Niu', 'Chunyan Miao', 'Xian-Sheng Hua', 'Hanwang Zhang']","['Nanyang Technological University', 'Columbia University', 'School of Computer Science and  Engineering, Nanyang Technological University', 'Terminus Group', 'Nanyang Technological University']",[]
https://iclr.cc/virtual/2022/poster/5892,Fairness & Bias,Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective,"Deep neural networks (DNNs) often rely on easy–to–learn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized based on their typical background scenery, such as lakes or streams. This phenomenon, also known as shortcut learning, is emerging as a key limitation of the current generation of machine learning models. In this work, we introduce a set of experiments to deepen our understanding of shortcut learning and its implications. We design a training setup with several shortcut cues, named WCST-ML, where each cue is equally conducive to the visual recognition problem at hand. Even under equal opportunities, we observe that (1) certain cues are preferred to others, (2) solutions biased to the easy–to–learn cues tend to converge to relatively flat minima on the loss surface, and (3) the solutions focusing on those preferred cues are far more abundant in the parameter space. We explain the abundance of certain cues via their Kolmogorov (descriptional) complexity: solutions corresponding to Kolmogorov-simple cues are abundant in the parameter space and are thus preferred by DNNs. Our studies are based on the synthetic dataset DSprites and the face dataset UTKFace. In our WCST-ML, we observe that the inborn bias of models leans toward simple cues, such as color and ethnicity. Our findings emphasize the importance of active human intervention to remove the inborn model biases that may cause negative societal impacts.","['generalization', 'flat minima', 'simplicity bias']",[],"['Luca Scimeca', 'Seong Joon Oh', 'Sanghyuk Chun', 'Michael Poli', 'Sangdoo Yun']","['Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'Parameter Lab', 'NAVER AI Lab', 'Stanford University', 'NAVER']",[]
https://iclr.cc/virtual/2022/poster/6163,Fairness & Bias,Knowledge Infused Decoding,"Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence. they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. Recent remedies to this problem focus on modifying either the pre-training or task fine-tuning objectives to incorporate knowledge, which normally require additional costly training or architecture modification of LMs for practical applications.We present Knowledge Infused Decoding (KID)---a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. On six diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART) armed with KID outperform many task-optimized state-of-the-art models, and show particularly strong performance in few-shot scenarios over seven related knowledge-infusion techniques. Human evaluation confirms KID's ability to generate more relevant and factual language for the input context when compared with multiple baselines. Finally, KID also alleviates exposure bias and provides stable generation quality when generating longer sequences.","['reinforcement learning', 'Generation', 'natural language']",[],"['Ruibo Liu', 'Guoqing Zheng', 'Shashank Gupta', 'Radhika Gaonkar', 'Chongyang Gao', 'Soroush Vosoughi', 'Milad Shokouhi', 'Ahmed Hassan Awadallah']","['Google DeepMind', 'Microsoft Research', 'Allen Institute for Artificial Intelligence', 'Microsoft', 'Northwestern University', 'Dartmouth College', 'Microsoft', 'Microsoft Research']",[]
https://iclr.cc/virtual/2022/poster/6140,Fairness & Bias,SUMNAS: Supernet with Unbiased Meta-Features for Neural Architecture Search,"One-shot Neural Architecture Search (NAS) usually constructs an over-parameterized network, which we call a supernet, and typically adopts sharing parameters among the sub-models to improve computational efficiency. One-shot NAS often repeatedly samples sub-models from the supernet and trains them to optimize the shared parameters. However, this training strategy suffers from multi-model forgetting. Training a sampled sub-model overrides the previous knowledge learned by the other sub-models, resulting in an unfair performance evaluation between the sub-models. We propose Supernet with Unbiased Meta-Features for Neural Architecture Search (SUMNAS), a supernet learning strategy based on meta-learning to tackle the knowledge forgetting issue. During the training phase, we explicitly address the multi-model forgetting problem and help the supernet learn unbiased meta-features, independent from the sampled sub-models. Once training is over, sub-models can be instantly compared to get the overall ranking or the best sub-model. Our evaluation on the NAS-Bench-201 and MobileNet-based search space demonstrate that SUMNAS shows improved ranking ability and finds architectures whose performance is on par with existing state-of-the-art NAS algorithms.",['neural architecture search'],[],"['Hyeonmin Ha', 'Ji-Hoon Kim', 'Semin Park', 'Byung-Gon Chun']","['Seoul National University', 'NAVER', 'Yonsei University', 'Seoul National University']",[]
https://iclr.cc/virtual/2022/poster/6095,Fairness & Bias,Deconstructing the Inductive Biases of Hamiltonian Neural Networks,"Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that don’t conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In this paper, we examine the inductive biases that make physics-inspired models successful in practice. We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation. We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems. We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control.",[],[],"['Nate Gruver', 'Marc Anton Finzi', 'Samuel Don Stanton', 'Andrew Gordon Wilson']","['New York University', 'Carnegie Mellon University', 'Genentech', 'Cornell University']",[]
https://iclr.cc/virtual/2022/poster/6009,Fairness & Bias,On Incorporating Inductive Biases into VAEs,"We explain why directly changing the prior can be a surprisingly ineffective mechanism for incorporating inductive biases into variational auto-encoders (VAEs), and introduce a simple and effective alternative approach: Intermediary Latent Space VAEs (InteL-VAEs). InteL-VAEs use an intermediary set of latent variables to control the stochasticity of the encoding process, before mapping these in turn to the latent representation using a parametric function that encapsulates our desired inductive bias(es). This allows us to impose properties like sparsity or clustering on learned representations, and incorporate human knowledge into the generative model. Whereas changing the prior only indirectly encourages behavior through regularizing the encoder, InteL-VAEs are able to directly enforce desired characteristics. Moreover, they bypass the computation and encoder design issues caused by non-Gaussian priors, while allowing for additional flexibility through training of the parametric mapping function. We show that these advantages, in turn, lead to both better generative models and better representations being learned.","['variational auto-encoders', 'representation learning', 'variational autoencoders', 'inductive biases']",[],"['Ning Miao', 'Emile Mathieu', 'Siddharth N', 'Yee Whye Teh', 'Tom Rainforth']","['University of Oxford', 'University of Cambridge', 'University of Edinburgh', 'University of Oxford and DeepMind', 'University of Oxford']",[]
https://iclr.cc/virtual/2022/poster/5891,Fairness & Bias,Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks,"Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.","['missing data', 'graph neural networks']",[],"['Andrea Cini', 'Ivan Marisca', 'Cesare Alippi']","['The Swiss AI Lab IDSIA', 'The Swiss AI Lab IDSIA', 'Università della Svizzera Italiana']",[]
https://iclr.cc/virtual/2022/poster/5932,Fairness & Bias,GraphENS: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification,"In many real-world node classification scenarios, nodes are highly class-imbalanced, where graph neural networks (GNNs) can be readily biased to major class instances. Albeit existing class imbalance approaches in other domains can alleviate this issue to some extent, they do not consider the impact of message passing between nodes. In this paper, we hypothesize that overfitting to the neighbor sets of minor class due to message passing is a major challenge for class-imbalanced node classification. To tackle this issue, we propose GraphENS, a novel augmentation method that synthesizes the whole ego network for minor class (minor node and its one-hop neighbors) by combining two different ego networks based on their similarity. Additionally, we introduce a saliency-based node mixing method to exploit the abundant class-generic attributes of other nodes while blocking the injection of class-specific features. Our approach consistently outperforms the baselines over multiple node classification benchmark datasets and architectures.","['class imbalance', 'deep learning', 'node classification', 'data augmentation']",[],"['Joonhyung Park', 'Jaeyun Song', 'Eunho Yang']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'KAIST']",[]
https://iclr.cc/virtual/2022/poster/5914,Fairness & Bias,Inductive Relation Prediction Using Analogy Subgraph Embeddings,"Prevailing methods for relation prediction in heterogeneous graphs aim at learning latent representations (i.e., embeddings) of observed nodes and relations, and thus are limited to the transductive setting where the relation types must be known during training.  Here,  we propose ANalogy  SubGraphEmbeddingLearning (GraphANGEL), a novel relation prediction framework that predicts relations5between each node pair based on the subgraphs containing the pair, as well as other  (analogy)  subgraphs with the same graph patterns.   Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relations and leads to more explainable predictive models. Moreover, our method also removes the limited neighborhood constraint of graph neural networks. Our model consistently outperforms existing models on heterogeneous graph based recommendation as well as knowledge graph completion.  We also empirically demonstrate our model’s capability in generalizing to new relations while producing explainable heat maps of attention scores across the discovered logic.","['link prediction', 'knowledge graphs']",[],"['Jiarui Jin', 'Yangkun Wang', 'Kounianhua Du', 'Weinan Zhang', 'Zheng Zhang', 'David Wipf', 'Yong Yu', 'Quan Gan']","['Shanghai Jiao Tong University', 'University of California, San Diego', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'New York University', 'Amazon AI Research Lab', 'Shanghai Jiaotong University', 'Amazon']",[]
https://iclr.cc/virtual/2022/poster/5912,Fairness & Bias,Continual Normalization: Rethinking Batch Normalization for Online Continual Learning,"Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and non-stationary nature of continual learning data, especially in the online setting, amplify the discrepancy between training and testing in BN and hinder the performance of older tasks. In this work, we study the cross-task normalization effect of BN in online continual learning where BN normalizes the testing data using moments biased towards the current task, resulting in higher catastrophic forgetting. This limitation motivates us to propose a simple yet effective method that we call Continual Normalization (CN) to facilitate training similar to BN while mitigating its negative effect. Extensive experiments on different continual learning algorithms and online scenarios show that CN is a direct replacement for BN and can provide substantial performance improvements. Our implementation will be made publicly available upon acceptance.","['batch normalization', 'continual learning']",[],"['Quang Pham', 'Chenghao Liu', 'Steven HOI']","['A*STAR', 'SalesForce.com', 'Singapore Management University']",[]
https://iclr.cc/virtual/2022/poster/6011,Fairness & Bias,Measuring CLEVRness: Black-box Testing of Visual Reasoning Models,"How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate.To answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a ``human-level'', can easily be fooled by our agent. Our results put in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning.","['computer vision', 'visual question answering', 'visual reasoning']",[],"['Spyridon Mouselinos', 'Henryk Michalewski', 'Mateusz Malinowski']","['University of Warsaw', 'Google', 'DeepMind']",[]
https://iclr.cc/virtual/2022/poster/7030,Fairness & Bias,Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond,"In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-Łojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings.","['convex optimization', 'stochastic optimization', 'distributed learning', 'large scale learning', 'local sgd', 'federated learning']",[],"['Chulhee Yun', 'Shashank Rajput', 'Suvrit Sra']","['Korea Advanced Institute of Science & Technology', 'University of Wisconsin, Madison', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/5931,Fairness & Bias,Towards Building A Group-based Unsupervised Representation Disentanglement Framework,"Disentangled representation learning is one of the major goals of deep learning, and is a key step for achieving explainable and generalizable models. The key idea of the state-of-the-art VAE-based unsupervised representation disentanglement methods is to minimize the total correlation of the joint distribution of the latent variables. However, it has been proved that their goal can not be achieved without introducing other inductive biases. The Group Theory based definition of representation disentanglement mathematically connects the data transformations to the representations using the formalism of group. In this paper, built on the group-based definition and inspired by the \emph{n-th dihedral group}, we first propose a theoretical framework towards achieving unsupervised representation disentanglement. We then propose a model based on existing VAE-based methods to tackle the unsupervised learning problem of the framework. In the theoretical framework, we prove three sufficient conditions on model, group structure, and data respectively in an effort to achieve, in an unsupervised way, disentangled representation per group-based definition. With these conditions, we offer an option, from the perspective of the group-based definition, for the inductive bias that existing VAE-based models lack. Experimentally, we train 1800 models covering the most prominent VAE-based methods on five datasets to verify the effectiveness of our theoretical framework. Compared to the original VAE-based methods, these Groupified VAEs consistently achieve better mean performance with smaller variances.","['group theory', 'vae', 'disentangled representation learning']",[],"['Tao Yang', 'Xuanchi Ren', 'Yuwang Wang', 'Wenjun Zeng', 'Nanning Zheng']","[""Xi'an Jiaotong University"", 'University of Toronto', 'Tsinghua University, Tsinghua University', 'Eastern Institute for Advanced Study', ""Xi'an Jiaotong University""]",[]
https://iclr.cc/virtual/2022/poster/6721,Fairness & Bias,Scaling Laws for Neural Machine Translation,"We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study.","['nmt', 'neural machine translation']",[],"['Behrooz Ghorbani', 'Orhan Firat', 'Markus Freitag', 'Ankur Bapna', 'Maxim Krikun', 'Xavier Garcia', 'Ciprian Chelba', 'Colin Cherry']","['Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2022/poster/7059,Fairness & Bias,Conditional Object-Centric Learning from Video,"Object-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built. Recent work on simple 2D and 3D datasets has shown that models with object-centric inductive biases can learn to segment and represent meaningful objects from the statistical structure of the data alone without the need for any supervision. However, such fully-unsupervised methods still fail to scale to diverse realistic data, despite the use of increasingly complex inductive biases such as priors for the size of objects or the 3D geometry of the scene. In this paper, we instead take a weakly-supervised approach and focus on how 1) using the temporal dynamics of video data in the form of optical flow and 2) conditioning the model on simple object location cues can be used to enable segmenting and tracking objects in significantly more realistic synthetic data. We introduce a sequential extension to Slot Attention which we train to predict optical flow for realistic looking synthetic scenes and show that conditioning the initial state of this model on a small set of hints, such as center of mass of objects in the first frame, is sufficient to significantly improve instance segmentation. These benefits generalize beyond the training distribution to novel objects, novel backgrounds, and to longer video sequences. We also find that such initial-state-conditioning can be used during inference as a flexible interface to query the model for specific objects or parts of objects, which could pave the way for a range of weakly-supervised approaches and allow more effective interaction with trained models.",[],[],"['Thomas Kipf', 'Gamaleldin Fathy Elsayed', 'Aravindh Mahendran', 'Austin Stone', 'Sara Sabour', 'Georg Heigold', 'Rico Jonschkowski', 'Alexey Dosovitskiy', 'Klaus Greff']","['Google', 'Google Research, Brain Team', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2022/poster/7090,Fairness & Bias,Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks,"We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow.  We show that in the underparameterized regime the network learns eigenfunctions of an integral operator $T_K$ determined by the Neural Tangent Kernel at rates corresponding to their eigenvalues.  For example, for uniformly distributed data on the sphere $S^{d - 1}$ and rotation invariant weight distributions, the eigenfunctions of $T_K$ are the spherical harmonics.  Our results can be understood as describing a spectral bias in the underparameterized regime. The proofs use the concept of ``Damped Deviations'' where deviations of the NTK matter less for eigendirections with large eigenvalues.  Aside from the underparameterized regime, the damped deviations point-of-view allows us to extend certain results in the literature in the overparameterized setting.","['implicit regularization', 'neural tangent kernel', 'implicit bias', 'gradient flow']",[],"['Benjamin Bowman', 'Guido Montufar']","['Amazon', 'UCLA ']",[]
https://iclr.cc/virtual/2022/poster/6112,Fairness & Bias,Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning,"Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving  generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space -- inter-class alignment, intra-class alignment, and uniformity -- and propose \textit{\textbf{finDML}}, the \textit{\textbf{f}}airness \textit{\textbf{i}}n \textit{\textbf{n}}on-balanced \textit{\textbf{DML}} benchmark to characterize representation fairness. Utilizing \textit{finDML}, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (\textit{\textbf{\pad}}) to disentangle feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics.","['representation learning', 'fairness']",[],"['Natalie Dullerud', 'Karsten Roth', 'Kimia Hamidieh', 'Nicolas Papernot', 'Marzyeh Ghassemi']","['Stanford University', 'University of Tuebingen', 'Massachusetts Institute of Technology', 'University of Toronto', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6754,Fairness & Bias,No One Representation to Rule Them All: Overlapping Features of Training Methods,"Despite being able to capture a range of features of the data, high accuracy models trained with supervision tend to make similar predictions. This seemingly implies that high-performing models share similar biases regardless of training methodology, which would limit ensembling benefits and render low-accuracy models as having little practical use. Against this backdrop, recent work has developed quite different training techniques, such as large-scale contrastive learning, yielding competitively high accuracy on generalization and robustness benchmarks. This motivates us to revisit the assumption that models necessarily learn similar functions. We conduct a large-scale empirical study of models across hyper-parameters, architectures, frameworks, and datasets. We find that model pairs that diverge more in training methodology display categorically different generalization behavior, producing increasingly uncorrelated errors. We show these models specialize in subdomains of the data, leading to higher ensemble performance: with just 2 models (each with ImageNet accuracy \~76.5\%), we can create ensembles with 83.4\% (+7\% boost). Surprisingly, we find that even significantly low-accuracy models can be used to improve high-accuracy models. Finally, we show diverging training methodology yield representations that capture overlapping (but not supersetting) feature sets which, when combined, lead to increased downstream performance.","['features', 'representation learning', 'diversity', 'Understanding Deep Learning', 'contrastive learning']",[],"['Raphael Gontijo-Lopes', 'Yann Dauphin', 'Ekin Dogus Cubuk']","['Google Brain', 'Google', 'Google']",[]
https://iclr.cc/virtual/2022/poster/6221,Fairness & Bias,On the Importance of Firth Bias Reduction in Few-Shot Classification,"Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classification suffers from the bias in the estimation of classifier parameters; however, an effective underlying bias reduction technique that could alleviate this issue in training few-shot classifiers has been overlooked. In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classification. Theoretically, Firth bias reduction removes the $O(N^{-1})$ first order term from the small-sample bias of the Maximum Likelihood Estimator. Here we show that the general Firth bias reduction technique simplifies to encouraging uniform class assignment probabilities for multinomial logistic classification, and almost has the same effect in cosine classifiers. We derive an easy-to-implement optimization objective for Firth penalized multinomial logistic and cosine classifiers, which is equivalent to penalizing the cross-entropy loss with a KL-divergence between the predictions and the uniform label distribution. Then, we empirically evaluate that it is consistently effective across the board for few-shot image classification, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes. Finally, we show the robustness of Firth bias reduction, in the case of imbalanced data distribution. Our implementation is available at https://github.com/ehsansaleh/firth_bias_reduction.",[],[],"['Saba Ghaffari', 'David Forsyth', 'Yu-Xiong Wang']","['University of Illinois, Urbana Champaign', 'University of Illinois, Urbana-Champaign', 'School of Computer Science, Carnegie Mellon University']",[]
https://iclr.cc/virtual/2022/poster/6387,Fairness & Bias,Approximation and Learning with Deep Convolutional Models: a Kernel Perspective,"The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks. These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias. We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities.","['Approximation', 'generalization', 'convolution', 'deep learning theory', 'kernel methods']",[],['Alberto Bietti'],['Flatiron Institute'],[]
https://iclr.cc/virtual/2022/poster/6666,Fairness & Bias,Fairness Guarantees under Demographic Shift,"Recent studies have demonstrated that using machine learning for social applications can lead to injustice in the form of racist, sexist, and otherwise unfair and discriminatory outcomes. To address this challenge, recent machine learning algorithms have been designed to limit the likelihood such unfair behaviors will occur. However, these approaches typically assume the data used for training is representative of what will be encountered once the model is deployed, thus limiting their usefulness. In particular, if certain subgroups of the population become more or less probable after the model is deployed (a phenomenon we call demographic shift), the fair-ness assurances provided by prior algorithms are often invalid. We consider the impact of demographic shift and present a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift. Shifty is the first technique of its kind and demonstrates an effective strategy for designing algorithms to overcome the challenges demographic shift poses. We evaluate Shifty-ttest, an implementation of Shifty based on Student’s 𝑡-test, and, using a real-world data set of university entrance exams and subsequent student success, show that the models output by our algorithm avoid unfair bias under demo-graphic shift, unlike existing methods. Our experiments demonstrate that our algorithm’s high-confidence fairness guarantees are valid in practice and that our algorithm is an effective tool for training models that are fair when demographic shift occurs.",['machine learning'],[],"['Stephen Giguere', 'Blossom Metevier', 'Bruno Castro da Silva', 'Yuriy Brun', 'Philip S. Thomas', 'Scott Niekum']","['University of Texas, Austin', 'University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'University of Massachusetts Amherst', 'College of Information and Computer Science, University of Massachusetts, Amherst', 'University of Massachusetts at Amherst']",[]
https://iclr.cc/virtual/2022/poster/6491,Fairness & Bias,Resolving Training Biases via Influence-based Data Relabeling,"The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that estimates the impacts of a training sample on the model’s predictions. Recent studies on \emph{data resampling} have employed influence functions to identify \emph{harmful} training samples that will degrade model's test performance. They have shown that discarding or downweighting the identified harmful training samples is an effective way to resolve training biases. In this work, we move one step forward and propose an influence-based relabeling framework named RDIA for reusing harmful training samples toward better model performance. To achieve this, we use influence functions to estimate how relabeling a training sample would affect model's test performance and further develop a novel relabeling function R. We theoretically prove that applying R to relabel harmful training samples allows the model to achieve lower test loss than simply discarding them for any classification tasks using cross-entropy loss. Extensive experiments on ten real-world datasets demonstrate RDIA outperforms the state-of-the-art data resampling methods and improves model's robustness against label noise.",['influence functions'],[],['Shuming Kong'],['Shanghai Jiao Tong University'],[]
https://iclr.cc/virtual/2022/poster/6785,Fairness & Bias,Amortized Implicit Differentiation for Stochastic Bilevel Optimization,"We study a class of algorithms for solving bilevel optimization problems in both stochastic and deterministic settings when the inner-level objective is strongly convex. Specifically, we consider  algorithms based on inexact implicit differentiation and we exploit a warm-start strategy to amortize the estimation of the exact gradient. We then introduce a unified theoretical framework inspired by the study of singularly perturbed systems to analyze such amortized algorithms. By using this framework, our analysis shows these algorithms to match the computational complexity of oracle methods that have access to an unbiased estimate of the gradient, thus outperforming many existing results for bilevel optimization.We illustrate these findings on synthetic experiments and demonstrate the efficiency of these algorithms on hyper-parameter optimization experiments involving several thousands of variables.","['bilevel optimization', 'stochastic optimization']",[],"['Michael Arbel', 'Julien Mairal']","['INRIA', 'Inria']",[]
https://iclr.cc/virtual/2022/poster/7038,Fairness & Bias,Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension,"Robust subspace recovery (RSR) is the problem of learning a subspace from sample data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is a robust subspace recovery method that aims to find a basis for the orthogonal complement of the subspace by minimizing the sum of the distances of the points to the subspaces subject to orthogonality constraints on the basis. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers as long as the true dimension of the subspace is known. In this paper, we show that if the orthogonality constraints --adopted in previous DPCP formulations-- are relaxed and random initialization is used instead of spectral one, DPCP can provably recover a subspace of \emph{unknown dimension}. Specifically, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach succeeds with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of the PSGM algorithm that allows us to perform RSR without knowing the subspace dimension",['representation learning'],[],"['Paris Giampouras', 'Rene Vidal']","['Johns Hopkins University', 'Johns Hopkins University']",[]
https://iclr.cc/virtual/2022/poster/6374,Fairness & Bias,Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?,"While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced  generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to $7.5$% and $6.7$% in $\ell_{\infty}$ and $\ell_2$ threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by $7.6$% on the CIFAR-10 dataset. We further demonstrate that different generative models brings a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact and further provide a deeper understanding of why diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.","['generative models', 'adversarial attacks', 'adversarial robustness']",[],"['Vikash Sehwag', 'Saeed Mahloujifar', 'Tinashe Handina', 'Sihui Dai', 'Chong Xiang', 'Prateek Mittal']","['Princeton University', 'Meta', 'Princeton University', 'Princeton University', 'Princeton University', 'Princeton University']",[]
https://iclr.cc/virtual/2022/poster/6820,Fairness & Bias,MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining,"Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g. the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ). {\em These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond.} In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision \& recall by 4.12\% \& 3.01\% and decreases gender bias by 41.2\%, without requiring labels or retraining.","['data augmentation', 'fairness']",[],"['Ahmed Imtiaz Humayun', 'Randall Balestriero', 'Richard Baraniuk']","['Google', 'Facebook', 'William Marsh Rice University']",[]
https://iclr.cc/virtual/2022/poster/5966,Fairness & Bias,Trivial or Impossible --- dichotomous data difficulty masks model differences (on ImageNet and beyond),"""The power of a generalization system follows directly from its biases"" (Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems---but to what degree have we understood how their inductive bias influences model decisions? We here attempt to disentangle the various aspects that determine how a model decides. In particular, we ask: what makes one model decide differently from another? In a meticulously controlled setting, we find that (1.) irrespective of the network architecture or objective (e.g. self-supervised, semi-supervised, vision transformers, recurrent models) all models end up with a similar decision boundary. (2.) To understand these findings, we analysed model decisions on the ImageNet validation set from epoch to epoch and image by image. We find that the ImageNet validation set, among others, suffers from dichotomous data difficulty (DDD): For the range of investigated models and their accuracies, it is dominated by 46.0% ""trivial"" and 11.5% ""impossible"" images (beyond label errors). Only 42.5%  of the images could possibly be responsible for the differences between two models' decision boundaries. (3.) Only removing the ""impossible"" and ""trivial"" images allows us to see pronounced differences between models. (4.) Humans are highly accurate at predicting which images are ""trivial"" and ""impossible"" for CNNs (81.4%). This implies that in future comparisons of brains, machines and behaviour, much may be gained from investigating the decisive role of images and the distribution of their difficulties.","['psychophysics', 'cognitive science', 'visual perception', 'CNNs', 'CIFAR', 'inductive bias', 'imagenet', 'neuroscience']",[],"['Kristof Meding', 'Luca M. Schulze Buschoff', 'Robert Geirhos', 'Felix A. Wichmann']","['Eberhard-Karls-Universität Tübingen', 'Helmholtz Munich', 'Google DeepMind', 'University of Tübingen']",[]
https://iclr.cc/virtual/2022/poster/7023,Fairness & Bias,Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy,"Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-the-art results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space & earth exploration, and water treatment.",['transformers'],[],"['Jiehui Xu', 'Haixu Wu', 'Jianmin Wang', 'Mingsheng Long']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University']",[]
https://iclr.cc/virtual/2022/poster/7085,Fairness & Bias,Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations,"Existing research on learning with noisy labels mainly focuses on synthetic label noise. The synthetic noise, though has clean structures which greatly enabled statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the existing efforts suffer from two caveats: firstly, the lack of ground-truth verification makes it hard to theoretically study the property and treatment of real-world label noise. Secondly, these efforts are often of large scales, which may result in unfair comparisons of robust methods within reasonable and accessible computation power. To better understand real-world label noise, it is important to establish controllable, easy-to-use, and moderate-sized real-world noisy datasets with both ground-truth and noisy labels. This work presents two new benchmark datasets, which we name as CIFAR-10N, CIFAR-100N (jointly we call them CIFAR-N), equipping the training datasets of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels we collected from Amazon Mechanical Turk. We quantitatively and qualitatively show that real-world noisy labels follow an instance-dependent pattern rather than the classically assumed and adopted ones (e.g.,  class-dependent label noise). We then initiate an effort to benchmarking a subset of the existing solutions using  CIFAR-10N and CIFAR-100N. We further proceed to study the memorization of correct and wrong predictions, which further illustrates the difference between human noise and class-dependent synthetic noise. We show indeed the real-world noise patterns impose new and outstanding challenges as compared to synthetic label noise. These observations require us to rethink the treatment of noisy labels, and we hope the availability of these two datasets would facilitate the development and evaluation of future learning with noisy label solutions. The corresponding datasets and the leaderboard are available at http://noisylabels.com.","['benchmark', 'Learning with noisy labels']",[],"['Jiaheng Wei', 'Zhaowei Zhu', 'Hao Cheng', 'Tongliang Liu', 'Gang Niu', 'Yang Liu']","['University of California, Santa Cruz', 'Docta.ai', 'University of California, Santa Cruz', 'University of Sydney', 'RIKEN', 'University of California, Santa Cruz']",[]
https://iclr.cc/virtual/2022/poster/6230,Fairness & Bias,"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer","Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets","['cnn', 'mobile', 'detection', 'efficient network', 'segmentation', 'transformer', 'imagenet']",[],"['Sachin Mehta', 'Mohammad Rastegari']","['Apple', 'Apple']",[]
https://iclr.cc/virtual/2022/poster/6736,Fairness & Bias,Hindsight: Posterior-guided training of retrievers for improved open-ended generation,"Many text generation systems benefit from retrieving passages from a textual knowledge corpus (e.g., Wikipedia) and using them to generate the output. For open-ended generation tasks, like generating informative utterances in conversations, many varied passages $z$ are relevant to the context $x$ but few are relevant to the observed next utterance $y$ (label). For such tasks, existing methods (that jointly train the retriever and generator) underperform: during training the top-k context-relevant retrieved passages might not contain the label-relevant passage and the generator may hence not learn a preference to ground its generated output in them. We propose using an additional guide-retriever that also conditions on the observed label $y$ and “in hindsight” retrieves label-relevant passages during training. We maximize the evidence lower bound (ELBo) to jointly train the guide-retriever $Q(z|x,y)$ with the standard retriever $P_\eta(z|x)$ and the generator $P_\theta(y|x,z)$ and find that ELBo has better inductive biases than prior work. For informative conversations from the Wizard of Wikipedia dataset, with our posterior-guided training, the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passage (19% relative improvement) and the end-to-end system produces better overall output (6.4% relative improvement).","['ELBO', 'Retrieval', 'Generation']",[],"['Ashwin Paranjape', 'Omar Khattab', 'Christopher Potts', 'Matei Zaharia', 'Christopher D Manning']","['Stanford University', 'Stanford University', 'Stanford University', 'University of California, Berkeley', 'Computer Science Department, Stanford University']",[]
https://iclr.cc/virtual/2022/poster/6261,Fairness & Bias,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings;  instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",[],[],"['Ofir Press', 'Noah A. Smith', 'Mike Lewis']","['Princeton University', 'University of Washington', 'Facebook AI Research']",[]
https://iclr.cc/virtual/2022/poster/6495,Fairness & Bias,Learning Synthetic Environments and Reward Networks for Reinforcement Learning,"We introduce Synthetic Environments (SEs) and Reward Networks (RNs), represented by neural networks, as proxy environment models for training Reinforcement Learning (RL) agents. We show that an agent, after being trained exclusively on the SE, is able to solve the corresponding real environment. While an SE acts as a full proxy to a real environment by learning about its state dynamics and rewards, an RN is a partial proxy that learns to augment or replace rewards. We use bi-level optimization to evolve SEs and RNs: the inner loop trains the RL agent, and the outer loop trains the parameters of the SE / RN via an evolution strategy. We evaluate our proposed new concept on a broad range of RL algorithms and classic control environments. In a one-to-one comparison, learning an SE proxy requires more interactions with the real environment than training agents only on the real environment. However, once such an SE has been learned, we do not need any interactions with the real environment to train new agents. Moreover, the learned SE proxies allow us to train agents with fewer interactions while maintaining the original task performance. Our empirical results suggest that SEs achieve this result by learning informed representations that bias the agents towards relevant states. Moreover, we find that these proxies are robust against hyperparameter variation and can also transfer to unseen agents.","['reinforcement learning', 'reward shaping', 'evolution strategies', 'meta-learning']",[],"['Fabio Ferreira', 'Thomas Nierhoff', 'Andreas Sälinger', 'Frank Hutter']","['Universität Freiburg', 'Universität Freiburg', 'University of Freiburg, Universität Freiburg', 'University of Freiburg & Bosch']",[]
https://iclr.cc/virtual/2022/poster/6910,Fairness & Bias,Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling,"Normalizing flows have shown great success as general-purpose density estimators. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality and continuity. Furthermore, we show that EMFs enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms a large number of alternative methods in common structured inference problems.","['variational inference', 'Normalizing flows', 'generative modeling']",[],"['Gianluigi Silvestri', 'Emily Fertig', 'Dave Moore', 'Luca Ambrogioni']","['OnePlanet Research Center', 'Google', 'University of California Berkeley', 'Radboud University Nijmegen']",[]
https://iclr.cc/virtual/2022/poster/6663,Fairness & Bias,Recursive Disentanglement Network,"Disentangled feature representation is essential for data-efficient learning. The feature space of deep models is inherently compositional. Existing $\beta$-VAE-based methods, which only apply disentanglement regularization to the resulting embedding space of deep models, cannot effectively regularize such compositional feature space, resulting in unsatisfactory disentangled results. In this paper, we formulate the compositional disentanglement learning problem from an information-theoretic perspective and propose a recursive disentanglement network (RecurD) that propagates regulatory inductive bias recursively across the compositional feature space during disentangled representation learning. Experimental studies demonstrate that RecurD outperforms $\beta$-VAE and several of its state-of-the-art variants on disentangled representation learning and enables more data-efficient downstream machine learning tasks.","['representation learning', 'disentanglement']",[],"['Yixuan Chen', 'Yubin Shi', 'Dongsheng Li', 'Yujiang Wang', 'Yingying Zhao', 'Robert P. Dick', 'Qin Lv', 'Fan Yang', 'Li Shang']","['Fudan University', 'Fudan University', 'Microsoft Research Asia', 'University of Oxford', 'Fudan University', 'University of Michigan', 'University of Colorado at Boulder', 'Fudan University', 'Fudan University']",[]
https://iclr.cc/virtual/2022/poster/5990,Fairness & Bias,Language-biased image classification: evaluation based on semantic representations,"Humans show language-biased image recognition for a word-embedded image, known as picture-word interference. Such interference depends on hierarchical semantic categories and reflects that human language processing highly interacts with visual processing. Similar to humans, recent artificial models jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased image classification. Exploring whether the bias leads to interference similar to those observed in humans can contribute to understanding how much the model acquires hierarchical semantic representations from joint learning of language and vision. The present study introduces methodological tools from the cognitive science literature to assess the biases of artificial models. Specifically, we introduce a benchmark task to test whether words superimposed on images can distort the image classification across different category levels and, if it can, whether the perturbation is due to the shared semantic representation between language and vision. Our dataset is a set of word-embedded images and consists of a mixture of natural image datasets and hierarchical word labels with superordinate/basic category levels. Using this benchmark test, we evaluate the CLIP model. We show that presenting words distorts the image classification by the model across different category levels, but the effect does not depend on the semantic relationship between images and embedded words. This suggests that the semantic word representation in the CLIP visual processing is not shared with the image representation, although the word representation strongly dominates for word-embedded images.",['cognitive science'],[],"['Yoann Lemesle', 'Masataka Sawayama', 'Guillermo Valle-Perez', 'Maxime Adolphe', 'Hélène Sauzéon', 'Pierre-Yves Oudeyer']","['Univeristé Paris-Dauphine', 'The University of Tokyo', 'University of Oxford', 'INRIA', 'University of Bordeaux', 'Inria']",[]
https://iclr.cc/virtual/2022/poster/5898,Fairness & Bias,Dynamic Token Normalization improves Vision Transformers,"Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN  makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, and PVT. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN on small ViT by $1.1\%$ top-1 accuracy on ImageNet.","['transformer', 'classification', 'normalization']",[],"['Wenqi Shao', 'Yixiao Ge', 'Zhaoyang Zhang', 'XUYUAN XU', 'Xiaogang Wang', 'Ying Shan', 'Ping Luo']","['Shanghai AI Laboratory', 'Tencent', 'The Chinese University of Hong Kong', 'Magic Light Inc. ', 'The Chinese University of Hong Kong', 'Tencent AI Lab Center of Visual Computing', 'The University of Hong Kong']",[]
https://iclr.cc/virtual/2022/poster/6831,Fairness & Bias,Goal-Directed Planning via Hindsight Experience Replay,"We consider the problem of goal-directed planning under a deterministic transition model. Monte Carlo Tree Search has shown remarkable performance in solving deterministic control problems. It has been extended from complex continuous domains through function approximators to bias the search of the planning tree in AlphaZero. Nonetheless, these algorithms still struggle with control problems with sparse rewards, such as goal-directed domains, where a positive reward is awarded only when reaching a goal state. In this work, we recast AlphaZero with Hindsight Experience Replay to tackle complex goal-directed planning tasks. We perform a thorough empirical evaluation in several simulated domains, including a novel application to a quantum compiling domain.","['reinforcement learning', 'Monte Carlo Tree Search']",[],"['Lorenzo Moro', 'Amarildo Likmeta', 'Enrico Prati', 'Marcello Restelli']","['Politecnico di Milano', ""Universita' di Bologna"", 'Consiglio Nazionale delle Ricerche', 'Politecnico di Milano']",[]
https://iclr.cc/virtual/2022/poster/6410,Fairness & Bias,Hybrid Random Features,"We propose a new class of random feature methods for linearizing softmax and Gaussian kernels called hybrid random features (HRFs) that automatically adapt the quality of kernel estimation to provide most accurate approximation in the defined regions of interest. Special instantiations of HRFs lead to well-known methods such as trigonometric (Rahimi & Recht, 2007) or (recently introduced in the context of linear-attention Transformers) positive random features (Choromanski et al., 2021). By generalizing Bochner’s Theorem for softmax/Gaussian kernels and leveraging random features for compositional kernels, the HRF-mechanism provides strong theoretical guarantees - unbiased approximation and strictly smaller worst-case relative errors than its counterparts.  We conduct exhaustive empirical evaluation of HRF ranging from pointwise kernel estimation experiments, through tests on data admitting clustering structure to benchmarking implicit-attention Transformers (also for downstream Robotics applications), demonstrating its quality in a wide spectrum of machine learning problems.","['random features', 'attention mechanism']",[],"['Krzysztof Marcin Choromanski', 'Han Lin', 'Arijit Sehanobish', 'Yuanzhe Ma', 'Deepali Jain', 'Jake Varley', 'Andy Zeng', 'Michael S Ryoo', 'Valerii Likhosherstov', 'Dmitry Kalashnikov', 'Vikas Sindhwani', 'Adrian Weller']","['Google Brain Robotics & Columbia University', 'Department of Computer Science, University of North Carolina at Chapel Hill', 'Kensho Technologies', 'Columbia University', 'Google', 'Google', 'Google', 'State University of New York, Stony Brook', 'Waymo', 'Google', 'Google', 'Alan Turing Institute']",[]
https://iclr.cc/virtual/2022/poster/7211,Fairness & Bias,Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies,"Human decision making is well known to be imperfect and the ability to analyse such processes individually is crucial when attempting to aid or improve a decision-maker's ability to perform a task, e.g. to alert them to potential biases or oversights on their part. To do so, it is necessary to develop interpretable representations of how agents make decisions and how this process changes over time as the agent learns online in reaction to the accrued experience. To then understand the decision-making processes underlying a set of observed trajectories, we cast the policy inference problem as the inverse to this online learning problem. By interpreting actions within a potential outcomes framework, we introduce a meaningful mapping based on agents choosing an action they believe to have the greatest treatment effect. We introduce a practical algorithm for retrospectively estimating such perceived effects, alongside the process through which agents update them, using a novel architecture built upon an expressive family of deep state-space models. Through application to the analysis of UNOS organ donation acceptance decisions, we demonstrate that our approach can bring valuable insights into the factors that govern decision processes and how they change over time.",['imitation learning'],[],"['Alex James Chan', 'Alicia Curth', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge']",[]
https://iclr.cc/virtual/2022/poster/6347,Fairness & Bias,FairCal: Fairness Calibration for Face Verification,"Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face. As a result, these models can disproportionately and negatively impact minority groups, particularly when used by law enforcement. The majority of bias reduction methods have several drawbacks: they use an end-to-end retraining approach, may not be feasible due to privacy issues, and often reduce accuracy. An alternative approach is post-processing methods that build fairer decision classifiers using the features of pre-trained models, thus avoiding the cost of retraining. However, they still have drawbacks: they reduce accuracy (AGENDA, FTC), or require retuning for different false positive rates (FSN). In this work, we introduce the Fairness Calibration (FairCal) method, a post-training approach that simultaneously: (i) increases model accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated probabilities, (iii) significantly reduces the gap in the false positive rates, (iv) does not require knowledge of the sensitive attribute, and (v) does not require retraining, training an additional model or retuning. We apply it to the task of Face Verification, and obtain state-of-the-art results with all the above advantages.","['calibration', 'bias', 'clustering', 'fairness']",[],"['Tiago Salvador', 'Stephanie Cairns', 'Vikram Voleti', 'Noah Marshall', 'Adam Oberman']","['McGill University', 'McGill University', 'Stability AI', 'McGill University', 'McGill University']",[]
https://iclr.cc/virtual/2022/poster/7208,Fairness & Bias,Permutation Compressors for Provably Faster Distributed Nonconvex Optimization,"In this work we study the MARINA method of Gorbunov et al (ICML, 2021) -- the current state-of-the-art distributed non-convex optimization method in terms of theoretical communication complexity. Theoretical superiority of this method can be largely attributed to two sources: a carefully engineered biased stochastic gradient estimator, which leads to a reduction in the number of communication rounds, and  the reliance on {\em independent} stochastic communication compression, which leads to a reduction in the number of  transmitted bits within each communication round. In this paper we  i) extend the theory of MARINA to support a much wider class of potentially {\em correlated} compressors, extending the reach of the method beyond the classical independent compressors setting,  ii) show that a new quantity, for which we coin the name {\em Hessian variance}, allows us to significantly refine the original analysis of MARINA without any additional assumptions, and iii) identify a special class of correlated compressors based on the idea of {\em random  permutations}, for which we coin the term Perm$K$, the use of which leads to up to $O(\sqrt{n})$ (resp. $O(1 + d/\sqrt{n})$) improvement in the theoretical communication complexity of MARINA in the low Hessian variance regime when $d\geq n$ (resp. $d \leq n$), where $n$ is the number of workers and $d$ is the number of parameters describing the model we are learning. We corroborate our theoretical results with carefully engineered synthetic experiments with minimizing the average of nonconvex quadratics, and on autoencoder training with the MNIST dataset.","['distributed training', 'nonconvex optimization']",[],"['Rafał Szlendak', 'Alexander Tyurin', 'Peter Richtárik']","['King Abdullah University of Science and Technology', 'KAUST', 'King Abdullah University of Science and Technology (KAUST)']",[]
https://iclr.cc/virtual/2022/poster/6432,Fairness & Bias,iFlood: A Stable and Effective Regularizer,"Various regularization methods have been designed to prevent overfitting of machine learning models. Among them, a surprisingly simple yet effective one, called Flooding, is proposed recently, which directly constrains the training loss on average to stay at a given level. However, our further studies uncover that the design of the loss function of Flooding can lead to a discrepancy between its objective and implementation, and cause the instability issue. To resolve these issues, in this paper, we propose a new regularizer, called individual Flood (denoted as iFlood). With instance-level constraints on training loss, iFlood encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. We theoretically show that the design of iFlood can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. We also theoretically link iFlood to some other regularizers by comparing the inductive biases they introduce. Our experimental results on both image classification and language understanding tasks confirm that models learned with iFlood can stably converge to solutions with better generalization ability, and behave consistently at instance-level.",['Overfitting'],[],"['Yuexiang Xie', 'Zhen WANG', 'Yaliang Li', 'Ce Zhang', 'Jingren Zhou', 'Bolin Ding']","['Alibaba Group', 'SUN YAT-SEN UNIVERSITY', 'Alibaba Group', 'University of Chicago', 'Alibaba Group', 'Alibaba Group']",[]
https://iclr.cc/virtual/2022/poster/7075,Fairness & Bias,"In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications","We address the problem of building agents whose goal is to learn to execute out-of distribution (OOD) multi-task instructions expressed in temporal logic (TL) by using deep reinforcement learning (DRL). Recent works provided evidence that the agent's neural architecture is a key feature when DRL agents are learning to solve OOD tasks in TL. Yet, the studies on this topic are still in their infancy. In this work, we propose a new deep learning configuration with inductive biases that lead agents to generate latent representations of their current goal, yielding a stronger generalization performance. We use these latent-goal networks within a neuro-symbolic framework that executes multi-task formally-defined instructions and contrast the performance of the proposed neural networks against employing different state-of-the-art (SOTA) architectures when generalizing to unseen instructions in OOD environments.",['deep reinforcement learning'],[],"['Borja G. León', 'Murray Shanahan', 'Francesco Belardinelli']","['Imperial College London', 'Google', 'Imperial College London']",[]
https://iclr.cc/virtual/2022/poster/6839,Fairness & Bias,Generalized Demographic Parity for Group Fairness,"This work aims to generalize demographic parity to continuous sensitive attributes while preserving tractable computation. Current fairness metrics for continuous sensitive attributes largely rely on intractable statistical independence between variables, such as Hirschfeld-Gebelein-Renyi (HGR) and mutual information. Statistical fairness metrics estimation relying on either tractable bounds or neural network approximation, however, are not sufficiently trustful to rank algorithms prediction bias due to lack of estimation accuracy guarantee. To make fairness metrics trustable, we propose \textit{\underline{G}eneralized \underline{D}emographic \underline{P}arity} (GDP), a group fairness metric for continuous and discrete attributes. We show the understanding of GDP from the probability perspective and theoretically reveal the connection between GDP regularizer and adversarial debiasing. To estimate GDP, we adopt hard and soft group strategies via the one-hot or the soft group indicator, representing the membership of each sample in different groups of the sensitive attribute. We provably and numerically show that the soft group strategy achieves a faster estimation error convergence rate. Experiments show the better bias mitigation performance of GDP regularizer, compared with adversarial debiasing, for regression and classification tasks in tabular and graph benchmarks.",[],[],"['Zhimeng Jiang', 'Xiaotian Han', 'Chao Fan', 'Fan Yang', 'Ali Mostafavi', 'Xia Hu']","['Texas A&M University', 'Texas A&M University', 'Clemson University', 'Wake Forest University', 'Texas A&M', 'Rice University']",[]
https://iclr.cc/virtual/2022/poster/6150,Fairness & Bias,Prototype memory and attention mechanisms for few shot image generation,"Recent discoveries indicate that the neural codes in the primary visual cortex (V1) of macaque monkeys are complex, diverse and sparse. This leads us to ponder the computational advantages and functional role of these “grandmother cells."" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing within the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized via a memory-based attention operation, which we define as Memory Concept Attention (MoCA). To test our proposal, we show in a few-shot image generation task, that having a prototype memory during attention can improve image synthesis quality, learn interpretable visual concept clusters, as well as improve the robustness of the model. Interestingly, we also find that our attentional memory mechanism can implicitly modify the horizontal connections by updating the transformation into the prototype embedding space for self-attention. Insofar as GANs can be seen as plausible models for reasoning about the top-down synthesis in the analysis-by-synthesis loop of the hierarchical visual cortex, our findings demonstrate a plausible computational role for these “prototype concept"" neurons in visual processing in the brain.","['deep learning', 'neuroscience']",[],"['Tianqin Li', 'Zijie Li', 'Andrew Luo', 'Harold Rockwell', 'Amir Barati Farimani', 'Tai Sing Lee']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'University of Chicago', 'CMU, Carnegie Mellon University', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2022/poster/6365,Fairness & Bias,Concurrent Adversarial Learning for Large-Batch Training,"Large-batch training has become a commonly used technique when training neural networks with a large number of GPU/TPU processors. As batch size increases, stochastic optimizers tend to converge to sharp local minima, leading to degraded test performance. Current methods usually use extensive data augmentation to increase the batch size, but we found the performance gain with data augmentation decreases as batch size increases, and data augmentation will become insufficient after certain point. In this paper, we propose to use adversarial learning to increase the batch size in large-batch training. Despite being a natural choice for smoothing the decision surface and biasing towards a flat region, adversarial learning has not been successfully applied in large-batch training since it requires at least two sequential gradient computations at each step, which will at least double the running time compared with vanilla training even with a large number of processors. To overcome this issue, we propose a novel Concurrent Adversarial Learning (ConAdv) method that decouple the sequential gradient computations in adversarial learning by utilizing staled parameters. Experimental results demonstrate that ConAdv can successfully  increase the batch size on both ResNet-50 and EfficientNet training on ImageNet while maintaining high accuracy. In particular, we show ConAdv along can achieve 75.3\% top-1 accuracy on ImageNet ResNet-50 training with 96K batch size, and the accuracy can be further improved to 76.2\% when combining ConAdv with data augmentation. This is the first work successfully scales ResNet-50 training batch size to 96K.",['adversarial learning'],[],"['Yong Liu', 'Xiangning Chen', 'Minhao Cheng', 'Cho-Jui Hsieh', 'Yang You']","['National University of Singapore', 'University of California, Los Angeles', 'Pennsylvania State University', 'Google', 'National University of Singapore']",[]
https://iclr.cc/virtual/2022/poster/6267,Fairness & Bias,CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention,"Transformers have made great progress in dealing with computer vision tasks. However, existing vision transformers have not yet possessed the ability of building the interactions among features of different scales, which is perceptually important to visual inputs. The reasons are two-fold: (1) Input embeddings of each layer are equal-scale, so no cross-scale feature can be extracted; (2) to lower the computational cost, some vision transformers merge adjacent embeddings inside the self-attention module, thus sacrificing small-scale (fine-grained) features of the embeddings and also disabling the cross-scale interactions. To this end, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). On the one hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features. On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings. Through the above two designs, we achieve cross-scale attention. Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs. Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks.",['architecture'],[],"['Wenxiao Wang', 'Lu Yao', 'Long Chen', 'Binbin Lin', 'Deng Cai', 'Xiaofei He', 'Wei Liu']","['Zhejiang University', 'Zhejiang University', 'Hong Kong University of Science and Technology', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Tencent']",[]
https://iclr.cc/virtual/2022/poster/7092,Fairness & Bias,Properties from mechanisms: an equivariance perspective on identifiable representation learning,"A key goal of unsupervised representation learning isinverting'' a data generating process to recover its latent properties.  Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the problem and ask,Can we instead identify latent properties by leveraging knowledge of the mechanisms that govern their evolution?'' We provide a complete characterization of the sources of non-identifiability as we vary knowledge about a set of possible mechanisms. In particular, we prove that if we know the exact mechanisms under which the latent properties evolve, then identification can be achieved up to any equivariances that are shared by the underlying mechanisms. We generalize this characterization to settings where we only know some hypothesis class over possible mechanisms, as well as settings where the mechanisms are stochastic. We demonstrate the power of this mechanism-based perspective by showing that we can leverage our results to generalize existing identifiable representation learning results. These results suggest that by exploiting inductive biases on mechanisms, it is possible to design a range of new identifiable representation learning approaches.","['independent component analysis', 'equivariance', 'autoencoders', 'representation learning']",[],"['Kartik Ahuja', 'Jason Hartford', 'Yoshua Bengio']","['FAIR (Meta)', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'University of Montreal']",[]
https://iclr.cc/virtual/2022/poster/7133,Fairness & Bias,On-Policy Model Errors in Reinforcement Learning,"Model-free reinforcement learning algorithms can compute policy gradients given sampled environment transitions, but require large amounts of data. In contrast, model-based methods can use the learned model to generate new data, but model errors and bias can render learning unstable or suboptimal. In this paper, we present a novel method that combines real-world data and a learned model in order to get the best of both worlds. The core idea is to exploit the real-world data for on-policy predictions and use the learned model only to generalize to different actions. Specifically, we use the data as time-dependent on-policy correction terms on top of a learned model, to retain the ability to generate data without accumulating errors over long prediction horizons. We motivate this method theoretically and show that it counteracts an error term for model-based policy improvement. Experiments on MuJoCo- and PyBullet-benchmarks show that our method can drastically improve existing model-based approaches without introducing additional tuning parameters.","['reinforcement learning', 'model learning', 'model-based reinforcement learning']",[],"['Lukas Froehlich', 'Maksym Lefarov', 'Melanie Zeilinger', 'Felix Berkenkamp']","['Swiss Federal Institute of Technology', 'Bosch', 'ETHZ - ETH Zurich', 'Bosch']",[]
https://iclr.cc/virtual/2022/poster/6609,Fairness & Bias,Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks,"We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we derive a continuous-time model for the dynamics of the data via the finite element method. The resulting graph neural network estimates the instantaneous effects of the unknown dynamics on each cell in a meshing of the spatial domain. Our model can incorporate prior knowledge via assumptions on the form of the unknown PDE, which induce a structural bias towards learning specific processes. Through this mechanism, we derive a transport variant of our model from the convection equation and show that it improves the transfer performance to higher-resolution meshes on sea surface temperature and gas flow forecasting against baseline models representing a selection of spatio-temporal forecasting methods. A qualitative analysis shows that our model disentangles the data dynamics into their constituent parts, which makes it uniquely interpretable.","['continuous', 'graph', 'spatio-temporal', 'forecasting', 'pde', 'gnn']",[],"['Marten Lienen', 'Stephan Günnemann']","['Technical University Munich', 'Technical University Munich']",[]
https://iclr.cc/virtual/2022/poster/6838,Fairness & Bias,Discovering Latent Concepts Learned in BERT,"A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-defined concepts that reinforce the traditional linguistic knowledge and do not reflect on how novel concepts are learned by the model. We address this limitation by discovering and analyzing latent concepts learned in neural network models in an unsupervised fashion and provide interpretations from the model's perspective. In this work, we study: i) what latent concepts exist in the pre-trained BERT model, ii) how the discovered latent concepts align or diverge from classical linguistic hierarchy and iii) how the latent concepts evolve across layers. Our findings show: i) a model learns novel concepts (e.g. animal categories and demographic groups), which do not strictly adhere to any pre-defined categorization (e.g. POS, semantic tags), ii) several latent concepts are based on multiple properties which may include semantics, syntax, and  morphology, iii) the lower layers in the model dominate in learning shallow lexical concepts while the higher layers learn semantic relations and iv) the discovered  latent concepts highlight potential biases learned in the model. We also release a novel BERT ConceptNet dataset consisting of 174 concept labels and 1M annotated instances.","['BERT', 'nlp']",[],"['Fahim Dalvi', 'Abdul Rafae Khan', 'Firoj Alam', 'Nadir Durrani', 'Jia Xu', 'Hassan Sajjad']","['Qatar Computing Research Institute', 'Stevens Institute of Technology', 'Qatar Computing Research Institute', 'Qatar Computing Research Institute', 'Stevens Institute of Technology', 'Dalhousie University']",[]
https://iclr.cc/virtual/2022/poster/6415,Fairness & Bias,Phase Collapse in Neural Networks,"Deep convolutional classifiers linearly separate image classes and improve accuracy as depth increases. They progressively reduce the spatial dimension whereas the number of channels grows with depth. Spatial variability is therefore transformed into variability along channels. A fundamental challenge is to understand the role of non-linearities together with convolutional filters in this transformation. ReLUs with biases are often interpreted as thresholding operators that improve discrimination through sparsity. This paper demonstrates that it is a different mechanism called \emph{phase collapse} which eliminates spatial variability while linearly separating classes. We show that collapsing the phases of complex wavelet coefficients is sufficient to reach the classification accuracy of ResNets of similar depths. However, replacing the phase collapses with thresholding operators that enforce sparsity considerably degrades the performance. We explain these numerical results by showing that the iteration of phase collapses progressively improves separation of classes, as opposed to thresholding non-linearities.","['classification', 'concentration', 'neural collapse', 'deep networks', 'imagenet']",[],"['Florentin Guth', 'John Zarka', 'Stéphane Mallat']","['New York University', 'Ecole Normale Superieure', 'College de France']",[]
https://iclr.cc/virtual/2022/poster/5973,Fairness & Bias,How to deal with missing data in supervised deep learning?,"The issue of missing data in supervised learning has been largely overlooked, especially in the deep learning community. We investigate strategies to adapt neural architectures for handling missing values. Here, we focus on regression and classification problems where the features are assumed to be missing at random. Of particular interest are schemes that allow reusing as-is a neural discriminative architecture. To address supervised deep learning with missing values, we propose to marginalize over missing values in a joint model of covariates and outcomes. Thereby, we leverage both the flexibility of deep generative models to describe the distribution of the covariates and the power of purely discriminative models to make predictions. More precisely, a deep latent variable model can be learned jointly with the discriminative model, using importance-weighted variational inference, essentially using importance sampling to mimick averaging over multiple imputations. In low-capacity regimes, or when the discriminative model has a strong inductive bias, we find that our hybrid generative/discriminative approach generally outperforms single imputations methods.",[],[],"['Niels Bruun Ipsen', 'Pierre-Alexandre Mattei', 'Jes Frellsen']","['Technical University of Denmark', 'INRIA', 'Technical University of Denmark']",[]
https://iclr.cc/virtual/2022/poster/6679,Fairness & Bias,Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect,"Recent empirical advances show that training deep models with large learning rate often improves generalization performance. However, theoretical justifications on the benefits of large learning rate are highly limited, due to challenges in analysis. In this paper, we consider using Gradient Descent (GD) with a large learning rate on a homogeneous matrix factorization problem, i.e., $\min_{X, Y} \|A - XY^\top\|_{\sf F}^2$. We prove a convergence theory for constant large learning rates well beyond $2/L$, where $L$ is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously establish an implicit bias of GD induced by such a large learning rate, termed `balancing', meaning that magnitudes of $X$ and $Y$ at the limit of GD iterations will be close even if their initialization is significantly unbalanced. Numerical experiments are provided to support our theory.","['convergence', 'matrix factorization', 'alignment', 'implicit regularization', 'gradient descent']",[],"['Yuqing Wang', 'Minshuo Chen', 'Tuo Zhao', 'Molei Tao']","['Georgia Institute of Technology', 'Princeton University', 'Georgia Institute of Technology', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6276,Fairness & Bias,Visual Representation Learning Does Not Generalize Strongly Within the Same Domain,"An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world.In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark.In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets.Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization.","['generalization', 'Composition', 'disentanglement']",[],"['Julius von Kügelgen', 'Frederik Träuble', 'Peter Vincent Gehler', 'Chris Russell', 'Matthias Bethge', 'Francesco Locatello', 'Wieland Brendel']","['Max Planck Institute for Intelligent Systems, Max-Planck Institute', ', Max Planck Institute for Intelligent Systems', 'Zalando SE', 'Amazon', 'University of Tuebingen', 'Institute of Science and Technology', 'ELLIS Institute Tübingen']",[]
https://iclr.cc/virtual/2022/poster/6557,Fairness & Bias,Controlling Directions Orthogonal to a Classifier,"We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than those of the given classifier). Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness. The orthogonal classifier enables desired style transfer when domains vary in multiple aspects, improves domain adaptation with label shifts and mitigates the unfairness as a predictor. The code is available at https://github.com/Newbeeer/orthogonal_classifier",['invariance'],[],"['Yilun Xu', 'Hao He', 'Tianxiao Shen', 'Tommi Jaakkola']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Washington', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/7045,Fairness & Bias,Fair Normalizing Flows,"Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets.",['fairness'],[],"['Mislav Balunovic', 'Anian Ruoss', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'DeepMind', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6209,Fairness & Bias,An Unconstrained Layer-Peeled Perspective on Neural Collapse,"Neural collapse is a highly symmetric geometry of neural networks that emerges during the terminal phase of training, with profound implications on the generalization performance and robustness of the trained networks. To understand how the last-layer features and classifiers exhibit this recently discovered implicit bias, in this paper, we introduce a surrogate model called the unconstrained layer-peeled model (ULPM). We prove that gradient flow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Moreover, we show that the ULPM with the cross-entropy loss has a benign global landscape for its loss function, which allows us to prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon. Empirically, we show that our results also hold during the training of neural networks in real-world tasks when explicit regularization or weight decay is not used.","['implicit regularization', 'neural collapse']",[],"['Wenlong Ji', 'Yiping Lu', 'Yiliang Zhang', 'Zhun Deng', 'Weijie J Su']","['Stanford University', 'Northwestern University', 'University of Pennsylvania', 'Columbia University', 'The Wharton School, University of Pennsylvania']",[]
https://iclr.cc/virtual/2022/poster/7048,Fairness & Bias,What Happens after SGD Reaches Zero Loss? --A Mathematical Framework,"Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function $L$ can form a manifold. Intuitively, with a sufficiently small learning rate $\eta$, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, $\text{tr}[\nabla^2 L]$. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold---i.e., the ""implicit bias""---using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a *global* analysis of the implicit bias valid for $\eta^{-2}$ steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for $\eta^{-1.6}$ steps and (2) allowing *arbitrary* noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires $O(\kappa\ln d)$ samples for learning an $\kappa$-sparse overparametrized linear model in $\mathbb{R}^d$ (Woodworth et al., 2020), while GD initialized in the kernel regime requires $\Omega(d)$ samples. This upper bound is minimax optimal and improves the previous $\widetilde{O}(\kappa^2)$ upper bound (HaoChen et al., 2020).","['manifold', 'sgd', 'deep learning', 'implicit regularization', 'generalization', 'implicit bias']",[],"['Zhiyuan Li', 'Tianhao Wang', 'Sanjeev Arora']","['Toyota Technological Institute at Chicago', 'Yale University', 'Princeton University']",[]
https://iclr.cc/virtual/2022/poster/7001,Fairness & Bias,Learning by Directional Gradient Descent,"How should state be constructed from a sequence of observations, so as to best achieve some objective? Most deep learning methods update the parameters of the state representation by gradient descent. However, no prior method for computing the gradient is fully satisfactory, for example consuming too much memory, introducing too much variance, or adding too much bias. In this work, we propose a new learning algorithm that addresses these limitations. The basic idea is to update the parameters of the representation by using the directional derivative along a candidate direction, a quantity that may be computed online with the same computational cost as the representation itself. We consider several different choices of candidate direction, including random selection and approximations to the true gradient, and investigate their performance on several synthetic tasks.","['credit assignment', 'recurrent networks']",[],"['David Silver', 'Anirudh Goyal', 'Ivo Danihelka', 'Matteo Hessel', 'Hado van Hasselt']","['University College London', 'Google DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']",[]
https://iclr.cc/virtual/2022/poster/6634,Fairness & Bias,Actor-critic is implicitly biased towards high entropy optimal policies,"We show that the simplest actor-critic method — a linear softmax policy updated with TD through interaction with a linear MDP, but featuring no explicit regularization or exploration — does not merely find an optimal policy, but moreover prefers high entropy optimal policies. To demonstrate the strength of this bias, the algorithm not only has no regularization, no projections, and no exploration like $\epsilon$-greedy, but is moreover trained on a single trajectory with no resets. The key consequence of the high entropy bias is that uniform mixing assumptions on the MDP, which exist in some form in all prior work, can be dropped: the implicit regularization of the high entropy bias is enough to ensure that all chains mix and an optimal policy is reached with high probability. As auxiliary contributions, this work decouples concerns between the actor and critic by writing the actor update as an explicit mirror descent, provides tools to uniformly bound mixing times within KL balls of policy space, and provides a projection-free TD analysis with its own implicit bias which can be run from an unmixed starting distribution.","['reinforcement learning', 'policy gradient', 'actor-critic', 'implicit bias']",[],"['Yuzheng Hu', 'Ziwei Ji', 'Matus Telgarsky']","['Peking University', 'Google', 'Department of Computer Science, University of Illinois, Urbana Champaign']",[]
https://iclr.cc/virtual/2022/poster/6564,Fairness & Bias,Overcoming The Spectral Bias of Neural Value Approximation,"Value approximation using deep neural networks is at the heart of off-policy deep reinforcement learning, and is often the primary module that provides learning signals to the rest of the algorithm.  While multi-layer perceptrons are universal function approximators, recent works in neural kernel regression suggest the presence of a \textit{spectral bias}, where fitting high-frequency components of the value function requires exponentially more gradient update steps than the low-frequency ones. In this work, we re-examine off-policy reinforcement learning through the lens of kernel regression and propose to overcome such bias via a composite neural tangent kernel. With just a single line-change, our approach, the Fourier feature networks (FFN) produce state-of-the-art performance on challenging continuous control domains with only a fraction of the compute. Faster convergence and better off-policy stability also make it possible to remove the target network without suffering catastrophic divergences, which further reduces (\text{TD}(0))'s bias to over-estimate the value.","['reinforcement learning', 'neural tangent kernels', 'q learning']",[],"['Ge Yang', 'Anurag Ajay', 'Pulkit Agrawal']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6204,Fairness & Bias,Online Hyperparameter Meta-Learning with Hypergradient Distillation,"Many gradient-based meta-learning methods assume a set of parameters that do not participate in inner-optimization, which can be considered as hyperparameters. Although such hyperparameters can be optimized using the existing gradient-based hyperparameter optimization (HO) methods, they suffer from the following issues. Unrolled differentiation methods do not scale well to high-dimensional hyperparameters or horizon length, Implicit Function Theorem (IFT) based methods are restrictive for online optimization, and short horizon approximations suffer from short horizon bias. In this work, we propose a novel HO method that can overcome these limitations, by approximating the second-order term with knowledge distillation. Specifically, we parameterize a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second-order term. Our method allows online optimization and also is scalable to the hyperparameter dimension and the horizon length. We demonstrate the effectiveness of our method on three different meta-learning methods and two benchmark datasets.","['hyperparameter optimization', 'meta-learning']",[],"['Hae Beom Lee', 'Hayeon Lee', 'JaeWoong Shin', 'Eunho Yang', 'Timothy Hospedales', 'Sung Ju Hwang']","['Korea Advanced Institute of Science and Technology', 'FAIR (Meta AI)', 'Lunit Inc.', 'KAIST', 'University of Edinburgh', 'Korea Advanced Institute of Science and Technology']",[]
https://iclr.cc/virtual/2022/poster/7159,Fairness & Bias,EigenGame Unloaded: When playing games is better than optimizing,"We build on the recently proposed EigenGame that views eigendecomposition as a competitive game. EigenGame's updates are biased if computed using minibatches of data, which hinders convergence and more sophisticated parallelism in the stochastic setting. In this work, we propose an unbiased stochastic update that is asymptotically equivalent to EigenGame, enjoys greater parallelism allowing computation on datasets of larger sample sizes, and outperforms EigenGame in experiments. We present applications to finding the principal components of massive datasets and performing spectral clustering of graphs. We analyze and discuss our proposed update in the context of EigenGame and the shift in perspective from optimization to games.","['singular value decomposition', 'svd', 'eigendecomposition', 'nash', 'principal components analysis', 'pca', 'games']",[],"['Ian Gemp', 'Brian McWilliams', 'Claire Vernade']","['Google DeepMind', 'Google', 'Eberhard-Karls-Universität Tübingen']",[]
https://iclr.cc/virtual/2022/poster/7166,Fairness & Bias,Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models,"Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\times$ to $80\times$ speed up.","['diffusion probabilistic models', 'generative models']",[],"['Fan Bao', 'Chongxuan Li', 'Jun Zhu', 'Bo Zhang']","['Tsinghua University, Tsinghua University', 'Renmin University of China', 'Tsinghua University', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2022/poster/6796,Privacy & Data Governance,"Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How","Optimizing an objective function with uncertainty awareness is well-known to improve the accuracy and confidence of optimization solutions. Meanwhile, another relevant but very different question remains yet open: how to model and quantify the uncertainty of an optimization algorithm (a.k.a., optimizer) itself? To close such a gap, the prerequisite is to consider the optimizers as sampled from a distribution, rather than a few prefabricated and fixed update rules. We first take the novel angle to consider the algorithmic space of optimizers, and provide definitions for the optimizer prior and likelihood, that intrinsically determine the posterior and therefore uncertainty. We then leverage the recent advance of learning to optimize (L2O) for the space parameterization, with the end-to-end training pipeline built via variational inference, referred to as uncertainty-aware L2O (UA-L2O). Our study represents the first effort to recognize and quantify the uncertainty of the optimization algorithm. The extensive numerical results show that, UA-L2O achieves superior uncertainty calibration with accurate confidence estimation and tight confidence intervals, suggesting the improved posterior estimation thanks to considering optimizer uncertainty. Intriguingly, UA-L2O even improves optimization performances for two out of three test functions, the loss function in data privacy attack, and four of five cases of the energy function in protein docking. Our codes are released at https://github.com/Shen-Lab/Bayesian-L2O.",[],[],"['Yuning You', 'Yue Cao', 'Tianlong Chen', 'Zhangyang Wang', 'Yang Shen']","['Texas A&M University', 'Texas A&M', 'Massachusetts Institute of Technology', 'University of Texas at Austin', 'Texas A&M University - College Station']",[]
https://iclr.cc/virtual/2022/poster/6513,Privacy & Data Governance,Increasing the Cost of Model Extraction with Calibrated Proof of Work,"In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.","['Model stealing', 'deep learning', 'model extraction', 'adversarial machine learning']",[],"['Muhammad Ahmad Kaleem', 'Yu Shen Lu', 'Nicolas Papernot']","['University of Toronto', 'Toronto University', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6471,Privacy & Data Governance,Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters,"The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly  due to the particularity of the task, \textit{i.e.},  broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo leads to more discriminative features. The proposed schemes are mathematically proved to be differential private, introduce a lightweight overhead as well as yield prominent performance boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method.",[],[],"['Qiang Meng', 'Feng Zhou', 'Hainan Ren', 'Guochao Liu', 'Yuanqing Lin']","['DiDi', 'NEC Labs', 'Aibee', 'DiDi', 'Aibee Inc.']",[]
https://iclr.cc/virtual/2022/poster/6094,Privacy & Data Governance,A Zest of LIME: Towards Architecture-Independent Model Distances,"Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights. While similarity of weights is attractive because it implies similarity of predictions in the limit, it suffers from being inapplicable to comparing models with different architectures. On the other hand, the similarity of predictions is broadly applicable but depends heavily on the choice of model inputs during comparison. In this paper, we instead propose to compute distance between black-box models by comparing their Local Interpretable Model-Agnostic Explanations (LIME). To compare two models, we take a reference dataset, and locally approximate the models on each reference point with linear models trained by LIME. We then compute the cosine distance between the concatenated weights of the linear models. This yields an approach that is both architecture-independent and possesses the benefits of comparing models in weight space. We empirically show that our method, which we call Zest, can be applied to two problems that require measurements of model similarity: detecting model stealing and machine unlearning.",['Model stealing'],[],"['Hengrui Jia', 'Hongyu Chen', 'Jonas Guan', 'Ali Shahin Shamsabadi', 'Nicolas Papernot']","['Toronto University', 'Cohere', 'Department of Computer Science, University of Toronto', 'Brave Software', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6035,Privacy & Data Governance,Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning,"The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \url{https://github.com/fshp971/robust-unlearnable-examples}.","['adversarial training', 'Unlearnable Examples', 'privacy']",[],"['Shaopeng Fu', 'Fengxiang He', 'Yang Liu', 'Li Shen', 'Dacheng Tao']","['King Abdullah University of Science and Technology', 'University of Edinburgh', 'Tsinghua University, Tsinghua University', 'JD Explore Academy', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6934,Privacy & Data Governance,Bayesian Framework for Gradient Leakage,"Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem.","['privacy', 'federated learning']",[],"['Mislav Balunovic', 'Dimitar Iliev Dimitrov', 'Robin Staab', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6703,Privacy & Data Governance,Shuffle Private Stochastic Convex Optimization,"In shuffle privacy, each user sends a collection of randomized messages to a trusted shuffler, the shuffler randomly permutes these messages, and the resulting shuffled collection of messages must satisfy differential privacy. Prior work in this model has largely focused on protocols that use a single round of communication to compute algorithmic primitives like means, histograms, and counts. In this work, we present interactive shuffle protocols for stochastic convex optimization. Our optimization protocols rely on a new noninteractive protocol for summing vectors of bounded $\ell_2$ norm. By combining this sum subroutine with techniques including mini-batch stochastic gradient descent, accelerated gradient descent, and Nesterov's smoothing method, we obtain loss guarantees for a variety of convex loss functions that significantly improve on those of the local model and sometimes match those of the central model.",['differential privacy'],[],"['Albert Cheu', 'Matthew Joseph', 'Jieming Mao', 'Binghui Peng']","['Google', 'Google', 'Google', 'Columbia University']",[]
https://iclr.cc/virtual/2022/poster/6020,Privacy & Data Governance,Knowledge Removal in Sampling-based Bayesian Inference,"The right to be forgotten has been legislated in many countries, but its enforcement in the AI industry would cause unbearable costs. When single data deletion requests come, companies may need to delete the whole models learned with massive resources. Existing works propose methods to remove knowledge learned from data for explicitly parameterized models, which however are not appliable to the sampling-based Bayesian inference, {\it i.e.}, Markov chain Monte Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we propose the first machine unlearning algorithm for MCMC. We first convert the MCMC unlearning problem into an explicit optimization problem. Based on this problem conversion, an {\it MCMC influence function} is designed to provably characterize the learned knowledge from data, which then delivers the MCMC unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not compromise the generalizability of the MCMC models. Experiments on Gaussian mixture models and Bayesian neural networks confirm the effectiveness of the proposed algorithm. The code is available at \url{https://github.com/fshp971/mcmc-unlearning}.","['Markov chain Monte Carlo', 'bayesian inference']",[],"['Shaopeng Fu', 'Fengxiang He', 'Dacheng Tao']","['King Abdullah University of Science and Technology', 'University of Edinburgh', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6746,Privacy & Data Governance,Hyperparameter Tuning with Renyi Differential Privacy,"For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm’s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.",['differential privacy'],[],"['Nicolas Papernot', 'Thomas Steinke']","['University of Toronto', 'Google']",[]
https://iclr.cc/virtual/2022/poster/6796,Security,"Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How","Optimizing an objective function with uncertainty awareness is well-known to improve the accuracy and confidence of optimization solutions. Meanwhile, another relevant but very different question remains yet open: how to model and quantify the uncertainty of an optimization algorithm (a.k.a., optimizer) itself? To close such a gap, the prerequisite is to consider the optimizers as sampled from a distribution, rather than a few prefabricated and fixed update rules. We first take the novel angle to consider the algorithmic space of optimizers, and provide definitions for the optimizer prior and likelihood, that intrinsically determine the posterior and therefore uncertainty. We then leverage the recent advance of learning to optimize (L2O) for the space parameterization, with the end-to-end training pipeline built via variational inference, referred to as uncertainty-aware L2O (UA-L2O). Our study represents the first effort to recognize and quantify the uncertainty of the optimization algorithm. The extensive numerical results show that, UA-L2O achieves superior uncertainty calibration with accurate confidence estimation and tight confidence intervals, suggesting the improved posterior estimation thanks to considering optimizer uncertainty. Intriguingly, UA-L2O even improves optimization performances for two out of three test functions, the loss function in data privacy attack, and four of five cases of the energy function in protein docking. Our codes are released at https://github.com/Shen-Lab/Bayesian-L2O.",[],[],"['Yuning You', 'Yue Cao', 'Tianlong Chen', 'Zhangyang Wang', 'Yang Shen']","['Texas A&M University', 'Texas A&M', 'Massachusetts Institute of Technology', 'University of Texas at Austin', 'Texas A&M University - College Station']",[]
https://iclr.cc/virtual/2022/poster/6513,Security,Increasing the Cost of Model Extraction with Calibrated Proof of Work,"In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.","['Model stealing', 'deep learning', 'model extraction', 'adversarial machine learning']",[],"['Muhammad Ahmad Kaleem', 'Yu Shen Lu', 'Nicolas Papernot']","['University of Toronto', 'Toronto University', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6471,Security,Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters,"The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly  due to the particularity of the task, \textit{i.e.},  broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo leads to more discriminative features. The proposed schemes are mathematically proved to be differential private, introduce a lightweight overhead as well as yield prominent performance boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method.",[],[],"['Qiang Meng', 'Feng Zhou', 'Hainan Ren', 'Guochao Liu', 'Yuanqing Lin']","['DiDi', 'NEC Labs', 'Aibee', 'DiDi', 'Aibee Inc.']",[]
https://iclr.cc/virtual/2022/poster/6094,Security,A Zest of LIME: Towards Architecture-Independent Model Distances,"Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights. While similarity of weights is attractive because it implies similarity of predictions in the limit, it suffers from being inapplicable to comparing models with different architectures. On the other hand, the similarity of predictions is broadly applicable but depends heavily on the choice of model inputs during comparison. In this paper, we instead propose to compute distance between black-box models by comparing their Local Interpretable Model-Agnostic Explanations (LIME). To compare two models, we take a reference dataset, and locally approximate the models on each reference point with linear models trained by LIME. We then compute the cosine distance between the concatenated weights of the linear models. This yields an approach that is both architecture-independent and possesses the benefits of comparing models in weight space. We empirically show that our method, which we call Zest, can be applied to two problems that require measurements of model similarity: detecting model stealing and machine unlearning.",['Model stealing'],[],"['Hengrui Jia', 'Hongyu Chen', 'Jonas Guan', 'Ali Shahin Shamsabadi', 'Nicolas Papernot']","['Toronto University', 'Cohere', 'Department of Computer Science, University of Toronto', 'Brave Software', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6035,Security,Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning,"The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \url{https://github.com/fshp971/robust-unlearnable-examples}.","['adversarial training', 'Unlearnable Examples', 'privacy']",[],"['Shaopeng Fu', 'Fengxiang He', 'Yang Liu', 'Li Shen', 'Dacheng Tao']","['King Abdullah University of Science and Technology', 'University of Edinburgh', 'Tsinghua University, Tsinghua University', 'JD Explore Academy', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6934,Security,Bayesian Framework for Gradient Leakage,"Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem.","['privacy', 'federated learning']",[],"['Mislav Balunovic', 'Dimitar Iliev Dimitrov', 'Robin Staab', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6703,Security,Shuffle Private Stochastic Convex Optimization,"In shuffle privacy, each user sends a collection of randomized messages to a trusted shuffler, the shuffler randomly permutes these messages, and the resulting shuffled collection of messages must satisfy differential privacy. Prior work in this model has largely focused on protocols that use a single round of communication to compute algorithmic primitives like means, histograms, and counts. In this work, we present interactive shuffle protocols for stochastic convex optimization. Our optimization protocols rely on a new noninteractive protocol for summing vectors of bounded $\ell_2$ norm. By combining this sum subroutine with techniques including mini-batch stochastic gradient descent, accelerated gradient descent, and Nesterov's smoothing method, we obtain loss guarantees for a variety of convex loss functions that significantly improve on those of the local model and sometimes match those of the central model.",['differential privacy'],[],"['Albert Cheu', 'Matthew Joseph', 'Jieming Mao', 'Binghui Peng']","['Google', 'Google', 'Google', 'Columbia University']",[]
https://iclr.cc/virtual/2022/poster/6020,Security,Knowledge Removal in Sampling-based Bayesian Inference,"The right to be forgotten has been legislated in many countries, but its enforcement in the AI industry would cause unbearable costs. When single data deletion requests come, companies may need to delete the whole models learned with massive resources. Existing works propose methods to remove knowledge learned from data for explicitly parameterized models, which however are not appliable to the sampling-based Bayesian inference, {\it i.e.}, Markov chain Monte Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we propose the first machine unlearning algorithm for MCMC. We first convert the MCMC unlearning problem into an explicit optimization problem. Based on this problem conversion, an {\it MCMC influence function} is designed to provably characterize the learned knowledge from data, which then delivers the MCMC unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not compromise the generalizability of the MCMC models. Experiments on Gaussian mixture models and Bayesian neural networks confirm the effectiveness of the proposed algorithm. The code is available at \url{https://github.com/fshp971/mcmc-unlearning}.","['Markov chain Monte Carlo', 'bayesian inference']",[],"['Shaopeng Fu', 'Fengxiang He', 'Dacheng Tao']","['King Abdullah University of Science and Technology', 'University of Edinburgh', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6746,Security,Hyperparameter Tuning with Renyi Differential Privacy,"For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm’s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.",['differential privacy'],[],"['Nicolas Papernot', 'Thomas Steinke']","['University of Toronto', 'Google']",[]