link,category,title,abstract,keywords,ccs_concepts,author_names,author_affilliations,author_countries
https://icml.cc/virtual/2017/workshop/20,Transparency & Explainability,Workshop on Human Interpretability in Machine Learning (WHI),"This is the Proceedings of the 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14, 2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda Vi√©gas, and Martin Wattenberg.",[],[],"['Kush Varshney', 'Adrian Weller', 'Been Kim', 'Dmitry Malioutov']",[],[]
https://icml.cc/virtual/2017/workshop/13,Fairness & Bias,Principled Approaches to Deep Learning,"This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.",[],[],"['Andrzej Pronobis', 'Robert Gens', 'Sham Kakade', 'Pedro Domingos']",[],[]
https://icml.cc/virtual/2017/workshop/14,Privacy & Data Governance,Private and Secure Machine Learning,"Federated Learning is the current state of the art in supporting secure multi-party machine learning (ML): data is maintained on the owner's device and the updates to the model are aggregated through a secure protocol. However, this process assumes a trusted centralized infrastructure for coordination, and clients must trust that the central service does not use the byproducts of client data. In addition to this, a group of malicious clients could also harm the performance of the model by carrying out a poisoning attack. As a response, we propose Biscotti: a fully decentralized peer to peer (P2P) approach to multi-party ML, which uses blockchain and cryptographic primitives to coordinate a privacy-preserving ML process between peering clients. Our evaluation demonstrates that Biscotti is scalable, fault tolerant, and defends against known attacks. For example, Biscotti is able to protect the privacy of an individual client's update and the performance of the global model at scale when 30% of adversaries are trying to poison the model. The implementation can be found at: this https URL",[],[],"['Antti Honkela', 'Kana Shimizu', 'Samuel Kaski']",[],[]
https://icml.cc/virtual/2017/workshop/14,Security,Private and Secure Machine Learning,"Federated Learning is the current state of the art in supporting secure multi-party machine learning (ML): data is maintained on the owner's device and the updates to the model are aggregated through a secure protocol. However, this process assumes a trusted centralized infrastructure for coordination, and clients must trust that the central service does not use the byproducts of client data. In addition to this, a group of malicious clients could also harm the performance of the model by carrying out a poisoning attack. As a response, we propose Biscotti: a fully decentralized peer to peer (P2P) approach to multi-party ML, which uses blockchain and cryptographic primitives to coordinate a privacy-preserving ML process between peering clients. Our evaluation demonstrates that Biscotti is scalable, fault tolerant, and defends against known attacks. For example, Biscotti is able to protect the privacy of an individual client's update and the performance of the global model at scale when 30% of adversaries are trying to poison the model. The implementation can be found at: this https URL",[],[],"['Antti Honkela', 'Kana Shimizu', 'Samuel Kaski']",[],[]
https://icml.cc/virtual/2017/workshop/2,Security,Deep Structured Prediction,"Generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. So far, most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. We introduce a novel flexible approach named Houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered, be it combinatorial and non-decomposable. We successfully apply Houdini to a range of applications such as speech recognition, pose estimation and semantic segmentation. In all cases, the attacks based on Houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation.",[],[],"['Aravind Rajeswaran', 'Isabelle Augenstein', 'Dan DeBlasio', 'Brandon Carter', 'Marcin Detyniecki', 'Ramya Srinivasan', 'Yi Sun', 'Dian Ang Yap', 'Kiran Koshy', 'Priyadarshini Panda', 'Tyler Scott', 'Erik Englesson', 'Rene Bidart', 'Yaniv Yacoby', 'Jonathan Aigrain', 'Isabelle Augenstein', 'Weiwei Pan', 'Kai-Wei Chang', 'Kai-Wei Chang', 'Gal Chechik', 'Gal Chechik', 'Bert Huang', 'Bert Huang', 'Andre Martins', 'Andre Filipe Torres Martins', 'Ofer Meshi', 'Ofer Meshi', 'Yishu Miao', 'Alex Schwing', 'Yishu Miao', 'Alex Schwing']",[],[]