link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://iclr.cc/virtual/2021/workshop/2142,Transparency & Explainability,Beyond Static Papers: Rethinking How We Share Scientific Understanding in ML,"Explainability techniques for data-driven predictive models based on artificial intelligence and machine learning algorithms allow us to better understand the operation of such systems and help to hold them accountable. New transparency approaches are developed at breakneck speed, enabling us to peek inside these black boxes and interpret their decisions. Many of these techniques are introduced as monolithic tools, giving the impression of one-size-fits-all and end-to-end algorithms with limited customisability. Nevertheless, such approaches are often composed of multiple interchangeable modules that need to be tuned to the problem at hand to produce meaningful explanations. This paper introduces a collection of hands-on training materials -- slides, video recordings and Jupyter Notebooks -- that provide guidance through the process of building and evaluating bespoke modular surrogate explainers for tabular data. These resources cover the three core building blocks of this technique: interpretable representation composition, data sampling and explanation generation.",[],[],"['Krishna Murthy Jatavallabhula', 'Bhairav Mehta', 'Tegan Maharaj', 'Amy Tabb', 'Khimya Khetarpal', 'Aditya Kusupati', 'Anna Rogers', 'Sara Hooker', 'Breandan Considine', 'Devi Parikh', 'Derek Nowrouzezahrai', 'Yoshua Bengio']",[],[]
https://iclr.cc/virtual/2021/workshop/2125,Fairness & Bias,"Synthetic Data Generation: Quality, Privacy, Bias","Insufficient training data is a major bottleneck for most deep learning practices, not least in medical imaging where data is difficult to collect and publicly available datasets are scarce due to ethics and privacy. This work investigates the use of synthetic images, created by generative adversarial networks (GANs), as the only source of training data. We demonstrate that for this application, it is of great importance to make use of multiple GANs to improve the diversity of the generated data, i.e. to sufficiently cover the data distribution. While a single GAN can generate seemingly diverse image content, training on this data in most cases lead to severe over-fitting. We test the impact of ensembled GANs on synthetic 2D data as well as common image datasets (SVHN and CIFAR-10), and using both DCGANs and progressively growing GANs. As a specific use case, we focus on synthesizing digital pathology patches to provide anonymized training data.",[],[],"['Sergul Aydore', 'Krishnaram Kenthapadi', 'Haipeng Chen', 'Edward Choi', 'Jamie Hayes', 'Mario Fritz', 'Rachel Cummings', 'Krishnaram Kenthapadi']",[],[]
https://iclr.cc/virtual/2021/workshop/2132,Security,Responsible AI (RAI),"Several recent works have studied the societal effects of AI; these include issues such as fairness, robustness, and safety. In many of these objectives, a learner seeks to minimize its worst-case loss over a set of predefined distributions (known as uncertainty sets), with usual examples being perturbed versions of the empirical distribution. In other words, aforementioned problems can be written as min-max problems over these uncertainty sets. In this work, we provide a general framework for studying these problems, which we refer to as Responsible AI (RAI) games. We provide two classes of algorithms for solving these games: (a) game-play based algorithms, and (b) greedy stagewise estimation algorithms. The former class is motivated by online learning and game theory, whereas the latter class is motivated by the classical statistical literature on boosting, and regression. We empirically demonstrate the applicability and competitive performance of our techniques for solving several RAI problems, particularly around subpopulation shift.",[],[],"['Ahmad Beirami', 'Emily Black', 'Krishna Gummadi', 'Hoda Heidari', 'Baharan Mirzasoleiman', 'Meisam Razaviyayn', 'Joshua Williams']",[],[]
https://iclr.cc/virtual/2021/workshop/2127,Security,Security and Safety in Machine Learning Systems,"To operate effectively in tomorrow's smart cities, autonomous vehicles (AVs) must rely on intra-vehicle sensors such as camera and radar as well as inter-vehicle communication. Such dependence on sensors and communication links exposes AVs to cyber-physical (CP) attacks by adversaries that seek to take control of the AVs by manipulating their data. Thus, to ensure safe and optimal AV dynamics control, the data processing functions at AVs must be robust to such CP attacks. To this end, in this paper, the state estimation process for monitoring AV dynamics, in presence of CP attacks, is analyzed and a novel adversarial deep reinforcement learning (RL) algorithm is proposed to maximize the robustness of AV dynamics control to CP attacks. The attacker's action and the AV's reaction to CP attacks are studied in a game-theoretic framework. In the formulated game, the attacker seeks to inject faulty data to AV sensor readings so as to manipulate the inter-vehicle optimal safe spacing and potentially increase the risk of AV accidents or reduce the vehicle flow on the roads. Meanwhile, the AV, acting as a defender, seeks to minimize the deviations of spacing so as to ensure robustness to the attacker's actions. Since the AV has no information about the attacker's action and due to the infinite possibilities for data value manipulations, the outcome of the players' past interactions are fed to long-short term memory (LSTM) blocks. Each player's LSTM block learns the expected spacing deviation resulting from its own action and feeds it to its RL algorithm. Then, the the attacker's RL algorithm chooses the action which maximizes the spacing deviation, while the AV's RL algorithm tries to find the optimal action that minimizes such deviation.",[],[],"['Xinyun Chen', 'Cihang Xie', 'Ali Shafahi', 'Bo Li', 'Ding Zhao', 'Tom Goldstein', 'Dawn Song']",[],[]