link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://iclr.cc/virtual/2023/workshop/12840,Fairness & Bias,AI for Agent-Based Modelling (AI4ABM),"Calibrating agent-based models (ABMs) to data is among the most fundamental requirements to ensure the model fulfils its desired purpose. In recent years, simulation-based inference methods have emerged as powerful tools for performing this task when the model likelihood function is intractable, as is often the case for ABMs. In some real-world use cases of ABMs, both the observed data and the ABM output consist of the agents' states and their interactions over time. In such cases, there is a tension between the desire to make full use of the rich information content of such granular data on the one hand, and the need to reduce the dimensionality of the data to prevent difficulties associated with high-dimensional learning tasks on the other. A possible resolution is to construct lower-dimensional time-series through the use of summary statistics describing the macrostate of the system at each time point. However, a poor choice of summary statistics can result in an unacceptable loss of information from the original dataset, dramatically reducing the quality of the resulting calibration. In this work, we instead propose to learn parameter posteriors associated with granular microdata directly using temporal graph neural networks. We will demonstrate that such an approach offers highly compelling inductive biases for Bayesian inference using the raw ABM microstates as output.",[],[],"['Christian Schroeder de Witt', 'Sumitra Ganesh', 'Ani Calinescu', 'Yang Zhang', 'Ayush Chopra', 'Swapneel Mehta', 'Pablo Samuel Castro', 'Jakob Foerster']",[],[]
https://iclr.cc/virtual/2023/workshop/12835,Fairness & Bias,"Machine Learning for IoT: Datasets, Perception, and Understanding","Mechanical defects in real situations affect observation values and cause abnormalities in multivariate time series, such as sensor values or network data. To perceive abnormalities in such data, it is crucial to understand the temporal context and interrelation between variables simultaneously. The anomaly detection task for time series, especially for unlabeled data, has been a challenging problem, and we address it by applying a suitable data degradation scheme to self-supervised model training. We define four types of synthetic outliers and propose the degradation scheme in which a portion of input data is replaced with one of the synthetic outliers. Inspired by the self-attention mechanism, we design a Transformer-based architecture to recognize the temporal context and detect unnatural sequences with high efficiency. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at this https URL.",[],[],"['Xinyu Zhang', 'Tauhidur Rahman', 'Yuan Yuan', 'Ke Sun', 'Yuanyuan Yang', 'Brad Campbell', 'Zhiting Hu']",[],[]
https://iclr.cc/virtual/2023/workshop/12827,Security,Trustworthy and Reliable Large-Scale Machine Learning Models,"ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.",[],[],"['Yu Cheng', 'Boxin Wang', 'Bo Li', 'Tom Goldstein', 'Sanmi Koyejo', 'Yejin Choi']",[],[]
https://iclr.cc/virtual/2023/workshop/12822,Security,Trustworthy Machine Learning for Healthcare,"Unpredictable ML model behavior on unseen data, especially in the health domain, raises serious concerns about its safety as repercussions for mistakes can be fatal. In this paper, we explore the feasibility of using state-of-the-art out-of-distribution detectors for reliable and trustworthy diagnostic predictions. We select publicly available deep learning models relating to various health conditions (e.g., skin cancer, lung sound, and Parkinson's disease) using various input data types (e.g., image, audio, and motion data). We demonstrate that these models show unreasonable predictions on out-of-distribution datasets. We show that Mahalanobis distance- and Gram matrices-based out-of-distribution detection methods are able to detect out-of-distribution data with high accuracy for the health models that operate on different modalities. We then translate the out-of-distribution score into a human interpretable CONFIDENCE SCORE to investigate its effect on the users' interaction with health ML applications. Our user study shows that the \textsc{confidence score} helped the participants only trust the results with a high score to make a medical decision and disregard results with a low score. Through this work, we demonstrate that dataset shift is a critical piece of information for high-stake ML applications, such as medical diagnosis and healthcare, to provide reliable and trustworthy predictions to the users.",[],[],"['Hao CHEN', 'Luyang Luo', 'Daguang Xu', 'Varut Vardhanabhuti', 'Yuyin Zhou', 'Jing Qin', 'Yueming Jin', 'Marius George Linguraru', 'Pheng-Ann Heng', 'Le Lu', 'Danny Chen', 'Kwang-Ting Cheng']",[],[]
https://iclr.cc/virtual/2023/workshop/12831,Security,Scene Representations for Autonomous Driving,"Autonomous driving requires a comprehensive understanding of the surrounding environment for reliable trajectory planning. Previous works rely on dense rasterized scene representation (e.g., agent occupancy and semantic map) to perform planning, which is computationally intensive and misses the instance-level structure information. In this paper, we propose VAD, an end-to-end vectorized paradigm for autonomous driving, which models the driving scene as a fully vectorized representation. The proposed vectorized paradigm has two significant advantages. On one hand, VAD exploits the vectorized agent motion and map elements as explicit instance-level planning constraints which effectively improves planning safety. On the other hand, VAD runs much faster than previous end-to-end planning methods by getting rid of computation-intensive rasterized representation and hand-designed post-processing steps. VAD achieves state-of-the-art end-to-end planning performance on the nuScenes dataset, outperforming the previous best method by a large margin. Our base model, VAD-Base, greatly reduces the average collision rate by 29.0% and runs 2.5x faster. Besides, a lightweight variant, VAD-Tiny, greatly improves the inference speed (up to 9.3x) while achieving comparable planning performance. We believe the excellent performance and the high efficiency of VAD are critical for the real-world deployment of an autonomous driving system. Code and models are available at this https URL for facilitating future research.",[],[],"['Hongyang Li', 'Mengye Ren', 'Li Chen', 'Chonghao Sima', 'Kashyap Chitta', 'Holger Caesar', 'Ping Luo', 'Wei Zhang']",[],[]
https://iclr.cc/virtual/2023/workshop/12825,Security,Backdoor Attacks and Defenses in Machine Learning,"As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy.",[],[],"['Guanhong Tao', 'Kaiyuan Zhang', 'Shawn Shan', 'Emily Wenger', 'Rui Zhu', 'Eugene Bagdasaryan', 'Naren Sarayu Manoj', 'Taylor Kulp-McDowall', 'Yousra Aafer', 'Shiqing Ma', 'Xiangyu Zhang']",[],[]