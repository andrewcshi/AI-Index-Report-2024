link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://iclr.cc/virtual/2019/workshop/632,Transparency & Explainability,"Safe Machine Learning: Specification, Robustness, and Assurance","Adversarial training is a training scheme designed to counter adversarial attacks by augmenting the training dataset with adversarial examples. Surprisingly, several studies have observed that loss gradients from adversarially trained DNNs are visually more interpretable than those from standard DNNs. Although this phenomenon is interesting, there are only few works that have offered an explanation. In this paper, we attempted to bridge this gap between adversarial robustness and gradient interpretability. To this end, we identified that loss gradients from adversarially trained DNNs align better with human perception because adversarial training restricts gradients closer to the image manifold. We then demonstrated that adversarial training causes loss gradients to be quantitatively meaningful. Finally, we showed that under the adversarial training framework, there exists an empirical trade-off between test accuracy and loss gradient interpretability and proposed two potential approaches to resolving this trade-off.",[],[],"['Silvia Chiappa', 'Victoria Krakovna', 'Adrià Garriga-Alonso', 'Andrew Trask', 'Jonathan Uesato', 'Christina Heinze-Deml', 'Ray Jiang', 'Adrian Weller']",[],[]
https://iclr.cc/virtual/2019/workshop/1207,Fairness & Bias,LatinX in AI and Black in AI Joint Workshop,"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",[],[],[''],[],[]
https://iclr.cc/virtual/2019/workshop/632,Security,"Safe Machine Learning: Specification, Robustness, and Assurance","Adversarial training is a training scheme designed to counter adversarial attacks by augmenting the training dataset with adversarial examples. Surprisingly, several studies have observed that loss gradients from adversarially trained DNNs are visually more interpretable than those from standard DNNs. Although this phenomenon is interesting, there are only few works that have offered an explanation. In this paper, we attempted to bridge this gap between adversarial robustness and gradient interpretability. To this end, we identified that loss gradients from adversarially trained DNNs align better with human perception because adversarial training restricts gradients closer to the image manifold. We then demonstrated that adversarial training causes loss gradients to be quantitatively meaningful. Finally, we showed that under the adversarial training framework, there exists an empirical trade-off between test accuracy and loss gradient interpretability and proposed two potential approaches to resolving this trade-off.",[],[],"['Silvia Chiappa', 'Victoria Krakovna', 'Adrià Garriga-Alonso', 'Andrew Trask', 'Jonathan Uesato', 'Christina Heinze-Deml', 'Ray Jiang', 'Adrian Weller']",[],[]